# 2. Dynamic Programming

Created: 2018-09-18 23:53:48 +0500

Modified: 2021-09-28 22:25:38 +0500

---

**Sports Betting**

![BELLMAYS DISCOVERY Any optim L policy can be'sub iuidUdi to two coffi onents: @ An optimal first action+ @ Followed by an optimatpoycy successor S'Ce Theorem (Principle of Optimality) A policy rr(als) achieves the optimal value from state s, Vr(s) = ifand only if @ For any state s' reachable from s @ rachieues the optimal ualue from state s' , Vr(s') = V*(s') ](media/2.-Dynamic-Programming-image1.png)


-   Dynamic programming refers to the collection of algorithms that can be used to compute optimal policies given a perfect model of the environment.
-   Value iteration is a type of dynamic programming algorithm that computes the optimal value function and consequently the optimal policy
-   Dynamic programming is useful, but limited because it requires a perfect environment model and is computationally expensive



# Bellman Advanced

Stochastic bellman equation, where each state transition has some probability for performing an action.

Ex - If we tell our RL agent to move left or right, it has 80% chance of moving left or right but 20% chance of moving up and down and vice-versa.


![V(s) = maxa R(s, a) + P(s, a, ](media/2.-Dynamic-Programming-image2.png)



# Dynamic Programming

Create a lookup table to solve Markov Decision Processes in stochastic environments.

Dynamic Programming Algorithms -
-   Policy Iteration - starts with a totally random policy and start taking actions, it then start estimating values for each square based on the reward received from this random actions updating the value table and using improved values to calculate and improve policy. This continues until both the value table stablizes and stops changing (algorithm converges)
-   Value Iteration - completly ignores policy and focuses on applying bellman equation



![Value Iteration Algorithm 1) Initialize a table V of value estimates for each square with all zeroes 2) Loop over every possible state s 3) From state s loop over every possible action a 4) Get a list of all (probability, reward, s') transition tuples from state s, action a 5) expected_reward = sum of all possible rewards multiplied by their probabilities 6) expected_value = lookup VCs'] for each possible s', multiply by probability, sum 7) action_value = expected_reward + GAMMA * expected_value 8) set VCs] to the best action_value found 9) Repeat steps 2-8 until the largest change in VCs] between iterations is below our threshold ](media/2.-Dynamic-Programming-image3.png)



3 Parts -
-   Policy Evaluation
-   Policy Iteration
-   Value Iteration



# Key Points

1.  **Policy iteration**includes:**policy evaluation**+**policy improvement**, and the two are repeated iteratively until policy converges.

2.  **Value iteration**includes:**finding optimal value function**+ one**policy extraction**. There is no repeat of the two because once the value function is optimal, then the policy out of it should also be optimal (i.e. converged).

3.  **Finding optimal value function**can also be seen as a combination of policy improvement (due to max) and truncated policy evaluation (the reassignment of v_(s) after just one sweep of all states regardless of convergence).

4.  The algorithms for**policy evaluation**and**finding optimal value function**are highly similar except for a max operation (as highlighted)

5.  Similarly, the key step to**policy improvement**and**policy extraction**are identical except the former involves a stability check.

In my experience,*policy iteration*is faster than*value iteration*, as a policy converges more quickly than a value function.



Why Discount Factor?

The idea of using discount factor is to prevent the total reward from going to infinity (because 0 <= Î³ <= 1). It also models the agent behavior when the agent prefers immediate rewards than rewards that are potentially received later in the future.



![Bellman optimality equation is used for Policy Improvement. VT(s) Bellman expectation equation is Bellman optimality equation is VT(S) max qr(s, a) ](media/2.-Dynamic-Programming-image4.png)



![Dynamic Programming Algorithms Algorithm Bellman Equation Problem Iterative Policy Evaluation Vk+l (S) S t'k(s') S' Bellman Expectation Equation Prediction Policy Iteration starting reed) Bellman Expectation Equation Policy Iteration + Greedy Policy Improvement Control Value Iteration t.'k+1(S) S Bellman Optimality Equation Control ](media/2.-Dynamic-Programming-image5.png)



<https://github.com/dennybritz/reinforcement-learning/tree/master/DP

<https://towardsdatascience.com/reinforcement-learning-demystified-solving-mdps-with-dynamic-programming-b52c8093c919>





