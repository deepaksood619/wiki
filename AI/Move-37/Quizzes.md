# Quizzes

Created: 2018-09-30 15:21:08 +0500

Modified: 2018-09-30 15:31:01 +0500

---

![#Policy Evaluation actions 0.0 .10 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -2.0 -2.0 1 4 5 2 6 3 7 on all transitions Undiscounted episodic MDP (y = 1) Nonterminal states 1, . 14 One terminal state (shown twice as shaded squares) Actions leading out of the grid leave state unchanged Reward is ---1 until the terminal state is reached Agent follows uniform random policy Tt(nl•) = Tl(el•) = Tt(sl•) = Tl(wl•) = 0.25 truncate to 1 decimal place t.'k for the Random Policy Greedy Policy w.r.t Vk 0.0 0.0 0.0 00 -1.0 .10 -1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ra ](media/Quizzes-image1.png)

![QI. Choose all that apply. (4 possible answers) A=-I.O 2. B=-2.O 3. c=-2.o 4. ](media/Quizzes-image2.png)

![QI Explanation> -1.0 -1.0 -1.0 -1.0 *ail -1.0 -1.0 -1.0 -1.0 0.0 -1.7 -2.0 -2.0 -1.7 -20 -2.0 -20 -1.7 oo [o] -2.0 t,'k for the Random Policy Greedy Policy w.r.t 0.0 0.0 0.0 0.0 -1.0 -1.0 -1.0 -2.0 -2.0 -2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -1.0 -1.0 -1.0 0.0 0.0 0.0 0.0 1.7 rat po opti 2. 3. 4. --- -1.7 (truncated to one decimal place) v = 1 x 0.25 x (-1 x (-1 + -1) = -1.7 (truncated to one decimal place) v = 1 x 0.25 x (-1 +0) +3 x 0.25 x (-1 + -1) v = 4 x 0.25 X (-1 + -1) [o] same values of -2.0 ](media/Quizzes-image3.png)

![#Policy Improvement #Policy Evaluation Q2. Choose all that apply. (4 possible answers) 1. 2. 3. 4. VT(s) + ...1st Policy Tt can be evaluated by Process of policy iteration always converges. Bellman expectation equation is used for Policy Improvement. Policy evaluation Vk+l max R.a + "iPVk aeA ](media/Quizzes-image4.png)

![](media/Quizzes-image5.png)

![9- = 0 S-= D •E S- = V (snmsue ölqssod •Kldde q ㄐ | 2 OSOOI-I) · £ 0 ewes Sl DISeg 1 | 61Jlsn spæM>l-oeq 」 0M pue leu!} 1e06 Sl 6 e 」 9 CIO 2 」 enpA# ](media/Quizzes-image6.png)

![Q3 Explanation> Problem 3. [01 c --5 4. [01 D = -6 aaaa Max(-1+Up, -1+Left, -1+Down, -1+Right) (using own value when it blocked Max(-3-1, -3-1 -5-1, -5-1) Max(-4-1, -4-1 -5-1, -5 1) Max(-4-1, -4-1 -5-1, -5 1) Max(-5-1, -5-1 -5-1, -5-1) ](media/Quizzes-image7.png)

![Q4. Choose all that apply. (4 possible answers) 1. 2. 3. 4. If we know the solution to subproblems v * (s') then solution v * (s) can one-step lookahead To find optimal policy TX, use iterative application of Bellman Expectation E there is no explicit policy in the value iteration. Value iteration is Vk+l max + 'pa Vk ](media/Quizzes-image8.png)

![](media/Quizzes-image9.png)

![Q5. Choose all that apply. (4 possible answers) Algorithm Policy Evaluation Policy Iteration Value Iteration Bellman Equation 1. 2. 3. 4. A = Bellman Expectation Equation B = Bellman Expectation Equation C = Greedy Policy Improvement D = Bellman Optimality Equation ](media/Quizzes-image10.png)

![](media/Quizzes-image11.png)











