# 2. Conditioning and Independence

---

Conditioning leads to revised ("conditional") probabilities that take into account partial information on the outcome of a probabilistic experiment. Conditioning is a very useful tool that allows us to "divide and conquer" complex problems. Independence is used to model situations involving non-interacting probabilistic phenomena and also plays an important role in building complex models from more elementary ones.

# Conditioning and Bayes' rule

- **Conditional Probability**

The conditional probability of an event given another event is the probability of their intersection divided by the probability of the conditioning event.

- Three important tools
  - Multiplication rule
  - Total probability theorem
  - Bayes' rule - provides a systematic way for incorporating new evidence into a probability model. Foundation of the field of inference. It is a guide on how to process data and make inferences about unobserved quantities or phenomena.
![image](media/Intro-Syllabus_2.-Conditioning-and-Independence-image1.png)
![image](media/Intro-Syllabus_2.-Conditioning-and-Independence-image2.png)

P ( A | B ) = Probability of **A given B**

P ( A | B ) = Here B is called the conditioning event

![image](media/Intro-Syllabus_2.-Conditioning-and-Independence-image3.png)

## Conditional Probabilities share probabilities of ordinary probabilities

- Conditional probabilities must be non-negative

![image](media/Intro-Syllabus_2.-Conditioning-and-Independence-image4.png)
![image](media/Intro-Syllabus_2.-Conditioning-and-Independence-image5.png)
![image](media/Intro-Syllabus_2.-Conditioning-and-Independence-image6.png)
![image](media/Intro-Syllabus_2.-Conditioning-and-Independence-image7.png)
![image](media/Intro-Syllabus_2.-Conditioning-and-Independence-image8.png)
![image](media/Intro-Syllabus_2.-Conditioning-and-Independence-image9.png)

Inference - Having observed B, we make inferences as to how likely a particular scenario Ai, is going to be.
