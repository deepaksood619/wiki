# 10. Conditioning on a random variable; Independence; Bayes' rule

Created: 2018-06-27 00:42:16 +0500

Modified: 2018-06-29 22:11:16 +0500

---

![LECTURE 10: Conditioning on a random variable; Conditioning X on Y --- Total probability theorem --- Total expectation theorem Independence independent normals A comprehensive example Four variants of the Bayes rule Indepen ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image1.png){width="8.552083333333334in" height="4.802083333333333in"}

![Conditional PDFs, given another r.v. py(y) if py(y) > O PxlY Definition: fxIY(x I y) if fy(y) > O fY(y) fxIA(x) • O, where P(A) > O ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image2.png){width="8.552083333333334in" height="4.802083333333333in"}

![Comments on conditional fxIY(x I y) --- fy(y) fxlY(x I y) 2 0 Think of value of Y as fixed at some y shape of fxy(. I y): slice of the joint fxlY(x I y) dx fy(y) Multiplication rule: ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image3.png){width="8.552083333333334in" height="4.802083333333333in"}

![Total probability and expectation px(x) I y) y Expected value rule. theorems fx(x) xfxlY(a ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image4.png){width="8.552083333333334in" height="4.802083333333333in"}

![Independence y) = px(x) .fx(x) fy(y), for all x, y for all x and y y) fxlY(x I Y) fy(y) equivalent to: fxly(x I y) = fx(x), for all y with fy(y) > O an E[XY] If X, Y are independent: ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image5.png){width="8.552083333333334in" height="4.802083333333333in"}

By analogy with the discrete case, we will say that two jointly continuous random variables are independent if the joint PDF is equal to the product of the marginal PDFs
![Stick-breaking example Break a stick of length C twice first break at X: uniform in --- second break at Y: uniform in [O, X] fx(x) 1 fylx(y I x) ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image6.png){width="8.552083333333334in" height="4.802083333333333in"}

![Stick-breaking example fy(y) Using total expectation theorem: J/ ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image7.png){width="8.552083333333334in" height="4.802083333333333in"}

Intuition - If we break a stick once, the expected value or what we're left with is half of what we started with. But if we break it once more, then we expect it on the average to be cut by a factor again of 1/2. And so we expect to be left with a stick that has length 1/4 of what we started with.
![Independent standard normals 277 277 ex p í -i (224 72)3 0.2 0.15 0.1 0.05 ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image8.png){width="8.552083333333334in" height="4.802083333333333in"}

![Independent normals 1 2770x0!] 1 202 0.1 0.08 c 0.06 0.04 0.02 ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image9.png){width="8.552083333333334in" height="4.802083333333333in"}

To conclude, the joint PDF of two independent normals has the shape of a bell. The center of the bell is determined by the means. Furthermore, the bell is stretched in the x and y directions by an amount that is determined by the variances of x and y. However, the stretching is always along the coordinate axes.

If you wanted a bell that stretches in some diagonal direction, or if you have contours that are ellipses but with some different kind of axes, then you will have dependence between the two random variables. In that case, we will be dealing with a so-called bivariate normal distribution.
![The Bayes rule pxIY( • I y) x unobserved value prior • ) --- a theme with variations Inference a: observed value y model ¯ px(x) I x) pxlY(x I Y) fx(x) fy(y) fx(x) ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image10.png){width="8.552083333333334in" height="4.802083333333333in"}

![The Bayes rule --- one discrete and one continuous random K: discrete - r ( 80) (F Y: continuous ! Y s 7+8) (K:E PK(k) I k) PIClY(k I y) = PRI ylk) = ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image11.png){width="8.552083333333334in" height="4.802083333333333in"}

![The Bayes rule --- discrete unknown, continuous measurem unkown K: equally likely to be ---1 or +1 measurement Y: Y=IC+VV; w Y 1K: I Ott (l, J) Probability that K = 1, given that Y y? PIC(k) = 1/2 I k) '¯ PIClY(k I y) fy(y) ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image12.png){width="8.552083333333334in" height="4.802083333333333in"}

![The Bayes rule --- continuous unknown, discrete measuren- measurement K: Bernoulli with parameter Y 7 1 fylK(y I k) = PK(k) unkown Y: uniform on [O, 1] Distribution of Y given that fy(y) PIC(I) PKIY(I I y) 7 2 ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image13.png){width="8.552083333333334in" height="4.802083333333333in"}

The joint PMF

The Conditional PMF given an event

Conditional PMF of one random variable given another
Joint PDF of X and Y

