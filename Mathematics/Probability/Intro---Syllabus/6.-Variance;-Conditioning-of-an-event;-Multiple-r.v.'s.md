# 6. Variance; Conditioning of an event; Multiple r.v.'s

Created: 2018-06-10 01:48:28 +0500

Modified: 2018-06-26 19:08:56 +0500

---

![LECTURE 6: Variance; Conditioning on an event; Variance and its properties --- Variance of the Bernoulli and uniform PMFs Conditioning a r.v. on an event Conditional PMF, mean, variance --- Total expectation theorem Geometric PMF Memorylessness Mean value Multiple random variables --- Joint and marginal PMFs ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image1.png){width="13.927083333333334in" height="7.885416666666667in"}
**Variance -** Is a quantity that measures the amount of spread, of a dispersion of a probability mass function. It is defined as the expected value of the squared distance from the mean. Always non negative.

**Intuition -** How far away the outcome of the random variable happens to be from the mean of that random variable

![Variance --- a measure of the spread of a PMF Random variable X, with mean • Distance from the mean: X --- p • Average distance from the mean? var(X) • Definition of variance: Calculation, using the expected value rule, ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image2.png){width="13.927083333333334in" height="7.885416666666667in"}

One way to calculate the distance from the mean is E[X-u] = E[X] - u (Using linearity of expressions), u-u = 0.

Therefore we want the average absolute value of the distance from the mean. Therefore we have created variance which is defined as the expected value of the squared distance from the mean.
Variance is a bit hard to interpret, because it is in wrong units. If capital X corresponds to meters, then the variance has units of meters squared. A more intuitive quantity is the square root of the variance, which is called **standard deviation.** It has the same units as the random variable and captures the width of the distribution.
![Properties of the variance var(aX + b) = a2var(X) Notation: Let Y = X+b vat (M) ( --- Q-f..4 Let A useful formula: var(X) --- -fJ)2 E[x2] - ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image3.png){width="13.927083333333334in" height="7.885416666666667in"}

If we add a constant to a random variable, the variance remains unchanged (Since constant just moves the entire PMF right or left by some amount, without changing its shape)

If we multiply a random variable by a, the variance gets multiplied by a squared
![Variance 1, 0, var(X) var(X) of the a: E[x2] Bernoulli Efriz p (l 17) z 0 p + (o-p)2• (ı- 2 2 (E CXI) 2 ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image4.png){width="13.927083333333334in" height="7.885416666666667in"}

**Intuition -** The variance is a measure of the amount of randomness. A coin is most random if it is fair, i.e. when p is equal to 1/2. The variance of a coin flip is biggest if that coin is fair. (as it is seen by the variance vs probability plot)
![Variance of the uniform px(x) 1 o 1 n a; var(x) -(gCY9) - 02 ; b -Q 1 12 x) ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image5.png){width="13.927083333333334in" height="7.885416666666667in"}

![Conditional PMF and expectation, given an event Condition on an event A use conditional probabilities px(x) = x) pxIA(x) = I A) EPXIA(X) I A] = Eg(x) pxIA(x) ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image6.png){width="13.927083333333334in" height="7.885416666666667in"}

![Example of conditioning • pxIA(x) 1 2 3 4 Let 1 2 3 E[X] = 5 4 3 ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image7.png){width="13.927083333333334in" height="7.885416666666667in"}

![Total expectation theorem P(B) = P(AI) I Al) + ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image8.png){width="13.927083333333334in" height="7.885416666666667in"}

![Total expectation theorem Al n {X P(B) = P(AI) I Al) + Px@) P (Al) p XI Al (x) + . ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image9.png){width="13.927083333333334in" height="7.885416666666667in"}

Total Expectation Theorem tells us that the expected value of a random variable can be calculated by considering different scenarios, finding the expected value under each of the possible scenarios and weigh the scenarios according to their respective probabilities.
![TotaI expectation exampIe Рх (т) 1/9 о 1/9 1 1/9 2 2/9 б 2/9 7 2/9 8 2 ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image10.png){width="13.927083333333334in" height="7.885416666666667in"}

![Conditioning a geometric random variable X: number of independent coin px(k) 1 2 3 '156789 tosses until first head; P(H) = Memorylessness: Number of remainin conditioned on Tails is Geometric, with Conditioned on X > 1, X--- 1 is geometric with parameter p 14 ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image11.png){width="13.927083333333334in" height="7.885416666666667in"}

![Conditioned on X > 1, X --- 1 is geometric with parameter p (L-p)tp (3) X-IIY>J ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image12.png){width="13.416666666666666in" height="3.375in"}

![Conditioned on X > n, X ---n is geometric with parameter p (L-p)tp (3) x-IIX>) ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image13.png){width="13.25in" height="3.5833333333333335in"}
![The mean of the geometric PX (k) 1 E[x] --- E kpx(k) 1 2 3 8 9 k ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image14.png){width="13.927083333333334in" height="7.885416666666667in"}

**Intuition -** If p is small, this means that the odds of seeing heads is small. Then in that case, we need to wait longer and longer until we see heads for the first time.
![Multiple random variables and joint PMFs rim a Q X: PX 2 20 4 1/20 2/20 2/ o Joint PMF: 20 20 px(x) --- 2 1 1/20 x ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image15.png){width="13.927083333333334in" height="7.885416666666667in"}

![More than two random variables z) = P (X = x and Y = y and Z PX(x) = E y, z) ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image16.png){width="13.927083333333334in" height="7.885416666666667in"}

![Functions of multiple random variables Z = g(x, Y) PMF: pz(z) = Y (#7, Expected value rule: E[g(X, Y)] E E.g(x, y) y) ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image17.png){width="13.927083333333334in" height="7.885416666666667in"}

![Linearity of expectations E[ax + b] = aE[x] + b + Y] E[x] + ELY] 22 Y CBD + ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image18.png){width="13.927083333333334in" height="7.885416666666667in"}

![Linearity of expectations E[ax + b] = aE[X] + b + Y] --- E[x] + ELY] ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image19.png){width="13.927083333333334in" height="7.885416666666667in"}

![The mean of the binomial X: binomial with parameters n, p number of successes in n independent trials Xi=l if ith trial is a success; E[x] = (indicator variable) Xi = O otherwise ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image20.png){width="13.927083333333334in" height="7.885416666666667in"}
**nu**

![](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image21.png){width="0.625in" height="0.6145833333333334in"}
Because X is a bernoulli, for X=0, X^2^ = 0 and for X = 1, X^2^ = 1
conditional mean or conditional expectation
Expected value rule, remain true in the conditional model
Same as Total Probability Theorem but translated in PMF notation

pX(x) = PMF at that particular x
By Symmetry, the expected value will be in the middle, so the expected value is 1 for conditional universe {0,1,2} and 7 for conditional universe {6,7,8}
by linearity of expectations
Interpretation of the above formula - With p X,Y a specific (x,y) pair will occur. And when that occurs, the value of our random variable is a certain number g(x,y). And the combination of these two terms gives us a contribution to the expected value. Now we consider all possible (x,y) pairs that may occur, and we sum over all these (x,y) pairs.
The expected value of the sum of two random variables is equal to the sum of their expectations.
