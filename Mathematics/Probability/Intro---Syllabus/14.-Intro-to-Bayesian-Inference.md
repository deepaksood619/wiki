# 14. Intro to Bayesian Inference

Created: 2018-08-17 20:32:04 +0500

Modified: 2022-04-07 21:18:39 +0500

---

**The power of Bayesian statistics**
-   Outcome characterization - This is the distribution of things that happened
-   Latent factor analysis - These are the things affect your outcome
-   Decision making - Given all the potential outcomes here's the most optimal choice we should make today

<https://www.youtube.com/watch?v=pJH_2y9J9-I>
a.  We apply the Bayes rule to find the posterior distribution of an unknown random variable given one or multiple observations of related random variables.

b.  We discuss the most common methods for coming up with a point estimate of the unknown random variable (Maximum a Posteriori probability estimate, Least Mean Squares estimate, and Linear Least Mean Squares estimate).

c.  We consider the question of performance analysis, namely, the calculation of the probability of error in hypothesis testing problems or the calculation of the mean squared error in estimation problems.

d.  To illustrate the methodology, we pay special attention to a few canonical problems such as linear normal models and the problem of estimating the unknown bias of a coin.
![LECTURE 14: Introduction to Bayesian inference The big picture motivation, applications --- problem types (hypothesis testing, estimation, etc.) The general framework Bayes' rule posterior (4 versions) --- point estimates (MAP, CMS) performance measures) ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image1.png)

![Inference: the big picture redictions Real world Decisions Data Probability theory (Analysis) Models Inference/ Statistics ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image2.png)

![Inference then and now Then: 10 patients were treated: 3 died 10 patients were not treated: 5 died Therefore Now: Big data Big models ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image3.png)

![A sample of application domains Design and interpretation of experiments --- polling STATE COUNT 17 SOLIDLY DEMOCRATIC 23 sot 11 TOSSUP ELECTORAL VOTE C 237 LIKELY DEMOCRATIC 191 Ll 110 TOSSUP ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image4.png)

![A sample of application domains marketing, advertising o recommendation systems Netflix competition persoms ononooaooos• aaaasaanoool aaaanaoaasol onoaoøsasaa• oaoaonosanol oaoøsnzøaøo• n onannøoøo• ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image5.png)

![](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image6.png)

![A • sample of application Life sciences genomics domains Cytokines EPC) --- systems bio Svvival F«tors (e.g- IGFI) Hormmes. Transmitters (e.g.. interleuk'b. ser«onin. eE.) Growth F Syntbol ASBII FTI-IL17 PHF8 O = GPCR = PKC Akka JAKS STABS Bc1-xL Caspase 9 Apoptosis BCI.2 Raf ME LOC16998) , SLC16A2 ZNF6 CAPZAIP ZETB33 MCTSI Et,F4 LOC266694 cyclase MEKK 17 h. .41 Myc: Mad: Max Max CREB Gene ARE mdm2 ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image7.png)

![A sample of application domains Modeling and monitoring the oceans Modeling and monitoring global climate Modeling and monitoring pollution Interpreting data from physics experiments ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image8.png)

![A sample of application domains Signal processing communication systems (noisy ... ) --- speech processing and understanding image processing and understanding --- tracking of objects positioning systems (e.g. GPS) ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image9.png)

![Model building versus inferring unobserved variables X • = as + W Model building: w S, observe X Predictions Real world Decisions Data Inference/St know "signal" infer a Variable estimation: ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image10.png)

![Hypothesis testing versus estimation Hypothesis testing: unknown takes one of few possible values aim at small probability of incorrect decision Is it an airplane or a bird? Estimation: numerical unknown(s) ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image11.png)

![The Bayesian inference Unknown e --- treated as a random framework variable prior distribution pe or fe Observation X observation model Pxle or f x. Use appropriate version of the Bayes rule to find pelx(.lX = x) or felx(.lX = x) Prior D Where does thc --- symmetry known range earlier studies --- subjective or ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image12.png)

![The output of Bayesian inference The complete answer is a posterior distribution: PMF pelx(• I x) felx(• I x) or PDF ELECT( ROM?' PelX(• I x) Prior D felX(• I x) 6% 3% 2% 1% 0% O o ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image13.png)

![Point estimates in Bayesian inference The complete answer is a posterior distribution: PMF pelx(• I x) felx(• I x) or PDF PelX(• I x) felX(• I x) estinu (numb estim (randc • Maximum a posteriori probability (MAP): I c) = maxpelx(0 1 c) ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image14.png)

![Discrete e, discrete X values of e: pelx(0 1 x) 0.1 1 MAP rule: alternative hypotheses 0.6 2 0.3 3 0 pelx(0 1 x) --- px(x) = El conditional prob of error smallest under the M/ overall probability of err e) = Ep(e ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image15.png)

![pelx(0 1 x) Discrete e, continuous x Standard example: --- send signal e e {1, 2, 3} w N indep. of O fxle(x | 0) = fw(x O) 0.1 0.6 0.3 pelx(0 1 x) fx(x) = Epe( conditional prob of e smallest under the overall probability o ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image16.png)

![Continuous e, continuous X linear normal models estimation of a noisy signal e and W: independent normals felx(0 1 x) fx(x) = fei multi-dimensional versions (many normal parameters, man estimating the parameter of a uniform intere ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image17.png)

![Inferring the unknown bias of a coin and the Beta distribut Standard example: --- coin with bias e; prior fe(•) fix n; K =number of heads Assume fe(.) is uniform in [O, 1] felK(0 1 k) 1 0 d(n, k) 01 k PFC k) "Beta 1 felK(0 1 k) = PK(k) = fe( distribution, with parameters (3 ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image18.png)

![Inferring the unknown bias of a coin: Standard example: --- coin with bias e; prior fe(•) fix n; K =number of heads Assume fe(.) is uniform in [O, 1] 1 Ok (1 --- O) feuc(0 1 k) d(n, k) MAP estimate: 0M A p = point estimates 0 ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image19.png)

![Summary • Problem data: pe(•), pxle(• I pelx(• I x) Given the value a; of X: find, e.g., using appropriate version of the Bayes rule (q cLl'ce Estimator e = g(X) Estimate D = g(x) MAP: 0M AP = gMAp(x) maximizes pelx(0 1 x) L MS: DLMs= Q Ms(x) = I X = ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image20.png)
[Bayes theorem, and making probability intuitive](https://www.youtube.com/watch?v=HZGCoVF3YvM)-   Bayes' theorem describe the probability of an event occurring, based upon prior knowledge of other variables related to that event
    -   In effect, it is a conditional probability, with the probability of an event conditioned on the information/knowledge you have
    -   Since the information/knowledge that different individuals can have about an event can vary, Bayes' thorem allows for differences in probability estimates for the same event across individuals
-   In Bayesian Inference, you update the probability of an event happening as you receive new evidence or information
    -   The probability that you assign to an event before you receive the new information represent your priors
    -   The probability that you assign to that same event after receiving and processing new information represent your posterior estimate

