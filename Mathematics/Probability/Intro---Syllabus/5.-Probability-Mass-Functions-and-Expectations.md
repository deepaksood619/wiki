# 5. Probability Mass Functions and Expectations

Created: 2018-06-09 14:45:43 +0500

Modified: 2018-06-13 23:39:51 +0500

---

![LECTURE 5: Discrete random variables: probability mass functions and expectations Random variables: the idea and the definition Discrete: take values in finite or countable set Probability mass function (PMF) Random variable examples Bernoulli Uniform Binomial ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image1.png){width="5.979166666666667in" height="3.3541666666666665in"}

![Random variables: the idea 62 ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image2.png){width="5.979166666666667in" height="3.3541666666666665in"}

![Random variables: the formalism A random variable ("r.v.") associates a value (a number) to every possible outcome Mathematically: A function from the sample space Q to the real It can take discrete or continuous values random variable X We can have several random variables defined on the same saml Notation: numerical value ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image3.png){width="5.979166666666667in" height="3.3541666666666665in"}

![Probability mass function (PMF) of a discrete r.v. X It is the "probability law" or "probability distribution" of X • If we fix some x, then "X = x" is an event px(x) = Properties: e Q s.t. x(w) --- px(x) 2 0 ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image4.png){width="5.979166666666667in" height="3.3541666666666665in"}

![PMF calculation • Two rolls of a tetrahedral die nana Y = Second roll 3 2 1 4 X = First roll pz(z) Let every possible outcome ha Find pz(z) repeat for all z: --- collect all possible outcomes for --- add their probabilities pa (3) = 3) a/ 16 ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image5.png){width="12.09375in" height="6.84375in"}
A random variable that takes a value of 0 or 1, with certain probabilities. Such a probability is called a Bernoulli random variable.

![The simplest random variable: Bernoulli with parameter p e [O, 1, w.p. p O, w.p. I---p px(l) = p • Models a trial that results in success/ failure, Heads/ Tails, etc. • Indicator rev. of an event A: IA = 1 iff A occurs ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image6.png){width="12.09375in" height="6.84375in"}
Some useful random variable -

1.  Discrete uniform random variable

It takes a value in a certain range, and each one of the values in that range has the same probability

2.  Binomial random variable

3.  Geometric random variable
![Discrete uniform random variable; Parameters: integers Experiment: Pick one of + 1, Sample space: {a, a + 1, . b} Random variable X: X (w) Model of: complete ignorance parameters a, b b at random; all equally lik J: 52 22 c Special case constant/det ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image7.png){width="12.09375in" height="6.84375in"}

![Binomial random variable; parameters: positive integer n; p e • n independent tosses of a coin with PC Heads) = p Experiment: Sample space: Set of sequences of H and T, of length n Random variable X: number of Heads observed Model of: number of successes in a given number of independer P P IIHH 11TH HTT 3 2 ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image8.png){width="12.09375in" height="6.84375in"}

![035 0.3 p = 0.5 075 0.04 0.2 ο. 15 0.05 0.7 0.6 p = 0.2 p = 0.2 0.5 04 05 1.5 2.5 3.5 025 0.2 0.15 ΟΤΕ 0 35 0.3 025 0.2 p = 0.5 ιο 017 0.06 0.03 0.02 0.01 10 o. 14 o. 12 20 ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image9.png){width="12.09375in" height="6.84375in"}

![Geometric random variable; parameter p: O < p 1 • Experiment: infinitely many independent tosses of a coin; P(HeaC Sample space: Set of infinite sequences of H and T Random variable X: number of tosses until the first Heads Model of: waiting times; number of trials until a success px(k) = px(k) P(no Heads ever) ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image10.png){width="12.09375in" height="6.84375in"}

In the above experiment where P(no heads ever) is extremely unlikely since (1-p)^k^ tends to 0 for k tends to infiinity.
Mean of a random variable is a single number that provides some kind of summary of a random variable by telling us what it is on the average

![Expectation/ mean of a random variable Motivation: Play a game 1000 times. Random gain at each play described by: "Average" gain: ) * 2004 2.5004 9-300 1000 1, 2, 4, w.p. z w.p. w.p. 2.110 • 10 Definition: E[x] 10 ¯ XPx(X) Interpretation: of independent repetiti ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image11.png){width="12.09375in" height="6.84375in"}
![Expectation of a Bernoulli r.v. 1, O, w.p. P w.p. 1 If X is the indicator of an event A, IS f A occuas ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image12.png){width="12.09375in" height="6.84375in"}

![Expectation of a uniform r.v. Uniform on O, 1 1 Definition: ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image13.png){width="12.09375in" height="6.84375in"}

Whenever we have a PMF which is symmetric around a certain point, then the expected value will be the center of symmetry.
![Expectation as a population average n students Weight of ith student: Experiment: pick a student at random, all equally likely Random variable X: weight of selected student assume the are distinct PX(Xi) = ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image14.png){width="12.09375in" height="6.84375in"}

![Elementary properties of expectations If X 0, then E[x] 2 0 for w: If a < x < b, Soc w: a E[x] then Definition: -Q > px(æ) ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image15.png){width="12.09375in" height="6.84375in"}
-   If X >= 0, then E[X] >= 0. If a random variable is non-negative, it's expected value is non-negative
-   Expectation has a linearity property
![The expected value rule, for calculating E[g(X)] • Let X be a r.v. and let Y = g(X) • Averaging over y: E[Y] = 3, (0.1+0.2) + q prob 3-0.) q •0.5 Averaging over Proof: E[x2] 0.4 0.3 0.2 0.1 2, ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image16.png){width="12.09375in" height="6.84375in"}

![Linearity of expectation: E[aX + b] = aE[X] + b aver Q oje SQ Q '7 +100 Intuitive • Derivation, based on the expected value rule: ELY | 2 cc) ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image17.png){width="12.09375in" height="6.84375in"}

![](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image18.png){width="0.23958333333333334in" height="0.4583333333333333in"}![](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image19.png){width="0.3229166666666667in" height="0.3229166666666667in"}![](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image20.png){width="0.3020833333333333in" height="0.2708333333333333in"}

No reason to believe that one value

is more likely than the other
PMF notation instead of simple probability notation