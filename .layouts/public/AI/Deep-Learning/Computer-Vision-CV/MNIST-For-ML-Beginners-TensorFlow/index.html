<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:image:alt" content="Accumulating thoughts, knowledge, tips and anything that is worth keeping a not of. These notes are build using the obsidian tool and deployed here as well for easy access."/><meta data-react-helmet="true" name="twitter:image" content="https://deepaksood619.github.io/wiki/graph-visualisation.jpg"/><meta data-react-helmet="true" name="twitter:description" content="When one learns how to program, there&#x27;s a tradition that the first thing you do is print &quot;Hello World.&quot;…"/><meta data-react-helmet="true" name="twitter:title" content="MNIST For ML Beginners | TensorFlow"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" property="article:section" content="None"/><meta data-react-helmet="true" property="article:author" content="http://examples.opengraphprotocol.us/profile.html"/><meta data-react-helmet="true" property="article:modified_time" content="2023-01-07T04:13:18.000Z"/><meta data-react-helmet="true" property="article:published_time" content="2022-12-27T15:23:59.000Z"/><meta data-react-helmet="true" property="og:description" content="When one learns how to program, there&#x27;s a tradition that the first thing you do is print &quot;Hello World.&quot;…"/><meta data-react-helmet="true" property="og:site_name"/><meta data-react-helmet="true" property="og:image:alt" content="Accumulating thoughts, knowledge, tips and anything that is worth keeping a not of. These notes are build using the obsidian tool and deployed here as well for easy access."/><meta data-react-helmet="true" property="og:image" content="https://deepaksood619.github.io/wiki/graph-visualisation.jpg"/><meta data-react-helmet="true" property="og:url" content="https://deepaksood619.github.io/wiki/wiki/AI/Deep-Learning/Computer-Vision-CV/MNIST-For-ML-Beginners-TensorFlow/"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="MNIST For ML Beginners | TensorFlow"/><meta data-react-helmet="true" name="image" content="https://deepaksood619.github.io/wiki/graph-visualisation.jpg"/><meta data-react-helmet="true" name="description" content="When one learns how to program, there&#x27;s a tradition that the first thing you do is print &quot;Hello World.&quot;…"/><meta name="generator" content="Gatsby 4.6.0"/><style data-href="/wiki/styles.d9e480e5c6375621c4fd.css" data-identity="gatsby-global-css">.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{background-color:#333;border-radius:4px;color:#fff;font-size:14px;line-height:1.4;outline:0;position:relative;transition-property:visibility,opacity,-webkit-transform;transition-property:transform,visibility,opacity;transition-property:transform,visibility,opacity,-webkit-transform;white-space:normal}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{border-top-color:initial;border-width:8px 8px 0;bottom:-7px;left:0;-webkit-transform-origin:center top;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:initial;border-width:0 8px 8px;left:0;top:-7px;-webkit-transform-origin:center bottom;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-left-color:initial;border-width:8px 0 8px 8px;right:-7px;-webkit-transform-origin:center left;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{border-right-color:initial;border-width:8px 8px 8px 0;left:-7px;-webkit-transform-origin:center right;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{color:#333;height:16px;width:16px}.tippy-arrow:before{border-color:transparent;border-style:solid;content:"";position:absolute}.tippy-content{padding:5px 9px;position:relative;z-index:1}.tippy-box[data-theme~=light]{background-color:#fff;box-shadow:0 0 20px 4px rgba(154,161,177,.15),0 4px 80px -8px rgba(36,40,47,.25),0 4px 4px -2px rgba(91,94,105,.15);color:#26323d}.tippy-box[data-theme~=light][data-placement^=top]>.tippy-arrow:before{border-top-color:#fff}.tippy-box[data-theme~=light][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#fff}.tippy-box[data-theme~=light][data-placement^=left]>.tippy-arrow:before{border-left-color:#fff}.tippy-box[data-theme~=light][data-placement^=right]>.tippy-arrow:before{border-right-color:#fff}.tippy-box[data-theme~=light]>.tippy-backdrop{background-color:#fff}.tippy-box[data-theme~=light]>.tippy-svg-arrow{fill:#fff}html{font-family:SF Pro SC,SF Pro Text,SF Pro Icons,PingFang SC,Helvetica Neue,Helvetica,Arial,sans-serif}body{word-wrap:break-word;-ms-hyphens:auto;-webkit-hyphens:auto;hyphens:auto;overflow-wrap:break-word;-ms-word-break:break-all;word-break:break-word}blockquote,body,dd,dt,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,html,iframe,legend,p,pre,textarea{margin:0;padding:0}h1,h2,h3,h4,h5,h6{font-size:100%;font-weight:400}button,input,select{margin:0}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}img,video{height:auto;max-width:100%}iframe{border:0}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style><style data-styled="" data-styled-version="5.3.5">.fnAJEh{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:14px;background-color:#005cc5;color:#ffffff;padding:16px;}/*!sc*/
.fnAJEh:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fnAJEh:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.czsBQU{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#ffffff;margin-right:16px;}/*!sc*/
.czsBQU:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.czsBQU:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kLOWMo{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#ffffff;}/*!sc*/
.kLOWMo:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kLOWMo:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kEUvCO{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;color:inherit;margin-left:24px;}/*!sc*/
.kEUvCO:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kEUvCO:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.fdzjHV{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#24292e;display:block;}/*!sc*/
.fdzjHV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fdzjHV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.bQLMRL{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:16px;display:inline-block;padding-top:4px;padding-bottom:4px;color:#586069;font-weight:medium;}/*!sc*/
.bQLMRL:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.bQLMRL:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.bQLMRL{font-size:14px;}}/*!sc*/
.ekSqTm{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;padding:8px;margin-left:-32px;color:#2f363d;}/*!sc*/
.ekSqTm:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.ekSqTm:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.cKRjba{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cKRjba:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.cKRjba:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.iLYDsn{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;margin-bottom:4px;}/*!sc*/
.iLYDsn:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.iLYDsn:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
data-styled.g1[id="Link-sc-1brdqhf-0"]{content:"fnAJEh,czsBQU,kLOWMo,kEUvCO,fdzjHV,bQLMRL,ekSqTm,cKRjba,iLYDsn,"}/*!sc*/
.EuMgV{z-index:20;width:auto;height:auto;-webkit-clip:auto;clip:auto;position:absolute;overflow:hidden;}/*!sc*/
.EuMgV:not(:focus){-webkit-clip:rect(1px,1px,1px,1px);clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;width:1px;margin:-1px;padding:0;}/*!sc*/
data-styled.g2[id="skip-link__SkipLink-sc-1z0kjxc-0"]{content:"EuMgV,"}/*!sc*/
.fbaWCe{display:none;margin-left:8px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.fbaWCe{display:inline;}}/*!sc*/
.bLwTGz{font-weight:600;display:inline-block;margin-bottom:4px;}/*!sc*/
.cQAYyE{font-weight:600;}/*!sc*/
.hHOTlN{text-align:center;font-size:14px;margin-top:8px;margin-bottom:16px;color:#6a737d;}/*!sc*/
.fWAlyn{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;font-size:14px;}/*!sc*/
.gHwtLv{font-size:14px;color:#444d56;margin-top:4px;}/*!sc*/
data-styled.g4[id="Text-sc-1s3uzov-0"]{content:"fbaWCe,bLwTGz,cQAYyE,hHOTlN,fWAlyn,gHwtLv,"}/*!sc*/
.ifkhtm{background-color:#ffffff;color:#24292e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.gSrgIV{top:0;z-index:1;position:-webkit-sticky;position:sticky;}/*!sc*/
.iTlzRc{padding-left:16px;padding-right:16px;background-color:#24292e;color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:66px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.iTlzRc{padding-left:24px;padding-right:24px;}}/*!sc*/
.kCrfOd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gELiHA{margin-left:24px;display:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gELiHA{display:block;}}/*!sc*/
.gYHnkh{position:relative;}/*!sc*/
.dMFMzl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhCmHN{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.jhCmHN{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.elXfHl{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gjFLbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gjFLbZ{display:none;}}/*!sc*/
.gucKKf{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;background-color:none;cursor:pointer;}/*!sc*/
.gucKKf:hover{fill:rgba(255,255,255,0.7);color:rgba(255,255,255,0.7);}/*!sc*/
.gucKKf svg{fill:rgba(255,255,255,0.7);}/*!sc*/
.fBMuRw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}/*!sc*/
.bQaVuO{color:#2f363d;background-color:#fafbfc;display:none;height:calc(100vh - 66px);min-width:260px;max-width:360px;position:-webkit-sticky;position:sticky;top:66px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.bQaVuO{display:block;}}/*!sc*/
.eeDmz{height:100%;border-style:solid;border-color:#e1e4e8;border-width:0;border-right-width:1px;border-radius:0;}/*!sc*/
.kSoTbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.nElVQ{padding:24px;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-top-width:1px;}/*!sc*/
.iXtyim{margin-left:0;padding-top:4px;padding-bottom:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.vaHQm{margin-bottom:4px;margin-top:4px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.hIjKHD{color:#586069;font-weight:400;display:block;}/*!sc*/
.icYakO{padding-left:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}/*!sc*/
.klfmeZ{max-width:1440px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
.TZbDV{padding:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;}/*!sc*/
@media screen and (min-width:544px){.TZbDV{padding:32px;}}/*!sc*/
@media screen and (min-width:768px){.TZbDV{padding:40px;}}/*!sc*/
@media screen and (min-width:1012px){.TZbDV{padding:48px;}}/*!sc*/
.DPDMP{display:none;max-height:calc(100vh - 66px - 24px);position:-webkit-sticky;position:sticky;top:90px;width:220px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:40px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.DPDMP{display:block;}}/*!sc*/
.gUNLMu{margin:0;padding:0;}/*!sc*/
.bzTeHX{padding-left:0;}/*!sc*/
.bnaGYs{padding-left:16px;}/*!sc*/
.meQBK{width:100%;}/*!sc*/
.jYYExC{margin-bottom:32px;background-color:#f6f8fa;display:block;border-width:1px;border-style:solid;border-color:#e1e4e8;border-radius:6px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.jYYExC{display:none;}}/*!sc*/
.hgiZBa{padding:16px;}/*!sc*/
.hnQOQh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gEqaxf{padding:16px;border-top:1px solid;border-color:border.gray;}/*!sc*/
.jNnzYF{position:relative;display:table;table-layout:fixed;width:100%;}/*!sc*/
.eLWiuh{padding:8px;position:absolute;top:0;right:0;}/*!sc*/
.iCCWeC{margin-top:0;margin-bottom:16px;padding:16px;border:0;border-radius:6px;}/*!sc*/
.ksEcN{margin-top:64px;padding-top:32px;padding-bottom:32px;border-style:solid;border-color:#e1e4e8;border-width:0;border-top-width:1px;border-radius:0;}/*!sc*/
.jsSpbO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g5[id="Box-nv15kw-0"]{content:"ifkhtm,gSrgIV,iTlzRc,kCrfOd,gELiHA,gYHnkh,dMFMzl,jhCmHN,elXfHl,gjFLbZ,gucKKf,fBMuRw,bQaVuO,eeDmz,kSoTbZ,nElVQ,iXtyim,vaHQm,hIjKHD,icYakO,klfmeZ,TZbDV,DPDMP,gUNLMu,bzTeHX,bnaGYs,meQBK,jYYExC,hgiZBa,hnQOQh,gEqaxf,jNnzYF,eLWiuh,iCCWeC,ksEcN,jsSpbO,"}/*!sc*/
.cjGjQg{position:relative;display:inline-block;padding:6px 16px;font-family:inherit;font-weight:600;line-height:20px;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;font-size:14px;}/*!sc*/
.cjGjQg:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cjGjQg:focus{outline:none;}/*!sc*/
.cjGjQg:disabled{cursor:default;}/*!sc*/
.cjGjQg:disabled svg{opacity:0.6;}/*!sc*/
data-styled.g6[id="ButtonBase-sc-181ps9o-0"]{content:"cjGjQg,"}/*!sc*/
.cCazJI{color:#24292e;background-color:#fafbfc;border:1px solid rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.04),inset 0 1px 0 rgba(255,255,255,0.25);padding-left:8px;padding-right:8px;}/*!sc*/
.cCazJI:hover{background-color:#f3f4f6;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cCazJI:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
.cCazJI:active{background-color:hsla(220,14%,94%,1);box-shadow:inset 0 0.15em 0.3em rgba(27,31,35,0.15);}/*!sc*/
.cCazJI:disabled{color:#959da5;background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g7[id="Button-xjtz72-0"]{content:"cCazJI,"}/*!sc*/
.cmMzjy{color:#6a737d;}/*!sc*/
.fafffn{margin-right:8px;}/*!sc*/
data-styled.g8[id="StyledOcticon-uhnt7w-0"]{content:"bhRGQB,cmMzjy,fafffn,"}/*!sc*/
.fTkTnC{font-weight:600;font-size:32px;margin:0;font-size:12px;font-weight:500;color:#959da5;margin-bottom:4px;text-transform:uppercase;font-family:Content-font,Roboto,sans-serif;}/*!sc*/
.ffNRvO{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g12[id="Heading-sc-1cjoo9h-0"]{content:"fTkTnC,ffNRvO,"}/*!sc*/
.ifFLoZ{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.ifFLoZ:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.ifFLoZ:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.ifFLoZ:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.ifFLoZ:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);}/*!sc*/
.fKTxJr:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.fKTxJr:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.fKTxJr:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.cXFtEt:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.cXFtEt:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.cXFtEt:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g13[id="ButtonOutline-sc-15gta9l-0"]{content:"ifFLoZ,fKTxJr,cXFtEt,"}/*!sc*/
.iEGqHu{color:rgba(255,255,255,0.7);background-color:transparent;border:1px solid #444d56;box-shadow:none;}/*!sc*/
data-styled.g14[id="dark-button__DarkButton-sc-bvvmfe-0"]{content:"iEGqHu,"}/*!sc*/
.ljCWQd{border:0;font-size:inherit;font-family:inherit;background-color:transparent;-webkit-appearance:none;color:inherit;width:100%;}/*!sc*/
.ljCWQd:focus{outline:0;}/*!sc*/
data-styled.g15[id="TextInput__Input-sc-1apmpmt-0"]{content:"ljCWQd,"}/*!sc*/
.dHfzvf{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;min-height:34px;font-size:14px;line-height:20px;color:#24292e;vertical-align:middle;background-repeat:no-repeat;background-position:right 8px center;border:1px solid #e1e4e8;border-radius:6px;outline:none;box-shadow:inset 0 1px 0 rgba(225,228,232,0.2);padding:6px 12px;width:240px;}/*!sc*/
.dHfzvf .TextInput-icon{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;color:#959da5;margin:0 8px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}/*!sc*/
.dHfzvf:focus-within{border-color:#0366d6;box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
@media (min-width:768px){.dHfzvf{font-size:14px;}}/*!sc*/
data-styled.g16[id="TextInput__Wrapper-sc-1apmpmt-1"]{content:"dHfzvf,"}/*!sc*/
.khRwtY{font-size:16px !important;color:rgba(255,255,255,0.7);background-color:rgba(255,255,255,0.07);border:1px solid transparent;box-shadow:none;}/*!sc*/
.khRwtY:focus{border:1px solid #444d56 outline:none;box-shadow:none;}/*!sc*/
data-styled.g17[id="dark-text-input__DarkTextInput-sc-1s2iwzn-0"]{content:"khRwtY,"}/*!sc*/
.bqVpte.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g19[id="nav-items__NavLink-sc-tqz5wl-0"]{content:"bqVpte,"}/*!sc*/
.kEKZhO.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g20[id="nav-items__NavBox-sc-tqz5wl-1"]{content:"kEKZhO,"}/*!sc*/
.gCPbFb{margin-top:24px;margin-bottom:16px;-webkit-scroll-margin-top:90px;-moz-scroll-margin-top:90px;-ms-scroll-margin-top:90px;scroll-margin-top:90px;}/*!sc*/
.gCPbFb .octicon-link{visibility:hidden;}/*!sc*/
.gCPbFb:hover .octicon-link,.gCPbFb:focus-within .octicon-link{visibility:visible;}/*!sc*/
data-styled.g22[id="heading__StyledHeading-sc-1fu06k9-0"]{content:"gCPbFb,"}/*!sc*/
.fGjcEF{margin-top:0;padding-bottom:4px;font-size:32px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g23[id="heading__StyledH1-sc-1fu06k9-1"]{content:"fGjcEF,"}/*!sc*/
.fvbkiW{padding-bottom:4px;font-size:24px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g24[id="heading__StyledH2-sc-1fu06k9-2"]{content:"fvbkiW,"}/*!sc*/
.cxpRJj{font-size:20px;}/*!sc*/
data-styled.g25[id="heading__StyledH3-sc-1fu06k9-3"]{content:"cxpRJj,"}/*!sc*/
.elBfYx{max-width:100%;box-sizing:content-box;background-color:#ffffff;}/*!sc*/
data-styled.g30[id="image__Image-sc-1r30dtv-0"]{content:"elBfYx,"}/*!sc*/
.jyryuZ{padding:0.2em 0.4em;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;font-size:85%;background-color:rgba(27,31,35,0.05);border-radius:3px;}/*!sc*/
data-styled.g31[id="inline-code__InlineCode-sc-5lgfx8-0"]{content:"jyryuZ,"}/*!sc*/
.dFVIUa{padding-left:2em;margin-bottom:4px;}/*!sc*/
.dFVIUa ul,.dFVIUa ol{margin-top:0;margin-bottom:0;}/*!sc*/
.dFVIUa li{line-height:1.6;}/*!sc*/
.dFVIUa li > p{margin-top:16px;}/*!sc*/
.dFVIUa li + li{margin-top:8px;}/*!sc*/
data-styled.g32[id="list__List-sc-s5kxp2-0"]{content:"dFVIUa,"}/*!sc*/
.iNQqSl{margin:0 0 16px;}/*!sc*/
data-styled.g34[id="paragraph__Paragraph-sc-17pab92-0"]{content:"iNQqSl,"}/*!sc*/
.drDDht{z-index:0;}/*!sc*/
data-styled.g37[id="layout___StyledBox-sc-7a5ttt-0"]{content:"drDDht,"}/*!sc*/
.flyUPp{list-style:none;}/*!sc*/
data-styled.g39[id="table-of-contents___StyledBox-sc-1jtv948-0"]{content:"flyUPp,"}/*!sc*/
.bPkrfP{grid-area:table-of-contents;overflow:auto;}/*!sc*/
data-styled.g40[id="post-page___StyledBox-sc-17hbw1s-0"]{content:"bPkrfP,"}/*!sc*/
</style><title data-react-helmet="true">MNIST For ML Beginners | TensorFlow - Everything that I know</title><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="icon" href="/wiki/favicon-32x32.png?v=5a308f5ba2cede7d3eae15d720a4e776" type="image/png"/><link rel="manifest" href="/wiki/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/wiki/icons/icon-48x48.png?v=5a308f5ba2cede7d3eae15d720a4e776"/><link rel="apple-touch-icon" sizes="72x72" href="/wiki/icons/icon-72x72.png?v=5a308f5ba2cede7d3eae15d720a4e776"/><link rel="apple-touch-icon" sizes="96x96" href="/wiki/icons/icon-96x96.png?v=5a308f5ba2cede7d3eae15d720a4e776"/><link rel="apple-touch-icon" sizes="144x144" href="/wiki/icons/icon-144x144.png?v=5a308f5ba2cede7d3eae15d720a4e776"/><link rel="apple-touch-icon" sizes="192x192" href="/wiki/icons/icon-192x192.png?v=5a308f5ba2cede7d3eae15d720a4e776"/><link rel="apple-touch-icon" sizes="256x256" href="/wiki/icons/icon-256x256.png?v=5a308f5ba2cede7d3eae15d720a4e776"/><link rel="apple-touch-icon" sizes="384x384" href="/wiki/icons/icon-384x384.png?v=5a308f5ba2cede7d3eae15d720a4e776"/><link rel="apple-touch-icon" sizes="512x512" href="/wiki/icons/icon-512x512.png?v=5a308f5ba2cede7d3eae15d720a4e776"/><link rel="sitemap" type="application/xml" href="/wiki/sitemap/sitemap-index.xml"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link as="script" rel="preload" href="/wiki/webpack-runtime-07e8faa2a261685889a0.js"/><link as="script" rel="preload" href="/wiki/framework-6c63f85700e5678d2c2a.js"/><link as="script" rel="preload" href="/wiki/f0e45107-3309acb69b4ccd30ce0c.js"/><link as="script" rel="preload" href="/wiki/0e226fb0-1cb0709e5ed968a9c435.js"/><link as="script" rel="preload" href="/wiki/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js"/><link as="script" rel="preload" href="/wiki/app-3f4d8f561104a074e339.js"/><link as="script" rel="preload" href="/wiki/commons-c89ede6cb9a530ac5a37.js"/><link as="script" rel="preload" href="/wiki/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"/><link as="fetch" rel="preload" href="/wiki/page-data/AI/Deep-Learning/Computer-Vision-CV/MNIST-For-ML-Beginners-TensorFlow/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/wiki/page-data/sq/d/2230547434.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/wiki/page-data/sq/d/2320115945.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/wiki/page-data/sq/d/3495835395.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/wiki/page-data/sq/d/451533639.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/wiki/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><a class="Link-sc-1brdqhf-0 fnAJEh skip-link__SkipLink-sc-1z0kjxc-0 EuMgV" color="auto.white" href="#skip-nav" font-size="1">Skip to content</a><div display="flex" color="text.primary" class="Box-nv15kw-0 ifkhtm"><div class="Box-nv15kw-0 gSrgIV"><div display="flex" height="66" color="header.text" class="Box-nv15kw-0 iTlzRc"><div display="flex" class="Box-nv15kw-0 kCrfOd"><a color="header.logo" mr="3" class="Link-sc-1brdqhf-0 czsBQU" href="/wiki/"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 bhRGQB" viewBox="0 0 16 16" width="32" height="32" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a color="header.logo" font-family="mono" class="Link-sc-1brdqhf-0 kLOWMo" href="/wiki/">Deepak Wiki</a><div display="none,,,block" class="Box-nv15kw-0 gELiHA"><div role="combobox" aria-expanded="false" aria-haspopup="listbox" aria-labelledby="downshift-search-label" class="Box-nv15kw-0 gYHnkh"><span class="TextInput__Wrapper-sc-1apmpmt-1 dHfzvf dark-text-input__DarkTextInput-sc-1s2iwzn-0 khRwtY TextInput-wrapper" width="240"><input type="text" aria-autocomplete="list" aria-labelledby="downshift-search-label" autoComplete="off" value="" id="downshift-search-input" placeholder="Search Deepak Wiki" class="TextInput__Input-sc-1apmpmt-0 ljCWQd"/></span></div></div></div><div display="flex" class="Box-nv15kw-0 dMFMzl"><div display="none,,,flex" class="Box-nv15kw-0 jhCmHN"><div display="flex" color="header.text" class="Box-nv15kw-0 elXfHl"><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://github.com/deepaksood619/wiki/" class="Link-sc-1brdqhf-0 kEUvCO">Github</a><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://twitter.com/deepaksood619" class="Link-sc-1brdqhf-0 kEUvCO">Twitter</a></div><button aria-label="Theme" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-sun" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z"></path></svg></button></div><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Search" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg fKTxJr iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z"></path></svg></button></div><button aria-label="Show Graph Visualisation" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg cXFtEt iEGqHu"><div title="Show Graph Visualisation" aria-label="Show Graph Visualisation" color="header.text" display="flex" class="Box-nv15kw-0 gucKKf"><svg t="1607341341241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path d="M512 512m-125.866667 0a125.866667 125.866667 0 1 0 251.733334 0 125.866667 125.866667 0 1 0-251.733334 0Z"></path><path d="M512 251.733333m-72.533333 0a72.533333 72.533333 0 1 0 145.066666 0 72.533333 72.533333 0 1 0-145.066666 0Z"></path><path d="M614.4 238.933333c0 4.266667 2.133333 8.533333 2.133333 12.8 0 19.2-4.266667 36.266667-12.8 51.2 81.066667 36.266667 138.666667 117.333333 138.666667 211.2C742.4 640 640 744.533333 512 744.533333s-230.4-106.666667-230.4-232.533333c0-93.866667 57.6-174.933333 138.666667-211.2-8.533333-14.933333-12.8-32-12.8-51.2 0-4.266667 0-8.533333 2.133333-12.8-110.933333 42.666667-189.866667 147.2-189.866667 273.066667 0 160 130.133333 292.266667 292.266667 292.266666S804.266667 672 804.266667 512c0-123.733333-78.933333-230.4-189.866667-273.066667z"></path><path d="M168.533333 785.066667m-72.533333 0a72.533333 72.533333 0 1 0 145.066667 0 72.533333 72.533333 0 1 0-145.066667 0Z"></path><path d="M896 712.533333m-61.866667 0a61.866667 61.866667 0 1 0 123.733334 0 61.866667 61.866667 0 1 0-123.733334 0Z"></path><path d="M825.6 772.266667c-74.666667 89.6-187.733333 147.2-313.6 147.2-93.866667 0-181.333333-32-249.6-87.466667-10.666667 19.2-25.6 34.133333-44.8 44.8C298.666667 942.933333 401.066667 981.333333 512 981.333333c149.333333 0 281.6-70.4 366.933333-177.066666-21.333333-4.266667-40.533333-17.066667-53.333333-32zM142.933333 684.8c-25.6-53.333333-38.4-110.933333-38.4-172.8C104.533333 288 288 104.533333 512 104.533333S919.466667 288 919.466667 512c0 36.266667-6.4 72.533333-14.933334 106.666667 23.466667 2.133333 42.666667 10.666667 57.6 25.6 12.8-42.666667 19.2-87.466667 19.2-132.266667 0-258.133333-211.2-469.333333-469.333333-469.333333S42.666667 253.866667 42.666667 512c0 74.666667 17.066667 142.933333 46.933333 204.8 14.933333-14.933333 32-27.733333 53.333333-32z"></path></svg><span display="none,,,inline" class="Text-sc-1s3uzov-0 fbaWCe">Show Graph Visualisation</span></div></button><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Menu" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path></svg></button></div></div></div></div><div display="flex" class="Box-nv15kw-0 layout___StyledBox-sc-7a5ttt-0 fBMuRw drDDht"><div display="none,,,block" height="calc(100vh - 66px)" color="auto.gray.8" class="Box-nv15kw-0 bQaVuO"><div height="100%" style="overflow:auto" class="Box-nv15kw-0 eeDmz"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><div class="Box-nv15kw-0 nElVQ"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><h2 color="text.disabled" font-size="12px" font-weight="500" class="Heading-sc-1cjoo9h-0 fTkTnC">Categories</h2><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">AI</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Algorithms</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer-Science</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Data-Structures</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Databases</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Knowledge</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Languages</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Mathematics</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Technologies</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/wiki/">wiki</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div></div></div></div></div><main class="Box-nv15kw-0 klfmeZ"><div id="skip-nav" display="flex" width="100%" class="Box-nv15kw-0 TZbDV"><div display="none,,block" class="Box-nv15kw-0 post-page___StyledBox-sc-17hbw1s-0 DPDMP bPkrfP"><span display="inline-block" font-weight="bold" class="Text-sc-1s3uzov-0 bLwTGz">On this page</span><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#mnist-for-ml-beginners--tensorflow" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">MNIST For ML Beginners | TensorFlow</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#implementing-the-regression" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Implementing the Regression</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#training" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Training</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#evaluating-our-model" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Evaluating Our Model</a></li></ul></li></ul></div><div width="100%" class="Box-nv15kw-0 meQBK"><div display="block,,none" class="Box-nv15kw-0 jYYExC"><div class="Box-nv15kw-0 hgiZBa"><div display="flex" class="Box-nv15kw-0 hnQOQh"><span font-weight="bold" class="Text-sc-1s3uzov-0 cQAYyE">On this page</span></div></div><div class="Box-nv15kw-0 gEqaxf"><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#mnist-for-ml-beginners--tensorflow" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">MNIST For ML Beginners | TensorFlow</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#implementing-the-regression" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Implementing the Regression</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#training" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Training</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#evaluating-our-model" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Evaluating Our Model</a></li></ul></li></ul></div></div><h1 id="mnist-for-ml-beginners--tensorflow" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#mnist-for-ml-beginners--tensorflow" color="auto.gray.8" aria-label="MNIST For ML Beginners | TensorFlow permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MNIST For ML Beginners | TensorFlow</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">When one learns how to program, there&#x27;s a tradition that the first thing you do is print &quot;Hello World.&quot; Just like programming has Hello World, machine learning has MNIST.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">MNIST is a simple computer vision dataset. It consists of images of handwritten digits like these:</p><figure class="gatsby-resp-image-figure">
    <span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:561px">
      <a class="Link-sc-1brdqhf-0 cKRjba gatsby-resp-image-link" style="display:block" target="_blank" rel="noopener noreferrer" href="/wiki/static/4868e0c2491978f3e2e3c54390187d32/9be90/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image1.png">
    <span class="gatsby-resp-image-background-image" style="padding-bottom:25%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABeUlEQVQY0x1OOW/iQBT2P0sb6vQpssUWUZSjSYqtIBWkQVhIpDExKxmvBIwFIigyBuxqkDWYSzYyhzyzZjgKhFY00Uovmrzm+97Tdzyp2WxebzabJmOsTAh5b7fbdwihiziOW8vlsjydTluEkNtKpXJBKf1YrVbl9Xr9jjH+lcvlzhljLUrpH0pp1bZtRQrDsBxFESSTSSCEwGw2Q67rpgAA0uk0KIoC+/2+Vq1WU0Inxvd9mM/ntq7r92I/HA5gGAZQSj8lz/MK4qgoylEEcs6LQRA8dTodME3zWKvVoN/vF3Vdf0QIgeM4R03TYLvdGqqq/hReWZb/eZ73udvt/kqj0Ugfj8eQyWRAYBRFxmQyebYsCwqFAjDGII5j1O12U6fTSXDAGItixzCMB8455PP578/DMPwv1ev1S03Tio7jyJxztdFo/EAIJQaDgdrr9WTGmGpZ1lWpVEoEQfDb933Zdd0327Zvstns2XA4VDHG+cVi8Wqa5ssXTIc27tEFEwgAAAAASUVORK5CYII=&#x27;);background-size:cover;display:block"></span>
  <img class="image__Image-sc-1r30dtv-0 elBfYx gatsby-resp-image-image" alt="image" title="image" src="/wiki/static/4868e0c2491978f3e2e3c54390187d32/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image1.png" srcSet="/wiki/static/4868e0c2491978f3e2e3c54390187d32/0d3e1/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image1.png 140w,/wiki/static/4868e0c2491978f3e2e3c54390187d32/6b1e2/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image1.png 281w,/wiki/static/4868e0c2491978f3e2e3c54390187d32/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image1.png 561w,/wiki/static/4868e0c2491978f3e2e3c54390187d32/9be90/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image1.png 636w" sizes="(max-width: 561px) 100vw, 561px" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0" loading="lazy" decoding="async"/>
  </a>
    </span>
    <figcaption font-size="1" color="auto.gray.5" class="Text-sc-1s3uzov-0 hHOTlN gatsby-resp-image-figcaption">image</figcaption>
  </figure><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">It also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">In this tutorial, we&#x27;re going to train a model to look at images and predict what digits they are. We&#x27;re going to start with a very simple model, called a Softmax Regression.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">The actual code for this tutorial is very short, and all the interesting stuff happens in just three lines. However, it is very important to understand the ideas behind it: both how TensorFlow works and the core machine learning concepts. Because of this, we are going to very carefully work through the code.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">What we will accomplish in this tutorial:</p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>Learn about the MNIST data and softmax regressions</li><li>Create a function that is a model for recognizing digits, based on looking at every pixel in the image</li><li>Use TensorFlow to train the model to recognize digits by having it &quot;look&quot; at thousands of examples (and run our first TensorFlow session to do so)</li><li>Check the model&#x27;s accuracy with our test data</li></ul><h3 id="the-mnist-data" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH3-sc-1fu06k9-3 ffNRvO gCPbFb cxpRJj Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#the-mnist-data" color="auto.gray.8" aria-label="The MNIST Data permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The MNIST Data</h3><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">The MNIST data is hosted on <a target="_blank" rel="noopener noreferrer" href="http://yann.lecun.com/exdb/mnist/" class="Link-sc-1brdqhf-0 cKRjba">Yann LeCun&#x27;s website</a>. If you are copying and pasting in the code from this tutorial, start here with these two lines of code which will download and read in the data automatically:</p><div class="Box-nv15kw-0 jNnzYF"><div class="Box-nv15kw-0 eLWiuh"><button aria-label="Copy to clipboard" class="ButtonBase-sc-181ps9o-0 Button-xjtz72-0 cjGjQg cCazJI"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 cmMzjy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></button></div><pre class="Box-nv15kw-0 iCCWeC prism-code language-python" style="color:#393A34;background-color:#f6f8fa;overflow:auto"><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">from tensorflow.examples.tutorials.mnist import input_data</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</span></div></pre></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">The MNIST data is split into three parts: 55,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test), and 5,000 points of validation data (mnist.validation). This split is very important: it&#x27;s essential in machine learning that we have separate data which we don&#x27;t learn from so that we can make sure that what we&#x27;ve learned actually generalizes!</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">As mentioned earlier, every MNIST data point has two parts: an image of a handwritten digit and a corresponding label. We&#x27;ll call the images &quot;x&quot; and the labels &quot;y&quot;. Both the training set and test set contain images and their corresponding labels; for example the training images are mnist.train.images and the training labels are mnist.train.labels.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:</p><figure class="gatsby-resp-image-figure">
    <span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:561px">
      <a class="Link-sc-1brdqhf-0 cKRjba gatsby-resp-image-link" style="display:block" target="_blank" rel="noopener noreferrer" href="/wiki/static/42964172e7f6fe3007f422cadb9e970f/84a90/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image2.png">
    <span class="gatsby-resp-image-background-image" style="padding-bottom:39.285714285714285%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABFklEQVQY022RTW7CMBBGcy+OwkHYsOAQXIQVgnOUTRFEivgrCDsmM0k8M/ZU2GlL1b7V2NKzP38u7ve7tdYY0/f94/EwCWutJ991XQhBEiGEGOPrsm3bAgA0sVqtLpeLqsYY67ruuq5pgJm998xsjPm4XpmZiPIONE2BiFkej8fr9VpVQwjOuVdZRHa73dtmQ4lBhiTHGFV1MpnM53NVFZHaWkRo4NfN79vt+Xxi5v6vPJvNptPpINcWEeFLJiJmqqrqeDyKyD/yYrEYjUb5za5+xgZEEeEEMZVlud/vc3IRQYSfwpxzy+UyH5QLAwQRIaZk0+12OxwObdsOMkDx/VXOOSLKs7W29/0Qmzyljp8pJE2UY8MnE4/Bmbp8RHYAAAAASUVORK5CYII=&#x27;);background-size:cover;display:block"></span>
  <img class="image__Image-sc-1r30dtv-0 elBfYx gatsby-resp-image-image" alt="media" title="media" src="/wiki/static/42964172e7f6fe3007f422cadb9e970f/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image2.png" srcSet="/wiki/static/42964172e7f6fe3007f422cadb9e970f/0d3e1/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image2.png 140w,/wiki/static/42964172e7f6fe3007f422cadb9e970f/6b1e2/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image2.png 281w,/wiki/static/42964172e7f6fe3007f422cadb9e970f/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image2.png 561w,/wiki/static/42964172e7f6fe3007f422cadb9e970f/99072/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image2.png 842w,/wiki/static/42964172e7f6fe3007f422cadb9e970f/84a90/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image2.png 982w" sizes="(max-width: 561px) 100vw, 561px" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0" loading="lazy" decoding="async"/>
  </a>
    </span>
    <figcaption font-size="1" color="auto.gray.5" class="Text-sc-1s3uzov-0 hHOTlN gatsby-resp-image-figcaption">media</figcaption>
  </figure><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">We can flatten this array into a vector of 28x28 = 784 numbers. It doesn&#x27;t matter how we flatten the array, as long as we&#x27;re consistent between images. From this perspective, the MNIST images are just a bunch of points in a 784-dimensional vector space, with a <a target="_blank" rel="noopener noreferrer" href="https://colah.github.io/posts/2014-10-Visualizing-MNIST/" class="Link-sc-1brdqhf-0 cKRjba">very rich structure</a> (warning: computationally intensive visualizations).</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Flattening the data throws away information about the 2D structure of the image. Isn&#x27;t that bad? Well, the best computer vision methods do exploit this structure, and we will in later tutorials. But the simple method we will be using here, a softmax regression (defined below), won&#x27;t.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">The result is that mnist.train.images is a tensor (an n-dimensional array) with a shape of <!-- -->[55000, 784]<!-- -->. The first dimension is an index into the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.</p><figure class="gatsby-resp-image-figure">
    <span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:561px">
      <a class="Link-sc-1brdqhf-0 cKRjba gatsby-resp-image-link" style="display:block" target="_blank" rel="noopener noreferrer" href="/wiki/static/7516810f065b8884a532ae23d2f2b207/0f586/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image3.png">
    <span class="gatsby-resp-image-background-image" style="padding-bottom:44.99999999999999%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABuElEQVQoz42RwWoaURSGf804TuMEE5OiXTY4aZvQTQnJonQVsjUP4Dvo0j7LgHShBUXc2IEBheKgI1c0I1eEUcxCcZwxT1BMSm+5dKQEkpAfzuKee/+fc76LRCKBYDAILtd1t4bDIWazGbrdLnK5HG/vADgE8CEQCLwD8B7AWwB7eEx+mKwoijQYDNBoNA5rtdppoVA4yefzx6qqHlUqleNqtSqpqrqxBfCcotHoAQDRtu0rQshtr9dbUEpvp9Pp0rZtdzQaXWuaJmmahmw2+3yYL7HZbJ4bhvFH13VGCGGLxYKt12s2mUyYZVkjxtgWY2zz/g2AbQAJAK/88z9FIpGPAI76/f5nx3F+mab5u9Vq3dXr9XvP89aWZbHlcnlTLBZDpmlubJxfGMAuH+YBz3Q6HeLg2+32F242DIOVy2U2n8+Z4zis0+kwz/O+8+l4pVKpF60cKpVKF7quFyil3yilP1zXrY7H45+EkK88KJPJYLVaQZbll+RhJ5lM8o8RfDbctS0Iwp7PTVQUZcMw6K/7VP2fUpKkg3A4HBMEISbLcoyzicfjr0VR3AfwCcAlgDMA/G7/sfoL/uPFcPWWfVUAAAAASUVORK5CYII=&#x27;);background-size:cover;display:block"></span>
  <img class="image__Image-sc-1r30dtv-0 elBfYx gatsby-resp-image-image" alt="image" title="image" src="/wiki/static/7516810f065b8884a532ae23d2f2b207/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image3.png" srcSet="/wiki/static/7516810f065b8884a532ae23d2f2b207/0d3e1/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image3.png 140w,/wiki/static/7516810f065b8884a532ae23d2f2b207/6b1e2/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image3.png 281w,/wiki/static/7516810f065b8884a532ae23d2f2b207/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image3.png 561w,/wiki/static/7516810f065b8884a532ae23d2f2b207/99072/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image3.png 842w,/wiki/static/7516810f065b8884a532ae23d2f2b207/62a6a/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image3.png 1122w,/wiki/static/7516810f065b8884a532ae23d2f2b207/0f586/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image3.png 1498w" sizes="(max-width: 561px) 100vw, 561px" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0" loading="lazy" decoding="async"/>
  </a>
    </span>
    <figcaption font-size="1" color="auto.gray.5" class="Text-sc-1s3uzov-0 hHOTlN gatsby-resp-image-figcaption">image</figcaption>
  </figure><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">For the purposes of this tutorial, we&#x27;re going to want our labels as &quot;one-hot vectors&quot;. A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. For example, 3 would be <!-- -->[0,0,0,1,0,0,0,0,0,0]<!-- -->. Consequently, mnist.train.labels is a <!-- -->[55000, 10]<!-- --> array of floats.</p><figure class="gatsby-resp-image-figure">
    <span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:561px">
      <a class="Link-sc-1brdqhf-0 cKRjba gatsby-resp-image-link" style="display:block" target="_blank" rel="noopener noreferrer" href="/wiki/static/ac8f12cbd1fead13e9a35b5c94907483/95e59/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image4.png">
    <span class="gatsby-resp-image-background-image" style="padding-bottom:34.285714285714285%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABOUlEQVQoz52RPWvCUBSGn8QU+qG0UEqn0qEV6dD+ARGXQiV2aUEyKNShP0LI7OYiilMQyeCSTI6OTkIGoQUFrUNACAj+AWO5JSUWK7TQvnB44Z6X5xzuAaDRaOB5HtlsFlVV8X2fcrmMECJo3wAJIA5cA+fAGb9IqtVqcc/znpLJZD6VShV83893Op2iYRj3/EOKZVnPs9lMOI4jer2e6Pf7wjRNUa1W3xRF2QlCmqat87uADOwBkdC/Ka7reqFer4vhcLgcjUarSqWyHAwGYrFYvACfwHQ6LYX5CyC64YltYKTZbBYDwHg8fnddV8zn89V0OhWTyeQ13OZPkkql0mW3283Ztv3QbrcfW61W4DnTNO+AIyAmy/JBuNHhDxUDTr6g21PCC6//6ArIALfAKbAfAqIbFbwdfwCXw3aX/XOfIgAAAABJRU5ErkJggg==&#x27;);background-size:cover;display:block"></span>
  <img class="image__Image-sc-1r30dtv-0 elBfYx gatsby-resp-image-image" alt="image" title="image" src="/wiki/static/ac8f12cbd1fead13e9a35b5c94907483/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image4.png" srcSet="/wiki/static/ac8f12cbd1fead13e9a35b5c94907483/0d3e1/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image4.png 140w,/wiki/static/ac8f12cbd1fead13e9a35b5c94907483/6b1e2/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image4.png 281w,/wiki/static/ac8f12cbd1fead13e9a35b5c94907483/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image4.png 561w,/wiki/static/ac8f12cbd1fead13e9a35b5c94907483/99072/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image4.png 842w,/wiki/static/ac8f12cbd1fead13e9a35b5c94907483/62a6a/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image4.png 1122w,/wiki/static/ac8f12cbd1fead13e9a35b5c94907483/95e59/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image4.png 1460w" sizes="(max-width: 561px) 100vw, 561px" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0" loading="lazy" decoding="async"/>
  </a>
    </span>
    <figcaption font-size="1" color="auto.gray.5" class="Text-sc-1s3uzov-0 hHOTlN gatsby-resp-image-figcaption">image</figcaption>
  </figure><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">We&#x27;re now ready to actually make our model!</p><h3 id="softmax-regressions" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH3-sc-1fu06k9-3 ffNRvO gCPbFb cxpRJj Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#softmax-regressions" color="auto.gray.8" aria-label="Softmax Regressions permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Softmax Regressions</h3><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">We know that every image in MNIST is of a handwritten digit between zero and nine. So there are only ten possible things that a given image can be. We want to be able to look at an image and give the probabilities for it being each digit. For example, our model might look at a picture of a nine and be 80% sure it&#x27;s a nine, but give a 5% chance to it being an eight (because of the top loop) and a bit of probability to all the others because it isn&#x27;t 100% sure.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">This is a classic case where a softmax regression is a natural, simple model. If you want to assign probabilities to an object being one of several different things, softmax is the thing to do, because softmax gives us a list of values between 0 and 1 that add up to 1. Even later on, when we train more sophisticated models, the final step will be a layer of softmax.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">A softmax regression has two steps: first we add up the evidence of our input being in certain classes, and then we convert that evidence into probabilities.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">To tally up the evidence that a given image is in a particular class, we do a weighted sum of the pixel intensities. The weight is negative if that pixel having a high intensity is evidence against the image being in that class, and positive if it is evidence in favor.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">The following diagram shows the weights one model learned for each of these classes. Red represents negative weights, while blue represents positive weights.</p><figure class="gatsby-resp-image-figure">
    <span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:561px">
      <a class="Link-sc-1brdqhf-0 cKRjba gatsby-resp-image-link" style="display:block" target="_blank" rel="noopener noreferrer" href="/wiki/static/e6b32d753208dc7befbeaf1a6d890ab5/c61d0/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image5.png">
    <span class="gatsby-resp-image-background-image" style="padding-bottom:50%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAACjUlEQVQozx2NzUtUcRiFz3zce3/vmZl7Z8aZuc7XTUWbsbRJHS1RGcvMGi1zNqUk2JdE0LJFRBRlERFB0Kp1tmsRBBmRSdSqP6Rtm8jo4w07u/OcBw4SDP9xWfmbYl2FoaNpmle9TP3Js6x52mdI91mBTS2zU4uCGxFmlrI8rEk58jug8xLCuRL5Y5mxbx4LXxGVIV3hAe3jMYX0z7QT7+Z4UIdYVZdY7mbw/CwP6yD3aFH23QGLK1cZaI1L2s7KmzCxUOa8jvKKlqS4DZH+7Qqnvhdo/4LEp1PivquxW/PMaIRYjsnAepHzmuaMOpy9C/av9LGhPRxQYc8GiNYYfa2z72+KyZ/wiP1hGwtGEnWXVsKmWymZ8KxjzKwTS6ddoqvi4FTWwSkrjkKGyEImpuFkTqeJCuin84KpURuLMZqD8Alc7gPAflC6YEsUg14RvrcA2yTgGKDhAXNJwDJAQkIIxXqR85vwBIB4KMUADYAiw0Ca9n2fWIP0PDTs2C0MX8pw1xOX408pZsQRZz5Lc69KrJFmSsTUE3QeJ+k/iIt1ziIrbYLb48T1EkM30SadOsYJbbKhUVluRmk2R1nS86yqL8ElsPyiIpPazRNKxh9RwqttHNdOmVZXRjbjRCvPqnZwUfcyULSzqi1OaoMZhdjNEnMfCryqRRlRkfxqjon1i8z+P01J9pFPrO7iSR2QGe1l4gMk1qpxtx7ipFbZoSD911Hp3YAU3kbEG/Qlcmsfq+/zUtiyBVNt9K7l2fgyLOUvgYQukNGjZRY+Z1j/6AjvgpGxYXE/HZfqVpLBJoypIQoEyD0EnBy6bOwkB8CxLKBmuztdgIATUcC2/u8OgC5YkR0LgO6gPQfiKfwD3fyZSVU+vM0AAAAASUVORK5CYII=&#x27;);background-size:cover;display:block"></span>
  <img class="image__Image-sc-1r30dtv-0 elBfYx gatsby-resp-image-image" alt="image" title="image" src="/wiki/static/e6b32d753208dc7befbeaf1a6d890ab5/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image5.png" srcSet="/wiki/static/e6b32d753208dc7befbeaf1a6d890ab5/0d3e1/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image5.png 140w,/wiki/static/e6b32d753208dc7befbeaf1a6d890ab5/6b1e2/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image5.png 281w,/wiki/static/e6b32d753208dc7befbeaf1a6d890ab5/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image5.png 561w,/wiki/static/e6b32d753208dc7befbeaf1a6d890ab5/99072/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image5.png 842w,/wiki/static/e6b32d753208dc7befbeaf1a6d890ab5/62a6a/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image5.png 1122w,/wiki/static/e6b32d753208dc7befbeaf1a6d890ab5/c61d0/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image5.png 1145w" sizes="(max-width: 561px) 100vw, 561px" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0" loading="lazy" decoding="async"/>
  </a>
    </span>
    <figcaption font-size="1" color="auto.gray.5" class="Text-sc-1s3uzov-0 hHOTlN gatsby-resp-image-figcaption">image</figcaption>
  </figure><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">We also add some extra evidence called a bias. Basically, we want to be able to say that some things are more likely independent of the input. The result is that the evidence for a class i given an input x is:</p><div class="Box-nv15kw-0 jNnzYF"><div class="Box-nv15kw-0 eLWiuh"><button aria-label="Copy to clipboard" class="ButtonBase-sc-181ps9o-0 Button-xjtz72-0 cjGjQg cCazJI"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 cmMzjy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></button></div><pre class="Box-nv15kw-0 iCCWeC prism-code language-python" style="color:#393A34;background-color:#f6f8fa;overflow:auto"><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">evidencei=∑jWi, jxj+bi</span></div></pre></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">where Wi is the weights and bi is the bias for class i, and j is an index for summing over the pixels in our input image x. We then convert the evidence tallies into our predicted probabilities y using the &quot;softmax&quot; function:</p><div class="Box-nv15kw-0 jNnzYF"><div class="Box-nv15kw-0 eLWiuh"><button aria-label="Copy to clipboard" class="ButtonBase-sc-181ps9o-0 Button-xjtz72-0 cjGjQg cCazJI"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 cmMzjy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></button></div><pre class="Box-nv15kw-0 iCCWeC prism-code language-python" style="color:#393A34;background-color:#f6f8fa;overflow:auto"><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">y=softmax(evidence)</span></div></pre></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Here softmax is serving as an &quot;activation&quot; or &quot;link&quot; function, shaping the output of our linear function into the form we want -- in this case, a probability distribution over 10 cases. You can think of it as converting tallies of evidence into probabilities of our input being in each class. It&#x27;s defined as:</p><div class="Box-nv15kw-0 jNnzYF"><div class="Box-nv15kw-0 eLWiuh"><button aria-label="Copy to clipboard" class="ButtonBase-sc-181ps9o-0 Button-xjtz72-0 cjGjQg cCazJI"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 cmMzjy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></button></div><pre class="Box-nv15kw-0 iCCWeC prism-code language-python" style="color:#393A34;background-color:#f6f8fa;overflow:auto"><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">softmax(evidence)=normalize(exp⁡(evidence))</span></div></pre></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">If you expand that equation out, you get:</p><div class="Box-nv15kw-0 jNnzYF"><div class="Box-nv15kw-0 eLWiuh"><button aria-label="Copy to clipboard" class="ButtonBase-sc-181ps9o-0 Button-xjtz72-0 cjGjQg cCazJI"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 cmMzjy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></button></div><pre class="Box-nv15kw-0 iCCWeC prism-code language-python" style="color:#393A34;background-color:#f6f8fa;overflow:auto"><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">softmax(evidence)i=exp⁡(evidencei)∑jexp⁡(evidencej)</span></div></pre></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">But it&#x27;s often more helpful to think of softmax the first way: exponentiating its inputs and then normalizing them. The exponentiation means that one more unit of evidence increases the weight given to any hypothesis multiplicatively. And conversely, having one less unit of evidence means that a hypothesis gets a fraction of its earlier weight. No hypothesis ever has zero or negative weight. Softmax then normalizes these weights, so that they add up to one, forming a valid probability distribution. (To get more intuition about the softmax function, check out the <a target="_blank" rel="noopener noreferrer" href="http://neuralnetworksanddeeplearning.com/chap3.html#softmax" class="Link-sc-1brdqhf-0 cKRjba">section</a> on it in Michael Nielsen&#x27;s book, complete with an interactive visualization.)</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">You can picture our softmax regression as looking something like the following, although with a lot more xs. For each output, we compute a weighted sum of the xs, add a bias, and then apply softmax.</p><figure class="gatsby-resp-image-figure">
    <span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:561px">
      <a class="Link-sc-1brdqhf-0 cKRjba gatsby-resp-image-link" style="display:block" target="_blank" rel="noopener noreferrer" href="/wiki/static/198e46cad7e7fee9b9fbf6a3e70a4fb5/e40ed/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image6.png">
    <span class="gatsby-resp-image-background-image" style="padding-bottom:40%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAACHVAAAh1QEEnLSdAAACfElEQVQozy2SaUiTcRzHf0FB9MaoyLDDDvJFL7IDQhZeaSmo1Yp0AzOPzIsxZ5tX2kZOH5+etjaPbe720VnThS7d6fkMn002UeZRIWQahl3QqwKJ+Mdjvfz84Pv9Hd8f+LSzXk2tmQ0Ae86dv7AXIQQisg6X+ZQoX1oUyzAA7JOIDWkuxzpt6w/nMDVqwpjw5dOob9KrPcuwt7UrMaDo5YPfEG7E+Yp4RnTsdHQMPibrb3E/JZoc2O3UvGsxuQ35mTv3Q5KkwXDL69rYfNkXKmMMpilz4Z+tEKLGdBkMezBNMS0n7bBoWQWsTgbYCHGxUicUiwebghUd/G3RZXY86xmlQMqAqqLlRQeg3wi6TQhs1hZ4s2jLerf0Cq28HYw39DyG0QYdrJFugGn9XFNtiuSEwCQk8TFZGAAOAcDuxJzkS9U99SqBVlQAAIcJfCDd69qYsFnnrjPNaJ+ZEwyQyE+ZUv6trEmfUfTWA6UNectTK1lFzx8oMQ/hiTgQcQQAojJKsu5hHuJrMV6WDQCRWKsl3+1Y/2w2UoXbBi713cV5K6KnjOkMO5s7C2g5aYEl8wdoVcpB7msvlQxJh2/w2NzI6MgoANhxMvYUq9mFh/TL3Xl69xB8/4hAo/oF1r4nsLwwkLUwb0XvV+wJJosYhoRtEFbbAMY7/SzpQ+IoAOzKruZWMOnyu6pEPFVlZnYNN66ur3GYLbrJM7T545zDq7YB62wSM9HCXD9n6+cMCtLkVYbHCd2VGaWlCgLG8KaqxnCfSflOOed4Eif5jEAvfNQeVCOeWpD2/20O6nWTpa6RNdTbTRczNbdDxf3xbQo57W3bNxyRdpRQhPH1X8TAU81LKvdeAAAAAElFTkSuQmCC&#x27;);background-size:cover;display:block"></span>
  <img class="image__Image-sc-1r30dtv-0 elBfYx gatsby-resp-image-image" alt="image" title="image" src="/wiki/static/198e46cad7e7fee9b9fbf6a3e70a4fb5/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image6.png" srcSet="/wiki/static/198e46cad7e7fee9b9fbf6a3e70a4fb5/0d3e1/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image6.png 140w,/wiki/static/198e46cad7e7fee9b9fbf6a3e70a4fb5/6b1e2/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image6.png 281w,/wiki/static/198e46cad7e7fee9b9fbf6a3e70a4fb5/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image6.png 561w,/wiki/static/198e46cad7e7fee9b9fbf6a3e70a4fb5/99072/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image6.png 842w,/wiki/static/198e46cad7e7fee9b9fbf6a3e70a4fb5/62a6a/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image6.png 1122w,/wiki/static/198e46cad7e7fee9b9fbf6a3e70a4fb5/e40ed/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image6.png 1378w" sizes="(max-width: 561px) 100vw, 561px" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0" loading="lazy" decoding="async"/>
  </a>
    </span>
    <figcaption font-size="1" color="auto.gray.5" class="Text-sc-1s3uzov-0 hHOTlN gatsby-resp-image-figcaption">image</figcaption>
  </figure><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">If we write that out as equations, we get:</p><figure class="gatsby-resp-image-figure">
    <span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:561px">
      <a class="Link-sc-1brdqhf-0 cKRjba gatsby-resp-image-link" style="display:block" target="_blank" rel="noopener noreferrer" href="/wiki/static/4c481e48637c4563a45480015d371bce/2f6f6/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image7.png">
    <span class="gatsby-resp-image-background-image" style="padding-bottom:22.857142857142858%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAACHVAAAh1QEEnLSdAAABoElEQVQY0wGVAWr+AMuSp4v/6v8AAHEAAN7TrgCQl3kAAAkJDIm/ja2f1aTA0azbvNRv4RuCuYVaktGWy726xcbUmuBpAAAABY7Fkqig0KXG0avct6hpnxm1tuFeAM2UqGv///8A8uHEC6Wkhg7//9IKVlpKGZDJlI+h1aab0qzdmtFl3hSPy5RLlteapL23xp/VmuBTABoABJjTnIqg0Kag0qrelJthkhO4ueREAMiRpIr///8A1dGrncnGotLTz6nVurqYh4PAi6ehz6XD0arbuspZ1xOIwIxmkc+Vy721xsbTmN5iAEQAB4/IlLOey6PG0abcs6FklBKoqNBXAMyUqH3/9f8A4NC1DtXTrBL+8sgNW2JQHonBjpmi06if0qvdm/Br/w2JwY1TlNOYqr+4yKPYnORPAF0AA5HKlZSh0Kel06jflbdtpQywsNlMAMaPo4T/6v8AI4sWAP/3ywCirYoAM00+DJHKlayezaO20KnasbtPxxONx5Fhk9KXwbuyxL7PltpfAD4AB5XPmamdyaK8z6XaqpRfixSqq9NL5dDriqkqduwAAAAASUVORK5CYII=&#x27;);background-size:cover;display:block"></span>
  <img class="image__Image-sc-1r30dtv-0 elBfYx gatsby-resp-image-image" alt="image" title="image" src="/wiki/static/4c481e48637c4563a45480015d371bce/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image7.png" srcSet="/wiki/static/4c481e48637c4563a45480015d371bce/0d3e1/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image7.png 140w,/wiki/static/4c481e48637c4563a45480015d371bce/6b1e2/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image7.png 281w,/wiki/static/4c481e48637c4563a45480015d371bce/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image7.png 561w,/wiki/static/4c481e48637c4563a45480015d371bce/99072/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image7.png 842w,/wiki/static/4c481e48637c4563a45480015d371bce/62a6a/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image7.png 1122w,/wiki/static/4c481e48637c4563a45480015d371bce/2f6f6/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image7.png 1377w" sizes="(max-width: 561px) 100vw, 561px" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0" loading="lazy" decoding="async"/>
  </a>
    </span>
    <figcaption font-size="1" color="auto.gray.5" class="Text-sc-1s3uzov-0 hHOTlN gatsby-resp-image-figcaption">image</figcaption>
  </figure><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">We can &quot;vectorize&quot; this procedure, turning it into a matrix multiplication and vector addition. This is helpful for computational efficiency. (It&#x27;s also a useful way to think.)</p><figure class="gatsby-resp-image-figure">
    <span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:561px">
      <a class="Link-sc-1brdqhf-0 cKRjba gatsby-resp-image-link" style="display:block" target="_blank" rel="noopener noreferrer" href="/wiki/static/1253c7d563e9b2a41832f408c0a9fd9b/e40ed/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image8.png">
    <span class="gatsby-resp-image-background-image" style="padding-bottom:24.28571428571429%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAACHVAAAh1QEEnLSdAAABoElEQVQY0wGVAWr+AM2WqpL/4/8AoMSHAOThtQCWd2sAAAAAA3ujfmiX0ZzDoNylSo3CkZud2KKTib2NUZbOmsaOxpFSsHy4Ncqi1KVLRzkKioqhILGu159dXGs4AM+VqnP///8A5927Er25lxb7/84UAAAADXiofFmd16G0mtWfQpfQnJGa1J+FlMyYTJvUn7h6un0/q261K8mh1JgAAAACb26LF7Kv24s2NUM2AMuTp47///8A0Mynq8rGoszW06zNsKWJU2+ieGSY0JzNm9agR5DHlbCY0ZyPj8STXpbNmtJ5unxBtX6/Pcef0aUAAAAGXl1zJLKv2p8tLDdYAMyTp4f/8P8A0b6qCMnGogn3/9EIAAAACnWieWKa1J+2ntmiQJLJlpyc16GBjsSTU5jQnL15vX06sXi8Ncmh1JQAAAABZmV/GrCu2JA3NkU5AMmUp3r///8ARoRGAP//+QBCGyQAAAAAB4Ovh2yY0p20l8+bQJXOmpyUzJmBlMyZU5fPm7uAtoJEu4vEQcOdzZoACwAGkZCoIKqnz5xbWmk3glDVe2Qj7v0AAAAASUVORK5CYII=&#x27;);background-size:cover;display:block"></span>
  <img class="image__Image-sc-1r30dtv-0 elBfYx gatsby-resp-image-image" alt="image" title="image" src="/wiki/static/1253c7d563e9b2a41832f408c0a9fd9b/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image8.png" srcSet="/wiki/static/1253c7d563e9b2a41832f408c0a9fd9b/0d3e1/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image8.png 140w,/wiki/static/1253c7d563e9b2a41832f408c0a9fd9b/6b1e2/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image8.png 281w,/wiki/static/1253c7d563e9b2a41832f408c0a9fd9b/410f3/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image8.png 561w,/wiki/static/1253c7d563e9b2a41832f408c0a9fd9b/99072/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image8.png 842w,/wiki/static/1253c7d563e9b2a41832f408c0a9fd9b/62a6a/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image8.png 1122w,/wiki/static/1253c7d563e9b2a41832f408c0a9fd9b/e40ed/Computer-Vision-CV_MNIST-For-ML-Beginners-TensorFlow-image8.png 1378w" sizes="(max-width: 561px) 100vw, 561px" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0" loading="lazy" decoding="async"/>
  </a>
    </span>
    <figcaption font-size="1" color="auto.gray.5" class="Text-sc-1s3uzov-0 hHOTlN gatsby-resp-image-figcaption">image</figcaption>
  </figure><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">More compactly, we can just write:</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><code class="inline-code__InlineCode-sc-5lgfx8-0 jyryuZ">y=softmax(Wx+b)</code></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Now let&#x27;s turn that into something that TensorFlow can use.</p><h2 id="implementing-the-regression" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#implementing-the-regression" color="auto.gray.8" aria-label="Implementing the Regression permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Implementing the Regression</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">To do efficient numerical computing in Python, we typically use libraries like <a target="_blank" rel="noopener noreferrer" href="http://www.numpy.org/" class="Link-sc-1brdqhf-0 cKRjba">NumPy</a> that do expensive operations such as matrix multiplication outside Python, using highly efficient code implemented in another language. Unfortunately, there can still be a lot of overhead from switching back to Python every operation. This overhead is especially bad if you want to run computations on GPUs or in a distributed manner, where there can be a high cost to transferring data.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">TensorFlow also does its heavy lifting outside Python, but it takes things a step further to avoid this overhead. Instead of running a single expensive operation independently from Python, TensorFlow lets us describe a graph of interacting operations that run entirely outside Python. (Approaches like this can be seen in a few machine learning libraries.)</p><div class="Box-nv15kw-0 jNnzYF"><div class="Box-nv15kw-0 eLWiuh"><button aria-label="Copy to clipboard" class="ButtonBase-sc-181ps9o-0 Button-xjtz72-0 cjGjQg cCazJI"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 cmMzjy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></button></div><pre class="Box-nv15kw-0 iCCWeC prism-code language-python" style="color:#393A34;background-color:#f6f8fa;overflow:auto"><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain"># To use TensorFlow, first we need to import it.</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">import tensorflow as tf</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain"># We describe these interacting operations by manipulating symbolic variables. Let&#x27;s create one:</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">x = tf.placeholder(tf.float32, [None, 784])</span></div></pre></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">x isn&#x27;t a specific value. It&#x27;s a placeholder, a value that we&#x27;ll input when we ask TensorFlow to run a computation. We want to be able to input any number of MNIST images, each flattened into a 784-dimensional vector. We represent this as a 2-D tensor of floating-point numbers, with a shape <!-- -->[None, 784]<!-- -->. (Here None means that a dimension can be of any length.)</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">We also need the weights and biases for our model. We could imagine treating these like additional inputs, but TensorFlow has an even better way to handle it: Variable. A Variable is a modifiable tensor that lives in TensorFlow&#x27;s graph of interacting operations. It can be used and even modified by the computation. For machine learning applications, one generally has the model parameters be Variables.</p><div class="Box-nv15kw-0 jNnzYF"><div class="Box-nv15kw-0 eLWiuh"><button aria-label="Copy to clipboard" class="ButtonBase-sc-181ps9o-0 Button-xjtz72-0 cjGjQg cCazJI"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 cmMzjy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></button></div><pre class="Box-nv15kw-0 iCCWeC prism-code language-python" style="color:#393A34;background-color:#f6f8fa;overflow:auto"><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">W = tf.Variable(tf.zeros([784, 10]))</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">b = tf.Variable(tf.zeros([10]))</span></div></pre></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">We create these Variables by giving tf.Variable the initial value of the Variable: in this case, we initialize both W and b as tensors full of zeros. Since we are going to learn W and b, it doesn&#x27;t matter very much what they initially are.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Notice that W has a shape of <!-- -->[784, 10]<!-- --> because we want to multiply the 784-dimensional image vectors by it to produce 10-dimensional vectors of evidence for the difference classes. b has a shape of <!-- -->[10]<!-- --> so we can add it to the output.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">We can now implement our model. It only takes one line to define it!</p><div class="Box-nv15kw-0 jNnzYF"><div class="Box-nv15kw-0 eLWiuh"><button aria-label="Copy to clipboard" class="ButtonBase-sc-181ps9o-0 Button-xjtz72-0 cjGjQg cCazJI"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 cmMzjy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></button></div><pre class="Box-nv15kw-0 iCCWeC prism-code language-python" style="color:#393A34;background-color:#f6f8fa;overflow:auto"><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">y = tf.nn.softmax(tf.matmul(x, W) + b)</span></div></pre></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">First, we multiply x by W with the expression tf.matmul(x, W). This is flipped from when we multiplied them in our equation, where we had Wx, as a small trick to deal with x being a 2D tensor with multiple inputs. We then add b, and finally apply tf.nn.softmax.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">That&#x27;s it. It only took us one line to define our model, after a couple short lines of setup. That isn&#x27;t because TensorFlow is designed to make a softmax regression particularly easy: it&#x27;s just a very flexible way to describe many kinds of numerical computations, from machine learning models to physics simulations. And once defined, our model can be run on different devices: your computer&#x27;s CPU, GPUs, and even phones!</p><h2 id="training" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#training" color="auto.gray.8" aria-label="Training permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">In order to train our model, we need to define what it means for the model to be good. Well, actually, in machine learning we typically define what it means for a model to be bad. We call this the cost, or the loss, and it represents how far off our model is from our desired outcome. We try to minimize that error, and the smaller the error margin, the better our model is.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">One very common, very nice function to determine the loss of a model is called &quot;cross-entropy.&quot; Cross-entropy arises from thinking about information compressing codes in information theory but it winds up being an important idea in lots of areas, from gambling to machine learning. It&#x27;s defined as:</p><div class="Box-nv15kw-0 jNnzYF"><div class="Box-nv15kw-0 eLWiuh"><button aria-label="Copy to clipboard" class="ButtonBase-sc-181ps9o-0 Button-xjtz72-0 cjGjQg cCazJI"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 cmMzjy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></button></div><pre class="Box-nv15kw-0 iCCWeC prism-code language-python" style="color:#393A34;background-color:#f6f8fa;overflow:auto"><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">Hy′(y)=−∑iyi′log⁡(yi)</span></div></pre></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Where y is our predicted probability distribution, and y′ is the true distribution (the one-hot vector with the digit labels). In some rough sense, the cross-entropy is measuring how inefficient our predictions are for describing the truth. Going into more detail about cross-entropy is beyond the scope of this tutorial, but it&#x27;s well worth <a target="_blank" rel="noopener noreferrer" href="https://colah.github.io/posts/2015-09-Visual-Information" class="Link-sc-1brdqhf-0 cKRjba">understanding</a>.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">To implement cross-entropy we need to first add a new placeholder to input the correct answers:</p><div class="Box-nv15kw-0 jNnzYF"><div class="Box-nv15kw-0 eLWiuh"><button aria-label="Copy to clipboard" class="ButtonBase-sc-181ps9o-0 Button-xjtz72-0 cjGjQg cCazJI"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 cmMzjy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></button></div><pre class="Box-nv15kw-0 iCCWeC prism-code language-python" style="color:#393A34;background-color:#f6f8fa;overflow:auto"><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">y_ = tf.placeholder(tf.float32, [None, 10])</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">Then we can implement the cross-entropy function, −∑y′log⁡(y):</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))</span></div></pre></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">First, tf.log computes the logarithm of each element of y. Next, we multiply each element of y_ with the corresponding element of tf.log(y). Then tf.reduce_sum adds the elements in the second dimension of y, due to the reduction_indices=<!-- -->[1]<!-- --> parameter. Finally, tf.reduce_mean computes the mean over all the examples in the batch.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Note that in the source code, we don&#x27;t use this formulation, because it is numerically unstable. Instead, we apply tf.nn.softmax_cross_entropy_with_logits on the unnormalized logits (e.g., we call softmax_cross_entropy_with_logits on tf.matmul(x, W) + b), because this more numerically stable function internally computes the softmax activation. In your code, consider using tf.nn.softmax_cross_entropy_with_logits instead.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Now that we know what we want our model to do, it&#x27;s very easy to have TensorFlow train it to do so. Because TensorFlow knows the entire graph of your computations, it can automatically use the <a target="_blank" rel="noopener noreferrer" href="https://colah.github.io/posts/2015-08-Backprop" class="Link-sc-1brdqhf-0 cKRjba">backpropagation algorithm</a> to efficiently determine how your variables affect the loss you ask it to minimize. Then it can apply your choice of optimization algorithm to modify the variables and reduce the loss.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">In this case, we ask TensorFlow to minimize cross_entropy using the <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Gradient_descent" class="Link-sc-1brdqhf-0 cKRjba">gradient descent algorithm</a> with a learning rate of 0.5. Gradient descent is a simple procedure, where TensorFlow simply shifts each variable a little bit in the direction that reduces the cost. But TensorFlow also provides <a target="_blank" rel="noopener noreferrer" href="https://www.tensorflow.org/api_guides/python/train#Optimizers" class="Link-sc-1brdqhf-0 cKRjba">many other optimization algorithms</a>: using one is as simple as tweaking one line.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">What TensorFlow actually does here, behind the scenes, is to add new operations to your graph which implement backpropagation and gradient descent. Then it gives you back a single operation which, when run, does a step of gradient descent training, slightly tweaking your variables to reduce the loss.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">We can now launch the model in an InteractiveSession:</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><code class="inline-code__InlineCode-sc-5lgfx8-0 jyryuZ">sess = tf.InteractiveSession()</code></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">We first have to create an operation to initialize the variables we created:</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><code class="inline-code__InlineCode-sc-5lgfx8-0 jyryuZ">tf.global_variables_initializer().run()</code></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Let&#x27;s train -- we&#x27;ll run the training step 1000 times!</p><div class="Box-nv15kw-0 jNnzYF"><div class="Box-nv15kw-0 eLWiuh"><button aria-label="Copy to clipboard" class="ButtonBase-sc-181ps9o-0 Button-xjtz72-0 cjGjQg cCazJI"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 cmMzjy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></button></div><pre class="Box-nv15kw-0 iCCWeC prism-code language-python" style="color:#393A34;background-color:#f6f8fa;overflow:auto"><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain">for _ in range(1000):</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain"> batch_xs, batch_ys = mnist.train.next_batch(100)</span></div><div class="token-line" style="color:#393A34"><span font-family="mono" font-size="1" class="Text-sc-1s3uzov-0 fWAlyn token plain"> sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})</span></div></pre></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Each step of the loop, we get a &quot;batch&quot; of one hundred random data points from our training set. We run train_step feeding in the batches data to replace the placeholders.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Using small batches of random data is called stochastic training -- in this case, stochastic gradient descent. Ideally, we&#x27;d like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that&#x27;s expensive. So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit.</p><h2 id="evaluating-our-model" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#evaluating-our-model" color="auto.gray.8" aria-label="Evaluating Our Model permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Evaluating Our Model</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">How well does our model do?</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Well, first let&#x27;s figure out where we predicted the correct label. tf.argmax is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. For example, tf.argmax(y,1) is the label our model thinks is most likely for each input, while tf.argmax(y_,1) is the correct label. We can use tf.equal to check if our prediction matches the truth.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><code class="inline-code__InlineCode-sc-5lgfx8-0 jyryuZ">correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))</code></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">That gives us a list of booleans. To determine what fraction are correct, we cast to floating point numbers and then take the mean. For example, <!-- -->[True, False, True, True]<!-- --> would become <!-- -->[1,0,1,1]<!-- --> which would become 0.75.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><code class="inline-code__InlineCode-sc-5lgfx8-0 jyryuZ">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</code></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Finally, we ask for our accuracy on our test data.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><code class="inline-code__InlineCode-sc-5lgfx8-0 jyryuZ">print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))</code></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">This should be about 92%.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Is that good? Well, not really. In fact, it&#x27;s pretty bad. This is because we&#x27;re using a very simple model. With some small changes, we can get to 97%. The best models can get to over 99.7% accuracy! (For more information, have a look at this <a target="_blank" rel="noopener noreferrer" href="https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results" class="Link-sc-1brdqhf-0 cKRjba">list of results</a>.)</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">What matters is that we learned from this model. Still, if you&#x27;re feeling a bit down about these results, check out <a target="_blank" rel="noopener noreferrer" href="https://www.tensorflow.org/get_started/mnist/pros" class="Link-sc-1brdqhf-0 cKRjba">the next tutorial</a> where we do a lot better, and learn how to build more sophisticated models using TensorFlow!</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.tensorflow.org/get_started/mnist/beginners" class="Link-sc-1brdqhf-0 cKRjba">https://www.tensorflow.org/get_started/mnist/beginners</a></p><div class="Box-nv15kw-0 ksEcN"><div display="flex" class="Box-nv15kw-0 jsSpbO"><a href="https://github.com/deepaksood619/wiki/tree/main/AI/Deep-Learning/Computer-Vision-CV/MNIST-For-ML-Beginners-TensorFlow.md" class="Link-sc-1brdqhf-0 iLYDsn"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 fafffn" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z"></path></svg>Edit this page</a><div><span font-size="1" color="auto.gray.7" class="Text-sc-1s3uzov-0 gHwtLv">Last updated on<!-- --> <b>1/7/2023</b></span></div></div></div></div></div></main></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id="></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/AI/Deep-Learning/Computer-Vision-CV/MNIST-For-ML-Beginners-TensorFlow/";window.___webpackCompilationHash="a90f4a594c9e502012ae";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2526e2a471eef3b9c3b2.js"],"app":["/app-3f4d8f561104a074e339.js"],"component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js":["/component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js-bd1c4b7f67a97d4f99af.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js-6ed623c5d829c1a69525.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"]};/*]]>*/</script><script src="/wiki/polyfill-2526e2a471eef3b9c3b2.js" nomodule=""></script><script src="/wiki/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js" async=""></script><script src="/wiki/commons-c89ede6cb9a530ac5a37.js" async=""></script><script src="/wiki/app-3f4d8f561104a074e339.js" async=""></script><script src="/wiki/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js" async=""></script><script src="/wiki/0e226fb0-1cb0709e5ed968a9c435.js" async=""></script><script src="/wiki/f0e45107-3309acb69b4ccd30ce0c.js" async=""></script><script src="/wiki/framework-6c63f85700e5678d2c2a.js" async=""></script><script src="/wiki/webpack-runtime-07e8faa2a261685889a0.js" async=""></script></body></html>