{"data":{"allMdx":{"nodes":[{"fields":{"slug":"/","title":"wiki"},"frontmatter":{"draft":false},"rawBody":"# wiki\nMy Second Brain\n"},{"fields":{"slug":"/Computer-Science/Courses/365-DS---Advanced-Stastistical-Methods-in-Python/","title":"365 DS - Advanced Stastistical Methods in Python"},"frontmatter":{"draft":false},"rawBody":"# 365 DS - Advanced Stastistical Methods in Python\n\nCreated: 2020-04-14 03:20:46 +0500\n\nModified: 2020-04-14 03:26:25 +0500\n\n---\n\n**Linear regression**\n\nWelcome to Advanced Statistics! (0:28)\n\nWelcome to the Course\n\nIntroduction to Regression Analysis (1:27)\n\nThe Linear Regression Model (5:50)\n\nCorrelation vs Regression (1:43)\n\nGeometrical Representation of the Linear Regression Model (1:25)\n\nPython Packages Installation (4:39)\n\nFirst Regression in Python (7:11)\n\nFirst Regression in Python Exercise\n\nUsing Seaborn for Graphs (1:21)\n\nHow to Interpret the Regression Table (5:47)\n\nDecomposition of Variability (3:37)\n\nWhat is the OLS? (3:13)\n\nR-Squared (5:30)\n**Multiple Linear Regression**\n\nMultiple Linear Regression (2:55)\n\nAdjusted R-Squared (6:00)\n\nMultiple Linear Regression Exercise\n\nTest for Significance of the Model (F-Test) (2:01)\n\nOLS Assumptions (2:21)\n\nA1: Linearity (1:50)\n\nA2: No Endogeneity (4:09)\n\nA3: Normality and Homoscedasticity (5:47)\n\nA4: No Autocorrelation (3:31)\n\nA5: No Multicollinearity (3:26)\n\nDealing with Categorical Data - Dummy Variables (6:43)\n\nDealing with Categorical Data - Dummy Variables Exercise\n\nMaking Predictions with the Linear Regression (3:29)\n**Linear Regression with sklearn**\n\nWhat is sklearn? (2:14)\n\nGame Plan for sklearn (1:55)\n\nSimple Linear Regression with sklearn (5:38)\n\nSimple Linear Regression with sklearn - Summary Table (4:48)\n\nA Note on Normalization\n\nMultiple Linear Regression with sklearn (3:10)\n\nAdjusted R-Squared (4:45)\n\nAdjusted R-Squared Exercise\n\nFeature Selection through p-values (F-regression) (4:41)\n\nA Note on Calculation of P-Values with sklearn\n\nCreating a Summary Table with the p-values (2:10)\n\nMultiple Linear Regression - Exercise\n\nFeature Scaling (5:38)\n\nFeature Selection through Standardization (5:22)\n\nMaking Predictions with Standardized Coefficients (3:52)\n\nFeature Scaling - Exercise\n\nUnderfitting and Overfitting (2:42)\n\nTraining and Testing (6:54)\n**Linear Regression - Practical Example**\n\nPractical Example (Part 1) (11:59)\n\nPractical Example (Part 2) (6:12)\n\nA Note on Multicollinearity\n\nPractical Example (Part 3) (3:15)\n\nDummies and VIF - Exercise\n\nPractical Example (Part 4) (8:09)\n\nDummy Variables Interpretation - Exercise\n\nPractical Example (Part 5) (7:34)\n\nLinear Regression - Exercise\n**Logistic Regression**\n\nIntroduction to Logistic Regression (1:19)\n\nA Simple Example in Python (4:42)\n\nLogistic vs Logit Function (4:00)\n\nBuilding a Logistic Regression (2:48)\n\nBulding a Logistic Regression Exercise\n\nAn Invaluable Coding Tip (2:26)\n\nUnderstanding Logistic Regression Tables (4:06)\n\nUnderstanding Logistic Regression Tables - Exercise\n\nWhat do the Odds Actually Mean (4:30)\n\nBinary Predictors in a Logistic Regression (4:32)\n\nBinary Predictors in a Logistic Regression - Exercise\n\nCalculating the Accuracy of the Model (3:21)\n\nCalculating the Accuracy of the Model - Exercise\n\nUnderfitting and Overfitting (3:43)\n\nTesting the Model (5:05)\n\nTesting the Model - Exercise\n**Cluster Analysis (Basics and Prerequisites)**\n\nIntroduction to Cluster Analysis (3:41)\n\nSome Examples of Clusters (4:31)\n\nDifference between Classification and Clustering (2:32)\n\nMath Prerequisites (3:19)\n**K-Means Clustering**\n\nK-Means Clustering (4:41)\n\nA Simple Example of Clustering (7:48)\n\nA Simple Example of Clustering - Exercise\n\nClustering Categorical Data (2:50)\n\nClustering Categorical Data - Exercise\n\nHow to Choose the Number of Clusters (6:11)\n\nHow to Choose the Number of Clusters - Exercise\n\nPros and Cons of K-Means Clustering (3:23)\n\nTo Standardize or to not Standardize (4:32)\n\nRelationship between Clustering and Regression (1:31)\n\nMarket Segmentation with Cluster Analysis (Part 1) (6:03)\n\nMarket Segmentation with Cluster Analysis (Part 2) (6:58)\n\nHow is Clustering Useful? (4:47)\n\nExercise - Species Segmentation with Cluster Analysis (Part 1)\n\nExercise - Species Segmentation with Cluster Analysis (Part 2)\n**Other Types of Clustering**\n\nTypes of Clustering (3:39)\n\nDendrogram (5:21)\n\nHeatmaps (4:34)\n<https://365datascience.teachable.com/courses/enrolled/362812>\n"},{"fields":{"slug":"/Computer-Science/Courses/365-DS---Mathematics/","title":"365 DS - Mathematics"},"frontmatter":{"draft":false},"rawBody":"# 365 DS - Mathematics\n\nCreated: 2020-04-14 03:32:19 +0500\n\nModified: 2020-04-14 03:32:34 +0500\n\n---\n\n**Introduction to Linear Algebra**\n\nWhat is a Matrix\n\nScalars and Vectors\n\nLinear Algebra and Geometry\n\nScalars, Vectors, and Matrices as Python Arrays\n\nWhat is a Tensor?\n\nAddition and Subtraction\n\nErrors when Adding Matrices\n\nTranspose of a Matrix\n\nDot Product\n\nDot Product of Matrices\n\nWhy is Linear Algebra Useful\n<https://365datascience.teachable.com/courses/enrolled/372258>\n"},{"fields":{"slug":"/Computer-Science/Courses/365-Data-Science-Program/","title":"365 Data Science Program"},"frontmatter":{"draft":false},"rawBody":"# 365 Data Science Program\n\nCreated: 2020-04-05 01:22:40 +0500\n\nModified: 2022-12-11 19:24:37 +0500\n\n---\n\n1.  Intro to Data and Data Science\n\n2.  Introduction to Microsoft Excel\n\n3.  Advanced Microsoft Excel\n\n4.  Probability\n\n5.  Statistics\n\n6.  Mathematics\n\n7.  PowerBI\n\n<https://365datascience.teachable.com/courses/enrolled/716963>\n\n8.  SQL\n\n9.  SQL + Tableau\n\n<https://365datascience.teachable.com/courses/enrolled/361448>\n\n10. Introduction to Python\n\n11. The Python Programmer\n\n12. Git and Github\n\n13. Introduction to R Programming\n\n14. Advanced Statistical Methods\n\n15. Deep Learning with TensorFlow\n\n<https://365datascience.teachable.com/courses/enrolled/284663>\n\n16. Deep Learning with TensorFlow 2.0\n\n<https://365datascience.teachable.com/courses/enrolled/614390>\n\n17. **Credit Risk Modeling in Python**\n\n18. **Time Series Analysis in Python**\n\n19. **Customer Analytics in Python**\r\n"},{"fields":{"slug":"/Computer-Science/Courses/Coursera---Algorithms-Part---1/","title":"Coursera - Algorithms Part - 1"},"frontmatter":{"draft":false},"rawBody":"# Coursera - Algorithms Part - 1\n\nCreated: 2018-06-02 14:02:02 +0500\n\nModified: 2022-12-11 19:27:35 +0500\n\n---\n\n# Syllabus\n\n**Week - 1**\n\n**Union - Find**\n\n1.  Dynamic Connectivity\n\n2.  Quick Find\n\n3.  Quick Union\n\n4.  Quick-Union Improvement\n\nWeighted Quick Union\n\nWeighted Quick Union with Path Compression\n\n5.  Quick-Find Applications\n**Assignment**\n\nPercolation\n**Analysis of Algorithms**\n\n1.  Scientific Method of Analysis\n\n    i.  Observe\n\n    ii. Hypothesis\n\n    iii. Predict\n\n    iv. Verify\n\n    v.  Validate\n\n2.  Empirical Analysis\n\nRun the program for various input sizes and measure running time\n\n3.  Mathematical Models\n\n4.  Order-of-Growth Classifications\n\n5.  Theory of Algorithms\n\n    i.  Best Case\n\n    ii. Average Case\n\n    iii. Worst Case\n\n6.  Memory**Week - 2**\n\n**Stacks and Queues**\n\n1.  Stacks\n\n2.  Resizing Arrays\n\n3.  Queues\n\n4.  Generics\n\n5.  Iterators\n\n6.  Stack and Queue Applications\n**Assignment**\n\nDequeues and Randomized Queues\n**Elementary Sort**\n\n1.  Introduction\n\n2.  Selection Sort\n\n3.  Insertion Sort\n\n4.  Shell Sort\n\n5.  Shuffling\n\n    i.  Shuffle Sort\n\n    ii. Knuth Shuffle\n\n6.  Convex Hull**Week - 3**\n\n**Mergesort**\n\n1.  Mergesort\n\n2.  Bottom-up Mergesort\n\n3.  Sorting Complexity\n\n4.  Comparators\n\n5.  Stability\n**Assignment**\n\nCollinear Points\n**Quicksort**\n\n1.  Quicksort\n\n2.  Selection (Quick Select)\n\n3.  Duplicate keys (3-way partition quick sort)\n\n4.  System sorts\n**Week - 4**\n\n**Priority Queues**\n\n1.  APIs and Elementary Implementation\n\n2.  Binary Heaps\n\n3.  Heapsort\n\n4.  Event-Driven Simulations\n**Assignment**\n\n8-Puzzle\n**Elementary Symbol Tables**\n\n1.  Symbol Table API\n\n2.  Elementary Implementations\n\nSorted array (Binary Search)\n\nUnordered List (Sequential Search)\n\n3.  Ordered Operations\n\n4.  Binary Search Trees\n\n5.  Ordered Operations in BSTs\n\n6.  Deletion in BSTs\n**Week - 5**\n\n**Balanced Search Trees**\n\n1.  2-3 Search Trees\n\n2.  Red-Black BSTs\n\n3.  B-Trees\n**Geometric Application of BSTs**\n\n1.  1d Range Search\n\n2.  Line Segment Intersection\n\n3.  Kd-Trees\n\n4.  Interval Search Trees\n\n5.  Rectangle Intersection\n**Assignment**\n\nKd-Trees\n**Week - 6**\n\n**Hash Tables**\n\n1.  Hash Tables\n\nUniform Hashing Assumption\n\n2.  Separate Chaining\n\n3.  Linear Probing\n\n4.  Hash Table Context\n**Symbol Table Applications**\n\n1.  Sets\n\n2.  Dictionary Clients\n\n3.  Indexing Clients\n\n4.  Sparse Vectors\n# Interview Questions\n\n## 1.1. Union Find\n\n1.  **Social network connectivity.**Given a social network containingnmembers and a log file containingmtimestamps at which times pairs of members formed friendships, design an algorithm to determine the earliest time at which all members are connected (i.e., every member is a friend of a friend of a friend ... of a friend). Assume that the log file is sorted by timestamp and that friendship is an equivalence relation. The running time of your algorithm should bemlognor better and use extra space proportional ton.\n\n2.  **Union-find with specific canonical element.**Add a methodğšğš’ğš—ğš()to the union-find data type so thatğšğš’ğš—ğš(ğš’)returns the largest element in the connected component containingi. The operations,ğšğš—ğš’ğš˜ğš—(),ğšŒğš˜ğš—ğš—ğšğšŒğšğšğš(), andğšğš’ğš—ğš()should all take logarithmic time or better. For example, if one of the connected components is{1,2,6,9}, then theğšğš’ğš—ğš()method should return9for each of the four elements in the connected components.\n\n3.  **Successor with delete.** Given a set ofnintegersS={0,1,...,nâˆ’1}and a sequence of requests of the following form:\n\nRemovexfromS\n\nFind thesuccessorofx: the smallestyinSsuch thatyâ‰¥x.\n\ndesign a data type so that all operations (except construction) take logarithmic time or better in the worst case.\n## 1.2. Analysis of Algorithms\n\n1.  **3-SUM in quadratic time.**Design an algorithm for the 3-SUM problem that takes time proportional ton2in the worst case. You may assume that you can sort thenintegers in time proportional ton2or better.\n\n2.  **Search in a bitonic array.**An array isbitonicif it is comprised of an increasing sequence of integers followed immediately by a decreasing sequence of integers. Write a program that, given a bitonic array ofndistinct integer values, determines whether a given integer is in the array.\n    -   Standard version: Useâˆ¼3lgncompares in the worst case.\n        -   Signing bonus: Useâˆ¼2lgncompares in the worst case (and prove that no algorithm can guarantee to perform fewer thanâˆ¼2lgncompares in the worst case).\n\n3.  **Egg drop.**Suppose that you have ann-story building (with floors 1 throughn) and plenty of eggs. An egg breaks if it is dropped from floorTor higher and does not break otherwise. Your goal is to devise a strategy to determine the value ofTgiven the following limitations on the number of eggs and tosses:\n    -   Version 0: 1 egg,â‰¤Ttosses.\n    -   Version 1:âˆ¼1lgneggs andâˆ¼1lgntosses.\n    -   Version 2:âˆ¼lgTeggs andâˆ¼2lgTtosses.\n    -   Version 3:2eggs andâˆ¼2âˆšntosses.\n    -   Version 4:2eggs andâ‰¤câˆšT tosses for some fixed constantc.\n## 2.1 Stack and Queues\n\n1.  **Queue with two stacks.**Implement a queue with two stacks so that each queue operations takes a constant amortized number of stack operations.\n2.  **Stack with max.**Create a data structure that efficiently supports the stack operations (push and pop) and also a return-the-maximum operation. Assume the elements are reals numbers so that you can compare them.\n3.  **Java generics.**Explain why Java prohibits generic array creation.\n## 2.2 Elementary Sorts\n\n1.  **Intersection of two sets.**Given two arraysğšŠ[]andğš‹[], each containingndistinct 2D points in the plane, design a subquadratic algorithm to count the number of points that are contained both in arrayğšŠ[]and arrayğš‹[].\n\n2.  **Permutation.**Given two integer arrays of sizen, design a subquadratic algorithm to determine whether one is a permutation of the other. That is, do they contain exactly the same entries but, possibly, in a different order.3.  **Dutch national flag (3-way partitioning).**Given an array ofnbuckets, each containing a red, white, or blue pebble, sort them by color. The allowed operations are:\n    -   **swap(i,j): swap the pebble in bucketiwith the pebble in bucketj.**\n    -   **color(i): determine the color of the pebble in bucketi.**\n\nThe performance requirements are as follows:\n-   At mostncalls tocolor().\n-   At mostncalls toswap().\n-   Constant extra space.\n## 3.1 Merge Sort\n\n1.  **Merging with smaller auxiliary array.**Suppose that the subarrayğšŠ[ğŸ¶]toğšŠ[ğš—âˆ’ğŸ·]is sorted and the subarrayğšŠ[ğš—]toğšŠ[ğŸ¸âˆ—ğš—âˆ’ğŸ·]is sorted. How can you merge the two subarrays so thatğšŠ[ğŸ¶]toğšŠ[ğŸ¸âˆ—ğš—âˆ’ğŸ·]is sorted using an auxiliary array of lengthn(instead of2n)?\n\n2.  **Counting inversions**\n    An *inversion* in an arraya[] is a pair of entries a[i] and a[j] such that i < j but a[i] > a[j]. Given an array, design a linearithmic algorithm to count the number of inversions.\n\n    <https://www.geeksforgeeks.org/counting-inversions>\n\n3.  **Shuffling a linked list.**\n    Given a singly-linked list containingnitems, rearrange the items uniformly at random. Your algorithm should consume a logarithmic (or constant) amount of extra memory and run in time proportional tonlognin the worst case.\n## 3.2. Quick Sort\n\n1.  **Nuts and bolts.**A disorganized carpenter has a mixed pile ofnnuts andnbolts. The goal is to find the corresponding pairs of nuts and bolts. Each nut fits exactly one bolt and each bolt fits exactly one nut. By fitting a nut and a bolt together, the carpenter can see which one is bigger (but the carpenter cannot compare two nuts or two bolts directly). Design an algorithm for the problem that usesnlogncompares (probabilistically).\n\nSolution -\n\nSuppose we choose a nut and partition all bolts in {1...n}, by comparing with this nut, into three intervals : {1...i-1}, {i}, {i+1, n} such that each bolt in {1, i-1} is smaller, bolt i matches and each bolt in {i+1, n} is larger than the chosen nut. This procedure is similar to the partition procedure used in quicksort and can be implemented in O(n). Now, we can use the matching bolt to partition all nuts in three intervals in a similar manner so that the nut i and bolt i match. We have reduced the problem of finding matchings in the interval {1...n} into two smaller subproblems: finding matchings in the intervals {1...i-1} and {i+1...n}.\n\nIf at each step, we choose the nut (to partition the bolts) randomly, we will get similar performance guarantees as quicksort i.e., randomized**O(n log n)**time.\n2.  **Selection in two sorted arrays.**Given two sorted arraysa[]andb[], of sizesn1andn2, respectively, design an algorithm to find thekthlargest key. The order of growth of the worst case running time of your algorithm should belogn, wheren=n1+n2.\n    -   **Version 1:n1=n2andk=n/2**\n    -   **Version 2:k=n/2**\n    -   **Version 3: no restrictions**\n3.  **Decimal dominants.**Given an array withnkeys, design an algorithm to find all values that occur more thann/10times. The expected running time of your algorithm should be linear.\n## 4.1. Priority Queues\n\n1.  **Dynamic median.**Design a data type that supports insert in logarithmic time, find-the-median in constant time, and remove-the-median in logarithmic time.\n2.  **Randomized priority queue.**Describe how to add the methodsğšœğšŠğš–ğš™ğš•ğš()andğšğšğš•ğšğšŠğš—ğšğš˜ğš–()to our binary heap implementation. The two methods return a key that is chosen uniformly at random among the remaining keys, with the latter method also removing that key. TheğšœğšŠğš–ğš™ğš•ğš()method should take constant time; theğšğšğš•ğšğšŠğš—ğšğš˜ğš–()method should take logarithmic time. Do not worry about resizing the underlying array.\n3.  **Taxicab numbers.**A*taxicab*number is an integer that can be expressed as the sum of two cubes of positive integers in two different ways:a3+b3=c3+d3. For example,1729is the smallest taxicab number:93+103=13+123. Design an algorithm to find all taxicab numbers less thann.\n    -   **Version 1: Use time proportional ton2lognand space proportional ton2.**\n    -   **Version 2: Use time proportional ton2lognand space proportional ton.**\n## 4.2. Elementary Symbol Table\n\n1.  **Java autoboxing and equals()**. Consider twoğšğš˜ğšğš‹ğš•ğšvaluesğšŠandğš‹and their corresponding <tt>Double</tt> valuesğš¡andğš¢.\n-   Find values such that(ğšŠ==ğš‹)isğšğš›ğšğšbutğš¡.ğšğššğšğšŠğš•ğšœ(ğš¢)isğšğšŠğš•ğšœğš.\n-   Find values such that(ğšŠ==ğš‹)isğšğšŠğš•ğšœğšbutğš¡.ğšğššğšğšŠğš•ğšœ(ğš¢)isğšğš›ğšğš.\n2.  **Check if a binary tree is a BST.**Given a binary tree where eachğ™½ğš˜ğšğšcontains a key, determine whether it is a binary search tree. Use extra space proportional to the height of the tree.\n3.  **Inorder traversal with constant extra space**. Design an algorithm to perform an inorder traversal of a binary search tree using only a constant amount of extra space.\n4.  **Web tracking.**Suppose that you are trackingnweb sites andmusers and you want to support the following API:\n-   User visits a website.\n-   How many times has a given user visited a given site?\n\nWhat data structure or data structures would you use?\n## 5.1. Balanced Search Trees\n\n1.  **Red--black BST with no extra memory.**Describe how to save the memory for storing the color information when implementing a red--black BST.\n\n2.  **Document search.**Design an algorithm that takes a sequence ofndocument words and a sequence ofmquery words and find the shortest interval in which themquery words appear in the document in the order given. The length of an interval is the number of words in that interval.\n\n3.  **Generalized queue.** Design a generalized queue data type that supports all of the following operations in logarithmic time (or better) in the worst case.\n    -   Create an empty data structure.\n    -   Append an item to the end of the queue.\n    -   Remove an item from the front of the queue.\n    -   Return theithitem in the queue.\n    -   Remove theithitem from the queue.\n## 6.1. Hash Tables\n\n1.  **4-SUM.**Given an arraya[]ofintegers, the 4-SUM problem is to determine if there exist distinct indicesi, j, k, andlsuch that a[i] + a[j] = a[k] + a[l]. Design an algorithm for the 4-SUM problem that takes time proportional ton^2(under suitable technical assumptions).\n2.  ![Hashing with wrong hashCode() or equals(). Suppose that you implement a data type OlympicAth1ete for use in a java. util . HashMap. â€¢ Describe what happens if you override hashCode ( ) but not equals ( ). â€¢ Describe what happens if you override equals ( ) but not hashCode ( ). â€¢ Describe what happens if you override hashCode ( ) but implement public boolean equals (OlympicAth1ete that) instead of public boolean equals (Object that) . ](media/Coursera---Algorithms-Part---1-image1.png)\n\n# Assignments\n\n**1.1. Union Find**\n\n**Percolation**\n-   N-by-N grid of sites\n-   Each site is open by probability p (or blocked with probability 1-p)\n-   System percolates iff top and bottom are connected by open sites\n-   **Percolation phase transition**\n-   **Monte Carlo simulation**\n-   **Dynamic connectivity solution to estimate percolation threshold**\n    -   Create an object for each site and name them 0 to N^2 - 1\n    -   Sites are in same component if connected by open sites\n    -   Trick - Introduce 2 virtual sites (and connections to top and bottom)\n        -   Percolates iff virtual top site is connected to virtual bottom site\n**3.1 Merge Sort**\n\n**Collinear Points**\n\nGiven a set of points in the plane, design an algorithm to find all line segments that contain 4 or more points.\n**4.1 Priority Queues**\n\n**8-Puzzle.**Your programming assignment is to implement the famous A* search algorithm to solve a combinatorial problem, and to substantially speed it up with an efficient priority queue implementation.\n**5.2 Geometric Applications of BSTs**\n\n**Kd-Trees.**Your programming assignment is to implement kd-trees, which can form the basis for fast search/insert in geometric applications and in multidimensional databases.\n\n1.  Range Search in a query rectangle\n\n2.  Nearest neighbor search (find a closest point to a query point)\n"},{"fields":{"slug":"/Computer-Science/Courses/Coursera---Algorithms-Part---2/","title":"Coursera - Algorithms Part - 2"},"frontmatter":{"draft":false},"rawBody":"# Coursera - Algorithms Part - 2\n\nCreated: 2018-06-02 14:07:39 +0500\n\nModified: 2018-07-01 22:55:14 +0500\n\n---\n\n# Syllabus\n\n**Week - 1**\n\n**Undirected Graph**\n\n1.  Introduction to graph\n\n    a.  Adjacency Matrix\n\n    b.  Adjacency List\n\n2.  Graph API\n\n3.  Depth-First Search\n\n4.  Breadth-First Search\n\n5.  Connected Components\n\n6.  Graph Challenges\n**Directed Graph**\n\n1.  Introduction to Digraphs\n\n2.  Digraph API\n\n3.  Digraph Search\n\n4.  Topological Sort\n\n    a.  Topological order of an acyclic digraph\n\n5.  Strong Components\n\n    a.  Kosaraju-Sharir algorithm for computing strong components of a digraph\n\n6.  Applications\n\n    a.  Garbage Collection\n\n    b.  Web Crawling\n**Assignment**\n\nWordNet\n**Week - 2**\n\n**Minimum Spanning Tree**\n\n1.  Introduction to MSTs\n\n2.  Greedy Algorithms\n\n3.  Edge-Weighted Graph API\n\n4.  Kruskal's Algorithm\n\n5.  Prim's Algorithm\n\n6.  MST Context\n**Shortest Path**\n\n1.  Shortest Path APIs\n\n2.  Shortest Path Properties\n\n3.  Dijkstra's Algorithm\n\n4.  Edge-Weighted DAGs\n\n5.  Negative Weights (Bellman Ford Algorithm)\n**Assignment**\n\nSeam Carving\n**Week - 3**\n\n**Maximum Flow and Minimum Cut**\n\n1.  Introduction to Maxflow\n\n2.  Ford-Fulkerson Algorithm\n\n3.  Maxflow-Mincut Theorem\n\n4.  Running Time Analysis\n\n5.  Java Implementation\n\n6.  Maxflow Applications\n**Assignment**\n\nBaseball Elimination\n**Radix Sorts**\n\n1.  Strings in Java\n\n2.  Key-Indexed Counting\n\n3.  LSD Radix Sort\n\n4.  MSD Radix Sort\n\n5.  3-way Radix Quicksort\n\n6.  Suffix Arrays\n**Week - 4**\n\n**Tries**\n\n1.  R-way Tries\n\n2.  Ternary Search Tries\n\n3.  Character-Based Operations\n**Substring Search**\n\n1.  Introduction to Substring Search\n\n2.  Brute-Force Substring Search\n\n3.  Knuth-Morris-Pratt\n\n4.  Boyer-Moore\n\n5.  Rabin-Karp\n**Assignment**\n\nBoggle\n**Week - 5**\n\n**Regular Expressions**\n\n1.  Regular Expressions\n\n2.  Res and NFAs\n\n3.  NFA Simulation\n\n4.  NFA Construction\n\n5.  RE Applications\n**Data Compression**\n\n1.  Introduction\n\n2.  Run-Length Coding\n\n3.  Huffman Compression\n\n4.  LZW Compression\n**Assignment**\n\nBurrows-Wheeler\n**Week - 6**\n\n**Reductions**\n\n1.  Introduction\n\n2.  Designing Algorithms\n\n3.  Establishing Lower Bounds\n\n4.  Classifying Problems\n**Linear Programming**\n\n1.  Brewer's Problem\n\n2.  Simplex Algorithm\n\n3.  Simplex Implementations\n\n4.  Linear Programming Reductions\n**Intractability**\n\n1.  Introduction\n\n2.  Search Problems\n\n3.  P vs NP\n\n4.  Classifying Problems\n\n5.  NP-Completeness\n\n6.  Coping with Intractability\n# Interview Questions\n\n# 1.1 Undirected Graphs\n\n1.  **Nonrecursive depth-first search.**Implement depth-first search in an undirected graph without using recursion.2.  **Diameter and center of a tree.**Given a connected graph with no cycles\n    -   ***Diameter*: design a linear-time algorithm to find the longest simple path in the graph.**\n    -   ***Center*: design a linear-time algorithm to find a vertex such that its maximum distance from any other vertex is minimized.**\n\n3.  **Euler cycle.**An Euler cycle in a graph is a cycle (not necessarily simple) that uses every edge in the graph exactly one.\n    -   **Show that a connected graph has an Euler cycle if and only if every vertex has even degree.**\n    -   **Design a linear-time algorithm to determine whether a graph has an Euler cycle, and if so, find one.**\n\n# \n\n# Assignments\n\n**WordNet:** is a semantic lexicon for the English language that is used extensively by computational linguists and cognitive scientists.\n\nWordNet groups words into sets of synonyms called*synsets*and describes semantic relationships between them.\n\nOne such relationship is the*is-a*relationship, which connects a*hyponym*(more specific synset) to a*hypernym*(more general synset).\n\nFor example,\n-   *animal*is a hypernym of both*bird*and*fish*;\n-   *bird*is a hypernym of*eagle*,*pigeon*, and*seagull*.\n-   Spoon is a hyponym of cutlery\nReferences -\n\n<http://coursera.cs.princeton.edu/algs4/assignments/wordnet.html>\n\n<http://coursera.cs.princeton.edu/algs4/checklists/wordnet.html>\n\n<https://github.com/CtheSky/Coursera-Algorithms/tree/master/Assignment6_WordNet>\n"},{"fields":{"slug":"/Computer-Science/Courses/Coursera---How-Google-does-ML/","title":"Coursera - How Google does ML"},"frontmatter":{"draft":false},"rawBody":"# Coursera - How Google does ML\n\nCreated: 2018-06-15 00:12:17 +0500\n\nModified: 2021-05-26 20:21:12 +0500\n\n---\n\n**Mathematical Models used in ML -**\n\n1.  Neural Network\n\n2.  Linear methods\n\n3.  Decision trees\n\n4.  Radial basis functions\n\n5.  Ensembles of trees\n\n6.  Radial basis functions followed by linear methods\n\n7.  Support vector machines\n\n8.  Bagged decision trees\n**Learning Objectives**\n\n1.  Build a data strategy around ML.\n\n2.  Discover where bias in machine learning models originates.\n\n3.  Explore Compute Engine and the basics of Cloud Storage.\n\n4.  Execute ad-hoc queries at scale.\n\n5.  Invoke pre-trained ML models from Datalab.**Phases of modelling**\n\nTraining phase\n\nPrediction / Inference Phase\n**Applications**\n\n![â€¢ a Travel and Hospitality Aircraft scheduling Dynamic pricing Social media - consumer feedback and interaction analysis Customer complaint resolution Traffic patterns and congestion management â€¢ alt Financial Services Risk analytics and regulation Customer Segmentation Cross-selling and up- selling Sales and marketing campaign management Credit worthiness evaluation â€¢ Energy, Feedstock and Utilities Power usage analytics Seismic data processing Carbon emissions and trading Customer-specific pricing Smart grid management Energy demand and supply optimization ](media/Coursera---How-Google-does-ML-image1.png)\n![â€¢ â€¢ Manufacturing Predictive maintenance or condition monitoring Warranty reserve estimation Propensity to buy Demand forecasting Process optimization Telematics â€¢ Retail Predictive inventory planning Recommendation engines Upsell and cross-channel marketing Market segmentation and targeting Customer ROI and lifetime value â€¢ â€¢ Healthcare and Life Sciences Alerts and diagnostics from real-time patient data Disease identification and risk satisfaction Patient triage optimization Proactive health management Healthcare provider sentiment analysis ](media/Coursera---How-Google-does-ML-image2.png)\n**Topics -**\n\n1.  Training and serving skew\n\n2.  Inclusive ML\n\n    a.  **Equality of opportunity**\n\n    b.  **How to find errors in your dataset using Facets**\n\n3.  Bias\n\n    a.  interaction bias (when a set of people are used they tend to draw generic things and computer then is biased towards non-generic things)\n\n    b.  latent bias (when fed pictures of scientists, model can be biased towards men)\n\n    c.  selection bias (select photos to train model from every place and not your album)\n\n4.  Ingest-transform-publish data\n**Path to ML**\n\n1.  Individual Contributor\n\n2.  Delegation - Gently ramp up to include more people\n\n3.  Digitization - Automate mundane parts of the process\n\n4.  Big Data and Analytics - Measure and achieve data-driven success\n\n5.  Machine Learning - Automated feedback loop that can outpace human scale\n![The Path to ML: Business processes that eventually end in ML typically go through 5 phases: 5 phases This is what changes in Phase 5 This is what changes in Phase 4 INSIGHT GENERATION â€¢ Individual contributor Delegation Digitization Big Data and Analytics Machine learning INPUT OPERATIONAL PARAMETERS UPDATED INSTRUCTIONS PROCESS -............................................................-..> OUTPUTS This is what changes in Phases 1-3 Google Cloud ](media/Coursera---How-Google-does-ML-image3.png)\n\n![Reviewing the Path to ML: 5 phases INDIVIDUAL CONTRIBUTOR O DELEGATION 1 DIGITIZATION BIG DATA MACHINE LEARNING WHO EXECUTES THE PROCESS? HOW ARE THE OPERATIONAL PARAMETERS CHOSEN? HOW ARE THE PARAMETERS, FED BACK TO EXECUTIONO ](media/Coursera---How-Google-does-ML-image4.png)\n![There are pre-trained machine learning services available on Google Cloud Custom MC models Pre-trained ML models 000 Vision API Translation API Speech API Natural Language API Jobs API ](media/Coursera---How-Google-does-ML-image5.png)\n![Unconscious bias gets reinforced in the training data Training data Algorithms are are collected programmed and classified Media are filtered, ranked, aggregated, or generated Unconscious bias affects the way we collect and classify data, design, and write code People (like us) are programmed Google CIC ](media/Coursera---How-Google-does-ML-image6.png)\n\n![](media/Coursera---How-Google-does-ML-image5.png)\n**Resources**\n\n<https://www.coursera.org/learn/google-machine-learning>\n"},{"fields":{"slug":"/Computer-Science/Courses/Data-Integration-Specialist---AWS/","title":"Data Integration Specialist - AWS"},"frontmatter":{"draft":false},"rawBody":"# Data Integration Specialist - AWS\n\nCreated: 2019-04-22 19:56:50 +0500\n\nModified: 2019-06-22 19:05:40 +0500\n\n---\n-   Big data topics including data architecture as well as techniques and tools for analysis, streaming and visualization.\n-   Data and data structures, data warehouses and lakes, as well as relational and non-relational data types\n-   Different scripting languages necessary for this role, including Java, Python, Javascript, Ruby, and using the command line interface\r\n"},{"fields":{"slug":"/Computer-Science/Courses/Intro-to-Microsoft-Excel---Google-Sheets/","title":"Intro to Microsoft Excel / Google Sheets"},"frontmatter":{"draft":false},"rawBody":"# Intro to Microsoft Excel / Google Sheets\n\nCreated: 2020-04-10 03:47:48 +0500\n\nModified: 2022-11-30 19:09:53 +0500\n\n---\n\n**Features**\n-   Timeline View\n\n<https://support.google.com/docs/answer/12935277><https://www.youtube.com/watch?v=Vl0H-qTclOg&feature=youtu.be&ab_channel=freeCodeCamp.org>\n-   enter data\n-   navigate through a spreadsheet\n-   create formulas to solve problems\n-   create charts and graphs\n-   understand relative vs absolute references\n-   import and export data\n-   implement VLOOKUP\n-   use pivot tables\n-   split and concatenate text\nColumns - 16,384\n\nRows - 1,048,576-   Text to column\n-   Pivot Tables\n-   Preferences > View > Developer Tab in the Ribbon (For macros)\n-   Fix cells (Absolute and Relative Cells, use dollar as prefix or postfix in functions for addresses of cells)\n-   Select special\n-   Dynamic names (=\"P&L:\"&C4)\n-   Named cell ranges\n-   Dropdown menu (Data validation)\n-   Conditional Formatting\n-   Custom cell formatting\n\n![Principles of custom formatting > Four fields separated by ; or , 1) Positive numbers 2) Negative numbers 3) How to display Os 4) How to display text # vs O: types of placeholders used #: flexible placeholder O: fixed placeholder ](media/Intro-to-Microsoft-Excel---Google-Sheets-image1.png)\n-   IF\n    -   SUMIF, SUMIFS, COUNTIF, COUNTIFS\n**Microsoft Excel - Useful Tools & Tips**\n\nHow to Make Your Spreadsheets Look Professional\n\nInserting, Deleting, and Modifying Rows & Columns\n\nExcel Formulas for Beginners\n\nExcel Functions\n\nWork Efficiently by Using Cut, Copy, and Paste\n\nFormat Cells\n\nPasting Values, Formulas and Formats with Paste Special\nInserting a Line Break with Alt + Enter\n\nDo More with Your Sales Data with Excel's Text to Columns Feature\n\nCreate Easily Printable Excel Documents\n\nHow to Wrap Text in Excel and Adjust a Cell's Size\n\nInsert Hyperlinks into Excel Spreadsheets\n\nUsing Excel's Freeze Panes to Handle Large Datasets\n\nFind Excel Functionalities in a Quicker Way - Tell Me What You Want to Do\n\nA Quick Introduction to Excel's Pivot Tables\n\nInitial Formatting Is Key for Creating Professional-looking Spreadsheets\n\nMacros Are a Great Timesaver! Here's Why\n\nHow to Use the Same Macro On Multiple Workbooks\n\nThe Secret to Faster Scrolling in Excel\n\nBe even quicker: F5 + Enter\n\nUsing Absolute and Relative Cell References\n\nFind and Select Cells That Meet Specific Conditions\n\nHow to Create Dynamic Names in Excel Spreadsheets\n\nUsing Named Ranges to Make Formulas More Readable\n\nHow to Add a Drop-down List in Excel\n\nUsing Custom-sort to Sort Multiple Columns Within a Table\n\nSaving Time in Excel and Doing Everything Faster by Using Excel Shortcuts\n\nMultiply by 1\n\nFind and Replace - References\n\nFind and Replace - Formatting\n\nGreen References\n\nBeauty Saving - The Professional Way of Saving Files\n\nThe Power of F2\n\nConditional Formatting\n\nIntroduction to Custom Cell Formatting\n\nCustom Formatting - An example\n**Microsoft Excel - Beginner, Intermediate & Advanced Functions**\n\nKey Excel Functions: If\n\nEnlarge the formula bar\n\nKey Excel Functions: Sum, Sumif, Sumifs\n\nKey Excel Functions: Count, Counta, Countif, Countifs\n\nKey Excel Functions: Average & Averageif\n\nKey Excel Functions: Left, Right, Mid, Upper, Lower, Proper, Concatenate, &\n\nWorking with text in Excel\n\nFind the Highest and Lowest Values in a Range: Max & Min\n\n= and + are interchangeable when you start typing a formula\n\nUse Round in Your Financial Models\n\nExcel's Lookup Functions: Vlookup & Hlookup Made Easy\n\nIndex, Match, and Their Combination - The Perfect Substitute for Vlookup\n\nUsing Excel's Iferror Function to Trap Spreadsheet Errors\n\nA Useful Tool for Financial Analysis - The Rank Function\n\nCreate Flexible Financial Models with Choose\n\nGoal Seek Will Help You Find the Result You Are Looking For\n\nPerform Sensitivity Analysis with Excel's Data Tables Functionality\n**Microsoft Excel - Practical Exercise \"Build a P&L From Scratch\"**\n\nIntroduction to the Case Study\n\nWhat You Will See Next\n\nUnderstand Your Data Source Before You Start Working on It\n\nOrdering the Source Worksheet\n\nCreate a Code: The Best Way to Organize Your Data and Work Efficiently with It\n\nLearn How to Create a Database\n\nUsing Lookup Functions (Vlookup) to Fill the Database Sheet\n\nUse Sumif to Complete the Database Sheet\n\nUsing Index & Match as a Substitute for Vlookup\n\nThe Mapping Exercise\n\nMapping the Rows in the Database Sheet\n\nBuilding the Structure of the P&L Sheet\n\nA Practical Example of Professional Formatting in Excel\n\nPopulate the P&L Sheet with Sumif\n\nLearn How to Find Mistakes with Countif\n\nCalculating Growth Rates in Excel\n**Microsoft Excel - Building Professional Charts in Excel**\n\nIntroduction to Excel Charts\n\nBeginner's Guide to Inserting Charts in Excel\n\nModifying Excel Charts - The Easy Way\n\nMaking Your Excel Charts Sexier - Proven Tips\n\nCreating a Bridge Chart in Excel 2016 - As Easy as It Gets\n\nNew Ways to Visualize Your Data - Treemap Charts\n\nHow to Represent Trends with Sparklines\n\nStacked Column Chart with a Secondary Axis\n\nDoughnut Chart\n\nArea Chart\n\nBridge Chart\n**Introduction to Pivot tables**\n\nIntro to Pivot tables\n\nEditing a Pivot table\n\nFormatting a Pivot table\n\nAdjusting a Pivot table\n\nGETPIVOTDATA\n\nSlicers\n**A practical case study with Pivot Tables**\n\nIntroduction to the case study\n\nWorking with an SAP data extraction\n\nPreliminary mapping of the data extraction\n\nCreating an output structure of the FMCG model\n\nImproving the layout and appearance of the FMCG report\n\nInserting formulas and automating calculations\n\nCreating a Master Pivot Table: The main data source for the FMCG report\n\nGetPivotData is great! Extracting data from the Master Pivot Table\n\nCombining Slicers and GetPivotData: The key to our success\n\nGetting fancy with Excel slicers\n\nThis is how the report can be used in practice by high-level executives\n<https://365datascience.teachable.com/courses/enrolled/526364>\n\n## Advanced Microsoft Excel**\n\n**Proficient Excel formatting**\n\nWhy Excel and why modeling\n\nLet's start from scratch and create a P&L sheet\n\nCell styles allows you to be faster\n\nPasting values, formulas, and formats with Paste Special\n\nFormatting Cells Part I - Working with data in Excel\n\nFormatting Cells Part II - Customize numbers the way you like\n\nHighlight key data with Excel Conditional Formatting\n\nFilter by color\n**How to be 3.0x faster than average users**\n\nUse multiple screens simultaneously\n\nF1 to F12 - Using Excel's function keys\n\nHow to select visible cells only\n\nGrouping Excel rows and columns - The correct way to do it!\n\nWorking on multiple sheets at the same time\n**Excel mechanics**\n\nFind & Replace - Our favourite Excel tool\n\nA great way to apply Find & Replace\n\nWhat Are Circular References in Excel\n\nCircular References - An example\n\nTrace precedents - Display the relationship between formulas and cells\n**Not so simple Excel functions**\n\nWhat is a nested function\n\nAdvanced Excel functions Index; Match; Index & Match\n\nAdvanced Excel functions Index, Match, Match\n\nAdvanced Excel functions Indirect; Vlookup & Indirect\n\nAdvanced Excel functions Rows; Columns; Vlookup & Columns\n\nAdvanced Excel functions Vlookup & Match\n\nAdvanced Excel functions Choose; Vlookup & Choose\n\nAdvanced Excel functions Offset; Offset & Match\n\nDate functions\n**Excel tips & tricks**\n\nExcel tips & tricks Part 1\n\nExcel tips & tricks Part 2\n\nExcel tips & tricks Part 3\n\nExcel tips & tricks Part 4\n<https://365datascience.teachable.com/courses/enrolled/233558>\n![1 STARTED THE WITH I-OTS OF PROBLEns. BUT NOW, AFTER HOVE AND HOVRS OF IADRK, 1 HAVE LOTS OF PROBLEt1S A SPREADSHEET y) ](media/Intro-to-Microsoft-Excel---Google-Sheets-image2.png)\n[Google Sheets - Full Course](https://www.youtube.com/watch?v=N2opj8XzYBY&ab_channel=freeCodeCamp.org)\n-   =GOOGLEFINANCE(\"GOOGL\")\n-   =image(\"https://image.jpg\")"},{"fields":{"slug":"/Computer-Science/Courses/Mordern-Algorithm-Design/","title":"Mordern Algorithm Design"},"frontmatter":{"draft":false},"rawBody":"# Mordern Algorithm Design\n\nCreated: 2018-05-03 23:59:05 +0500\n\nModified: 2020-02-10 11:07:10 +0500\n\n---\n\n1.  Randomization\n\n2.  Amortized Analysis\n\n3.  Disjoint Set\n\n4.  Suffix Tree and Suffix Array\n\n5.  Linear Programming\n\n6.  Maximum Flow Algorithms\n\n7.  Bipartite Matching and Stable Matching\n\n8.  Approximation Algorithms\n\n9.  Streaming Algorithms\n\n    a.  sliding window sampling algorithms\n\n10. External Memory Algorithm\n\n11. Online Algorithms\n\n12. Web Search Algorithms\n\n13. Data Compression Algorithms\n**Optimal Stopping Problem**\n\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics), the theory ofoptimal stoppingorearly stoppingis concerned with the problem of choosing a time to take a particular action, in order to[maximise](https://en.wikipedia.org/wiki/Optimization_(mathematics))an expected reward or minimise an expected cost. Optimal stopping problems can be found in areas of[statistics](https://en.wikipedia.org/wiki/Statistics),[economics](https://en.wikipedia.org/wiki/Economics), and[mathematical finance](https://en.wikipedia.org/wiki/Mathematical_finance)(related to the pricing of[American options](https://en.wikipedia.org/wiki/American_options)). A key example of an optimal stopping problem is the[secretary problem](https://en.wikipedia.org/wiki/Secretary_problem). Optimal stopping problems can often be written in the form of a[Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation), and are therefore often solved using[dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming).\n**Apartment hunting - 37%**\n<https://en.wikipedia.org/wiki/Optimal_stopping>\n\n## Secretary Problem**\n\nThesecretary problemis a problem that demonstrates a scenario involving[optimal stopping](https://en.wikipedia.org/wiki/Optimal_stopping)theory.The problem has been studied extensively in the fields of[applied probability](https://en.wikipedia.org/wiki/Applied_probability),[statistics](https://en.wikipedia.org/wiki/Statistics), and[decision theory](https://en.wikipedia.org/wiki/Decision_theory). It is also known as themarriage problem, thesultan's dowry problem, thefussy suitor problem,the googol game, and thebest choice problem.\nThe basic form of the problem is the following: imagine an administrator who wants to hire the best secretary out ofnrankable applicants for a position. The applicants are interviewed one by one in random order. A decision about each particular applicant is to be made immediately after the interview. Once rejected, an applicant cannot be recalled. During the interview, the administrator can rank the applicant among all applicants interviewed so far, but is unaware of the quality of yet unseen applicants. The question is about the optimal strategy ([stopping rule](https://en.wikipedia.org/wiki/Stopping_rule)) to maximize the probability of selecting the best applicant. If the decision can be deferred to the end, this can be solved by the simple maximum[selection algorithm](https://en.wikipedia.org/wiki/Selection_algorithm)of tracking the running maximum (and who achieved it), and selecting the overall maximum at the end. The difficulty is that the decision must be made immediately.\nThe problem has an elegant solution, and the shortest rigorous proof known so far is provided by the[odds algorithm](https://en.wikipedia.org/wiki/Odds_algorithm)(Bruss 2000). An easy analysis implies that the optimal win probability is always at least1/e, and that the latter holds even in a much greater generality (2003). The optimal stopping rule prescribes always rejecting the first ~n/e applicants that are interviewed (where[e](https://en.wikipedia.org/wiki/E_(mathematical_constant))is the base of the[natural logarithm](https://en.wikipedia.org/wiki/Natural_logarithm)) and then stopping at the first applicant who is better than every applicant interviewed so far (or continuing to the last applicant if this never occurs). Sometimes this strategy is called the 1/estopping rule, because the probability of stopping at the best applicant with this strategy is about1/ealready for moderate values ofn. One reason why the secretary problem has received so much attention is that the optimal policy for the problem (the stopping rule) is simple and selects the single best candidate about 37% of the time, irrespective of whether there are 100 or 100 million applicants.\n<https://en.wikipedia.org/wiki/Secretary_problem>\n"},{"fields":{"slug":"/Computer-Science/Courses/Nutanix-Hybrid-Cloud/","title":"Nutanix Hybrid Cloud"},"frontmatter":{"draft":false},"rawBody":"# Nutanix Hybrid Cloud\n\nCreated: 2020-06-23 20:57:11 +0500\n\nModified: 2021-02-28 22:22:29 +0500\n\n---\n\nUdacity - Hybrid Cloud Scholarship Foundation Course Nanodegree Program (23 June 2020)\n\n<https://www.udacity.com/scholarships/nutanix-hybrid-cloud-scholarship-program>\n\n## Hyper-converged infrastructure(HCI)**\n\nHyper-converged infrastructure(HCI) is a[software-defined](https://www.wikiwand.com/en/Software-defined_infrastructure)[IT infrastructure](https://www.wikiwand.com/en/IT_infrastructure)that virtualizes all of the elements of conventional \"[hardware](https://www.wikiwand.com/en/Computer_hardware)-defined\" systems. HCI includes, at a minimum,[virtualized computing](https://www.wikiwand.com/en/Virtualization)(a[hypervisor](https://www.wikiwand.com/en/Hypervisor)),[software-defined storage](https://www.wikiwand.com/en/Software-defined_storage)and virtualized networking ([software-defined networking](https://www.wikiwand.com/en/Software-defined_networking)).[[citation needed](https://www.wikiwand.com/en/Wikipedia:Citation_needed)]HCI typically runs on[commercial off-the-shelf](https://www.wikiwand.com/en/Commercial_off-the-shelf)(COTS) servers.\nThe primary difference between[converged infrastructure (CI)](https://www.wikiwand.com/en/Converged_infrastructure)and hyper-converged infrastructure is that in HCI, both the storage area network and the underlying storage abstractions are implemented virtually in software (at or via the hypervisor) rather than physically, in hardware.[[citation needed](https://www.wikiwand.com/en/Wikipedia:Citation_needed)]Because all of the software-defined elements are implemented within the context of the hypervisor, management of all resources can be[federated](https://www.wikiwand.com/en/Federation_(information_technology))(shared) across all instances of a hyper-converged infrastructure.\n<https://www.wikiwand.com/en/Hyper-converged_infrastructure>\n\n<https://github.com/rancher/harvester>\n1.  The Journey to the Modern Hybrid Cloud\n\n2.  Introduce you to the Nutanix HCI\n\n3.  Hybrid Cloud Security\n\n4.  Networking\n\n5.  Managing Virtual Machines in the Hybrid Cloud\n\n6.  Data Protection\nNIST defines cloud computing as \"a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources. For instance: networks, servers, storage, applications, and services that can be rapidly provisioned and released with minimal management effort or service provider interaction.\"\n[According to NIST](https://classroom.udacity.com/nanodegrees/nd321-1/parts/cd9ca74f-cbf3-40f8-9726-289a03b5560a/modules/c1b1466d-dba6-4e06-9014-b1cca87f5ca4/lessons/81e12a4b-5f16-4a67-8da0-8fe5eea1f483/concepts/The%20cloud%20is%20an%20experience%20and%20a%20mindset%20%5bshow:%20https:/csrc.nist.gov/publications/detail/sp/800-145/final%5d), the cloud model has 5 essential characteristics, 3 service models, and 4 deployment models.\n\n![Essential Characteristics On-demand self-service Broad network access Resource pooling Rapid elasticity Measured service Service Models Infrastructure: laaS Platform: Software: paaS saas Deployment Models Private Cloud Community Cloud Public Cloud Hybrid Cloud ](media/Nutanix-Hybrid-Cloud-image1.png)\n![Service Models Traditional On- premises IT Apolications Databases Doerating System Virtualization Physical Servers Network & Storage Data Center Colocation Data Apolications Databases Operating System Virtualization Physical Servers Network & Storage Data Center Hosting Data Apolications Doerating System Virtualization Physical Servers Network & Storage Data Center laaS Data Applications Databases Ooerating System Virtualization Physical Servers Network & Storage Data Center Data Applications Databases Ooerating System Virtualization Physical Servers Network & Storage Data Center Self-Managed Provider-Suoplied SaaS Data Apolications Databases Doerating System Virtualization Physical Servers Network & Storage Data Center ](media/Nutanix-Hybrid-Cloud-image2.png)\n![Monolith Data Application Database Operating System Virtualization Physical Servers Network & Storage Data Center ](media/Nutanix-Hybrid-Cloud-image3.png)\n\n![Web Load Balancer Webl Web Presentation Tier Web3 Web2 Web4 AppServer Load Balancer AppServer1 Application Logic Tier AppServer3 Databas Database Clu Database3 AppServer2 Server4 Database2 Database4 ](media/Nutanix-Hybrid-Cloud-image4.png)\n\nThere are first two major aspects of cloud consumption that organizations need to take into consideration: fiscal consumption and workload predictability.\n-   Rent vs Buy\n**Virtualization**\n\n![VIRTUAL MACHINES HYPERVISOR SERVER STORAGE NETWORK STORAGE Virtualized 3 Tier Architecture ](media/Nutanix-Hybrid-Cloud-image5.png)\n\nVirtualizationuses an abstraction layer and resource scheduler called a hypervisor to run virtual machines on shared hardware resources. Virtual machines, or VMs, can be run at 80% or higher resource utilization without contention, solving one of the major problems of distributed and 3-tier architecture.\nIn addition to resource utilization efficiency, VMs offer other advantages as well. They can be moved between hosts without downtime. And cloning, backup, and recovery of both servers and applications are simpler.\nx86 hardwareis a good fit for virtualization because, since the mid 2000s, each generation of the x86 processor has added features to improve virtualization performance. The latest generation supports virtualized network, storage, and server infrastructure, with performance that matches dedicated Application Specific Integrated Circuit, or ASIC-based hardware.\n**Advantages**\n\nx86 virtualization dramatically changed infrastructure resource utilization for the better:\n\ni.  The ability to utilize 80% or more of the available infrastructure resources meant that businesses also experienced huge ROI gains with drastically lower TCO.\n\nii. An added benefit was that VMs could be moved between hosts without downtime, and operational processes such as cloning, deployment, backup, and recovery of servers and applications were simplified.\n**Disadvantages**\n\na.  VM sprawl and the need for high availability resulted in high demands on storage and network resources.\n\nb.  If VM sprawl goes unmanaged, it can result in more complexity and increased costs.\n\nc.  Mixed environments -- which include both virtualized and non-virtualized workloads -- can result in even more management complexity.\n**Software Defined Datacenter (SDDC)**\n\nanSDDCis an integrated abstraction layer that defines a complete datacenter by means of a layer of software that presents the resources of the datacenter as pools of virtual and physical resources and allows their composition into arbitrary user-defined services. A modern SDDC deployment is defined by virtualized, software-defined resources that can be scaled up or down as required and can be deployed as needed in a number of distinct ways.\nThere are three key components to the SDDC:\n-   Software defined computing\n-   Software defined networking\n-   Software defined storage\nThere's also often a fourth layer known as the orchestration management layer. Gartner's John Morency describes it as \"The intelligence that enables the operations team to do the initial configuration in terms of defining the virtual machines, the storage, the network interconnections, and if they need to, support a specific application or set of applications.\"\n**Hyperconverged Infrastructure (HCI)**\n\nHyperconverged Infrastructure (HCI)converges the entire datacenter stack, including compute, storage, networking, and virtualization. Some key points to remember about HCI are:\n-   Software running on each server node distributes all operating functions across a cluster. This allows incremental scaling, so that a cluster can be grown one node at a time; performance increases as the environment grows.\n-   The software also creates clusters and pools local storage, eliminating the need for SAN or NAS infrastructureâ€‹.\n-   The removal of the physical storage fully unleashes the power of the SDDC.\n-   The use ofsolid state drives (SSDs), combined with data locality, provides excellent cluster performance without bottlenecks.\n\nNote that unlike its 3-tier incarnation -- which was only 66% converged -- HCI is fully converged.\n**SSDs and HCI**\n-   SSDs improve performance of the storage tier if they are bottlenecking on read-write performance at the disk level.\n-   To leverage the extreme advances in performance, the controllers and network need to be able to handle the vast I/O capabilities of SSDs.\n-   Data locality is a key component of HCI; not having to transit the network provides better utilization of an SSDs capabilities.\n**HCI and Private Cloud**\n\nOut of the box, HCI isn't a true private cloud. To have a fully operational private cloud, you also need to have:\n-   Unified management support\n-   One-click upgrades\n-   Customizable security\n-   Built-in data services for file, block, and object storage\n-   Sophisticated backup and disaster recovery solutions\n-   Tools for automation and self-service; and cost governance\n**Compliance and Security**\n\nCompanies are subject to many regulations regarding security and data-handling---and these regulations can have a significant impact on how you utilize the cloud. These are some of the main regulations that it's important to be familiar with:\n-   **GDPR:** TheGeneral Data Protection Regulation (GDPR)protects EU citizens from data breaches and misuse. It applies to all companies with data for EU citizens, even if those companies are not located in the EU.\n-   **HIPAA:** TheHealth Insurance Portability and Accountability Act (HIPAA)regulates the data security of healthcare patients. Companies that handle healthcare data (e.g., hospitals, clinics and insurance companies) are required to comply with HIPAA regulations.\n-   **Sarbanes-Oxley Act (SOX):** TheSarbanes-Oxley Act (SOX)requires U.S. company boards, management, and accounting firms to follow best practices and maintain financial records for seven years. The intent is to prevent incidents like the Enron scandal.\n-   **FISMA:**The Federal Information Security Management Act of 2002 (FISMA)treats information security as a matter of national security. All federal agencies are required to develop compliant data protection methods.\n-   **PCI-DSS:** ThePayment Card Industry Data Security Standard (PCI-DSS)regulations reduce fraud by protecting customer credit card information. PCI-DSS compliance is required for all companies handling credit card information.\n-   **GPG13:** TheProtective Monitoring for HMG ICT Systems regulation (GPG13)is a U.K. general data-protection regulation for business processes. It is compulsory for businesses managing high-impact data.\n**Introduction to Nutanix HCI**\n\nNutanix provides the public cloud benefits that organizations want with the control that they need on-prem. There aresixmajor benefits to Nutanix HCI specifically:\n\na.  **Full-cloud:**It's a full-cloud stack that integrates all compute, storage, virtualization, and networking resources to run any application.\n\nb.  **One-click simplicity:**This entire stack is managed via a single pane of glass that streamlines IT lifecycle management and makes hybrid and multi-cloud management easy .\n\nc.  **Deployed in minutes:**The applications themselves can be deployed in minutes, instead of weeks or months. This is true for new infrastructure as well.\n\nd.  **Automation application management:**Application management can also be automated, along with other common IT tasks. Application owners and developers can also be given on-demand IT services.\n\ne.  **Lower cloud costs:**You can also reduce your datacenter TCO by up to 60%. This will help optimize your public cloud spend with lower cloud costs.\n\nf.  **True hybrid cloud:**This refers to the ability for you to combine both public and private cloud operations with unified management.\n![Interfaces Management HTML 5 UI prism Acropolis REST API CLI VMS 1 Policy Services Analytics App Mobility Fabric (AMF) Storage Services Distributed Storage Fabric (DSF) t: ](media/Nutanix-Hybrid-Cloud-image6.png)\n\n1.  Acropolis:The data plane\n\n2.  Prism:The management plane\n**Hybrid Cloud Security**\n\n**Networking**\n\n**Managing Virtual Machines in the Hybrid Cloud**\n\n**Data Protection**\n"},{"fields":{"slug":"/Computer-Science/Courses/SE-Radio/","title":"SE Radio"},"frontmatter":{"draft":false},"rawBody":"# SE Radio\n\nCreated: 2018-06-15 10:47:34 +0500\n\nModified: 2021-10-24 15:38:11 +0500\n\n---\n\n**346: Streaming Architecture**\n-   Apache Flink\n**333: Software Design Decoded: 66 Ways Experts Think**\n**331: Architecture and Organizational Design**\n-   Correspondence between organizational design and software architecture.\n-   Conway's Law\n    -   If you have a 4 person team building a compiler, you will build a 4-pass compiler\n    -   Organizations which design systems ... are constrained to produce designs which are copies of the communication[structures](https://en.wikipedia.org/wiki/Organizational_structure)of these organizations.\n    -   The law is based on the reasoning that in order for a[software module](https://en.wikipedia.org/wiki/Modular_programming)to function, multiple authors must communicate frequently with each other. Therefore, the[software interface](https://en.wikipedia.org/wiki/Software_interface)structure of a system will reflect the social boundaries of the organization(s) that produced it, across which communication is more difficult.\n-   Inverse Conway Maneuver\n    -   'Inverse Conway Maneuver' recommends evolving your team and organizational structure to promote your desired architecture. Ideally your technology architecture will display isomorphism with your business architecture.\n**330: Attack Surface Reduction**\n-   Theattack surfaceof a[software](https://en.wikipedia.org/wiki/Software)environment is the sum of the different points (the \"attack[vectors](https://en.wikipedia.org/wiki/Vector_(malware))\") where an unauthorized user (the \"attacker\") can try to enter data to or extract data from an environment.Keeping the attack surface as small as possible is a basic security measure.\n-   The basic strategies of attack surface reduction include the following: reduce the amount of[code](https://en.wikipedia.org/wiki/Software)running, reduce entry points available to untrusted users, and eliminate services requested by relatively few users. One approach to improving[information security](https://en.wikipedia.org/wiki/Information_security)is to reduce the attack surface of a system or software. By turning off unnecessary functionality, there are fewer[security risks](https://en.wikipedia.org/wiki/Security_risk). By having less code available to unauthorized actors, there will tend to be fewer failures. Although attack surface reduction helps prevent security failures, it does not mitigate the amount of damage an attacker could inflict once a vulnerability is found.\n**327: Developer Productivity with Open Source**\n-   Package management and Distribution\n-   Semantic versioning\n**322: Property Based Test**\n\nIn property based tests we specify what is the condition that needs to be tested. And we write a concrete different implementation of that to check both for equality.\nExample -\n-   If we wrote a function that given an integer return square root of that, we can check this function using the built in function Math.sqrt and compare both values together.\n**294: Machine Learning in Log Analysis**\n\nClusterization\n\nClassifiers\n\nUsing classifiers and clusterization, find different problems in logs and also solutions to those using stackoverflow and other discussion forums.\n<http://www.se-radio.net>\n\n## Cloud Search with Liam Cavanagh, 17 Oct 2018**\n\nSearch is part of almost every application. Users search for movies to watch. Engineers search through terabytes of log messages to find exceptions. Drivers search through maps to find a destination. Search remains an unsolved problem, with lots of room for optimization.\nMany search applications have been built[Elasticsearch, an open source distributed search engine](https://softwareengineeringdaily.com/2017/04/12/elasticsearch-with-philipp-krenn/). Elasticsearch is the code that powers some search-as-a-service products offered by major cloud providers. After eight years of open source development, Elasticsearch is excellent at core search functionalities, such as indexing data, sharding, and serving queries.\nWith improved access to machine learning tools, search applications can advance in new and interesting ways. For example,an incoming search query can be sent to an API for natural language processing before being served by the search engine. A natural language processing API can derive additional meaning from the query, adding metadata to a search query. Machine learning can also be applied to better understand how people are searching across your search index, and to optimize the search index to incorporate those user preferences.\nLiam Cavanagh is the principal program manager on Azure Search. He joins the show to talk about the architecture of a search index, how search queries are served by an index, and how machine learning APIs can be used to improve queries.\n-   tf-idf\n-   word2vec\n-   ElasticSearch\n**Druid Analytical Database with Fangjin Yang, 14 Sep 2018**\n\nModern applications produce large numbers of events. These events can be users clicking, IoT sensors accumulating data, or log messages.\n\nThe cost of cloud storage and compute continues to drop, so engineers can afford to build applications around these high volumes of events, and a variety of tools have been developed to process them. Apache Kafka is widely used to store and queue these streams of data, and Apache Spark and Apache Flink are stream processing systems that are used to perform general purpose computations across this event stream data.\n\nKafka, Spark, and Flink are great general purpose tools, but there is also room for a more narrow set of distributed systems tools to support high volume event data. Apache Druid is an open source database built for high performance, read only analytic workloads. Druid has a useful combination of features for event data workloads, including a column-oriented storage system, automatic search indexing, and a horizontally scalable architecture.\n\nDruid's feature set allows for new types of analytics applications to be built on top of it, including search applications, dashboards, and ad-hoc analytics.\n<https://softwareengineeringdaily.com/category/data>\n"},{"fields":{"slug":"/Computer-Science/Courses/Self-Driving-Nanodegree/","title":"Self-Driving Nanodegree"},"frontmatter":{"draft":false},"rawBody":"# Self-Driving Nanodegree\n\nCreated: 2019-03-05 19:53:27 +0500\n\nModified: 2022-01-29 18:45:02 +0500\n\n---\n\n**Part - 1**\n\n[Computer Vision, Deep Learning, and Sensor Fusion](https://classroom.udacity.com/nanodegrees/nd013/parts/edf28735-efc1-4b99-8fbb-ba9c432239c8)\n\nIn this term, you'll first become an expert in applying Computer Vision and Deep Learning on automotive problems. You will teach the car to detect lane lines, predict steering angle, and more all based on just camera data!\n\nNext, you'll learn Sensor Fusion, or how to use an array of sensor data to perceive the environment around the vehicle.\n-   Project:[Finding Lane Lines](https://classroom.udacity.com/nanodegrees/nd013/parts/edf28735-efc1-4b99-8fbb-ba9c432239c8/modules/5d1efbaa-27d0-4ad5-a67a-48729ccebd9c/lessons/7c075239-1f65-4952-bde8-1810354d7988/project)\n-   Project:[Advanced Lane Finding](https://classroom.udacity.com/nanodegrees/nd013/parts/edf28735-efc1-4b99-8fbb-ba9c432239c8/modules/5d1efbaa-27d0-4ad5-a67a-48729ccebd9c/lessons/7cb63828-36aa-4cea-9239-700b5ea41f0b/project)\n-   Project:[Traffic Sign Classifier](https://classroom.udacity.com/nanodegrees/nd013/parts/edf28735-efc1-4b99-8fbb-ba9c432239c8/modules/6b6c37bc-13a5-47c7-88ed-eb1fce9789a0/lessons/7ee8d0d4-561e-4101-8615-66e0ab8ea8c8/project)\n-   Project:[Behavioral Cloning](https://classroom.udacity.com/nanodegrees/nd013/parts/edf28735-efc1-4b99-8fbb-ba9c432239c8/modules/6b6c37bc-13a5-47c7-88ed-eb1fce9789a0/lessons/3fc8dd70-23b3-4f49-86eb-a8707f71f8dd/project)\n-   Project:[Extended Kalman Filters](https://classroom.udacity.com/nanodegrees/nd013/parts/edf28735-efc1-4b99-8fbb-ba9c432239c8/modules/49d8fda9-69c7-4f10-aa18-dc3a2d790cbe/lessons/3feb3671-6252-4c25-adf0-e963af4d9d4a/project)\n**Lectures**\n-   **Computer Vision Fundamentals**\n-   **Camera Calibration**\n-   **Gradients and Color Spaces**\n-   **Advanced Computer Vision**\n-   **Neural Networks**\n-   **TensorFlow**\n-   **Deep Neural Networks**\n-   **Convolutional Neural Networks**\n-   **LeNet for Traffic Signs**\n-   **Keras**\n-   **Transfer Learning**\n-   **Sensors**\n-   **Kalman Filters**\n-   **C++ Checkpoint**\n-   **Geometry and Trigonometry Refresher**\n-   **Extended Kalman Filters**\n**Part - 2**\n\n[Localization, Path Planning, Control, and System Integration](https://classroom.udacity.com/nanodegrees/nd013/parts/30260907-68c1-4f24-b793-89c0c2a0ad32)\n\nIn this term, you'll expand on your sensor knowledge to localize and control the vehicle. You'll evaluate sensor data from camera, radar, lidar, and GPS, and use these in closed-loop controllers that actuate the vehicle.\n\nAfter that, you'll learn how to plan where the vehicle should go, and how the vehicle systems work together to get it there.\n-   Project:Kidnapped Vehicle\n-   Project:Highway Driving\n-   Project:PID Controller\n-   Project:Improve Your LinkedIn Profile\n-   Project:Optimize Your GitHub Profile\n-   Project:Programming a Real Self-Driving Car\n**Computer Vision Fundamentals**\n-   Color selection\n-   Region masking\n-   Gradient of an image\n-   Canny edge detection\n-   Hough Transform to find lines from Canny Edges\n\nWe will use equation of a line to model the line and then find lines using this model in our Gradient grayscale image\n-   Image Space (x vs y plot)\n-   Parameter space called as Hough Space (m vs b)\n-   Hough Transform is just a conversion from Image Space to Hough Space\n    -   Characterization of a line in image space will be a single point at the position (m, b) in Hough space.\n    -   A point in an image space will be a line in Hough Space\n**For edge detection - HED - Holistically Nested Edge Detection**\n\n<https://www.pyimagesearch.com/2019/03/04/holistically-nested-edge-detection-with-opencv-and-deep-learning>\n\n## Project**\n-   Project:[Finding Lane Lines](https://classroom.udacity.com/nanodegrees/nd013/parts/edf28735-efc1-4b99-8fbb-ba9c432239c8/modules/5d1efbaa-27d0-4ad5-a67a-48729ccebd9c/lessons/7c075239-1f65-4952-bde8-1810354d7988/project)\n\nTools\n-   Color selection\n-   Region of interest selection\n-   Grayscaling\n-   Gaussian smoothing\n-   Canny edge detection\n-   Hough transform line detection-   Project:[Advanced Lane Finding](https://classroom.udacity.com/nanodegrees/nd013/parts/edf28735-efc1-4b99-8fbb-ba9c432239c8/modules/5d1efbaa-27d0-4ad5-a67a-48729ccebd9c/lessons/7cb63828-36aa-4cea-9239-700b5ea41f0b/project)\n-   Project:[Traffic Sign Classifier](https://classroom.udacity.com/nanodegrees/nd013/parts/edf28735-efc1-4b99-8fbb-ba9c432239c8/modules/6b6c37bc-13a5-47c7-88ed-eb1fce9789a0/lessons/7ee8d0d4-561e-4101-8615-66e0ab8ea8c8/project)\n-   Project:[Behavioral Cloning](https://classroom.udacity.com/nanodegrees/nd013/parts/edf28735-efc1-4b99-8fbb-ba9c432239c8/modules/6b6c37bc-13a5-47c7-88ed-eb1fce9789a0/lessons/3fc8dd70-23b3-4f49-86eb-a8707f71f8dd/project)\n-   Project:[Extended Kalman Filters](https://classroom.udacity.com/nanodegrees/nd013/parts/edf28735-efc1-4b99-8fbb-ba9c432239c8/modules/49d8fda9-69c7-4f10-aa18-dc3a2d790cbe/lessons/3feb3671-6252-4c25-adf0-e963af4d9d4a/project)\n<https://www.freecodecamp.org/news/perception-for-self-driving-cars-deep-learning-course>\n"},{"fields":{"slug":"/Computer-Science/Decentralized-Applications/Bitcoin---Cryptocurrency---Web3/","title":"Bitcoin / Cryptocurrency / Web3"},"frontmatter":{"draft":false},"rawBody":"# Bitcoin / Cryptocurrency / Web3\n\nCreated: 2019-04-07 09:57:52 +0500\n\nModified: 2022-08-06 12:17:15 +0500\n\n---\n\n**Web3**\n\n<https://www.notboring.co/p/braintrust-fighting-capitalism-with>\n\n## Bitcoin (2008): first combination of proof-of-work / distributed ledger / hashchain**\n-   Solves Byzantine agreemeent, permissionless, Sybil-proof, reasonable cryptographic assumptions\n-   Elegant in its simplicity\n-   Limited scripting for transaction confirmation\n**Ethereum (2015): full-blown, Turing-complete smart contracts**\n-   Built-in currency caller **Ether**\n-   Operations in smart contracts have a **gas cost** that gets paid to the miners\n**Cryptocurrency**\n\n<https://hackernoon.com/crypto-baby-talk-first-50-terms-that-you-should-know-about-30a829320b4b>\n\n## Scaling Problem**\n\nBitcoin has a scaling problem. Bitcoin is designed to store all transactions in a data structure called a[block](https://en.bitcoin.it/wiki/Block). A block contains information about the previous block, miscellaneous data about mining rewards, and most of the block is just transaction data. Blocks are also fixed at a maximum of 1 MB in size. This last bit is where the trouble is.\nBecause blocks are 1 MB in size, and a block is created every 10 minutes, assuming the transactions are not SegWit (coming up later) the network can process a maximum of[between 3.3 and 7 transactions per second](https://link.springer.com/chapter/10.1007/978-3-662-53357-4_8). For a currency designed for mass use by billions of humans and their machines, 7 transactions a second just isn't up to par. Visa, on the other hand, claims to be able to process[24,000 transactions](https://web.archive.org/web/20181023104241/https:/usa.visa.com/run-your-business/small-business-tools/retail.html)per second.\n<https://towardsdatascience.com/the-blockchain-scalability-problem-the-race-for-visa-like-transaction-speed-5cce48f9d44>\n\n## Bitcoin Lightning Network**\n\nLightning Network is a second-layer network that transmits signed, but unbroadcast, transactions among peers and relies on the Bitcoin blockchain only for final settlement of funds.\n**How BLN work?**\n\nThe Lightning Network uses a network of nodes that hold funds in multi-sig wallets (\"channels\") and exchange signed, but unbroadcast, transactions.\n**Blockchain Trilemma**\n\nThe blockchain trilemma is a concept coined by[Vitalik Buterin](https://coinmarketcap.com/alexandria/glossary/vitalik-buterin)that proposes a set of three main issues --- decentralization, security and scalability --- that developers encounter when building blockchains, forcing them to ultimately sacrifice one \"aspect\" for as a trade-off to accommodatethe other two\n**The Scalability Trilemma**\n\n![Scalability Pick one side of the triangle Security Decentralization ](media/Bitcoin---Cryptocurrency---Web3-image1.jpg)\n<https://medium.com/logos-network/everything-you-know-about-the-scalability-trilemma-is-probably-wrong-bc4f4b7a7ef>\n<https://www.toptal.com/bitcoin/intro-to-bitcoin-lightning-network>\n\n<https://www.freecodecamp.org/news/create-cryptocurrency-using-python>\n\n[ETHEREUM 2.0 - A GAME CHANGER? Proof Of Stake, The Beacon Chain, Sharding, Docking Explained](https://youtu.be/ctzGr58_jeI)\n-   Solidity\n-   Ether = Token (is the fuel to run this Dapps)\n-   Ethereum - Platform to develop our own Dapps\n-   Gas fees\n-   Smart contracts\n-   DeFi - Decentralized Finance\n-   Proof of work -> Proof of stake\n-   Sharding will increasing 1000 TPS\n-   EIP 1559\n![Bitcoin BTC is a store Of value and can be used to exchange value for goods/services. Bitcoin blockchain only keeps records of transactions. Decentralizing the payment system. Time taken to confirm transactions is 10 minutes. There is limit of 21 Million to the maximum supply of Bitcoins. Ethereum Ether is primarily used to pay for Dapps running on ethereum network. Ethereum blockchain can run lines of codes. Decentralizing the internet. Time taken to confirm transactions is few seconds. There is no limit to the maximum supply of Ether. But there is a limit that only a maximum 18 million ether can enter the supply annually. ](media/Bitcoin---Cryptocurrency---Web3-image2.jpeg)\n**Is a greener, faster and more decentralised alternative to Bitcoin possible?**\n\nThis piece is a counter of sorts to this week's Long Read 3 about how tech is taking over geopolitics, at the core of whose argument lies smart contracts (self-executing rules, not enforced by an intermediary or an authority) that run on blockchains (decentralised or distributed databases). The most popular application of this concept so far has been cryptocurrencies. This article argues that even cryptocurrencies including the popular Bitcoin has flaws which threaten not only its own sustainability but the larger extension to other spheres of life, let alone replacement of nation states.\nThe three main issues the article points to are that the technology is energy intensive and hence not particularly eco-friendly, that they are not fast enough to claim supremacy over the status quo and finally that it is not decentralised enough as mining is still done by a few large pools (as access to resources is an edge).\n\"Hence the quest to come up with better blockchains. Chia, for instance, is a system based on \"proof of space and time\". As with Bitcoin, the carrot is that participating users earn coins. Yet the stick is different: instead of wasting computing power, Chia wastes digital storage. It is not yet clear, though, whether Chia will prove more sustainable and less centralised than Bitcoin if it becomes widely used.\nThe smart digital money is therefore on another approach: proof of stake. Here decisions about updating the blockchain are made not through a computing arms race, but by a vote among the holders of a cryptocurrency. Voting power as well as the share of the rewards depend on how much holders are willing to bet on the outcome. This stake can be destroyed if a participant misbehaves. In this system both carrot and stick are the cryptocurrency itself.\nProof of stake does use much less energy. And its latest incarnations are much faster than Bitcoin: Avalanche, a blockchain that uses the approach, processes thousands of transactions a second. But it still has big problems. Coders have been attempting to shift Ethereum, the preferred blockchain for DeFi apps, from proof of work to proof of stake. Even Vitalik Buterin, one of the inventors of Ethereum, admits that proof of stake is \"surprisingly complex\". That means that lots can go wrong, especially when nearly $100bn in capital in DeFi apps must switch over. After several delays, the coders hope to make the move in 2022.\nYet this system would still tend towards centralisation. Bigger holders can reap more rewards, increasing their holdings further. This concentrates power among early buyers of a cryptocurrency and could allow them to take control of the blockchain. Newer projects that use proof of stake are trying to find ways to avoid this. Hedera Hashgraph is governed by a consortium, much like the one that runs Visa, a credit-card network. Avalanche and Tezos seek to ensure decentralisation by making it easy for \"validators\", participants who maintain the blockchain, to join.\nTo critics, centralisation is inevitable, even if energy inefficiency and complexity are not. The problem of increasing returns to scale will raise its head for any popular blockchain, predicts David Rosenthal, an early practitioner. \"You waste all these resources only to end up with a system that is controlled by people you have even less reason to trust than those who run conventional financial institutions,\" he says.\nTo others, a degree of centralisation may simply be a price to pay for the other advantages of blockchains. Emin GÃ¼n Sirer of Cornell University, who co-founded Ava Labs, which created Avalanche, says that the main benefit is that governments will find it harder to influence blockchains than they do conventional banks. Kevin Werbach of the Wharton School of the University of Pennsylvania says that the openness of blockchains makes it easier to develop innovative financial services. Still, if the quest to come up with better blockchains shows one thing, it is that even in crypto-paradise there is no free lunch.\"\n[https://www.economist.com/finance-and-economics/2022/01/01/is-a-greener-faster-and-more-decentralised-alternative-to-bitcoin-possible](https://linkedin.us19.list-manage.com/track/click?u=52ada2ee20240692fbeb44407&id=bb6a2a2c5c&e=153268bbf0)\n**Crypto Investing / Investment (Party Fund)**\n-   **Only invest 1 lakh i.e. 10K for 12 weeks (3 months) and hold it long term for like years**-   **Coins**\n    -   Bitcoin (BTH)\n    -   Ethereum (ETH)\n    -   Tether (USDT) - stable coin, it will be always $1\n    -   Binance (BNB)\n    -   USD Coin (USDC) - stable coin\n\nUSDC is the world's fastest-growing, fully regulated dollar digital stablecoin.\n-   Solana (SOL)\n    -   Scalable - 50,000 TPS\n    -   Low transaction fees\n-   Cardano (ADA)\n-   CRO Coin (bitcoin.com)\n-   Alt coins (Alternative coins - coins apart from Bitcoin)\n-   Meme coin\n-   Portfolio\n    -   40% - Ethereum\n    -   20% - Polygon\n    -   20% - Cardano\n    -   10% - Cartezi\n    -   10% - Litecoin-   40% - Bitcoin\n-   15% - Ethereum\n-   15% - DeFi\n-   10% - Centralized Exchanges\n-   10% - Smart Contract Projects\n-   5% - NFT (Mana / Chiliz / Axie infinity)\n-   5% - Shitcoins/Memecoins-   **NFT (Non Fungible Token)**-   **Platforms**\n    -   <https://coinmarketcap.com/rankings/exchanges>\n    -   **Binance (Sagar)**\n    -   Crypto.com\n    -   Vauld (FDs)\n    -   Wazirx (Vishal)\n    -   CoinDCX\n    -   **coinbase (us based)**\n    -   Bitfinex (Exchange)\n    -   Metamask (A crypto wallet & gateway to blockchain apps)-   **Chains**\n    -   Blockchain\n    -   Smart chain\n    -   Smart contract\n    -   Ronin (Side Chain)\n    -   Lightning network-   **Others**\n    -   NFT\n    -   DeFi wallet-   **Tools**\n    -   <https://coinmarketcap.com>\n    -   <https://www.coingecko.com/en>\n    -   <https://www.intotheblock.com>\n    -   <https://interest.coinmarketcap.com> (Earn crypto from crypto)\n    -   **<https://cryptopanic.com> (All news in one page)**\n    -   <https://coinmarketcal.com/en> (Events)\n    -   <https://studio.glassnode.com/metrics> (Analytics)\n    -   <https://messari.io> (Analytics)-   **Technical Analysis**\n    -   Total Market Cap\n    -   24 hours volume\n    -   BTC Dominance (if BTC dominance decreases, alt coins take lead)\n    -   On-chain metrics-   Thoughts\n    -   Blockchain is important bitcoin not\n    -   Focus on fundamentals\n-   Pros\n    -   Huge Upside\n-   Cons\n    -   Currently a pyramid scheme\n        -   No one is accepting it\n    -   Gambling\n    -   Highly volatile\n    -   Very new\n    -   Speculation\n    -   Lot of new companies\n    -   Advertisements (Paid influencer marketing campaigns)\n    -   No regulation\n    -   Non productive asset - doesn't generate anything\n    -   Timing the market is very important\n    -   More cannot be generated (printing currency)\n    -   Cannot be taken from one person to give to other person\n    -   Doesn't increase productivity of the world\n    -   Doesn't create value (invest in businesses that creates value)\n    -   Source code managed by Blockstream\n    -   [Why I lost faith in Bitcoin (as an early adopter and software engineer)](https://www.youtube.com/watch?v=vjwVtl-VBDw)\n    -   [The Money Flower and why Bitcoin is a ponzi scheme | Morten Bech | TEDxBasel](https://www.youtube.com/watch?v=UK0ATammdRo)-   **References**\n\nwhiteboardcrypto\n"},{"fields":{"slug":"/Computer-Science/Courses/Udemy---Python-for-data-structures-algorithms/","title":"Udemy - Python for data structures algorithms"},"frontmatter":{"draft":false},"rawBody":"# Udemy - Python for data structures algorithms\n\nCreated: 2018-04-03 23:16:15 +0500\n\nModified: 2019-06-23 20:19:26 +0500\n\n---\n\n1.  Array Sequences\n\n2.  Stacks, Queues, and Deques\n\n3.  Linked Lists\n\n4.  Recursion\n\n5.  Trees\n\n6.  Searching and Sorting\n\n7.  Graph\n\n8.  Riddles\n**References**\n\n<https://www.udemy.com/python-for-data-structures-algorithms-and-interviews>\n"},{"fields":{"slug":"/Computer-Science/Decentralized-Applications/Blockchain/","title":"Blockchain"},"frontmatter":{"draft":false},"rawBody":"# Blockchain\n\nCreated: 2018-04-01 21:58:10 +0500\n\nModified: 2022-03-14 20:48:37 +0500\n\n---\n\nInventor - Satoshi Nakamoto\n\"Most technologies tend to automate workers on the periphery who are doing menial tasks. But blockchains automate away the center. Instead of putting taxi drivers out of a job, blockchain puts Uber out of a job, and lets the taxi drivers work with the customer directly.\" --- Vitalik Buterin, Creator of Ethereum\nA blockchain is a data structure that enables identifying and tracking transactions digitally and sharing this information across a distributed network of computers, creating a distributed trust network.\n**Key Points**\n-   Trapdoor function\n\nA trapdoor function is a function that is easy to compute in one direction but difficult to compute in the opposite direction unless you have special information. Trapdoor functions are essential for public key encryption---that's why they are commonly used in blockchain development to represent the ideas of addresses and private keys.-   Chain Fork\n\nBlocks in the ledger are included in such a way as to build the longest chain, i.e., the chain with the greatest cumulative difficulty. Forking is a situation where there are two candidate blocks competing to form the longest blockchain and two miners discover a solution to the proof-of-work problem within a short period of time from each other. The network is then divided, because some nodes get blocks from miner #1 and some from miner #2.\n\nA fork usually gets resolved in one block, because the probability that this situation happens again gets extremely lower with the next blocks that arise, so soon there is a new longest chain that will be considered as main.\n\n(Note: This type of fork is distinct from ahard fork, which is where some developers decide to create a backward-incompatible change to the blockchain protocol, resulting in two forever-distinct blockchains.)\n**Featues of Blockchain**\n-   Distributed / Decentralized - Data is replicated on all the nodes in a distributed P2P network, and each copy of the ledger is identical to others. It can also be decentralised with some lighter nodes not having full data storage with limited connection.\n-   Consensus mechanism - All users in the network can come to a pre-determined programmable agreement on the method of validation and can be by consensus.\n-   Irreversibility and crypto security - One would need to command at least 51% of the computing power (or nodes or stake) to take control of the bitcoin blockchain.\n-   Cryptographically secure - That means that the minting of digital currency is secured by mathematical algorithms that make it very difficult to break. It prevents bad actors from creating fake transactions, erasing transactions, stealing funds, etc.\n-   Finite-state machines - In Computer Science, a state machine is a machine that will analyze a series of inputs and based on those inputs will transition to a new state. Blockchains have one instance responsible for all the transactions being created by the system. There's one global truth that all nodes adhere to, they all share the same state.\n**Points**\n-   Blockchains all start with a 'genesis state'.\n-   Each transaction in the Ethereum network is grouped into what are called blocks. A single block contains a set of transactions and each block points to the next block. This makes a chain of blocks aka blockchain.\n-   Mining is when a specific group of nodes (miners) uses their computing power to create a block of valid transactions.\n-   The processes of validating each block by having a miner provide a mathematical proof are called the 'proof of work' algorithm.\n-   Miners who validate new blocks get rewarded with an intrinsic digital token called 'Ether'. Every single time a miner proves a block as valid, the network generates new Ethers and awards the miner.\n-   In scenarios where multiple paths are generated by miners, a 'fork' happens. Forks are to be avoided since they are disruptive to the system and force nodes to choose which chain they believe to be the most valid.\n-   Ethereum's mechanism to choose the most valid chain is called the \"GHOST protocol\". GHOST stands for \"Greedy Heaviest Observed Subtree\". Essentially, it picks the path that has had the most computation done on it. The protocol uses the block number of the most recent block, this represents the total number of blocks in the current path. The higher the block number, the longer the path and as such the larger the mining effort that had to have gone into arriving at the most recent block. This allows the network to agree on the correct version of the current state.\n**Components**\n-   Accounts\n    -   Externally Owned Account\n        -   Ether Balance\n    -   Smart Contract Account\n        -   Ether Balance\n        -   Contract Code\n    -   Every account has a state\n        -   nonce\n            -   represents number of transactions sent from the address of the account\n        -   balance\n            -   amount of Ether owned by the address\n        -   storageRoot\n            -   root node of its Merkle tree\n        -   codeHash\n            -   hash of the Ethereum Virtual Machine code\n    -   Merkle Tree\n\nA Merkle tree is a type of binary tree composed of a set of nodes with lots of leaf nodes at the bottom of the tree that contains data. Intermediate nodes in a Merkle tree consist of nodes that have a hash of its two child nodes, and the root node is made up of the hash of its two child nodes, representing the top of the tree.\n\n![](media/Blockchain-image1.png)\nData at the bottom of the Merkle tree is generated by splitting it into chunks, splitting chunks into buckets, and repeating the process using bucket hashes until there is only a single hash remaining (the root hash).\n![usw ](media/Blockchain-image2.png)\nEach node of the tree has a key with an associated value; the key tells us which child node to follow to get to the corresponding value, which is stored in the leaf nodes. In the case of ethereum, the key/value mapping for the state tree is between the addresses and their accounts.\n\nEvery blocks header stores the hash of the root node of what are essentially three different Merkle tree structures for the state, transactions, and receipts.\n![Block 124 State root Txgroup root Block 125 State root Txgroup root Block 126 State root \"662 Txgroup root Shard 10: state: Ret e g mot : . state 835680Â« Receipt Of57 ](media/Blockchain-image3.png)\nMerkle trees are supremely useful because its a very efficient way to store all this information, especially for light nodes. Ethereum uses light nodes and full nodes; full nodes must download the full blockchain, executing all the transactions contained in it. Light nodes must download only the chain of headers, without needing to execute any transactions or retrieving any associated state. This allows them to easily generate and receive verifiable answers about balances, events, etc.\n\nMerkle trees serve to help secure the blockchain because hashes in the Merkle tree propagate upward. If a bad actor tries to swap a fraudulent transaction into the bottom of the Merkle tree, it will cause a chain reaction in all the hashes in all the nodes above it.\n![2f9c 48a5 1328 d187 d063 a8b5 48a5 d8ca 94 bc 4a2f 2f9c d063 ERROR: hash(94bC â€¢a8b5) d8Ca a8b5 d8ca e74b 12c5 20 BTC 1328 d187 Alice EÂ« 20 BTC ](media/Blockchain-image4.png)\nA node that wants to verify some data can use whats called a 'Merkle proof\". This contains some data to be verified, the root hash of the tree, and the tree branch (partner hashes going up along the path from the data chunk to the root hash). People who read the proof can verify that branch hashing is consistent throughout the tree.-   **Fees**\n    -   Every single computation/transaction on the Ethereum blockchain requires a fee. That fee is paid in whats called 'gas'. Gas is the unit Ethereum uses to measure computation fees. Gas price is an amount of Ether a node is willing to spend on every gas unit, measured in 'gwei' (since 'wei' is the smallest unit of Ether).\n    -   For each transaction, a sender sets a gas limit and a gas price. The gas limit represents the maximum gas the sender is willing to pay. If the sender doesn't provide the necessary gas to execute a transaction, the transaction is considered invalid. And since the Ethereum network had to expend computational effort to run the calculations before running out of gas, none of the gas gets refunded to the sender. The money spent on gas by the sender is sent to some miners address since miners are expanding the computational effort to validate transactions. The gas fee acts as a reward for the miners. Importantly, gas is used to pay for storage usage as well.\n    -   Fees help prevent users from overtaxing the Ethereum network. Its very computationally expensive to run computational steps on the Ethereum Virtual Machine, so smart contracts should be used for simple tasks like verifying ownership instead of more complex tasks like machine learning or file storage. Fees also help protect the network from malicious attacks. Ethereum has its own Turing complete programming language called Solidity for creating smart contracts. Turing complete means it can simulate any computer algorithm. It allows for-loops, so a bad actor could disrupt the network by executing an infinite loop within a transaction, but thanks to fees this becomes infeasible.\n    -   There are two types of transactions in Ethereum, the message call and the contract creation (creates new smart contracts). Both are initiated by externally owned accounts and submitted to the blockchain. They are what bridge the external world to the internal state of Ethereum. Contracts can talk to other contracts via messages. Messages are like internal transactions. These messages are generated by contracts, and when one contract sends a message to another, the code that exists on the recipient contract account is executed.-   **Ethereum Virtual Machine**\n\n![EVM ROM](media/Blockchain-image5.png)\nThe EVM is a complete virtual machine, and its only limitation is that its bound by gas. Meaning the total amount of computation it can do is limited by the amount of gas provided. Its a stack-based architecture (last-in, first-out). It has temporary memory and long-term storage. It even has its own language! (called EVM bytecode). When we write smart contracts, it's in a higher level language like Solidity, but this compiles down to EVM bytecode.\n**Hyperledger**\n\nHyperledger is an open source community focused on developing a suite of stable frameworks, tools and libraries for enterprise-grade blockchain deployments.\nIt serves as a neutral home for various distributed ledger frameworks including Hyperledger Fabric, Sawtooth, Indy, as well as tools like Hyperledger Caliper and libraries like Hyperledger Ursa.\n<https://www.hyperledger.org>\n\n## Hyperledger Fabric**\n\n**Type: Distributed ledger software**\nHyperledger Fabric is intended as a foundation for developing applications or solutions with a modular architecture. Hyperledger Fabric allows components, such as consensus and membership services, to be plug-and-play. Its modular and versatile design satisfies a broad range of industry use cases. It offers a unique approach to consensus that enables performance at scale while preserving privacy.\n<https://www.hyperledger.org/use/fabric>\nBelow are some of the key features of Hyperledger Fabric and what differentiates it from other distributed ledger technologies\n-   Permissioned architecture\n-   Highly modular\n-   Pluggable consensus\n-   Open smart contract model --- flexibility to implement any desired solution model (account model, UTXO model, structured data, unstructured data, etc)\n-   Low latency of finality/confirmation\n-   Flexible approach to data privacy : data isolation using 'channels', or share private data on a need-to-know basis using private data 'collections'\n-   Multi-language smart contract support: Go, Java, Javascript\n-   Support for EVM and Solidity\n-   Designed for continuous operations, including rolling upgrades and asymmetric version sup-port\n-   Governance and versioning of smart contracts\n-   Flexible endorsement model for achieving consensus across required organizations\n-   Queryable data (key-based queries and JSON queries)\n[Blockchain 102 and The Dark Side of Blockchain](https://www.youtube.com/watch?v=-so3AtnToek)\n\n**Proof of Authority (PoA)**\n-   Blocks must be signed by a sufficient quorum of \"authoritative\" nodes\n-   Very simple,, very efficient\n-   Requires trust in the authorities\n    -   How are they chosen?\n-   Basically: A PKI, a databases, and a hashchain in a trenchcoat\n-   Standard construction for \"private\"/\"permissioned\" blockchains\n**Proof of Stake**\n-   Like PoA, but authority is indirectly established by \"stake\", suh as token holdings\n-   May offer integrated punishement mechanism for malicious nodes: slashing\n-   Current **holy grail** to get of the nergy waste stigma (both Ethereum and IOTA plan to pivot to PoS)\n-   Slight problems\n    -   How is stake initally distributed?\n        -   Popular: Sale of tokens\n        -   Alternative: Bootstrap off of existing blockchains\n    -   No built-in defence against centralization. No Sybil resistance\n        -   Even if bootstrapped correctly, may become centralized without any visible sign\n**Proof of Storage**\n-   Example: Chia (2018)\n    -   Increased wear destroys SSDs after less than a year\n    -   No use beyond price speculation\n**Proof of Elapsed Time (2016)**\n-   Instead of computing hashes for PoW, just do: nothing! Go to sleep!\n-   Incredible enery savings compared to PoW\n-   Drop-in solution, behaves exactly like PoW\n-   Used in permissioned environments\n-   Small questions: What guarantees do other nodes have that no node wakes up before its time?\n    -   Intel SGX\n-   See also: Proof of Luck (PoL)\n**mobilecoin (2020): Proof of complexity**\n-   Mobile-device focused\n-   Pulls every cryptographic register there is\n    -   Stellar consensus\n    -   Zero-knowledge proofs for everything\n    -   Ristretoo, Schnorr anonymous signatures, Pedersen commitments\n-   Security against double-spending completely relies on Intel SGX enclaves\n-   Entire token supply is pre-mined\n**DAO - Decentralized Autonomous Organization**\n-   Governance implemented in Ethereum smart contracts. **Code is Law**, etc\n-   Own token: DAO\n-   Crowdfunding in June 2016: $150 million\n-   Vulnerability in the split function: time-of-check/time-of-use vs recursive calls\n    -   Anyone can propose to create a child DAO\n    -   Contract first retrieves Ether from main DAO, then checks against proponent balance\n    -   Recursive call allows this to be nested/magnified\n    -   Mid June 2016: $60 million worth in rogue child DAO\n    -   Build in 48 day period before funds can be transferred out\n-   Heated discussion led to hard fork of the entire Ethereum Blockchain\n**ERC-20 tokens**\n-   Principal mechanism to create custom money-like tokens on Ethereum\n-   Specifies a smart contract API that allows balance query and transfer of amounts\n-   All details (ICO, governance, etc) left to the creator of the contract\n-   May have own value independent of Ether, traded on several different platforms\n**Crypto-enabled financial trickery**\n-   Smart contracts allow automatic execution of contract stipulations, no matter their form\n-   Classic: Arbitrage between different platforms\n-   New: Uncollateralized flash loans\n**Decentralized Finance - DeFi**\n-   Allow most operations traditional financial instruments, but on The Blockchain\n    -   Borrowing, Lending\n    -   Price speculation\n    -   Swapping once ERC-20 token for another\n        -   F.e. Uniswap\n**Self-soverign identities (SSI)**\n-   Self-sovereign identity (SSI) is an approach to digital identity that gives individuals control of their digital identities\n-   No single standard, specification, or shared understanding **what** that may be\n**Oracle problem**\n-   Cryptography works only within the system\n-   Any interface to the real world requires trust in a node other than self\n    -   In which case no distributed consensus is required\n-   Cryptocurrencies have, more or less by definition, only one use case: **Ponzi schemes and other scams**\n-   A pure timestamping hash chain/Merkle tree (without consensus overhead) can be useful\n![Start Can i trust them to accurately report sensor data to the distributed ledger interface? yes I can trust them to accurately report sensor data to a Rain Old Database interface. oonâ€¢t need consensus. NO j can't trust them. The ' distributed ledger isnâ€¢t going to help with that. ](media/Blockchain-image6.jpg)\n<https://blog.chain.link/what-is-the-blockchain-oracle-problem>\n"},{"fields":{"slug":"/Computer-Science/Decentralized-Applications/Ethereum/","title":"Ethereum"},"frontmatter":{"draft":false},"rawBody":"# Ethereum\n\nCreated: 2018-09-20 23:50:23 +0500\n\nModified: 2022-03-13 15:42:41 +0500\n\n---\n\nEthereum is a**decentralized platform** that runs smart contracts: applications that run exactly as programmed without any possibility of downtime, censorship, fraud or third-party interference.\nUses Solidity as a programming language.\nEthereum's mechanism to choose the most valid chain is called the \"GHOST protocol\". GHOST stands for \"Greedy Heaviest Observed Subtree\". Essentially, it picks the path that has had the most computation done on it. The protocol uses the block number of the most recent block, this represents the total number of blocks in the current path. The higher the block number, the longer the path and as such the larger the mining effort that had to have gone into arriving at the most recent block. This allows the network to agree on the correct version of the current state.-   Ethereum Virtual Machine is a decentralized virtual machine that can run crypto economically secured bits of code called smart contract.\n-   The smart contracts language Solidity is tailored to the ethereum blockchain since it compiles down to bytecode specific for Ethereum's stack machine.\n-   Solidity is Turing-complete meaning it can theoretically any kind of computation.\n<https://www.theschool.ai/wp-content/uploads/2018/09/Reading-Assignment-Semantics-of-the-EVM-.pdf>\n\n## Blinkist - Ethereum by Henning Diedrich**\n-   blockchain - secure and shared database that contains transactions\n-   smart contracts\n-   bitcoin\n-   ethereum - most advanced blockchain available\n-   The first block of data is called the genesis block\n-   Decentralized\n-   Blockchains will be to finance what the Internet was to music and video\n-   Consensus protocol known as proof-of-work\n-   Agree on true version of the blockchain\n-   Follow the longest chain or heaviest chain\n-   Problems\n    -   Loss of data\n    -   Confidentiality\n-   Ethereum is the blockchain of blockchains\n**Ethereum Push Notification Service (EPNS)**\n\nProtocol forblockchainbasednotificationsthat arechain agnostic,platform independentandincentivized!\n<https://epns.io>\n"},{"fields":{"slug":"/Computer-Science/Decentralized-Applications/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-09-20 18:13:12 +0500\n\nModified: 2022-04-07 20:49:34 +0500\n\n---\n\n1.  Centralized\n\n2.  Decentralised\n\n3.  Distributed\n**2.0 stack**\n-   AWS S3 for storage\n-   Aws EC2 for compute\n-   Stripe for payments\n-   Third party services for other services\n**Web 3.0 stack (dApps)**\n-   Etherium, truebit (scalable computation)\n-   IPFS (inter planetary file system) / FileCoin (storage)\n-   Oracles (External data)\n-   Token Model (Monetization)\n-   Bitcoin (payments)\n**Features of decentralised application**\n-   Open source\n-   Use of cryptocurrency\n    -   Allocate a scarce resource to monetize.\n    -   Example - Steemit\n-   Decentralised consensus\n    -   Ability of a network to agree about on something\n    -   Proof of work algorithm\n    -   51% of all the nodes must approve the work\n    -   Big files are stored in a distributed hash table (DHT) (IPFS)\n    -   Block chain helps the DHT reach consensus\n    -   Smart contracts which are crypoeconomically secured code\n    -   Etherium has a tiring complete block chain\n-   No central point of failure (IPFS Stack)\n    -   Instead of IP addressing our content, we should content address it\n    -   Merkel DAG\n    -   Resiliency happening with bits and shards of data replicated across network (multiple nodes)\n**Decentralized Chat (Whisper)**\n\n1.  Building first whisper chat app\n\n2.  Sending messages with Geth + Whisper\n\n3.  Off chain P2P communication protocol\n\n4.  Scalability issues\n\n5.  Peer to peer chat\n![alt text](media/Intro-image1.jpeg)\n\n**What is Whisper**\n\n![alt text](media/Intro-image2.png)\n-   Darkness as a feature\n<https://github.com/llSourcell/Decentralized_Chat/blob/master/Decentralized%20Chat.ipynb>\nDecentralized News\n\nDecentralized Games\n\nDecentralized Rides\n\nDecentralized Music\n\nDecentralized Search Engine App\n\nDecentralized Marketplace\n\nDecentralized Social Network\n\nDecentralized Artificial Intelligence\n**References**\n\n<https://www.toptal.com/blockchain/interview-questions>\n\n<https://www.youtube.com/channel/UCY0xL8V6NzzFcwzHCgB8orQ>"},{"fields":{"slug":"/Computer-Science/Decentralized-Applications/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-01-20 12:30:30 +0500\n\nModified: 2022-03-13 16:54:15 +0500\n\n---\n\n**IPFS (InterPlanatery File System)**\n\n**InterPlanetary File System(IPFS)** is a[protocol](https://en.wikipedia.org/wiki/Communications_protocol)and network designed to create a[content-addressable](https://en.wikipedia.org/wiki/Content-addressable_storage),[peer-to-peer](https://en.wikipedia.org/wiki/Peer-to-peer)method of storing and sharing[hypermedia](https://en.wikipedia.org/wiki/Hypermedia)in a[distributed file system](https://en.wikipedia.org/wiki/Distributed_file_system).\nIPFS is a peer-to-peer distributed file system that seeks to connect all computing devices with the same system of files. IPFS could be seen as a single[BitTorrent](https://en.wikipedia.org/wiki/BitTorrent)swarm, exchanging objects within one[Git](https://en.wikipedia.org/wiki/Git_(software))repository. In other words, IPFS provides a high-throughput, content-addressed[block storage](https://en.wikipedia.org/wiki/Block_storage)model, with content-addressed[hyperlinks](https://en.wikipedia.org/wiki/Hyperlink).Distributed Content Delivery saves bandwidth and prevents[DDoS attacks](https://en.wikipedia.org/wiki/Denial-of-service_attack), which HTTP struggles with.\n**Working**\n-   Each file and all of theblocks within itare given aunique fingerprintcalled acryptographic hash.\n-   IPFSremoves duplicationsacross the network.\n-   Eachnetwork nodestores only content it is interested in, and some indexing information that helps figure out who is storing what.\n-   Whenlooking up files, you're asking the network to find nodes storing the content behind a unique hash.\n-   Every file can be found byhuman-readable namesusing a decentralized naming system calledIPNS.\n<https://ipfs.io>\n\n<https://en.wikipedia.org/wiki/InterPlanetary_File_System>\n\n## Orbit Chat**\n\nA distributed, serverless, peer-to-peer chat application on IPFS\n\n<https://github.com/orbitdb/orbit>\n\n## OrbitDB**\n\nPeer-to-Peer Databases for the Decentralized Web\n\n<https://github.com/orbitdb>\n\n## OpenBazaar**\n\n**OpenBazaar**is an open source project developing a protocol for[e-commerce](https://en.wikipedia.org/wiki/E-commerce)transactions in a fully[decentralized](https://en.wikipedia.org/wiki/Decentralization)marketplace.[^[1]^](https://en.wikipedia.org/wiki/OpenBazaar#cite_note-1)It uses[cryptocurrencies](https://en.wikipedia.org/wiki/Cryptocurrency)as medium of exchange and was inspired by a[hackathon](https://en.wikipedia.org/wiki/Hackathon)project called**DarkMarket**.\n<https://en.wikipedia.org/wiki/OpenBazaar>\n\n<https://openbazaar.org>\n\n## EOS Storage**\n\nEOS.IO Storage is a proposed decentralized file system designed to give everyone the ability to permanently store and host files accessible by any web browser.\n\n<https://medium.com/eosio/start-an-eos-meetup-in-your-community-761be355fce>\n\nEarth Observation System - <https://eos.com/eos-storage>\n\n<https://github.com/cern-eos/eos>\n\n## XRootD**\n\nThe XROOTD project aims at giving high performance, scalable fault tolerant access to data repositories of many kinds. The typical usage is to give access to file-based ones. It is based on a scalable architecture, a communication protocol, and a set of plugins and tools based on those. The freedom to configure it and to make it scale (for size and performance) allows the deployment of data access clusters of virtually any size, which can include sophisticated features, like authentication/authorization, integrations with other systems, WAN data distribution, etc.\nXRootD software framework is a fully generic suite for fast, low latency and scalable data access, which can serve natively any kind of data, organized as a hierarchical filesystem-like namespace, based on the concept of directory. As a general rule, particular emphasis has been put in the quality of the core software parts.\n<http://xrootd.org>\n\n## Bittorrent**\n-   ğŸ’­ Who Created BitTorrent?\n-   ğŸ¥Š BitTorrent vs Cient-Server Downloading\n-   ğŸ“‘ High Level Overview\n-   ğŸ“ What's in a Torrent Descriptor File, Anyway?\n-   ğŸ§€ The Piece Selection Algorithm of BitTorrent\n-   ğŸŒ† What Are Sub-Pieces and the Piece Selection Algorithm?\n-   ğŸŒ± Resource Allocation Using Tit-For-Tat\n-   ğŸ The Choking Algorithm\n-   ğŸ˜ Optimistic Unchoking\n-   ğŸ¤• Anti-Snubbing\n-   ğŸ¤” What If We Upload Only?\n-   ğŸ What Is a Tracker?\n-   ğŸ§² Magnet Links - Trackerless Torrents\n-   ğŸ Distributed Hash Tables\n-   ğŸ“Œ Routing Table\n-   ğŸ¤º Attacks on BitTorrent\n<https://dev.to/brandonskerritt/how-does-bittorrent-work-a-plain-english-guide-16a2>\n\n<https://skerritt.blog/bit-torrent>\nOthers - **WebTorrent**\n\n<https://github.com/webtorrent/webtorrent>\n\n## DApps**\n\n**Decentralized database**\n\n<https://gun.eco>\n\n[GUN Decentralized Graph DB in 100 Seconds](https://www.youtube.com/watch?v=oTQXzhm8w_8)\n"},{"fields":{"slug":"/Computer-Science/Distributed-System/CAP-Theorem/","title":"CAP Theorem"},"frontmatter":{"draft":false},"rawBody":"# CAP Theorem\n\nCreated: 2019-04-21 13:20:23 +0500\n\nModified: 2021-09-05 15:32:05 +0500\n\n---\n\n**Proposed by Eric Brewer (Berkeley)**\n\n**C - Consistency**\n-   Every read receives the most recent write or an error\n-   all nodes see same data at any time, or reads return latest written value by any client\n\n**A - Availability**\n-   Every request receives a (non-error) response -- without guarantee that it contains the most recent write\n-   the system allows operations all the time, and operations return quickly\n\n**P - Partition Tolerence**\n-   The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes\n-   the system continues to work inspite of network partitions\nCan only achieve two of these.\nIn the presence of a network partition, you must choose betwen consistency and availability\n**Why is Availability Important?**\n-   Availability = Reads/writes complete reliable and quickly\n-   Measurements have shown that a 500 ms increase in latency for operations at Amazon.com or at Google.com can cause a 20% drop in revenue.\n-   At Amazon, each added millisecond of latency implies a $6M yearly loss\n-   **User cognitive drift:** If more than a second elapses between clicking and material appearing, the user's mind is already somewhere else\n-   SLAs (Service Level Agreements) written by providers predominantly deal with latencies faced by clients.\n**Why is Consistency Important?**\n-   Consistency = all nodes see same data at any time, or reads return latest written value by any client.\n-   When you access your bank or investment account via multiple clients, you want the updates done from one client to be visible to other clients.\n-   When thousands of customers are looking to book a flight, all updates from any client (e.g., book a flight) should be accessible by other clients.\n**Why is Partition-Tolerance Important?**\n-   Partitions happen. They happen for countless reasons. Switches fail, NICs fail, link layers fail, servers fail, processes fail. Partitions happen even when systemsdon'tfail due to GC pauses or prolonged I/O latency for example. Let's acceptthis as fact andmove on. What this means is that a \"CA\" system is CAonly until it's not. Once that partition happens, all your assumptions and all your guarantees hit the fan in spectacular fashion. Where does this leave us?\n-   Partitions can happen across datacenters when the internet gets disconnected\n    -   Internet router outages\n    -   Under-sea cables cut\n    -   DNS not working\n-   Partitions can also occur within a datacenter, e.g. a rack switch outage\n-   Still desire system to continue functioning normally under this scenario\n-   Partition tolerance ensures that system still functions in face of partition or network failure\n**CAP Theorem Fallout**\n-   Since partition-tolerance is essential in today's cloud computing systems, CAP theorem implies that a system has to choose between consistency and availability\n**Cassandra**\n-   Eventual (weak) consistency, Availability, Partition-tolerance\n**Traditional RDMSs**\n-   Strong consistency over availability under a partition\n**CAP Tradeoff**\n-   Starting point for NoSQL Revolution\n-   A distributed storage system can achieve at most two of C, A, and P.\n-   When partition-tolerance is important, you have to choose between consistency and availability\n\n![Con istency HBase, HyperTable, BigTable, Spanner Pick 2 Partition-tolerance RDBMSs (non-replicated) Availability Cassandra, RIAK, Dynayo, Voldemort ](media/CAP-Theorem-image1.png)\n**Eventual Consistency**\n-   If all writes stop (to a key), then all its values (replicas) will converge eventually.\n-   If writes continue, then system always tries to keep converging.\n    -   Moving wave of updated values lagging behind the latest values sent by clients, but always trying to catch up.\n-   May still return stale values to clients (e.g., if many back-to-back writes).\n-   But works well when there a few periods of low writes - system converges quickly\n**RDBMS vs Key-value stores**\n-   While RDBMS provide ACID\n    -   Atomicity\n    -   Consistency\n    -   Isolation\n    -   Durability\n-   Key-value stores like Cassandra provide BASE\n    -   Basically Available Soft-state Eventual Consistency\n    -   Prefers Availability over Consistency\nDespite your best efforts, your system will experience enough faults that it will have to make a choice between reducing yield (i.e., stop answering requests) and reducing harvest (i.e., giving answers based on incomplete data). This decision should be based on business requirements.\n**PACELC (pass-elk) Theorem**\n\nIn[theoretical computer science](https://en.wikipedia.org/wiki/Theoretical_computer_science), thePACELC theoremis an extension to the[CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem). It states that in case of network partitioning (P) in a[distributed computer system](https://en.wikipedia.org/wiki/Distributed_computing), one has to choose between availability (A) and consistency (C) (as per the CAP theorem), but else (E), even when the system is running normally in the absence of partitions, one has to choose between latency (L) and consistency (C).\n**Database PACELC ratings**\n-   The default versions of[DynamoDB](https://en.wikipedia.org/wiki/Amazon_DynamoDB),[Cassandra](https://en.wikipedia.org/wiki/Apache_Cassandra),[Riak](https://en.wikipedia.org/wiki/Riak)and[Cosmos DB](https://en.wikipedia.org/wiki/Cosmos_DB)are **PA/EL systems**: if a partition occurs, they give up consistency for availability, and under normal operation they give up consistency for lower latency.\n-   Fully ACID systems such as[VoltDB](https://en.wikipedia.org/wiki/VoltDB)/H-Store, Megastore and**[MySQL Cluster](https://en.wikipedia.org/wiki/MySQL_Cluster)are PC/EC**: they refuse to give up consistency, and will pay the availability and latency costs to achieve it.[BigTable](https://en.wikipedia.org/wiki/Bigtable)and related systems such as[HBase](https://en.wikipedia.org/wiki/Apache_HBase)are also PC/EC.\n-   [Couchbase](https://docs.couchbase.com/server/6.0/learn/clusters-and-availability/clusters-and-availability.html)provides a range of consistency and availability options during a partition, and equally a range of latency and consistency options with no partition. Unlike most other databases, Couchbase doesn't have a single API set nor does it scale/replicate all data services homogeneously. For writes, Couchbase favors Consistency over Availability making it formally CP, but on read there is more user-controlled variability depending on index replication, desired consistency level and type of access (single document lookup vs range scan vs full-text search, etc). On top of that, there is then further variability depending on cross-datacenter-replication (XDCR) which takes multiple CP clusters and connects them with asynchronous replication and Couchbase Lite which is an embedded database and creates a fully multi-master (with revision tracking) distributed topology.\n-   [Cosmos DB](https://en.wikipedia.org/wiki/Cosmos_DB)supports five tunable consistency levels that allow for tradeoffs between C/A during P, and L/C during E.[Cosmos DB](https://en.wikipedia.org/wiki/Cosmos_DB)never violates the specified consistency level, so it's formally CP.\n-   [MongoDB](https://en.wikipedia.org/wiki/MongoDB)can be classified as a PA/EC system. In the baseline case, the system guarantees reads and writes to be consistent.\n-   PNUTS is a PC/EL system.\n-   Hazelcast IMDG and indeed most in-memory data grids are an implementation of a PA/EC system; Hazelcast can be configured to be EL rather than EC.Concurrency primitives (Lock, AtomicReference, CountDownLatch, etc.) can be either PC/EC or PA/EC.\n-   [FaunaDB](https://news.ycombinator.com/item?id=18257128) implements [Calvin](http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf), a transaction protocol created by Dr. Daniel Abadi and author of PACELC theorem, and offers users adjustable controls for LC tradeoff. It is PC/EC for strictly serializable transactions, and EL for serializable reads.\n\n<table>\n<colgroup>\n<col style=\"width: 35%\" />\n<col style=\"width: 17%\" />\n<col style=\"width: 17%\" />\n<col style=\"width: 17%\" />\n<col style=\"width: 10%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>DDBS</th>\n<th>P+A</th>\n<th>P+C</th>\n<th>E+L</th>\n<th>E+C</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>DynamoDB</td>\n<td>Yes</td>\n<td></td>\n<td><p>Yes</p>\n</td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>Cassandra</td>\n<td>Yes</td>\n<td></td>\n<td><p>Yes</p>\n</td>\n<td></td>\n</tr>\n<tr class=\"odd\">\n<td>Cosmos DB</td>\n<td>Yes</td>\n<td></td>\n<td>Yes</td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>Couchbase</td>\n<td></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr class=\"odd\">\n<td>Riak</td>\n<td>Yes</td>\n<td></td>\n<td><p>Yes</p>\n</td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>VoltDB/H-Store</td>\n<td></td>\n<td>Yes</td>\n<td></td>\n<td>Yes</td>\n</tr>\n<tr class=\"odd\">\n<td>Megastore</td>\n<td></td>\n<td>Yes</td>\n<td></td>\n<td>Yes</td>\n</tr>\n<tr class=\"even\">\n<td>BigTable/HBase</td>\n<td></td>\n<td>Yes</td>\n<td></td>\n<td>Yes</td>\n</tr>\n<tr class=\"odd\">\n<td>MySQL Cluster</td>\n<td></td>\n<td>Yes</td>\n<td></td>\n<td>Yes</td>\n</tr>\n<tr class=\"even\">\n<td>MongoDB</td>\n<td>Yes</td>\n<td></td>\n<td></td>\n<td>Yes</td>\n</tr>\n<tr class=\"odd\">\n<td>PNUTS</td>\n<td></td>\n<td>Yes</td>\n<td>Yes</td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>Hazelcast IMDG</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr class=\"odd\">\n<td>FaunaDB</td>\n<td></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n</tbody>\n</table>\n\n[PACELC theorem - Wikipedia](https://en.wikipedia.org/wiki/PACELC_theorem)\n[CAP Twelve Years Later: How the \"Rules\" Have Changed](https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed)\n\n"},{"fields":{"slug":"/Computer-Science/Distributed-System/Clocks/","title":"Clocks"},"frontmatter":{"draft":false},"rawBody":"# Clocks\n\nCreated: 2020-03-17 09:29:04 +0500\n\nModified: 2020-03-18 18:49:53 +0500\n\n---\n\n**Atomic Broadcast**\n\nIn[fault-tolerant](https://en.m.wikipedia.org/wiki/Fault-tolerant)[distributed computing](https://en.m.wikipedia.org/wiki/Distributed_systems), anatomic broadcastortotal order broadcastis a[broadcast](https://en.m.wikipedia.org/wiki/Broadcasting_(networking))where all correct processes in a system of multiple processes receive the same set of messages in the same order; that is, the same sequence of messages.The broadcast is termed \"[atomic](https://en.m.wikipedia.org/wiki/Atomic_(computer_science))\" because it either eventually completes correctly at all participants, or all participants abort without side effects. Atomic broadcasts are an important distributed computing primitive.\n**Total Order and Partial Order, Causal ordering**\n\nWhen events in a system follow a total order, then every event in that system has a specific order in which it occurred. In other words, when we know exactly wheneachof the events occurred, we know the total order of all the events in a system.\nIn apartial order, we can't be sure of the exact order ofallthe events in the system. Instead, all we can be sure about is the order of events that are reliant upon one another.\nIn a distributed system, we mostly deal with partially ordered events, simply because individual nodes can be sure about how to order local events, but they can't always be sure about how to order events that are happening on a different node. A node in our distributed system will send messages out to other nodes, who can't necessarily be sure of when they occurred. Similarly a node in our system will receive its own set of incoming messages, and it can't be sure of what time those messages were sent, either!\nSo how do we reconcile this lack of knowledge throughout our system? Well, it involves some reframing of how we think about time...and whether we even need it at all (?!).\nThe answer to this mystery is**causal ordering**, which helps us order events not based on thetimethat they occurred, but rather, based on cause and effect. Causal ordering reframes how we think about events. If we can just figure out which events causeotherevents, we can come up with a loose ordering of how those events occurred.\n<https://medium.com/baseds/ordering-distributed-events-29c1dd9d1eff>\n\n## Lamport Timestamps**\n\nThe algorithm of**Lamport timestamps**is a simple algorithm used to determine the order of events in a[distributed computer system](https://en.wikipedia.org/wiki/Distributed_computer_system). As different nodes or processes will typically not be perfectly synchronized, this algorithm is used to provide a[partial ordering](https://en.wikipedia.org/wiki/Partially_ordered_set)of events with minimal overhead, and conceptually provide a starting point for the more advanced[vector clock](https://en.wikipedia.org/wiki/Vector_clock)method. They are named after their creator,[Leslie Lamport](https://en.wikipedia.org/wiki/Leslie_Lamport).\nLamport's solution is to shift our thinking. He presents a novel idea: we don't actually need to think about causality in the context of total ordering to start. Instead, he says that we can start with a partial ordering of events, and then just deal with figuring out which events happened before other events. Once we figure out a partial ordering, we can turn it into a consistent total ordering.\nLamport's logical clocks allow us to shift from happened **when** to happened **before**\n\nwe need to shift from thinking about**when**an event happened to what the event happened**before**\nThe idea of one event happening before another is central to Lamport's paper. He uses theâ†’shorthand notation to indicate the**happens before**relationship, or the fact that one event happened before another. For example, if we know that one event,a, happened before another event,b, then we can say thataâ†’b, orahappened beforeb.\n<https://en.wikipedia.org/wiki/Lamport_timestamps>\n\n<https://medium.com/baseds/logical-time-and-lamport-clocks-part-1-d0317e407112>\n\n## Logical Clock**\n\nAlogical clockis a mechanism for capturing chronological and causal relationships in a[distributed system](https://en.wikipedia.org/wiki/Distributed_system). Distributed systems may have no physically synchronous global clock, so a logical clock allows global ordering on events from different processes in such systems.\nIn logical clock systems each process has two data structures:*logical local time*and*logical global time*. Logical local time is used by the process to mark its own events, and logical global time is the local information about global time. A special protocol is used to update logical local time after each local event, and logical global time when processes exchange data.\nLamport suggests using something different from the typical physical clock that we all think of. Instead of using each process's physical clock to track the order of events, we can instead use acounter. The counter can start with an initial time (like 0), and we can treat that counter as the processes own local clock.\nLamport continues with this idea by proposing that, not only will every process within a distributed system have its own counter clock, but eacheventthat is recorded on a process should also have avalueon that process's local clock. Furthermore, the value of each of these events on the clock must mirror any happened before relationships. For example, if eventaâ†’ b, then the clock time for when eventaoccurred must be less than the clock time for whenever eventboccurred; in other words,clock(a) < clock(b).\nBy using basic counters instead of physical clocks, Lamport simplifies clocks into something a little easier to deal with. These counter clocks are called logical clocks. Alogical clockis quite different from a physical clock in that there is no central notion of time, and the clock is just a counter that increments based on events in the system.\n![](media/Clocks-image1.jpeg)\n<https://en.wikipedia.org/wiki/Logical_clock>\n\n<https://medium.com/baseds/ticking-clocks-in-a-distributed-system-ef2aa4df07a3>\n\n<https://medium.com/baseds/logical-time-and-lamport-clocks-part-2-272c097dcdda>\n\n## Interval Tree Clocks: A Logical Clock for Dynamic Systems**\n\n<https://github.com/catwell/itc.lua>\n\n## Leap Second**\n\nBefore we dive into the details of our NTP service, we need to look at a phenomenon called a leap second. Because of the Earth's rotation irregularities, we occasionally need to add or remove a second from time, or a[leap second](https://caps.gsfc.nasa.gov/simpson/time/leapseconds.html). For humans, adding or removing a second creates an almost unnoticeable hiccup when watching a clock. Servers, however, can miss a ton of transactions or events or experience a serious software malfunction when they expect time to go forward continuously. One of the most popular approaches for addressing that is to \"[smear\" the leap second](https://docs.ntpsec.org/latest/leapsmear.html), which means to change the time in very small increments across multiple hours.\n\n"},{"fields":{"slug":"/Computer-Science/Distributed-System/Consensus-Protocols/","title":"Consensus Protocols"},"frontmatter":{"draft":false},"rawBody":"# Consensus Protocols\n\nCreated: 2019-06-24 06:31:57 +0500\n\nModified: 2022-03-09 22:19:23 +0500\n\n---\n\nThere are a number of ways we can go about replicating the log data. Broadly speaking, we can group the techniques into two different categories:\n**Gossip/Multicast Protocols**\n\nThese tend to be eventually consistent and/or stochastic.\n-   Epidemic broadcast trees\n-   Bimodal multicast\n-   SWIM\n-   HyParView\n-   NeEM\n**Consensus Protocols**\n\nThese tend to favor strong consistency over availability.\n-   2PC/3PC\n-   Paxos\n-   Raft\n-   Zab (Zookeeper atomic broadcast)\n-   Chain replication\n**Paxos - Consensus over distributed hosts**\n\nEx - doing a leader election among a distributed host.\n\nMulti-paxos - models the log as a series of consensus problems, one for each slot in the log\n[Paxos](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)is the original distributed consensus algorithm, with modified versions used by[Chubby](https://static.googleusercontent.com/media/research.google.com/en/archive/chubby-osdi06.pdf)and many others. (Zookeeper's[ZAB](https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zab+vs.+Paxos)is similar to Paxos as well.)\n<https://lamport.azurewebsites.net/tla/paxos-algorithm.html>\n\n[Paxos lecture (Raft user study)](https://www.youtube.com/watch?v=JEpsBg0AO6o)\n\n<https://engineering.fb.com/2022/03/07/core-data/augmenting-flexible-paxos-logdevice>\n\n## Raft - Consensus Algorithm**\n\n[RAFT](https://raft.github.io/raft.pdf)is a much simpler consensus algorithm.\nUsed by -\n-   Consul\n-   Used by etcd (distributed key value store)\n<https://raft.github.io>\n\n<http://thesecretlivesofdata.com/raft>\n\n## SWIM**\n\n[SWIM](https://www.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf)is a distributed gossip protocol for group membership (e.g. for determining members of a caching ring, etc)\n**Two-Phase Commit Protocol**\n\nIn[transaction processing](https://en.wikipedia.org/wiki/Transaction_processing),[databases](https://en.wikipedia.org/wiki/Database), and[computer networking](https://en.wikipedia.org/wiki/Computer_networking), thetwo-phase commit protocol(2PC) is a type of[atomic commitment protocol](https://en.wikipedia.org/wiki/Atomic_commit)(ACP). It is a[distributed algorithm](https://en.wikipedia.org/wiki/Distributed_algorithm)that coordinates all the processes that participate in a[distributed atomic transaction](https://en.wikipedia.org/wiki/Distributed_transaction)on whether to[commit](https://en.wikipedia.org/wiki/Commit_(data_management))orabort(roll back) the transaction (it is a specialized type of[consensus](https://en.wikipedia.org/wiki/Consensus_(computer_science))protocol). The protocol achieves its goal even in many cases of temporary system failure (involving either process, network node, communication, etc. failures), and is thus widely used.However, it is not resilient to all possible failure configurations, and in rare cases, manual intervention is needed to remedy an outcome. To accommodate recovery from failure (automatic in most cases) the protocol's participants use[logging](https://en.wikipedia.org/wiki/Server_log)of the protocol's states. Log records, which are typically slow to generate but survive failures, are used by the protocol's[recovery procedures](https://en.wikipedia.org/wiki/Recovery_procedure). Many protocol variants exist that primarily differ in logging strategies and recovery mechanisms. Though usually intended to be used infrequently, recovery procedures compose a substantial portion of the protocol, due to many possible failure scenarios to be considered and supported by the protocol.\nIn a \"normal execution\" of any single[distributed transaction](https://en.wikipedia.org/wiki/Distributed_transaction)(i.e., when no failure occurs, which is typically the most frequent situation), the protocol consists of two phases:\n\na.  **Thecommit-request phase(orvoting phase / prepare phase)**, in which acoordinatorprocess attempts to prepare all the transaction's participating processes (namedparticipants,cohorts, orworkers) to take the necessary steps for either committing or aborting the transaction and tovote, either \"Yes\": commit (if the transaction participant's local portion execution has ended properly), or \"No\": abort (if a problem has been detected with the local portion), and\n\nb.  **Thecommit phase**, in which, based onvotingof the participants, the coordinator decides whether to commit (only ifallhave voted \"Yes\") or abort the transaction (otherwise), and notifies the result to all the participants. The participants then follow with the needed actions (commit or abort) with their local transactional resources (also calledrecoverable resources; e.g., database data) and their respective portions in the transaction's other output (if applicable).\n![9 ã‚’ ã® æœˆ 0 â€² ã‚¹ 0 â€² V 0 ãƒ• ãƒ ã¿ ãƒ ã ãƒ¼ ã‚¹ åŠ  5 ã® ? ã‚¢ ã‚¢ å· ã‚£ ãƒ¼ 0 ãƒ• çŸ¥ >IO d æœˆ 0 ãƒ• äºº å± ãƒ¼ ã„ ã„ VS ãƒ¬ ã€‚ ç”² äºº ã€‚ - ã® } , å· â…¢ ã® ãƒ• ç‰© % è¨€ ](media/Consensus-Protocols-image1.png)\n![](media/Consensus-Protocols-image2.png)\n\nTwo-phase commit is a blocking protocol. The coordinator blocks waiting for votes from its cohorts, and cohorts block waiting for a commit/rollback message from the coordinator. Unfortunately, this means 2PC can, in some circumstances, result in a deadlock, e.g. the coordinator dies while cohorts wait or a cohort dies while the coordinator waits. Another problematic scenario is when a coordinator and cohort simultaneously fail. Even if another coordinatortakes its place, it won't be able to determine whether to commit or rollback.\n<https://en.wikipedia.org/wiki/Two-phase_commit_protocol>\n\n[**https://bravenewgeek.com/understanding-consensus/**](https://bravenewgeek.com/understanding-consensus/)\n**3 Phase Commit**\n\n![g Compu/ f (O - oydu Can VoHcÄ‚ (OrMWf,â€¢ DO (O 1 2 ](media/Consensus-Protocols-image3.png)\n![](media/Consensus-Protocols-image4.png)\n\nThree-phase commit (3PC) is designed to solve the problems identified in two-phase by implementing a non-blocking protocol with an added \"prepare\" phase. Like 2PC, it relies on a coordinator which relays messages to its cohorts.\nUnlike 2PC, cohorts do not executea transaction during the voting phase. Rather, they simply indicate if they are prepared to perform the transaction. If cohorts timeout during this phase or there is one or more \"no\" vote, the transaction is aborted. If the vote is unanimously \"yes,\" the coordinator moves on to the \"prepare\" phase, sending a message to its cohorts to acknowledge the transaction will be committed. Again, if an ack times out, the transactionis aborted. Once all cohorts have acknowledged the commit, we are guaranteed to be in a state where*all*cohorts have agreed to commit. At this point, if the commit message from the coordinator is not received in the third phase, the cohort will go ahead and commit anyway. This solves the deadlocking problems described earlier. However, 3PC is still susceptible to network partitions. If a partition occurs, the coordinator will timeout and progress will not be made.\n<https://bravenewgeek.com/understanding-consensus>\n\n## State Replication**\n\nProtocols like[Raft](https://ramcloud.stanford.edu/raft.pdf),[Paxos](http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf), and[Zab](http://web.stanford.edu/class/cs347/reading/zab.pdf)are popular and widely used solutions to the problem of distributed consensus. These implement state replication or primary-backup using leaders, quorums, and replicas of operation logs or incremental delta states.\n\n![c set x Client Quorum c set x = 5 Client ](media/Consensus-Protocols-image5.png)\n\nThese protocols workby electing a leader (coordinator). Like multi-phase commit, all changes must go through thatleader, who then broadcasts the changes to the group. Changes occur by appending a log entry, and each node has its own log replica. Where multi-phase commit falls down in the face of network partitions, these protocols are able to continue working by relying on a quorum (majority). The leader commits the change once the quorum has acknowledged it.\nThe use of quorums provide partition tolerance by fencing minority partitions while the majority continues to operate.This is the pessimistic approach to solving split-brain, so it comes with an inherent availability trade-off. This problem is mitigated by the fact that each node hosts a replicated state machine which can be rebuilt or reconciled once the partition is healed.\nGoogle relies on Paxos for its high-replication datastore in App Engine as well as its[Chubby lock service](http://static.googleusercontent.com/media/research.google.com/en/us/archive/chubby-osdi06.pdf). The distributed key-value store[etcd](https://github.com/coreos/etcd)uses Raft to manage highly available replicated logs. Zab, which differentiates itself from the former by implementing a primary-backup protocol, was designed for the[ZooKeeper](http://zookeeper.apache.org/)coordination service. In general, there are several different implementations of these protocols, such as the[Go implementation](https://github.com/goraft/raft)of Raft.\n<https://bravenewgeek.com/understanding-consensus>\n\n## SAGA (Asynchronous Distributed Transactions)**\n\n**Context**\n\nYou have applied the[Database per Service](https://microservices.io/patterns/data/database-per-service.html)pattern. Each service has its own database. Some business transactions, however, span multiple service so you need a mechanism to implement transactions that span services. For example, let's imagine that you are building an e-commerce store where customers have a credit limit. The application must ensure that a new order will not exceed the customer's credit limit. Since Orders and Customers are in different databases owned by different services the application cannot simply use a local ACID transaction.\n**Solution**\n\nImplement each business transaction that spans multiple services is a saga. A saga is a sequence of local transactions. Each local transaction updates the database and publishes a message or event to trigger the next local transaction in the saga. If a local transaction fails because it violates a business rule then the saga executes a series of compensating transactions that undo the changes that were made by the preceding local transactions.\n**Types of SAGA Implementation**\n\n1.  Choreography - Event based\n\nEach local transaction publishes domain events that trigger local transactions in other services\n![Choreography ORDER_CREATE O ORDER_CREATED ORDER_PAID ORDER_PREPARED O Order service Payment service Restaurant service ORDER_DELIVERED Delivery service ](media/Consensus-Protocols-image6.png)\n2.  Orchestration - Command based\n\nAn orchestrator (object) tells the participants what local transactions to execute\n![ORDER_CREATE Order service ORDER_CREATED Orchestration Restaurant Payment service Delivery service OR ER_PAID Orchestrator service service ORPZR_PREPARED ORDER_DELIVERED ](media/Consensus-Protocols-image7.png)\nYoutube - [SAGA | Microservices Architecture Patterns | Tech Primers](https://www.youtube.com/watch?v=WnZ7IcaN_JA)\n\nYoutube - [Do you know Distributed transactions?](https://www.youtube.com/watch?v=S4FnmSeRpAY)\n\n<https://microservices.io/patterns/data/saga.html>\n\n## Gossip / Gossiping**\n\nAgossip protocolis a procedure or process of computer peer-to-peer communication that is based on the way that[epidemics](https://en.wikipedia.org/wiki/Epidemic)spread. Some[distributed systems](https://en.wikipedia.org/wiki/Distributed_computing)use peer-to-peer gossip to ensure that data is routed to all members of an ad-hoc network. Some ad-hoc networks have no central registry and the only way to spread common data is to rely to each member to pass it along to their neighbors.\n\nThe termepidemic protocolis sometimes used as a synonym for a gossip protocol, because gossip spreads information in a manner similar to the spread of a[virus](https://en.wikipedia.org/wiki/Virus)in a biological community.\n<https://en.wikipedia.org/wiki/Gossip_protocol>\n\n## FLP Impossibility**\n\nStates that reaching a multi-party consensus in a asynchronous system is not possible and, in order for consensus to be reachable, we need**Failure Detectors**.\nIn order for processes to agree, several invariants have to be persevered:\n-   value that's being agreed on has to be \"proposed\" by one of the participants\n-   all active (non-crashed) participants have to decide on the value\n-   value they eventually decide on has to be the same for all processes\nIn a[paper](https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf)by Fisher, Lynch and Paterson, famously known asFLP Impossibility Problem(derived from the first letters of authors' last names), authors discuss a weak form of consensus in which processes start with an initial value and have to achieve an agreement on a new value. This new value has to be the same for all non-faulty processes.\nPaper concludes that in anasynchronous system, no consensus protocol can be totally correct in presence of even a single fault. If we do not consider an upper time bound for process to complete the algorithm steps and if process failures can't be reliably detected, FLP paper shows that there's no deterministic algorithm to reach a consensus.\nHowever, FLP proof does not mean we have to pack our things and go home, as reaching consensus is not possible. It only means that it can't always be reached in bounded time. In practice, systems exhibit partial synchronicity, which puts partially synchronous system between the cases of asynchronous and synchronous ones. Nancy Lynch, one of the FLP proof authors, has later authored[Consensus in the Presence of Partial Synchrony](http://groups.csail.mit.edu/tds/papers/Lynch/jacm88.pdf)paper, where several partially synchronous models are discussed, one of them holding timing assumptions that are not known in advance and the other one, where timing assumptions are known, but start holding up at an unknown time.\n<https://medium.com/databasss/on-ways-to-agree-part-1-links-and-flp-impossibility-f6bd8a6a0980>\n\n"},{"fields":{"slug":"/Computer-Science/Distributed-System/Consistency/","title":"Consistency"},"frontmatter":{"draft":false},"rawBody":"# Consistency\n\nCreated: 2018-07-05 17:05:50 +0500\n\nModified: 2019-12-31 21:33:04 +0500\n\n---\n\nWhen we're talking about *consistency* in distributed systems, we are referring to the concept that you will have some data distributed in different nodes of your system, and each one of those might have a copy of your data. If it's a read-only dataset, any client connecting to any of the nodes will always receive the same data, so there is no consistency problem. When it comes to read-write datasets, some conflicts can arise. Each one of the nodes can update its own copy of the data, so if a client connects to different nodes in your system, it might receive different values for the same data.\n**Consistency levels**\n\nConsistency levels from Werner Vogel's[Eventually Consistent](http://delivery.acm.org/10.1145/1440000/1435432/p40-vogels.pdf?ip=67.170.235.99&id=1435432&acc=OPEN&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1565542564_c4a7ccbc0971346102d83294a77ed4a2):\n-   **Strong consistency-** after an update completes, all further operations correctly use the new value\n-   **Weak conssistency-** operations after an update completes may not correctly use the new value\n-   **Eventual consistency-** if no new updates are made to an object, at some point in future it will return correct value\n-   **Inconsistency window-** the period between an update and the system guaranteeing the correct value will be returned\n-   **Casual consistency-** once an updated version is communicated to a process, it is guaranteed to not use older versions\n-   **Monotonic read consistency-** similar to casual consistency, but from perspective of receiving process: once a process has seen a version, it will never use previous versions\n-   **Monotonic write consistency-** writes within a process are serialized\n-   **Read-your-writes consistency-** reads after a write return the updated version\n-   **Session consistency-** reads after writes are always correct within a given session\n-   **Read level,write level**and**replica level**are the number of nodes in a distributed storage system involved in the reading, writing and replication of a piece of data. The[Dynamo paper](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf)describes this approach in some detail, and it's used heavily by both Cassandra and MongoDB as well (among many others)\n| Strong Consistency   | See all previous writes.            |\n|----------------------|-------------------------------------|\n| Eventual Consistency | See subset of previous writes.      |\n| Consistent Prefix    | See initial sequence of writes.     |\n| Bounded Staleness    | See all \"old\" writes.               |\n| Monotonic Reads      | See increasing subset of writes.    |\n| Read My Writes       | See all writes performed by reader. |\n1.  **Strong consistency** is particularly easy to understand. It guarantees that a read operation returns the value that was last written for a given object. If write operations can modify or extend portions of a data object, such as appending data to a log, then the read returns the result of applying all writes to that object. In other words, a read observes the effects of all previously completed writes.\n2.  **Eventual consistency** is the weakest of the guarantees, meaning that it allows the greatest set of possible return values. For whole-object writes, an eventually consistent read can return any value for a data object that was written in the past. More generally, such a read can return results from a replica that has received an arbitrary subset of the writes to the data object being read.\n3.  By requesting a **consistent prefix,** a reader is guaranteed to observe an ordered sequence of writes starting with the first write to a data object. For example, the read may be answered by a replica that receives writes in order from a master replica but has not yet received an unbounded number of recent writes. In other words, the reader sees a version of the data store that existed at the master at some time in the past. This is similar to the \"snapshot isolation\" consistency offered by many database management systems.\n4.  **Bounded staleness** ensures that read results are not too out-of-date. Typically, staleness is defined by a time period T, say 5 minutes. The storage system guarantees that a read operation will return any values written more than T minutes ago or more recently written values. Alternative, some systems have defined staleness in terms of the number of missing writes or even the amount of inaccuracy in a data value. I find that time-bounded staleness is the most natural concept for application developers.\n5.  **Monotonic Reads** is a property that applies to a sequence of read operations that are performed by a given storage system client. As such, it is often called a \"session guarantee.\" With monotonic reads, a client can read arbitrarily stale data, as with eventual consistency, but is guaranteed to observe a data store that is increasingly up-to-date over time. In particular, if the client issues a read operation and then later issues another read to the same object(s), the second read will return the same value(s) or the results of later writes.\n6.  **Read My Writes** is a property that also applies to a sequence of operations performed by a single client. It guarantees that the effects of all writes that were performed by the client are visible to the client's subsequent reads. If a client writes a new value for a data object and then reads this object, the read will return the value that was last written by the client (or some other value that was later written by a different client). (Note: In other papers, this has been called \"Read Your Writes,\" but I have chosen to rename it to more accurately describe the guarantee from the client's viewpoint.)\n**Convergence**\n\nConvergence is the state in which all the nodes of the system have eventually achieved consistency.-   **Eventual Consistency (NoSQL, Higher Availability)**\n\n**Read will see some write and eventually it will see the latest write**\n\nEventual consistency is a model in distributed computing that guarantees that given an update to a data item in your dataset, *eventually*, at a point in the future, all access to this data item in any node will return the same value. Because each one of the nodes can update its own copy of the data item, if two or more nodes modify the same data item, you will have a conflict. ***Conflict resolution algorithms*** are then required to achieve convergence.\nOne special case for eventual consistency is when you have your data distributed in multiple nodes, but only one of them is allowed to make updates to the data. If one node is the canonical source of information for the updates, you won't have conflicts in the other nodes as long as they are able to apply the updates in the exact same order as the canonical information source. You add the possibility of write unavailability, but that's a bearable trade-off for some business use cases.\n**Eventually Consistent Resolution Strategies (conflict resolution algorithms)**\n\n1.  **LWW (last write wins)**\n\nIf we are able to add a synchronized timestamp or counter to all of our updates, the last update always wins the conflict.\n\nA last write wins (or all other writes lose) strategy is one way to converge on a single version for all copies of a value in a distributed system. But what are you losing by dropping those other versions? Ignoring clock skew for a moment, you can't even be sure the last write saw earlier writes from other clients. This is data loss.\n2.  **Semantic Resolution**\n\nAnother strategy is storing multiple versions of the datum and using semantic resolution. Use the semantics of the domain to define a path to a single value. An example being union operation that takes two divergent copies of a value and creates a single consistent version. But this passes the pain to the developer to build ad-hoc resolution strategies for the use case at hand.\n3.  **CRDTs**\n\nWhat if someone built a series of reusable data types for you? Convergent Replicated Data Types are those data types, and offer a principled approach to eventually consistent data modelling. Some very cool maths ensures these defined data types always converge to a single correct value.\n\nCRDTs can be considered one of the key building blocks of a distributed system, enabling strong eventual consistency and a highly available, low latency application.-   **Strong Consistency (SQL)**\n\n**Our reads will read the latest writes**\n\nStrong consistency is a model that is most familiar to database developers, given that it resembles the traditional transaction model with its Atomicity, Consistency, Isolation, and Durability (ACID) properties. In this model, any update in any node requires that all nodes agree on the new value before making it visible for client reads. It sounds naively simple, but it also introduces the requireâ€ ment of blocking all the nodes until they converge. It might be espeâ€ cially problematic depending on network latency and throughput.-   **External Consistency**\n\nUnder external consistency, the system behaves as if all transactions were executed sequentially, even though Cloud Spanner actually runs them across multiple servers (and possibly in multiple datacenters) for higher performance and availability.\n<https://cloud.google.com/spanner/docs/true-time-external-consistency>\n![Consistency Solutions Faster reads and writes More consistency Eventua Big Data Computing Strong e.g., Sequential Design of Key-Value Stores ](media/Consistency-image1.png)\n\n![Eventual Consistency â€¢ Cassandra offers Eventual Consistency â€¢ If writes to a key stop, all replicas of key will converge Originally from Amazon's Dynamo and Linkedln's Voldemort systems Faster reads and writes More consistency Ev tual Big Data Computing Strong (e.g., Sequential) Design of Key-Value Stores ](media/Consistency-image2.png)\n\n![Newer Consistency Models â€¢ Striving towards strong consistency â€¢ While still trying to maintain high availability and partition-tolerance Red-Blue Causal Probabilistic er-key sequentia Eventua CRDTs Big Data Computing trong (e.g., equenti I) Design of Key-Value Stores ](media/Consistency-image3.png)\n\n![Newer Consistency Models (Contd.) â€¢ Per-key sequential: Per key, all operations have a global order â€¢ CRDTs (Commutative Replicated Data Types): Data structures for which commutated writes give same result [INRIA, France] E.g., value == int, and only op allowed is +1 Effectively, servers don't need to worry about consistency Red-Blue Causal Probabilistic Per-key sequential Eventual Big Data Computing CRDTs Strong (e.g., Sequential) Design of Key-Value Stores ](media/Consistency-image4.png)\n\n![Newer Consistency Models (Contd.) â€¢ Red-blue Consistency: Rewrite client transactions to separate operations into red operations vs. blue operations [MPI-SWS Germany] Blue operations can be executed (commutated) in any order across DCs Red operations need to be executed in the same order at each DC Red-Blue Causal robabilistic Per-key sequential Eventual Big Data Computing CRDTs Strong (e.g., Sequential) Design of Key-Value Stores ](media/Consistency-image5.png)\n\n![Newer Consistency Models (Contd.) Causal Consistency: Reads must respect partial order based on informatâ€¢ w [Princeton, CMU W(KI, ) Client W(K2, 55) Client (Klhr turns Client Time W(KI, 2b 361) may return â€¢s must return 33 22 or 33 R(K2) returns 55 Causality, not messages Red-Blue Causal Probabilistic Per-key sequential Eventual CRDTs Big Data Computing Strong (e.g., Sequential) Design of Key-Value Stores ](media/Consistency-image6.png)\n\n![Which Consistency Model should you use? â€¢ Use the lowest consistency (to the left) consistency model that is \"correct\" for your application â€¢ Gets you fastest availability Red-Blue Causal Probabilistic Per-key sequential Eventual Big Data Computing CRDTs Strong (e.g., Sequential) Design of Key-Value Stores ](media/Consistency-image7.png)\n\nStrong Consistency Models\n-   Linearizability: Each operation by a client is visible (or available) instantaneously to all other clients\n-   Sequential Consistency [Lamport]:\n    -   \"... the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.\n    -   After the fact, find a reasonable ordering of the operations (can reorder operations) that obeys sanity (consistency) at all clients, and across clients\n-   Transaction ACID properties, example: newer key-value / NoSQL stores called as NewSQL (NoSQL + ACID)\n    -   Hyperdex [Cornell]\n    -   Spanner [Spanner]\n    -   Transaction chains [Microsoft Reasearch]\nConclusion\n-   Traditional databases (RDMSs) work with strong consistency, and offer ACID\n-   Modern workloads don't need such strong guarantees, but do need fast response times (availability)\n-   Unfortunately, CAP theorem\n-   Key-value ? NoSQL systems offer BASE\n    -   Basically Available Soft-state Eventual Consistency\n    -   Eventual consistency, and a variety of other consistency models striving towards strong consistency\n**References**\n\n<https://www.dotconferences.com/2015/06/dan-brown-convergent-replicated-data-types>\n\n"},{"fields":{"slug":"/Computer-Science/Distributed-System/Designing-Distributed-Systems/","title":"Designing Distributed Systems"},"frontmatter":{"draft":false},"rawBody":"# Designing Distributed Systems\n\nCreated: 2018-10-11 11:28:16 +0500\n\nModified: 2020-01-21 16:20:18 +0500\n\n---\n\n# Designing Distributed Systems\n\n#### *Patterns and Paradigms for Scalable, Reliable Services by Brendon Burns (cofounder of Kubernetes)*\n-   Both an object and interface for expressing core distributed system patterns\n-   side-car, adapter and ambassador patterns to split application into a group of containers on a single machine\n-   Loosely coupled multi-node distributed patterns for replication, scaling, and communication between the components\n-   Work queues, event-based processing and coordinated workflows# Event-Driven Batch Processing\n"},{"fields":{"slug":"/Computer-Science/Distributed-System/Distributed-Logging/","title":"Distributed Logging"},"frontmatter":{"draft":false},"rawBody":"# Distributed Logging\n\nCreated: 2018-09-13 18:44:15 +0500\n\nModified: 2019-10-14 11:32:39 +0500\n\n---\n\n**Logs -**\n\nA log is perhaps the simplest possible storage abstraction. It is an append-only, totally-ordered sequence of records ordered by time. It looks like this:\n![1st Record Next Record Written ](media/Distributed-Logging-image1.png)\nDatabase people generally differentiate between*physical*and*logical*logging. **Physical logging** means logging the contents of each row that is changed. **Logical logging** means logging not the changed rows but the SQL commands that lead to the row changes (the insert, update, and delete statements).\nEvery programmer is familiar with another definition of logging---the unstructured error messages or trace info an application might write out to a local file using syslog or log4j. For clarity I will call this **\"application logging\".**\nLogging can be used for -\n\n1.  **Data Integration---**Making all of an organization's data easily available in all its storage and processing systems.\n\n2.  **Real-time data processing---**Computing derived data streams.\n\n3.  **Distributed system design---**How practical systems can by simplified with a log-centric design.\n**Logs Type**\n\n1.  Common log format\n\n2.  Combined log format - (Common log format + referrer + user-agent)\n**Basics of logging**\n\n1.  Aggregate your logs\n\n2.  Trace the flow\n\n3.  Explicitly log control flow events\n\n4.  Log metrics\n\n5.  Structured log format\n**Logging Best Practices**\n-   Who was using the system when it failed?\n-   Where in the code did the application fail?\n-   What was the system doing when it failed?\n-   When did the failure occur?\n-   Why did the application fail?\n**Common Log Format**\n\nThe format is extended by the[Combined Log Format](https://en.wikipedia.org/w/index.php?title=Combined_Log_Format&action=edit&redlink=1)with[referrer](https://en.wikipedia.org/wiki/Referrer)and[user-agent](https://en.wikipedia.org/wiki/User-agent)fields.\n**Example**\n\n127.0.0.1 user-identifier frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif HTTP/1.0\" 200 2326\nA \"-\" in a field indicates missing data.-   127.0.0.1is the IP address of the client (remote host) which made the request to the server.\n-   user-identifieris the[RFC 1413](https://tools.ietf.org/html/rfc1413)[identity](https://en.wikipedia.org/wiki/Ident_Protocol)of the client. Usually \"-\".\n-   frankis the userid of the person requesting the document. Usually \"-\" unless .htaccess has requested authentication.\n-   [10/Oct/2000:13:55:36 -0700]is the date, time, and time zone that the request was received, by default in[strftime](https://en.wikipedia.org/wiki/Strftime)format%d/%b/%Y:%H:%M:%S %z.\n-   \"GET /apache_pb.gif HTTP/1.0\"is the request line from the client. The methodGET,/apache_pb.gifthe resource requested, andHTTP/1.0the[HTTP protocol](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol).\n-   200is the[HTTP status code](https://en.wikipedia.org/wiki/HTTP_status_code)returned to the client. 2xx is a successful response, 3xx a redirection, 4xx a client error, and 5xx a server error.\n-   2326is the size of the object returned to the client, measured in[bytes](https://en.wikipedia.org/wiki/Byte).<https://en.wikipedia.org/wiki/Common_Log_Format>\n\n## Tools**\n\n1.  ELK\n**References**\n\n<https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying>\n\n<https://github.com/magnhaug/blogposts/blob/master/distributed-logging.md>\n\n<https://blog.treasuredata.com/blog/2016/08/03/distributed-logging-architecture-in-the-container-era>\n<https://devops.stackexchange.com/questions/422/how-to-have-multiple-log-streams-in-docker>\n\n"},{"fields":{"slug":"/Computer-Science/Distributed-System/Fallacies-and-Problems/","title":"Fallacies and Problems"},"frontmatter":{"draft":false},"rawBody":"# Fallacies and Problems\n\nCreated: 2019-06-27 17:04:31 +0500\n\nModified: 2020-03-26 01:22:03 +0500\n\n---\n\n**Fallacies of distributed computing**\n\nThe**fallacies of distributed computing**are a set of assertions made by[L Peter Deutsch](https://en.wikipedia.org/wiki/L_Peter_Deutsch) and others at[Sun Microsystems](https://en.wikipedia.org/wiki/Sun_Microsystems)describing false assumptions that[programmers](https://en.wikipedia.org/wiki/Programmer)new to [distributed](https://en.wikipedia.org/wiki/Distributed_computing) [applications](https://en.wikipedia.org/wiki/Application_software) invariably make.\nThe[fallacies](https://en.wikipedia.org/wiki/Fallacy)are:\n\n1.  The[network](https://en.wikipedia.org/wiki/Computer_network)is reliable.\n\n2.  [Latency](https://en.wikipedia.org/wiki/Latency_(engineering))is zero.\n\n3.  [Bandwidth](https://en.wikipedia.org/wiki/Throughput)is infinite.\n\n4.  The network is[secure](https://en.wikipedia.org/wiki/Computer_security).\n\n5.  [Topology](https://en.wikipedia.org/wiki/Network_topology)doesn't change.\n\n6.  There is one[administrator](https://en.wikipedia.org/wiki/Network_administrator).\n\n7.  Transport cost is zero.\n\n8.  The network is homogeneous.\nThe effects of the fallacies\n\n1.  Software applications are written with little error-handling on networking errors. During a network outage, such applications may stall or infinitely wait for an answer packet, permanently consuming memory or other resources. When the failed network becomes available, those applications may also fail to retry any stalled operations or require a (manual) restart.\n\n2.  Ignorance of network latency, and of the[packet loss](https://en.wikipedia.org/wiki/Packet_loss)it can cause, induces application- and transport-layer developers to allow unbounded traffic, greatly increasing dropped packets and wasting bandwidth.\n\n3.  Ignorance of bandwidth limits on the part of traffic senders can result in bottlenecks.\n\n4.  Complacency regarding network security results in being blindsided by malicious users and programs that continually adapt to security measures.\n\n5.  Changes in[network topology](https://en.wikipedia.org/wiki/Network_topology)can have effects on both bandwidth and latency issues, and therefore can have similar problems.\n\n6.  Multiple administrators, as with[subnets](https://en.wikipedia.org/wiki/Subnetwork)for rival companies, may institute conflicting policies of which senders of network traffic must be aware in order to complete their desired paths.\n\n7.  The \"hidden\" costs of building and maintaining a network or subnet are non-negligible and must consequently be noted in budgets to avoid vast shortfalls.\n\n8.  If a system assumes a homogeneous network, then it can lead to the same problems that result from the first three fallacies.\n<https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing>\n\n<https://medium.com/baseds/foraging-for-the-fallacies-of-distributed-computing-part-1-1b35c3b85b53>\n\n<https://medium.com/baseds/foraging-for-the-fallacies-of-distributed-computing-part-2-b8ff29beed56>\n\n## Byzantine Generals' Problem / Two Generals' Problem**\n\n![COMMANDER \"he said 'retreatâ€¢\" Figure 1 \"attek\" LIEUTENANT LIEUTENANT LIEUTENANT The one with stripes is the traitor because of which a consensus cannot be reached COMMANDER *'he said â€¢retreat*\" Figure 2 \" retreat\" LIEUTENANT 2 ](media/Fallacies-and-Problems-image1.jpg)\n\nAll participating nodes have to agree upon every message that is transmitted between the nodes. If a group of nodes is corrupt or the message that they transmit is corrupt then still the network as a whole should not be affected by it and should resist this 'Attack'. In short, the network in its entirety has to agree upon every message transmitted in the network. This agreement is called as**consensus**.\n**Two Generals Problem**\n\n**Two Generalsproblem** tells us that two communicating processes will always be one step away from being certain that the other party has received a derivative acknowledgement.\nIt is related to the more general[Byzantine Generals](https://en.wikipedia.org/wiki/Byzantine_Generals)Problem and appears often in introductory classes about[computer networking](https://en.wikipedia.org/wiki/Computer_networking)(particularly with regard to the[Transmission Control Protocol](https://en.wikipedia.org/wiki/Transmission_Control_Protocol), where it shows that TCP can't guarantee state consistency between endpoints and why), though it applies to any type of two-party communication where failures of communication are possible.\n**Problem**\n\nImagine two armies, led by generals, preparing to attack a fortified city. Armies are located on the two sides of the city and can succeed in their siege only if their attack is synchronized. They can communicate by sending messengers and already have a devised attack plan. Now they only have to agree on the fact that they both will proceed with the attack, otherwise the attack can not succeed.\nGeneralAsends a messageMSG(attack at 7PM)stating that their army will proceed with the attack. Once messenger is dispatched,Adoesn't know whether messenger has arrived or no. GeneralB, upon receiving the message, has to send an acknowledgementACK(MSG(attack at 7PM)). However, messenger carrying this acknowledgement might get captured or fail to deliver, so nowBdoesn't have any way of knowing if the messenger has successfully delivered it. To be sure about it,Bhas to wait for a second-order acknowledgementACK(ACK(MSG(attack at 7PM)stating that A had received an acknowledgement for the acknowledgement.\n\n![GeneraI Ğ Fortified City General Ğ’ ](media/Fallacies-and-Problems-image2.png)\n\nNo amount of further confirmations can solve the problem, as the generals will be oneACKaway from knowing if they can safely proceed with the attack. Generals are doomed to wonder if the message carrying this last acknowledgment has reached the destination.\n<https://medium.com/all-things-ledger/the-byzantine-generals-problem-168553f31480>\n\n<https://en.wikipedia.org/wiki/Byzantine_fault_tolerance>\n\n<https://en.wikipedia.org/wiki/Quantum_Byzantine_agreement>\n\n<https://en.wikipedia.org/wiki/Two_Generals%27_Problem>\n\n[**https://bravenewgeek.com/understanding-consensus/**](https://bravenewgeek.com/understanding-consensus/)"},{"fields":{"slug":"/Computer-Science/Distributed-System/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-04-19 17:48:38 +0500\n\nModified: 2021-06-11 00:18:38 +0500\n\n---\n\nAndrew S. Tanenbaum - A Collection of independent computers that appears to its users as one computer.\n![Centralized Systems Legend Decentralized Systems Server connected Centralized to main central Server server Distributed Systems Individual nodes or terminals ](media/Intro-image1.jpg)-   In**Centralized Systems**there is one central authority or server and all the other nodes act like clients or entities who accept message and enact accordingly\n-   In**Decentralized Systems**there are multiple servers who receive messages from one central server. The individual nodes are connected to the secondary servers.However, in some systems, all servers can be of equal in hierarchy with no central server as well.\n-   In**Distributed systems**there is no central authority. Each node is connected to every other node and has the exact same authority. Of course, in terms of computing distributed systems the processing power of each node might vary to a huge extent.\n**Characteristics**\n-   The computers operate concurrently\n-   The computers fail independently\n-   The computers do not share a global clock\n**Main Topics**\n\n1.  Distributed Storage\n\nSingle Master Storage - Database lives in one server\n\nIn a typical system there are always more read than write.\n\nRelational database have reads cheaper than writes\n\nTo scale up we do following things -\n\n1.  Read Replication - Can cause inconsistency\n    -   Create a master database that handle all the writes and there will be multiple databases where data can be read from. Master will synchronize all databases with each other.\n\n2.  Sharding - Find a key and break-up the database into multiple databases using that key. (Like a-k, k-s, s-z)\n    -   Broke data modal\n    -   Cannot join across shards\n\n3.  Add index to the database\n\n4.  De-normalize\n\n5.  Consistent Hashing\n\n6.  CAP Theorem (can only have 2 of these 3)\n    -   Consistency\n    -   Availability\n    -   Partition Tolerance2.  Distributed Computation\n\n    a.  MapReduce\n\n        1.  Map (Tokenize words and count using key-value pairs)\n\n        2.  Shuffle (Get same words near each other)\n\n        3.  Reduce\n\n        4.  Hadoop\n\n        5.  Spark\n\n        6.  Kafka\n\n3.  Messaging\n\n    a.  Means of loosely coupling subsystems\n\n    b.  Messages consumed by subscribers\n\n    c.  Created by one or more producers\n\n    d.  Organized into topics\n\n    e.  Processed by brokers\n\n    f.  Usually persistent over the short term\n**Problems**\n\n1.  What if a topic gets too big for one computer?\n\n2.  What if one computer is not reliable enough?\n\n3.  How strongly can be guarantee delivery?\n\n**Solution - Apache Kafka**\n\n1.  Messaging broker\n\n2.  Message: an immutable array of bytes\n\n3.  Topic: a feed of messages\n\n4.  Producer: a process that publishes messages to a topic\n\n5.  Consumer: a single-threaded process that subscribes to a topic\n\n6.  Broker: one of the servers that comprises a cluster\n**Links**\n\n1.  **Fair-Loss Link**\n\nDoes not give any visibility in terms of delivery for a sender\n\n2.  **Stubborn Link**\n\nKeep re-transmitting messages indefinitely, hoping that eventually one of the messages will get delivered\n\n3.  **Perfect Link**\n\nEventually deliver messages and make sure this delivery is done at most once\n**Questions while designing a distributed system**\n\nThe following qualities are all important in the design of a realtime data system\n\n1.  Ease of use\n\nHow complex are the processing requirements? Is SQL enough? Or is a general-purpose procedural language (such as C++ or Java) essential? How fast can a user write, test, and deploy a new application?\n\n2.  Performance\n\nHow much latency is ok? Milliseconds? Seconds? Or minutes? How much throughput is required, per machine and in aggregate?\n\n3.  Fault-tolerance\n\nWhat kinds of failures are tolerated?What semantics are guaranteed for the number of times that data is processed or output? How does the system store and recover in-memory state?\n\n4.  Scalability\n\nCan data be sharded and resharded to pro-cess partitions of it in parallel? How easily can the system adapt to changes in volume, both up and down?Can it reprocess weeks worth of old data?\n\n5.  Correctness\n\nAre ACID guarantees required? Must all data that is sent to an entry point be processed and appear in results at the exit point?\n**Principles of Distributed System Design**\n\n[From the March 14, 2006, press release for the S3 (Simple Storage Service) launch](https://press.aboutamazon.com/news-releases/news-release-details/amazon-web-services-launches-amazon-s3-simple-storage-service)2\nAmazon used the following principles of distributed system design to meet Amazon S3 requirements:\n-   **Decentralization.** Use fully decentralized techniques to remove scaling bottlenecks and single points of failure.\n-   **Asynchrony.** The system makes progress under all circumstances.\n-   **Autonomy.** The system is designed such that individual components can make decisions based on local information.\n-   **Local responsibility.** Each individual component is responsible for achieving its consistency; this is never the burden of its peers.\n-   **Controlled concurrency.** Operations are designed such that no or limited concurrency control is required.\n-   **Failure tolerant.** The system considers the failure of components to be a normal mode of operation and continues operation with no or minimal interruption.\n-   **Controlled parallelism.** Abstractions used in the system are of such granularity that parallelism can be used to improve performance and robustness of recovery or the introduction of new nodes.\n-   **Decompose into small, well-understood building blocks.** Do not try to provide a single service that does everything for everyone, but instead build small components that can be used as building blocks for other services.\n-   **Symmetry.** Nodes in the system are identical in terms of functionality, and require no or minimal node-specific configuration to function.\n-   **Simplicity.** The system should be made as simple as possible, but no simpler.\n**References -**\n\n<https://www.youtube.com/watch?v=Y6Ev8GIlbxc>\n<https://www.researchgate.net/publication/282914203_A_comparative_evaluation_of_AMQP_and_MQTT_protocols_over_unstable_and_mobile_networks>\n\n<https://dev.to/uyouthe/scalable-architecture-without-magic-and-how-to-build-it-if-youre-not-google-336a>\n<https://en.wikipedia.org/wiki/High_availability>\n<https://lethain.com/distributed-systems-vocabulary>\n<https://www.kislayverma.com/post/code-review-checklist-for-distributed-systems>\n\n"},{"fields":{"slug":"/Computer-Science/Distributed-System/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-06-27 17:04:54 +0500\n\nModified: 2020-03-26 00:33:29 +0500\n\n---\n\n**CRDTs (Conflict-free Replicated Data Types)**\n\n**A conflict-free replicated data type (CRDT)** is an abstract data type, with a well defined interface, designed to be replicated at multiple processes and exhibiting the following properties:\n\n1.  Any replica can be modified without coordinating with another replicas;\n\n2.  When any two replicas have received the same set of updates, they reach the same state, deterministically, by adopting mathematically sound rules to guarantee state convergence.\nRiak is the most popular open source library of CRDT's and is used by Bet365 and League of Legends.\n**Types of CRDTs**\n\nThere are two approaches to CRDTs, both of which can provide[strong](https://en.wikipedia.org/wiki/Strong_consistency)[eventual consistency](https://en.wikipedia.org/wiki/Eventual_consistency): **operation-based CRDTsand state-based CRDTs.**\n\nThe two alternatives are equivalent, as one can emulate the other.Operation-based CRDTs require additional guarantees from the[communication middleware](https://en.wikipedia.org/wiki/Communications_protocol);namely that the operations not be dropped or duplicated when transmitted to the other replicas, though they can be delivered in any order. State-based CRDTs also have a disadvantage, which is that the entire state must be transmitted to the other replicas, which may be costly.\n**Operation-based CRDTs**\n\nOperation-based CRDTs are referred to as**commutative replicated data types, orCmRDTs.** CmRDT replicas propagate state by transmitting only the update operation. For example, a CmRDT of a single integer might broadcast the operations (+10) or (âˆ’20). Replicas receive the updates and apply them locally. The operations are[commutative](https://en.wikipedia.org/wiki/Commutative). However, they are not[idempotent](https://en.wikipedia.org/wiki/Idempotent). The communications infrastructure must therefore ensure that all operations on a replica are delivered to the other replicas, without duplication, but in any order.\n\nPureoperation-based CRDTsare a variant of operation-based CRDTs that reduces the metadata size.\n**State-based CRDTs**\n\nState-based CRDTs are called**convergent replicated data types, orCvRDTs**. In contrast to CmRDTs, CvRDTs send their full local state to other replicas, where the states are merged by a function which must be[commutative](https://en.wikipedia.org/wiki/Commutative),[associative](https://en.wikipedia.org/wiki/Associative), and[idempotent](https://en.wikipedia.org/wiki/Idempotent). Themergefunction provides a[join](https://en.wikipedia.org/wiki/Join_(mathematics))for any pair of replica states, so the set of all states forms a[semilattice](https://en.wikipedia.org/wiki/Semilattice). Theupdatefunction must[monotonically increase](https://en.wikipedia.org/wiki/Monotonic_function)the internal state, according to the same[partial order](https://en.wikipedia.org/wiki/Partial_order)rules as the semilattice.\n\nDelta stateCRDTs(or simply Delta CRDTs) are optimized state-based CRDTs where only recently applied changes to a state are disseminated instead of the entire state.\n**Comparison**\n\nWhile CmRDTs place more requirements on the protocol for transmitting operations between replicas, they use less bandwidth than CvRDTs when the number of transactions is small in comparison to the size of internal state. However, since the CvRDT merge function is associative, merging with the state of some replica yields all previous updates to that replica.[Gossip protocols](https://en.wikipedia.org/wiki/Gossip_protocol) work well for propagating CvRDT state to other replicas while reducing network use and handling topology changes.\nSome lower bounds on the storage complexity of state-based CRDTs are known.\n<https://arxiv.org/abs/1805.06358>\n\n<https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type>\n\n<http://christophermeiklejohn.com/crdt/2014/07/22/readings-in-crdts.html>\n\n## Actor Model**\n\nThe actor model provides a higher level of abstaction for writing concurrent and distributed systems, which shields the developer from explicit locking and thread management. It provides the core functionality of reactive systems, defined in the Reactive Manifesto as responsive, resilient, elastic, and message-driven. Akka is an actor-based framework that is easy to implement with full Java 8 Lambda support. Actors enable developers to design and implement systems in ways that help focus more on the core functionality and less on the plumbing. Actor-based systems are the perfect foundation for quickly evoling microservices architectures.\nActor (encapsulate 3 things)\n\n1.  Processing\n\n2.  Storage\n\n3.  Communication\nIf an actor receive a message it can do 3 things\n\n1.  Create more actors\n\n2.  Send messages to actors it knows\n\n3.  Designate what to do with the next message\n<https://www.brianstorti.com/the-actor-model>\n\n<https://www.youtube.com/watch?v=7erJ1DV_Tlo>\n\n## Multi-Tenancy**\n\nMulti-tenancy is an architecture in which a single instance of a software application serves multiple customers. Each customer is called a tenant. Tenants may be given the ability to customize some parts of the application, such as color of the user interface ([UI](http://searchsoa.techtarget.com/definition/user-interface)) or[business rules](http://whatis.techtarget.com/definition/business-rule), but they cannot customize the application's[code](http://whatis.techtarget.com/definition/code).\nMulti-tenancy can be economical because software development and maintenance costs are shared. It can be contrasted with single-tenancy, an architecture in which each customer has their own software instance and may be given access to code. With a multi-tenancy architecture, the provider only has to make updates once. With a single-tenancy architecture, the provider has to touch multiple instances of the software in order to make updates.\nIn[cloud computing](http://searchcloudcomputing.techtarget.com/definition/cloud-computing), the meaning of multi-tenancy architecture has broadened because of new service models that take advantage of[virtualization](http://searchservervirtualization.techtarget.com/definition/virtualization)and[remote access](http://searchmidmarketsecurity.techtarget.com/definition/remote-access). A software-as-a-service ([SaaS](http://whatis.techtarget.com/definition/SaaS)) provider, for example, can run one instance of its application on one instance of a database and provide web access to multiple customers. In such a scenario, each tenant's data is isolated and remains invisible to other tenants.\n<https://whatis.techtarget.com/definition/multi-tenancy>\n\n<https://www.computerworld.com/article/2517005/data-center/multi-tenancy-in-the-cloud--why-it-matters.html>\n\n## Failure Modes**\n\nThis brings us to an important subject of Failure Detectors, which are widely used in practical consensus algorithms and help to solve consensus problem in a partial synchronous or synchronous system.Failure Detectoris an abstraction that helps to reason about liveness in the system, detect and mark participants as active or failed.\nIf processesAandBcommunicate through perfect link and all process B stops receiving messages fromAandAdoes not receive any messages fromB, most of the time from the process perspective it's impossible to know whetherBhas crashed,Bis simply running very slow or there's a network partition. If two processes are separated by the network partition, for both of them it will seem as if the other process just crashed.\nThe simplest way for a process to fail isCrash-Failure, where the process stops executing steps required by the algorithm. Here, the assumption is that processes are executing algorithm correctly, but stop at some point and never recover. In real-life system, this type of failure occurs less often than, say,Crash-Recovery, where the process stops executing steps required by the algorithm, but recovers at the later point and tries to execute further steps. For correctness, some algorithms still assume crashed and recovered process as failed and further steps do not influence the outcome of the algorithm.\nThis means that the algorithm should be designed in a way that does not rely for on process recovery for correctness, since it may never recover or recover too late.\n\nAnother type of failure isOmission Fault. This failure model assumes that the process omits some of the algorithm steps, is not able to execute algorithm steps or this execution is not visible for other participants.\nThe hardest failures to overcome areArbitraryorByzantine Failures, where the process continues executing algorithm steps, but in a way that contradicts the algorithm in some way (for example, by deciding on a value that was never proposed).\n<https://medium.com/databasss/on-ways-to-agree-part-1-links-and-flp-impossibility-f6bd8a6a0980>\n\n## Shared-Nothing architecture (SN)**\n\nAshared-nothing architecture(SN) is a[distributed-computing](https://en.wikipedia.org/wiki/Distributed_computing)architecture in which each update request is satisfied by a single node (processor/memory/storage unit). The intent is to eliminate contention among nodes. Nodes do not share (independently access) memory or storage. One alternative architecture is shared everything, in which requests are satisfied by arbitrary combinations of nodes. This may introduce contention, as multiple nodes may seek to update the same data at the same time.\nSN eliminates[single points of failure](https://en.wikipedia.org/wiki/Single_point_of_failure), allowing the overall system to continue operating despite failures in individual nodes and allowing individual nodes to upgrade without a system-wide shutdown.\nA SN system can scale simply by adding nodes, since no central resource bottlenecks the system.Another term for SN is[sharding](https://en.wikipedia.org/wiki/Sharding). A SN system typically partitions its data among many nodes. A refinement is to replicate commonly used but infrequently modified data across many nodes, allowing more requests to be resolved on a single node.\n<https://en.wikipedia.org/wiki/Shared-nothing_architecture>\n\n## Redundancy, Replication, Transparency**\n\nreplicationtakes a redundant node one step further; it ensures that the redundant node (areplica) is identical to all of its other copies.\n[Transparency](https://medium.com/baseds/transparency-illusions-of-a-single-system-part-1-b01c25f7dddd)in[a system](https://medium.com/baseds/transparency-illusions-of-a-single-system-part-2-2b21c5047774)means that all the replicas and the original node must behave similarly, which means that a consumer of the system (like an end user or another node) could potentially write to one replica, while another consumer of the system could read fromanotherreplica!\n<https://medium.com/baseds/redundancy-and-replication-duplicating-in-a-distributed-system-7ab4322d7378>\n"},{"fields":{"slug":"/Computer-Science/Distributed-System/Vocabulary/","title":"Vocabulary"},"frontmatter":{"draft":false},"rawBody":"# Vocabulary\n\nCreated: 2019-11-16 00:13:39 +0500\n\nModified: 2020-03-17 13:46:32 +0500\n\n---\n\nHarvest and Yield\n\nCALM - ConsistencyasLogicalMonotonicity\n\nA program has a consistent, coordination-free distributed implementation if any only if it is monotonic.\n**Availability configurations**\n-   Active-activeimplies that a request routed to any node will be handled properly.\n-   Active-passiveimplies that a request will always be routed to a single active node, but that it's possible to quickly elect a new active node if the current active node becomes degraded.\n**Conflict resolution**\n\nConflict resolution mechanisms for distributed systems:\n-   **[last writer wins](https://dl.acm.org/citation.cfm?doid=1435417.1435432)-** the most recently written version is the correct version\n-   **[read repair](https://en.wikipedia.org/wiki/Eventual_consistency)-** inconsistencies fixed at read time, slowing reads\n-   **write repair-** inconsistencies fixed at write, slowing writes\n-   **asynchronous repair-** inconsistencies fixed out of band somehow, not synchronously within read or write operation\n-   **vector clocks**create a logical clock to reconcile writes, described in[Dynamo paper](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf)\n**Other stuff**\n-   **Single point of failure**(SPOF) a single component that causes dependent services to fail\n-   [**Fault domains**](https://lethain.com/fault-domains/)is a set of components that share a SPOF\n-   **Fault tolerant**describes a system that has multiple fault domains at the same level of functionality\n-   **Replication**is streaming changes from one process to another\n-   **Synchronous replication**is commiting change on a replica at same time as committing on the primary (e.g. MySQL's[semisynchronous replication](https://dev.mysql.com/doc/refman/5.5/en/replication-semisync.html). Typically very, very slow\n<https://lethain.com/distributed-systems-vocabulary>\n\n## State Machine Replication**\n\nIn[computer science](https://en.wikipedia.org/wiki/Computer_science),**state machine replication**or**state machine approach**is a general method for implementing a fault-tolerant service by replicating servers and coordinating client interactions with server replicas. The approach also provides a framework for understanding and designing replication management protocols.\n<https://en.wikipedia.org/wiki/State_machine_replication>\n\n## Highy Available and Fault Tolerant**\n\n![](media/Vocabulary-image1.jpg)\n\n<https://medium.com/baseds/ready-and-available-distributed-systems-161023aca378>\n\n## Fault**\n\nAfaultis really just anything in our system that is different from what we expect it to be. Whenever some piece of our system deviates from its expected behavior, or whenever something unexpectedly occurs in our system, that behavior itself is a fault!\n\n![](media/Vocabulary-image2.jpg)\n\n![](media/Vocabulary-image3.jpg)\nA fault, which can originate in any part of a system, can cause unexpected behavior, which results in an unexpected result, or an error. If that error isn't handled in some way or hidden from the rest of the system, the originating node --- where the fault first presented itself --- will return that error, which is what we also call afailure. When we talk about different kinds of failures in a system, which could come from different kinds of faults, we can categorize them in different ways.\nThe different classifications for the kinds of failures we see in a distributed system are also known as failure modes.Failure modesare how we can identify the exact way that a system has failed. Interestingly, failure modes are classified somewhat holistically; that is to say, when we try to identify what kind of failure we're dealing with, we take the whole system into account.\n**Types of Failures**\n-   **timing failures**\n    -   If a node in a system delivers a response correctly, but that response is outside the expected time interval.\n    -   A node with a timing failure could deliver a response that is earlier or later than expected; these are also called Performance failures\n-   **omission failures**\n    -   It is a failure where the node's reponse never appears to be sent (or, in other words, is an \"infinitely late\" timing failure)\n    -   Omission failures come in two forms, since a node can both send and receive responses\n        -   Send Omission Failure - A node fails to send a response\n        -   Receive Omission Failure - A node fails to receive an incoming message/another node's response\n-   **crash failures**\n    -   A crash failure occurs if a node experiences an omission failure once and stops responding completely, and becomes unresponsive (aka, crashes)\n-   **response failures**\n    -   occurs when a node actually does respond, but its response is incorrect\n    -   There are 2 forms of response failures; the node's response may be:\n        -   incorrect in value (Value Failure)\n        -   incorrect in state, indicating something went wrong with the flow control/logic of the system (state transition failure)\n-   arbitrary failures\n    -   occurs when a node sends different responses through the system, and can produce arbitrary messages at arbitrary times\n    -   also called a Byzantine failure\n    -   In a Byzantine failure, a node can send differing responses to other nodes, and it can forge responses/messages from other nodes\n    -   A subset of this failure is Authentication Detectable Byzantine Failures, where a node cannot forge a message on the behalf of another node\n**Error**\n\nErrorsare manifestations of faults within our system, and when an error occurs and then spreadsor propagates through the system\n![](media/Vocabulary-image4.jpeg)\n<https://medium.com/baseds/fantastic-faults-and-what-to-call-them-56d91a1b198c>\n\n<https://medium.com/baseds/modes-of-failure-part-1-6687504bfed6>\n\n<https://medium.com/baseds/modes-of-failure-part-2-4d050794be2f>\n\n<https://medium.com/baseds/weeding-out-distributed-system-bugs-28a01e37f70c>\n\n"},{"fields":{"slug":"/Computer-Science/General/Coding-Guidelines---Code-Reviews---Clean-Code/","title":"Coding Guidelines / Code Reviews / Clean Code"},"frontmatter":{"draft":false},"rawBody":"# Coding Guidelines / Code Reviews / Clean Code\n\nCreated: 2020-10-10 18:58:58 +0500\n\nModified: 2022-12-09 23:51:56 +0500\n\n---\n\nCode reviews are fundamental to the software development process, even when there's only one engineer.\n\n<https://www.chakshunyu.com/blog/this-is-my-10-questions-react-code-reviewing-routine>\n\"I first look for security, functionality, and readability. Is the code simple, or cluttered, bloated, and inefficient? How many lines of unnecessary code will I need to re-write or remove? I check for any weaknesses that could cause vulnerabilities and confirm that regulatory requirements have been met.\"\nEveryone has their own coding style and every developer or team will have requirements that are specific to their codebase. Effective code reviews often have checklists. Below is a limited list of general suggestions you could consider including:\n-   The software passes automated and manual testing\n-   Code follows applicable conventions and is easy to understand\n-   Code is not duplicated\n-   No negatively named boolean variables\n-   Scrutinize[methods with boolean parameters](https://medium.com/@amlcurran/clean-code-the-curse-of-a-boolean-parameter-c237a830b7a3)\n-   Blocks of code inside loops are as small as possible\n-   No memory leaks\n\nBut more important than which exact points a candidate brings up is their reasoning for doing so. Be wary of candidates who get stuck on tabs-versus-spaces bikeshedding at the expense of more crucial engineering elements.\nWrite deterministic code - Given an input, code always produces the same output\n-   Code should be consistent\n-   Code should be self-descriptive\n-   Code should be well documented\n-   Code should utilize stable modern features\n-   Code shouldn't be unnecessarily complex\n-   Code shouldn't be un-performant (don't write intentionally slow code)\n<https://www.freecodecamp.org/news/clean-coding-for-beginners>\n[**https://www.toptal.com/software/six-commandments-of-good-code**](https://www.toptal.com/software/six-commandments-of-good-code)\n-   Treat Your Code the Way You Want Other's Code to Treat You\n-   Good Code Is Easily Read and Understood, in Part and in Whole\n-   Good Code Has a Well Thought-out Layout and Architecture to Make Managing State Obvious\n-   Good Code Doesn't Reinvent the Wheel, It Stands on the Shoulders of Giants\n-   Don't Cross the Streams\n-   When Possible, Let the Computer Do the Work\n**Error Handling**\n-   Bugs\n-   Error Handling\n    -   Corner Cases\n    -   Disk full, etc\n-   Input Validation both client side and server side\n**NIH Syndrome (Not Invented Here)**\n\nIn programming, it is also common to refer to the \"NIH syndrome\" as the tendency towards[reinventing the wheel](https://en.wikipedia.org/wiki/Reinventing_the_wheel)(reimplementing something that is already available) based on the belief that in-house developments are inherently better suited, more secure, more controlled, quicker to develop, and incur lower overall cost (including maintenance cost) than using existing implementations.\n<https://en.wikipedia.org/wiki/Not_invented_here>\n\n## Enterprise Programming Tricks for clean code**\n-   Singleton configuration\n-   Singletons\n-   Verbose naming\n-   Noisy logging\n-   Log and throw\n-   Repetition and duplication\n-   Unnecessary code\n-   Mixed levels of abstraction\n-   Legacy coding habits\n-   Programming by coincidence\n-   Programming by superstition\n[Enterprise Programming Tricks For Clean Code](https://www.youtube.com/watch?v=dC9vdQkU-xI)\n\n[Naming Things in Code](https://www.youtube.com/watch?v=-J3wNP6u5YU)\n-   Don't abbreviate names\n-   Don't put types in variable names\n-   Add units to variables unless the type tells you\n-   Don't put types in your types (e.g. AbstractX, BaseX)\n-   Refactor if you find yourself naming code \"Utils\"\n**Others**\n\n<https://www.fluentcpp.com/2019/08/27/extract-function-should-i-extract-the-condition-too>\n\n<https://www.fluentcpp.com/2016/12/15/respect-levels-of-abstraction>\n\n[Reasons & Ways to Improve Code Quality â€¢ Venkat Subramaniam â€¢ GOTO 2021](https://www.youtube.com/watch?v=znZlF4uQBN0)\n\n[Writing Code You Won't Hate Tomorrow](https://www.youtube.com/watch?v=qjtMs7jQxEo)\n\n[Why You Shouldn't Nest Your Code](https://www.youtube.com/watch?v=CFRhGnuXG-4)\n-   Never Nester\n-   If you need more than 3 levels of indentation, you're screwed anyway, and should fix your program\n\n"},{"fields":{"slug":"/Computer-Science/General/Common/","title":"Common"},"frontmatter":{"draft":false},"rawBody":"# Common\n\nCreated: 2017-11-01 12:17:26 +0500\n\nModified: 2022-07-18 09:31:51 +0500\n\n---\n\nCAPTCHA - Completely Automated Public Turing test to tell Computers and Humans Apart\nAPI - Application Programming Interface\n\nA set of subroutine definitions, protocols, and tools for building application software\nAJAX - Asynchronous JavaScript and XML\n\nWeb applications can send and retrieve data from a server asynchronously without interfering with the display and behavior of the existing page.\nAccessibility\n\n**ARIA - Accessible Rich Internet Applications**\n\nDefines different front-end methodologies that make web content accessible for disabled people who access the web with Assistive Technologies (AT), such as screen readers.\nBOM - Browser Object Model\n\nA browser specific convention referring to all the objects exposed by the web browser. Unlike DOM, there is no standard for implementation and no strict definition, so browser vendors are free to implement the BOM in any way they wish.\nCTA - Call-To-Action\n\nA CTA is a message directed to users or prospective customers to take an immediate action.\nDOM - Document Object Model\n\nAn API for HTML and XML documents, used by browsers to render these documents. The DOM specifies the logical structure of a document, and represents it as a node tree in which nodes are objects that represent different parts of the document.\nMIME - Multipurpose Internet Mail Extensions\n\nAn Internet standard that extends the format of email to support:\n-   Text in character sets other than ASCII\n-   Non-text attachments: audio, video, images, application programs, etc.\n-   Message bodies with multiple parts\n-   Header information in non-ASCII character sets\nORM - Object-Relational Mapping\n\nA technique that lets us query and manipulate data from a database using an object-oriented paradigm.\nSOAP - Simple Object Access Protocol\n\nA messaging protocol specification for exchanging structured information in the implementation of web services in computer networks. SOAP allows processes running on disparate operating systems (such as Windows and Linux) to communicate using XML. Since web protocols like HTTP are installed and running on all operating systems, SOAP allows clients to invoke web services and receive responses independent of language and platforms.\nSCM - Source Control Management\n\nFTU - First Time Use\n\nAMP - Accelerated Mobile Pages\n\nSHA - Secure Hash\n\nJSX - Javascript XML\n**mebiByte / kibibyte / gibibyte**\n\nA mebibyte is a unit of data storage that equals 2 to the 20th power, or 1,048,576 bytes.\nWhile a[megabyte](https://techterms.com/definition/megabyte)can be estimated as 10^6 or 1,000,000 bytes, a mebibyte is exactly 1,048,576 bytes. This is to avoid the ambiguity associated with the size of megabytes. A mebibyte is 1,024[kibibytes](https://techterms.com/definition/kibibyte)and precedes the[gibibyte](https://techterms.com/definition/gibibyte)unit of measurement.**YK38 Problem**\n\nTheYear 2038 problem(also called Y2038, Epochalypse,Y2k38, or Unix Y2K) relates to representing time in many digital systems as the number of seconds passed since 00:00:00[UTC](https://www.wikiwand.com/en/Coordinated_Universal_Time)on[1 January 1970](https://www.wikiwand.com/en/Unix_time)and storing it as a[signed 32-bit integer](https://www.wikiwand.com/en/Signed_number_representations). Such implementations cannot encode times after 03:14:07 UTC on 19 January 2038. Similar to the[Y2K problem](https://www.wikiwand.com/en/Year_2000_problem), the Year 2038 problem is caused by insufficient capacity used to represent time.\n**RFC**\n\n**4122** - UUID/GUID <https://tools.ietf.org/html/rfc4122.html>\n\n## Epoch Time / Unix Time**\n\nUnix time(also known asEpoch time,POSIX time,seconds since the Epoch,orUNIX Epoch time) is a system for describing a[point in time](https://www.wikiwand.com/en/Timestamp). It is the number of[seconds](https://www.wikiwand.com/en/Second)that have elapsed since theUnix epoch, minus[leap seconds](https://www.wikiwand.com/en/Leap_second); the Unix epoch is 00:00:00[UTC](https://www.wikiwand.com/en/Coordinated_Universal_Time)on 1 January 1970 (an arbitrary date); leap seconds are ignored, with a leap second having the same Unix time as the second before it, and every day is treated as if it contains exactly86400seconds.Due to this treatment Unix time is not a true representation of UTC.\nIt should also be pointed out that this point in time technically does not change no matter where you are located on the globe. This is very useful to computer systems for tracking and sorting dated information in dynamic and distributed applications both online and client side. The reason why Unix timestamps are used by many webmasters is because they can represent all time zones at once.\n<https://www.wikiwand.com/en/Unix_time>\n\n<https://www.epochconverter.com>\n\n<https://everytimezone.com>\n\n## Time - ISO 8601**\n\nISO 8601Data elements and interchange formats -- Information interchange -- Representation of dates and times is an[international standard](https://en.wikipedia.org/wiki/International_standard)covering the exchange of[date](https://en.wikipedia.org/wiki/Calendar_date)- and[time](https://en.wikipedia.org/wiki/Time)-related data. It was issued by the [International Organization for Standardization](https://en.wikipedia.org/wiki/International_Organization_for_Standardization) (ISO) and was first published in 1988. The purpose of this standard is to provide an unambiguous and well-defined method of representing dates and times, so as to avoid misinterpretation of numeric representations of dates and times, particularly when data is transferred between [countries with different conventions](https://en.wikipedia.org/wiki/Date_and_time_notation_by_country) for writing numeric dates and times.\n**General Principles**\n-   Date and time values are ordered from the largest to smallest unit of time: year, month (or week), day, hour, minute, second, and fraction of second. The[lexicographical order](https://www.wikiwand.com/en/Lexicographical_order)of the representation thus corresponds to chronological order, except for date representations involving negative years or time offset. This allows dates to be naturally[sorted](https://www.wikiwand.com/en/Sorting)by, for example, file systems.\n-   Each date and time value has a fixed number of digits that must be padded with[leading zeros](https://www.wikiwand.com/en/Leading_zero).\n-   Representations can be done in one of two formats-- a basic format with a minimal number of separators or an extended format with separators added to enhance human readability.The standard notes that \"The basic format should be avoided in[plain text](https://www.wikiwand.com/en/Plain_text).\"The separator used between date values (year, month, week, and day) is the[hyphen](https://www.wikiwand.com/en/Hyphen), while the[colon](https://www.wikiwand.com/en/Colon_(punctuation))is used as the separator between time values (hours, minutes, and seconds). For example, the 6th day of the 1st month of the year 2009 may be written as\"2009-01-06\"in the extended format or simply as \"20090106\" in the basic format without ambiguity.\n-   For reduced precision,any number of values may be dropped from any of the date and time representations, but in the order from the least to the most significant. For example, \"2004-05\" is a valid ISO 8601 date, which indicates May (the fifth month) 2004. This format will never represent the 5th day of an unspecified month in 2004, nor will it represent a time-span extending from 2004 into 2005.\n-   If necessary for a particular application, the standard supports the addition of a[decimal fraction](https://www.wikiwand.com/en/Decimal_fractions)to the smallest time value in the representation.\n<https://en.wikipedia.org/wiki/ISO_8601>\n\n## IEC Prefixes**\n\n| **Factor** | **Name** | **Symbol** | **Origin**              | **Derivation**  |\n|------------|------------|-------------|----------------------|---------------|\n| 2^10^      | kibi      | Ki          | kilobinary: (2^10^)^1^  | kilo: (10^3^)^1^ |\n| 2^20^      | mebi      | Mi          | megabinary: (2^10^)^2^ | mega: (10^3^)^2^ |\n| 2^30^      | gibi      | Gi          | gigabinary: (2^10^)^3^  | giga: (10^3^)^3^ |\n| 2^40^      | tebi      | Ti          | terabinary: (2^10^)^4^  | tera: (10^3^)^4^ |\n| 2^50^      | pebi      | Pi          | petabinary: (2^10^)^5^  | peta: (10^3^)^5^ |\n| 2^60^      | exbi      | Ei          | exabinary: (2^10^)^6^   | exa: (10^3^)^6^  |\n<https://physics.nist.gov/cuu/Units/binary.html>\n\n## shim** in computing means application compatibility workaround.\n**IMP Coding Snippets**\n\n**Sweep Line Algorithm (Find intersection of two lines)**\n\nUsing 1d range search\n**Parameterization**\n\n<https://www.toptal.com/python/python-parameterized-design-patterns>\nThis is a solution to python challenge level 7.\n\nThis is the first code that i wrote that involves Image Processing and i came to it by myself.\nProblem - <http://www.pythonchallenge.com/pc/def/oxygen.html>\n\nSolution - python 7.py\n<https://slate.com/technology/2019/10/consequential-computer-code-software-history.html>\n\n<https://medium.com/the-atlantic/the-coming-software-apocalypse-4ffb43f3b288>Æ’\n<https://github.com/RockstarLang/rockstar>\n\n[The Art of Code - Dylan Beattie](https://www.youtube.com/watch?v=6avJHaC3C2U)\n<https://www.youtube.com/watch?v=0vPt7GI-2kc>\n<https://blog.codinghorror.com/new-programming-jargon>\n\n## Typosquatting**\n\n<https://medium.com/@williambengtson/python-typosquatting-for-fun-not-profit-99869579c35d>\n\n## Three-way comparison Operator (SpaceShip Operator < = >)**\n\nIn[computer science](https://www.wikiwand.com/en/Computer_science), athree-way comparisontakes two values A and B belonging to a type with a[total order](https://www.wikiwand.com/en/Total_order)and determines whether A < B, A = B, or A > B in a single operation, in accordance with the mathematical[law of trichotomy](https://www.wikiwand.com/en/Trichotomy_(mathematics)).\n<https://www.wikiwand.com/en/Three-way_comparison>\n\n## FP64, FP32, FP16, BFloat16, TF32 (Floating Points)**\n\n<https://medium.com/@moocaholic/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407>\n\n## Valid Emails**\n\n<https://www.youtube.com/watch?v=60BPETbra9U>\n\n## [Zawinski's Law](https://en.wikipedia.org/wiki/Jamie_Zawinski#Principles)---** \"Every program attempts to expand until it can read mail. Those programs which cannot so expand are replaced by ones which can.\" (related:[Greenspun's tenth rule](https://en.wikipedia.org/wiki/Greenspun%27s_tenth_rule)--- \"any sufficiently complicated C or Fortran program contains an ad hoc, informally-specified, bug-ridden, slow implementation of half of Common Lisp.\")\n**[Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law)---** \"The observation that the number of transistors in a dense integrated circuit doubles approximately every two years.\"\n\n**Eroom's Law**\n\nEroom's lawis the observation that drug discovery is becoming slower and more expensive over time, despite improvements in technology (such as[high-throughput screening](https://en.wikipedia.org/wiki/High-throughput_screening),[biotechnology](https://en.wikipedia.org/wiki/Biotechnology),[combinatorial chemistry](https://en.wikipedia.org/wiki/Combinatorial_chemistry), and computational[drug design](https://en.wikipedia.org/wiki/Drug_design)), a trend first observed in the 1980s. The cost of developing a new drug roughly doubles every nine years (inflation-adjusted).In order to highlight the contrast with the exponential advancements of other forms of technology (such as[transistors](https://en.wikipedia.org/wiki/Transistor)) over time, the law was deliberately spelled as[Moore's law](https://en.wikipedia.org/wiki/Moore%27s_law)spelled backwards.\n\nSoftware also getting slower with improved processors because developers are writing inefficient code.\n**[Metcalfe's Law](https://en.wikipedia.org/wiki/Metcalfe%27s_law#Limitations)---** \"The value of a telecommunications network is proportional to the square of the number of connected users of the system...Within the context of social networks, many, including Metcalfe himself, have proposed modified models using (nÃ— logn) proportionality rather thann^2proportionality.\"\n**[Clarke's Third Law](https://en.wikipedia.org/wiki/Clarke%27s_three_laws)---** \"Any sufficiently advanced technology is indistinguishable from magic.\""},{"fields":{"slug":"/Computer-Science/General/Newsletter---Learning-Resources/","title":"Newsletter / Learning Resources"},"frontmatter":{"draft":false},"rawBody":"# Newsletter / Learning Resources\n\nCreated: 2020-10-10 00:43:31 +0500\n\nModified: 2022-05-30 23:23:24 +0500\n\n---\n\n<https://www.freecodecamp.org/news/how-to-create-an-email-newsletter-design-layout-send>\n\n## #1**\n\nThis is a weeklynewsletter for our tech team. This will contain links worth reading in and around tech.\n\n**So this week it's all about databases**\n\n1.<https://medium.com/analytics-vidhya/categories-of-databases-a-primer-9781a3b24285>\n\n2.<https://www.toptal.com/database/database-design-bad-practices>\n\n3.<https://www.prisma.io/dataguide/intro/comparing-database-types>\n\n4.<https://medium.com/@rakyll/things-i-wished-more-developers-knew-about-databases-2d0178464f78>\n\n5.<https://www.youtube.com/watch?v=W2Z7fbCLSTw&ab_channel=Fireship>\n\n## Advanced**\n\n1.<https://www.geeksforgeeks.org/indexing-in-databases-set-1>\n\n2.<https://www.toptal.com/database/sql-indexes-explained-pt-1>\n\n3.<https://fauna.com/blog/introduction-to-transaction-isolation-levels>\n\nSome links are prettydeep and if you have any difficulty understanding any concept feelfree to reach out to me.\n**Quote of the Week:**\"Computer programming is an art because it applies accumulated knowledge to the world, because it requires skill and ingenuity, and especially because it produces objects of beauty. Programmers who subconsciously view themselves as artists will enjoy what they do and will do it better.\" -- Donald Knuth\n**Websites worth checking out**\n\n1.<https://www.freecodecamp.org>\n\n2.<https://medium.com>\n\n3. Awesome Repositories -<https://github.com/ossu/computer-science>\n\n## Youtube channelsworth subscribing**\n\n1. GotoConf -<https://www.youtube.com/user/GotoConferences>\n\n2. InfoQ -<https://www.youtube.com/user/MarakanaTechTV>\n\n3. Coding Tech -<https://www.youtube.com/channel/UCtxCXg-UvSnTKPOzLH4wJaQ>\n\n4. Tech Dummies -<https://www.youtube.com/channel/UCn1XnDWhsLS5URXTi5wtFTA>\n\n## #2**\n\n**This is all about Coding**\n-   **Codeigniter**\n\n* Hooks\n\n* Helpers\n\n* Core controller model\n* Libraries\n\n* Caching\n\n* Logging-   High Cohesion\n-   Low Coupling\n-   Object Oriented Programming\n-   Algorithms\n-   Data Structures\n-   Design Patterns\n-   Testable Code\n**Newsletter**\n\nInterview Cake Weekly Problem\n\nDaily coding\n\n<http://gamedevjsweekly.com>\n\n<https://dailyproductprep.com>\n\n<https://programmingdigest.net>\n\nMedium Daily Digest\n**Learning Resources / Bootcamps**\n\n<https://github.com/jwasham/coding-interview-university>\n\n<https://www.freecodecamp.org>\n\n<https://www.freecodecamp.org/news/best-coding-games-online-adults-learn-to-code>\n\n<https://www.masaischool.com>\n<https://repl.it>\n[**https://www.freecodecamp.org/news/tech-talks-software-development-conferences/**](https://www.freecodecamp.org/news/tech-talks-software-development-conferences/)\n\n<https://roadmap.sh/backend>\n\n[**https://github.com/yangshun/tech-interview-handbook**](https://github.com/yangshun/tech-interview-handbook)\n\n[**https://github.com/kdn251/interviews**](https://github.com/kdn251/interviews)\n\n<https://github.com/sdmg15/Best-websites-a-programmer-should-visit>\n\n## Blogs**\n\n<https://about.gitlab.com/2019/08/27/tyranny-of-the-clock>\n\n<https://fremtidensuddannelser.dk/en>\n\n## Repositories**\n\n<https://gitstar-ranking.com/repositories>\n\n<https://github.com/danistefanovic/build-your-own-x>\n\n<https://github.com/gothinkster/realworld>\n\n<https://codebase.show/projects/realworld>\nMake sure to finish following documentation and POC in next 30-45 days, since after that you will not get time, since tasks will be very aggressive\n-   Python (OOPS concept)\n-   Writing Clean Code\n-   System Design\n-   Fast API + Django\n-   SQLAlchemy + Django ORM\n-   Security (AAA concepts)\n-   Basic understanding of Frontend i.e. React\n-   Competitive programming daily\n-   Design patterns (Singleton/observer/factory,etc)\n-   Databases - Redis + MySQL\n-   Writing SQL queries (Joins / Views / Procedures, etc)\n-   Basic understanding of PHP for reading PHP Codes\n-   Docker\n-   Git (Highly proficient)\n"},{"fields":{"slug":"/Computer-Science/General/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-01-14 15:04:47 +0500\n\nModified: 2021-10-24 16:03:56 +0500\n\n---\n\nISO 3103 - How to brew tea?\n**Pairwise distinct -** All elements are unique\n**UUID (Universally Unique IDentifier) / GUID (Globally Unique IDentifier)**\n\nAuniversally unique identifier(UUID) is a[128-bit](https://en.wikipedia.org/wiki/128-bit)number used to identify information in computer systems. The termglobally unique identifier(GUID) is also used, typically in software created by Microsoft.\nWhen generated according to the standard methods, UUIDs are for practical purposes unique, without depending for their uniqueness on a central registration authority or coordination between the parties generating them, unlike most other[numbering schemes](https://en.wikipedia.org/wiki/Numbering_scheme). While the probability that a UUID will be duplicated is not zero, it is close enough to zero to be negligible.\nThus, anyone can create a UUID and use it to identify something with near certainty that the identifier does not duplicate one that has already been, or will be, created to identify something else. Information labeled with UUIDs by independent parties can therefore be later combined into a single database or transmitted on the same channel, with a negligible probability of duplication.\n**Format**\n\nIn its canonical textual representation, the 16[octets](https://en.wikipedia.org/wiki/Octet_(computing))of a UUID are represented as 32[hexadecimal](https://en.wikipedia.org/wiki/Hexadecimal)(base-16) digits, displayed in 5 groups separated by hyphens, in the form 8-4-4-4-12 for a total of 36 characters (32 alphanumeric characters and 4 hyphens). For example:\n\n123e4567-e89b-12d3-a456-426655440000\n\nxxxxxxxx-xxxx-Mxxx-Nxxx-xxxxxxxxxxxx\n\nThe 4 bits of digitMindicate the UUID version, and the 1--3 most significant bits of digitNindicate the UUID variant. In the example,*M*is1, and*N*isa(10xx~2~), meaning that the UUID is a variant-1, version-1 UUID; that is, a time-based DCE/RFC 4122 UUID.\n\nThe canonical 8-4-4-4-12 format string is based on the \"record layout\" for the 16 bytes of the UUID:\n\n| **Name**                           | **Length (bytes)** | **Length (hex digits)** | **Contents**                                                                                 |\n|-------------------|-----------|------------|------------------------------|\n| time_low                           | 4                  | 8                       | integer giving the low 32 bits of the time                                                   |\n| time_mid                           | 2                  | 4                       | integer giving the middle 16 bits of the time                                                |\n| time_hi_and_version                | 2                  | 4                       | 4-bit \"version\" in the most significant bits, followed by the high 12 bits of the time     |\n| clock_seq_hi_and_res clock_seq_low | 2                  | 4                       | 1--3-bit \"variant\" in the most significant bits, followed by the 13--15-bit clock sequence |\n| node                               | 6                  | 12                      | the 48-bit node id                                                                           |\n![Timestamp (48) Address Family (8) 573E010013647A49B8F6B3E35E8B3C4 Reserved (16) Host ID (56) ](media/Others-image1.png)\n\n<https://tools.ietf.org/html/rfc4122.html>\n\n## UUID V1 : Uniqueness**\n\nUUID v1 is generated by using a combination the host computers MAC address and the current date and time. In addition to this, it also introduces another random component just to be sure of its uniqueness.\nThis means you are guaranteed to get a completely unique ID, unless you generate it from thesamecomputer, and at the exactsametime. In that case, the chance of collision changes from impossible to very very small because of the random bits.\nThis guaranteed uniqueness comes at the cost of anonymity. Because UUID v1 takes the time and your MAC address into consideration, this also means that someone could potentially identify the time and place(i.e. computer) of creation.\n**UUID V4 : Randomness**\n\nThe generation of a v4 UUID is much simpler to comprehend. The bits that comprise a UUID v4 are generated randomly and with no inherent logic. Because of this, there is no way to identify information about the source by looking at the UUID.\nHowever, there is now a chance that a UUID could be duplicated. The question is, do you need to worry about it?\nThe short answer is no. With the sheer number of possible combinations (2^128), it would be almost impossible to generate a duplicate unless you are generating trillions of IDs every second, for many years.\nIf your application is mission critical (for example, bank transactions or medical systems), you should still add a uniqueness constraint to avoid UUIDv4 collision\n**UUID V5: Non-Random UUIDs**\n\nIf you want a unique ID that's not random, UUID v5 could be the right choice.\nUnlike v1 or v4, UUID v5 is generated by providing two pieces of input information:\n\ni.  Input string:Any string that can change in your application.\n\nii. Namespace:A fixed UUID used in combination with the input string to differentiate between UUIDs generated in different applications, and to prevent[rainbow table hacks](https://www.hackingloops.com/what-are-rainbow-tables/)\nThese two pieces of information are converted to a UUID using the SHA1 hashing algorithm.\nAn important point to note is UUID v5 isconsistent. This means that any given combination of input and namespace will result in the same UUID, every time.\nThis is great if you want to, for example, maintain a mapping of your users to their UUIDs without explicitly persisting that information to storage.\nHowever, remember that these IDs arenot random, and their uniqueness is now your responsibility.\n**Which Version Should You Use?**\n\nIf you don't know what to go with, go with v4. It's good enough, and the chances of collision are practically none.\nIf you actuallywantyour UUID to give some indication of the date and computer in which it was created, then UUID v1 may be for you (although it is).\nUUID v5 is normally used only for very specific use cases, when you want to derive a UUID from another piece of information on the fly.\n<https://www.sohamkamani.com/uuid-versions-explained>\n\n## FlakeIDs**\n\nMany distributed systems have a requirement to generate time sorted, unique ids of some kind - for distinguishing incoming events, for resolving conflicts, for using as keys in key/value stores, for logging, and a whole bunch more.\nThe basic idea behind flake ids is simple: instead of incrementing a counter each time you need an ID, use some of the top bits in an id to represent time, and then some others to represent a \"node id\", such that id generation across nodes is unique. The wonderful thing about the node id is that you can just coordinateonce- very often just by writing to config files inside your orchestration tool (chef/puppet/ansible/etc).\n**Indirection**\n\nIn[computer programming](https://en.wikipedia.org/wiki/Computer_programming),**indirection**(also called**dereferencing**) is the ability to reference something using a name, reference, or container instead of the value itself. The most common form of indirection is the act of manipulating a value through its[memory address](https://en.wikipedia.org/wiki/Memory_address). For example, accessing a[variable](https://en.wikipedia.org/wiki/Variable_(programming))through the use of a[pointer](https://en.wikipedia.org/wiki/Pointer_(computer_programming)). A stored pointer that exists to provide a reference to an object by double indirection is called an*indirection node*. In some older computer architectures, indirect words supported a variety of more-or-less complicated[addressing modes](https://en.wikipedia.org/wiki/Addressing_mode).\n<https://en.wikipedia.org/wiki/Indirection>\n\n## Frequency Cap**\n\nIt is a counter for unique users on a website based on session data (like cookies). There might be millions or tens of millions of users visiting a website. Frequency capping means you only show each user your ad once per day.\nRedis has a[HyperLogLog data type](https://redis.io/commands/pfcount)that is perfect for a frequency cap. It approximates set membership with a very small error rate, in exchange for O(1) time and a very small memory footprint.PFADDadds an element to a HyperLogLog set. It returns 1 if your element is not in the set already, and 0 if it is in the set.\n**Variadic**\n\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics)and in[computer programming](https://en.wikipedia.org/wiki/Computer_programming), a**variadic function**is a[function](https://en.wikipedia.org/wiki/Function_(programming))of indefinite[arity](https://en.wikipedia.org/wiki/Arity), i.e., one which accepts a variable number of[arguments](https://en.wikipedia.org/wiki/Argument_(computer_science)). Support for variadic functions differs widely among[programming languages](https://en.wikipedia.org/wiki/Programming_language).\n<https://en.wikipedia.org/wiki/Variadic_function>\n\n## Internationalization (\"i18n\")**\n\nInternationalization (\"i18n\") is the process of adapting app code to other languages. For example, an app might need to display data in a different format, prices, or numbers with the correct decimal or thousands separators, or even handling whether text is written right-to-left or left-to-right. It also requires taking into account the user's timezone.\n**Localization (\"l10n\")**\n\nLocalization (\"l10n\"), is the process of preparing the content of the app to be available in different languages. This includes translating most of the app's resources, like texts, images, and sounds. Resources for different languages are usually kept in different files or directories, and the OS chooses the right one based on user settings.\nTranslation (T9N)\n\n**Globalization (G11N) -> T9N + L10N + I18N**\n**Time in human terms (Real cost)**\n\n| **Access type**                | **Actual time** | **Approximated time** |\n|--------------------------------|-----------------|-----------------------|\n| 1 CPU cycle                    | 0.3 ns          | 1 s                   |\n| Level 1 cache access           | 0.9 ns          | 3 s                   |\n| Level 2 cache access           | 2.8 ns          | 9 s                   |\n| Level 3 cache access           | 12.9 ns         | 43 s                  |\n| Main memory access             | 120 ns          | 6 min                 |\n| Solid-state disk I/O           | 50-150 Î¼s       | 2-6 days              |\n| Rotational disk I/O            | 1-10 ms         | 1-12 months           |\n| Internet: SF to NYC            | 40 ms           | 4 years               |\n| Internet: SF to UK             | 81 ms           | 8 years               |\n| Internet: SF to Australia      | 183 ms          | 19 years              |\n| OS virtualization reboot       | 4 s             | 423 years             |\n| SCSI command time-out          | 30 s            | 3000 years            |\n| Hardware virtualization reboot | 40 s            | 4000 years            |\n| Physical system reboot         | 5 m             | 32 millenia           |\n<https://blog.codinghorror.com/the-infinite-space-between-words>\n\n## Responsiveness matters**\n-   Walmart has found a sharp decline in conversion rates as latency climbs from 1s to 4s.\n-   Amazon found that every 100ms of latency cost them 1% in sales.\n-   Estimates from Akamai show that a 1s delay in page response can\n    result in a 7% reduction in conversions.\n\n![100ms is the holy grail from the RAIL model User Perception Of Performance Delays Oto 16ms Oto 100ms 100 to 300ms 300 to IOOOms 1000ms or more 10000ms or more Users are exceptionally good at tracking motion, and they dislike it when animations aren't smooth. They perceive animations as smooth so long as 60 new frames are rendered every second. That's 16ms per frame, including the time it takes for the browser to paint the new frame to the screen, leaving an app about 1 Oms to produce a frame. Respond to user actions within this time window and users feel like the result is immediate. Any longer, and the connection between action and reaction is broken. Users experience a slight perceptible delay. Within this window, things feel part of a natural and continuous progression of tasks. For most users on the web, loading pages or changing views represents a task. Beyond 1000 milliseconds (1 second), users lose focus on the task they are performing. Beyond 10000 milliseconds (10 seconds), users are frustrated and are likely to abandon tasks. They may or may not come back later. ](media/Others-image2.jpeg)\n\nRAIL Model - <https://web.dev/rail>\n\n## Linear-feedback shift register (LSFR)**\n\nIn[computing](https://en.wikipedia.org/wiki/Computing), alinear-feedback shift register(LFSR) is a[shift register](https://en.wikipedia.org/wiki/Shift_register)whose input bit is a[linear function](https://en.wikipedia.org/wiki/Linear#Boolean_functions)of its previous state.\nThe most commonly used linear function of single bits is[exclusive-or](https://en.wikipedia.org/wiki/Exclusive-or)(XOR). Thus, an LFSR is most often a shift register whose input bit is driven by the XOR of some bits of the overall shift register value.\n\nThe initial value of the LFSR is called the seed, and because the operation of the register is deterministic, the stream of values produced by the register is completely determined by its current (or previous) state. Likewise, because the register has a finite number of possible states, it must eventually enter a repeating cycle. However, an LFSR with a[well-chosen feedback function](https://en.wikipedia.org/wiki/Primitive_polynomial_(field_theory))can produce a sequence of bits that appears random and has a[very long cycle](https://en.wikipedia.org/wiki/Maximal_length_sequence).\nApplications of LFSRs include generating[pseudo-random numbers](https://en.wikipedia.org/wiki/Pseudorandomness),[pseudo-noise sequences](https://en.wikipedia.org/wiki/Pseudorandom_noise), fast digital counters, and[whitening sequences](https://en.wikipedia.org/wiki/Whitening_sequences). Both hardware and software implementations of LFSRs are common.\nThe mathematics of a[cyclic redundancy check](https://en.wikipedia.org/wiki/Cyclic_redundancy_check), used to provide a quick check against transmission errors, are closely related to those of an LFSR.In general, the arithmetics behind LFSRs makes them very elegant as an object to study and implement. One can produce relatively complex logics with simple building blocks. However, other methods, that are less elegant but perform better, should be considered as well.\n<https://en.wikipedia.org/wiki/Linear-feedback_shift_register>"},{"fields":{"slug":"/Computer-Science/General/Outline/","title":"Outline"},"frontmatter":{"draft":false},"rawBody":"# Outline\n\nCreated: 2018-11-14 00:34:53 +0500\n\nModified: 2022-12-10 01:19:35 +0500\n\n---\n-   [Logic in computer science](https://en.wikipedia.org/wiki/Logic_in_computer_science)\n    -   [Formal methods](https://en.wikipedia.org/wiki/Formal_methods)([Formal verification](https://en.wikipedia.org/wiki/Formal_verification))\n    -   [Logic programming](https://en.wikipedia.org/wiki/Logic_programming)\n    -   [Multi-valued logic](https://en.wikipedia.org/wiki/Multi-valued_logic)\n        -   [Fuzzy logic](https://en.wikipedia.org/wiki/Fuzzy_logic)\n    -   [Programming language semantics](https://en.wikipedia.org/wiki/Formal_semantics_of_programming_languages)\n    -   [Type theory](https://en.wikipedia.org/wiki/Type_theory)\n-   [Algorithms](https://en.wikipedia.org/wiki/Algorithm)\n    -   [Computational geometry](https://en.wikipedia.org/wiki/Computational_geometry)\n    -   [Distributed algorithms](https://en.wikipedia.org/wiki/Distributed_algorithms)\n    -   [Parallel algorithms](https://en.wikipedia.org/wiki/Parallel_algorithms)\n    -   [Randomized algorithms](https://en.wikipedia.org/wiki/Randomized_algorithms)\n-   [Artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence)([outline](https://en.wikipedia.org/wiki/Outline_of_artificial_intelligence))\n    -   [Cognitive science](https://en.wikipedia.org/wiki/Cognitive_science)\n        -   [Automated reasoning](https://en.wikipedia.org/wiki/Automated_reasoning)\n        -   [Computer vision](https://en.wikipedia.org/wiki/Computer_vision)([outline](https://en.wikipedia.org/wiki/Outline_of_computer_vision))\n        -   [Machine learning](https://en.wikipedia.org/wiki/Machine_learning)\n            -   [Artificial neural network](https://en.wikipedia.org/wiki/Artificial_neural_network)\n            -   [Support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine)\n        -   [Natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing)([Computational linguistics](https://en.wikipedia.org/wiki/Computational_linguistics))\n    -   [Expert systems](https://en.wikipedia.org/wiki/Expert_systems)\n    -   [Robotics](https://en.wikipedia.org/wiki/Robotics)([outline](https://en.wikipedia.org/wiki/Outline_of_robotics))\n-   [Data structures](https://en.wikipedia.org/wiki/Data_structures)\n-   [Computer architecture](https://en.wikipedia.org/wiki/Computer_architecture)\n-   [Computer graphics](https://en.wikipedia.org/wiki/Computer_graphics)\n    -   [Image processing](https://en.wikipedia.org/wiki/Image_processing)\n    -   [Scientific visualization](https://en.wikipedia.org/wiki/Scientific_visualization)\n-   [Computer communications (networks)](https://en.wikipedia.org/wiki/Computer_networking)\n    -   [Cloud computing](https://en.wikipedia.org/wiki/Cloud_computing)\n    -   [Information theory](https://en.wikipedia.org/wiki/Information_theory)\n    -   [Internet](https://en.wikipedia.org/wiki/Internet),[World Wide Web](https://en.wikipedia.org/wiki/World_Wide_Web)\n    -   [Ubiquitous computing](https://en.wikipedia.org/wiki/Ubiquitous_computing)\n    -   [Wireless computing](https://en.wikipedia.org/wiki/Wireless_computing)([Mobile computing](https://en.wikipedia.org/wiki/Mobile_computing))\n-   [Computer security](https://en.wikipedia.org/wiki/Computer_security)and[reliability](https://en.wikipedia.org/wiki/High_availability)\n    -   [Cryptography](https://en.wikipedia.org/wiki/Cryptography)\n    -   [Fault-tolerant computing](https://en.wikipedia.org/wiki/Fault-tolerant_system)\n-   Computing in[mathematics](https://en.wikipedia.org/wiki/Mathematics),[natural sciences](https://en.wikipedia.org/wiki/Natural_science),[engineering](https://en.wikipedia.org/wiki/Engineering), and[medicine](https://en.wikipedia.org/wiki/Medicine)\n    -   [Algebraic (symbolic) computation](https://en.wikipedia.org/wiki/Symbolic_computation)\n    -   [Computational biology (bioinformatics)](https://en.wikipedia.org/wiki/Computational_biology)\n    -   [Computational chemistry](https://en.wikipedia.org/wiki/Computational_chemistry)\n    -   [Computational mathematics](https://en.wikipedia.org/wiki/Computational_mathematics)\n    -   [Computational neuroscience](https://en.wikipedia.org/wiki/Computational_neuroscience)\n    -   [Computational number theory](https://en.wikipedia.org/wiki/Computational_number_theory)\n    -   [Computational physics](https://en.wikipedia.org/wiki/Computational_physics)\n    -   [Computer-aided engineering](https://en.wikipedia.org/wiki/Computer-aided_engineering)\n        -   [Computational fluid dynamics](https://en.wikipedia.org/wiki/Computational_fluid_dynamics)\n        -   [Finite element analysis](https://en.wikipedia.org/wiki/Finite_element_analysis)\n    -   [Numerical analysis](https://en.wikipedia.org/wiki/Numerical_analysis)\n    -   [Scientific computing (Computational science)](https://en.wikipedia.org/wiki/Scientific_computing)\n-   Computing in[social sciences](https://en.wikipedia.org/wiki/Social_science),[arts](https://en.wikipedia.org/wiki/The_arts),[humanities](https://en.wikipedia.org/wiki/Humanities), and[professions](https://en.wikipedia.org/wiki/Profession)\n    -   [Community informatics](https://en.wikipedia.org/wiki/Community_informatics)\n    -   [Computational economics](https://en.wikipedia.org/wiki/Computational_economics)\n    -   [Computational finance](https://en.wikipedia.org/wiki/Computational_finance)\n    -   [Computational sociology](https://en.wikipedia.org/wiki/Computational_sociology)\n    -   [Digital humanities](https://en.wikipedia.org/wiki/Digital_humanities)(Humanities computing)\n    -   [History of computer hardware](https://en.wikipedia.org/wiki/History_of_computer_hardware)\n    -   [History of computer science](https://en.wikipedia.org/wiki/History_of_computer_science)([outline](https://en.wikipedia.org/wiki/Outline_of_computer_science#History_of_computer_science))\n    -   [Humanistic informatics](https://en.wikipedia.org/wiki/Humanistic_informatics)\n    -   [Database](https://en.wikipedia.org/wiki/Database)([outline](https://en.wikipedia.org/wiki/Outline_of_databases))\n        -   [Distributed database](https://en.wikipedia.org/wiki/Distributed_database)\n        -   [Object database](https://en.wikipedia.org/wiki/Object_database)\n        -   [Relational database](https://en.wikipedia.org/wiki/Relational_database)\n    -   [Data management](https://en.wikipedia.org/wiki/Data_management)\n    -   [Data mining](https://en.wikipedia.org/wiki/Data_mining)\n    -   [Information management](https://en.wikipedia.org/wiki/Information_management)\n    -   [Information retrieval](https://en.wikipedia.org/wiki/Information_retrieval)\n    -   [Knowledge management](https://en.wikipedia.org/wiki/Knowledge_management)\n    -   [Multimedia](https://en.wikipedia.org/wiki/Multimedia),[hypermedia](https://en.wikipedia.org/wiki/Hypermedia)\n        -   [Sound and music computing](https://en.wikipedia.org/wiki/Sound_and_music_computing)\n-   [Distributed computing](https://en.wikipedia.org/wiki/Distributed_computing)\n    -   [Grid computing](https://en.wikipedia.org/wiki/Grid_computing)\n-   [Human-computer interaction](https://en.wikipedia.org/wiki/Human-computer_interaction)\n-   [Operating systems](https://en.wikipedia.org/wiki/Operating_systems)\n-   [Parallel computing](https://en.wikipedia.org/wiki/Parallel_computing)\n    -   [High-performance computing](https://en.wikipedia.org/wiki/High-performance_computing)\n-   [Programming languages](https://en.wikipedia.org/wiki/Programming_languages)\n    -   [Compilers](https://en.wikipedia.org/wiki/Compilers)\n    -   [Programming paradigms](https://en.wikipedia.org/wiki/Programming_paradigms)\n        -   [Concurrent programming](https://en.wikipedia.org/wiki/Concurrent_programming_language)\n        -   [Functional programming](https://en.wikipedia.org/wiki/Functional_programming)\n        -   [Imperative programming](https://en.wikipedia.org/wiki/Imperative_programming)\n        -   [Logic programming](https://en.wikipedia.org/wiki/Logic_programming)\n        -   [Object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming)\n    -   [Program semantics](https://en.wikipedia.org/wiki/Program_semantics)\n    -   [Type theory](https://en.wikipedia.org/wiki/Type_theory)\n-   [Quantum computing](https://en.wikipedia.org/wiki/Quantum_computing)\n\n[The Map of Quantum Computing | Quantum Computers Explained](https://youtu.be/-UlxHPIEVqA)-   [Software engineering](https://en.wikipedia.org/wiki/Software_engineering)\n    -   [Formal methods](https://en.wikipedia.org/wiki/Formal_methods)([Formal verification](https://en.wikipedia.org/wiki/Formal_verification))\n-   [Theory of computation](https://en.wikipedia.org/wiki/Theory_of_computation)\n    -   [Automata theory](https://en.wikipedia.org/wiki/Automata_theory)([Formal languages](https://en.wikipedia.org/wiki/Formal_languages))\n    -   [Computability theory](https://en.wikipedia.org/wiki/Computability_theory_(computer_science))\n    -   [Computational complexity theory](https://en.wikipedia.org/wiki/Computational_complexity_theory)\n    -   [Concurrency theory](https://en.wikipedia.org/wiki/Concurrency_(computer_science)#Theory)\n-   [VLSI design](https://en.wikipedia.org/wiki/Very-large-scale_integration)\n"},{"fields":{"slug":"/Computer-Science/General/Standards/","title":"Standards"},"frontmatter":{"draft":false},"rawBody":"# Standards\n\nCreated: 2019-04-21 23:29:05 +0500\n\nModified: 2022-02-05 19:03:23 +0500\n\n---\n\n**ASCII**\n\n48-57 - 0-9\n\n65-90 - A-Z\n\n97-122 - a-z\n**base64**\n\nIn[computer science](https://en.wikipedia.org/wiki/Computer_science),Base64is a group of[binary-to-text encoding](https://en.wikipedia.org/wiki/Binary-to-text_encoding)schemes that represent[binary data](https://en.wikipedia.org/wiki/Binary_data)in an[ASCII](https://en.wikipedia.org/wiki/ASCII)string format by translating it into a[radix](https://en.wikipedia.org/wiki/Radix)-64 representation. The termBase64originates from a specific[MIME content transfer encoding](https://en.wikipedia.org/wiki/MIME#Content-Transfer-Encoding). Each Base64 digit represents exactly 6 bits of data. Three 8-bit bytes (i.e., a total of 24 bits) can therefore be represented by four 6-bit Base64 digits.\nCommon to all binary-to-text encoding schemes, Base64 is designed to carry data stored in binary formats across channels that only reliably support text content. Base64 is particularly prevalent on the[World Wide Web](https://en.wikipedia.org/wiki/World_Wide_Web)[[1]](https://en.wikipedia.org/wiki/Base64#cite_note-1)where its uses include the ability to embed[image files](https://en.wikipedia.org/wiki/Image_files)or other binary assets inside textual assets such as[HTML](https://en.wikipedia.org/wiki/HTML) and [CSS](https://en.wikipedia.org/wiki/CSS) files.\nThe difference between Base64 and hex is really just how bytes are represented. Hex is another way of saying \"Base16\". **Hex will take two characters for each byte - Base64 takes 4 characters for every 3 bytes, so it's more efficient than hex.** Assuming you're using UTF-8 to encode the XML document, a 100K file will take 200K to encode in hex, or 133K in Base64.\n<https://en.wikipedia.org/wiki/Base64>\n\n## Base64 vs UTF-8/UTF-16**\n\nUTF-8andUTF-16are methods to encode Unicode strings to byte sequences.\n\nBase64is a method to encode a byte sequence to a string.\nBase64 is a way to encode binary data, while UTF8 and UTF16 are ways to encode Unicode text.\nThings to keep in mind:\n-   Not every byte sequence represents an Unicode string encoded in UTF-8 or UTF-16.\n-   Not every Unicode string represents a byte sequence encoded in Base64.\n<https://stackoverflow.com/questions/3866316/whats-the-difference-between-utf8-utf16-and-base64-in-terms-of-encoding>\n\n## Unicode**\n\nA **character** is a minimal unit of text that has semantic value.\nA **character set** is a collection of characters that might be used by multiple languages. For example, the Latin character set is used by English and most European languages, though the Greek character set is used only by the Greek language.\nA**coded character set**is a character set where each character is assigned a unique number.\nA**code point**is a value that can be used in a coded character set. A code point is a 32-bitintdata type, where the lower 21 bits represent a valid code point value and the upper 11 bits are 0. Code point is a character and this is represented by one or more code units depending on the encoding.\n**Unicode**\n\nUnicode is a computing industry standard for the consistent encoding, representation, and handling of text expressed in most of the world's writing systems.\nThe latest version contains a repertoire of 136,755[characters](https://en.wikipedia.org/wiki/Character_(computing))covering 139 modern and historic[scripts](https://en.wikipedia.org/wiki/Script_(Unicode)), as well as multiple symbol sets.\nA Unicode**code unit**is a 16-bitcharvalue. For example, imagine aStringthat contains the letters \"abc\" followed by the Deseret LONG I, which is represented with twocharvalues. That string contains four characters, four code points, but five code units. Code unit is the number of bits an encoding uses. So UTF-8 would use 8 and UTF-16 would use 16 units.\nTo express a character in Unicode, the hexadecimal value is prefixed with the string U+. The valid code point range for the Unicode standard is U+0000 to U+10FFFF, inclusive. The code point value for the Latin character A is U+0041. The character â‚¬ which represents the Euro currency, has the code point value U+20AC. The first letter in the Deseret alphabet, the LONG I, has the code point value U+10400.\n\nThe following table shows code point values for several characters:\n\n| Character       | Unicode Code Point | Glyph                                                                                                                                                                                                              |\n|--------------------------|----------------------------------|------------|\n| Latin A         | U+0041             | ![The Latin character A](media/Standards-image1.gif)                                                     |\n| Latin sharp S   | U+00DF             | ![The Latin small letter sharp S](media/Standards-image2.gif)                                            |\n| Han for East    | U+6771             | ![The Han character for east, eastern or eastward](media/Standards-image3.gif) |\n| Deseret, LONG I | U+10400            | ![The Deseret capital letter long I](media/Standards-image4.gif)               |\n\nAs previously described, characters that are in the range U+10000 to U+10FFFF are called supplementary characters. The set of characters from U+0000 to U+FFFF are sometimes referred to as theBasic Multilingual Plane (BMP).\n<https://www.asciitohex.com>\n\n## Control Characters**\n\nAcontrol[character](https://en.wikipedia.org/wiki/Character_(computing))ornon-printing character(NPC) is a[code point](https://en.wikipedia.org/wiki/Code_point)(a[number](https://en.wikipedia.org/wiki/Number)) in a[character set](https://en.wikipedia.org/wiki/Character_encoding), that does not represent a written symbol. They are used as[in-band signaling](https://en.wikipedia.org/wiki/In-band_signaling)to cause effects other than the addition of a symbol to the text. All other characters are mainly printing, printable, or[graphic characters](https://en.wikipedia.org/wiki/Graphic_character), except perhaps for the \"space\" character (see[ASCII printable characters](https://en.wikipedia.org/wiki/ASCII_printable_characters)).\nThe control characters in ASCII still in common use include:\n-   0 ([null](https://en.wikipedia.org/wiki/Null_character),NUL,[0](https://en.wikipedia.org/wiki/%5C0),[^@](https://en.wikipedia.org/wiki/%5E@)), originally intended to be an ignored character, but now used by many[programming languages](https://en.wikipedia.org/wiki/Programming_language)including[C](https://en.wikipedia.org/wiki/C_programming_language)to mark the end of a string.\n-   7 ([bell](https://en.wikipedia.org/wiki/Bell_character),BEL,[a](https://en.wikipedia.org/wiki/%5Ca),[^G](https://en.wikipedia.org/wiki/%5EG)), which may cause the device to emit a warning such as a bell or beep sound or the screen flashing.\n-   8 ([backspace](https://en.wikipedia.org/wiki/Backspace),BS,b,[^H](https://en.wikipedia.org/wiki/%5EH)), may overprint the previous character.\n-   9 ([horizontal tab](https://en.wikipedia.org/wiki/Tab_key),HT,[t](https://en.wikipedia.org/wiki/%5Ct),[^I](https://en.wikipedia.org/wiki/%5EI)), moves the printing position right to the next tab stop.\n-   10 ([line feed](https://en.wikipedia.org/wiki/Newline),LF,[n](https://en.wikipedia.org/wiki/%5Cn),[^J](https://en.wikipedia.org/wiki/%5EJ)), moves the print head down one line, or to the left edge and down. Used as the end of line marker in most[UNIX systems](https://en.wikipedia.org/wiki/Unix)and variants.\n-   11 ([vertical tab](https://en.wikipedia.org/wiki/Tab_key),VT,[v](https://en.wikipedia.org/wiki/%5Cv),[^K](https://en.wikipedia.org/wiki/%5EK)), vertical tabulation.\n-   12 ([form feed](https://en.wikipedia.org/wiki/Page_break),FF,[f](https://en.wikipedia.org/wiki/%5Cf),[^L](https://en.wikipedia.org/wiki/%5EL)), to cause a printer to eject paper to the top of the next page, or a video terminal to clear the screen.\n-   13 ([carriage return](https://en.wikipedia.org/wiki/Carriage_return),CR,[r](https://en.wikipedia.org/wiki/%5Cr),[^M](https://en.wikipedia.org/wiki/%5EM)), moves the printing position to the start of the line, allowing overprinting. Used as the end of line marker in[Classic Mac OS](https://en.wikipedia.org/wiki/Classic_Mac_OS),[OS-9](https://en.wikipedia.org/wiki/OS-9),[FLEX](https://en.wikipedia.org/wiki/FLEX_(operating_system))(and variants). ACR+LFpair is used by[CP/M](https://en.wikipedia.org/wiki/CP/M)-80 and its derivatives including[DOS](https://en.wikipedia.org/wiki/DOS)and[Windows](https://en.wikipedia.org/wiki/Microsoft_Windows), and by[Application Layer](https://en.wikipedia.org/wiki/Application_Layer)[protocols](https://en.wikipedia.org/wiki/Communications_protocol)such as[FTP](https://en.wikipedia.org/wiki/File_Transfer_Protocol),[SMTP](https://en.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol), and[HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol).\n-   26 ([Control-Z](https://en.wikipedia.org/wiki/Control-Z),SUB,EOF,[^Z](https://en.wikipedia.org/wiki/%5EZ)). Acts as an end-of-file for the Windows text-mode file i/o.\n-   27 ([escape](https://en.wikipedia.org/wiki/Escape_character),ESC,[e](https://en.wikipedia.org/wiki/%5Ce)([GCC](https://en.wikipedia.org/wiki/GCC_(software))only),^[). Introduces an[escape sequence](https://en.wikipedia.org/wiki/Escape_sequence).\n[**https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/**](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)\n\nIt does not make sense to have a string without knowing what encoding it uses.\n**ISO/IEC 5218**\n-   0 = Not known;\n-   1 = Male;\n-   2 = Female;\n-   9 = Not applicable.\n<https://www.wikiwand.com/en/ISO/IEC_5218>\n\n## Licenses**\n\nGPL Gnu General Public License\n\nCDDL Common Development and Distribution License\n\nApache License (APL)\n<https://choosealicense.com/appendix>\n"},{"fields":{"slug":"/Computer-Science/Interview-Question/Blogs---Conferences---Blogging---Presentation---Tech-Thursdays/","title":"Blogs / Conferences / Blogging / Presentation / Tech Thursdays"},"frontmatter":{"draft":false},"rawBody":"# Blogs / Conferences / Blogging / Presentation / Tech Thursdays\n\nCreated: 2019-04-17 20:37:48 +0500\n\nModified: 2022-09-16 00:32:48 +0500\n\n---\n\n**Startup onboarding series**\n\n1.  Setting up laptop (mac)\n\n2.  Setting up git and tutorial (hands on) with branching strategy (github)\n\n3.  Setting up kubernetes cluster access in terminal\n\n4.  Setting up kubernetes infrastructure\n\n    a.  prometheus\n\n    b.  elk\n\n    c.  istio\n\n    d.  vault (secrets manager)\n\n    e.  airflow\n\n5.  Setting up aws infra and access (console, s3, athena, glue, etc)\n\n6.  Python/flask best practices\n\n7.  Coding best practices\n\n8.  Data infra - kafka and all\n\n9.  Setting up staging, QA and prod\n<https://www.freecodecamp.org/news/how-to-start-a-blog-book>\nQuestion-Based vs Statement-Based Headline\n\nProblem based vs Solution based Headline\n\nCurious vs Non-curious headlines\n**Series**\n\n1.  Docker\n\n2.  Kubernetes\n\n    a.  Operators\n\n    b.  Helm\n\n3.  Prometheus\n\n4.  Grafana\n\n5.  InfluxDB\n\n6.  Telegraf\n\n7.  ELK\n\n    a.  ElasticSearch\n\n    b.  Logstash\n\n    c.  FileBeat\n\n    d.  Kibana\n\n8.  API Gateway\n\n    a.  Kong9.  **Deploy everything**\n**Brokers**\n\n1.  EMQX\n\n2.  Kafka (series)\n\n    a.  Connect\n\n    b.  Kafka\n\n    c.  Producers / Consumers\n\n3.  vernemq\nGraphQL\nHow I Learned Series\nLevel up - As a Developer, series\n**Conferences**\n\n1.  How to be a detective in Kubernetes (Debugging in kubernetes)\n\n2.  Failcon\n\n3.  Pycon\n<https://dev.to/stephsmithio/writing-is-thinking-learning-to-write-with-confidence-1ijh>\n\n## Course maker**\n\n<https://teachable.com>\n\n## Projects**\n-   Monte carlo simulation for dice roll with one dice and two dice\n**Presentations**\n\n<https://www.slideshare.net/billkarwin/extensible-data-modeling>\n\n## Tech Thursdays**\n\n**Icebreakers**\n-   Typing Games (TypingCat, TypeRacer)\n-   GitGames\n-   **Hackathon / Coding**\n-   **Guess the word**\n-   Capture The Flag\n-   1 GB Sort\n-   Scavenger/Treasure Hunt\n-   PythonChallenge\n-   <http://flaws.cloud>\n-   **Game Days**\n    -   Killing something in production\n-   Blue/Green teams\n    -   Security testing\n-   Productivity\n    -   Onenote (note taking)\n    -   Podcast\n    -   Blinkist\n-   Bombsquad game - mac mini\n-   Gully Cricket - Parking lot\n-   Git\n\n<https://github.com/git-game/git-game>\n\n<https://github.com/deepaksood619/GitGames>\n\n## SQL Games**\n\n<https://mystery.knightlab.com>\n\n<https://selectstarsql.com>\n\n<https://www.hackerrank.com/domains/sql>\n\n<https://www.w3schools.com/sql/trysql.asp?filename=trysql_asc>\n\n<https://www.w3schools.com/mysql/trymysql.asp?filename=trysql_select_where_and_or>\n\n<https://use-the-index-luke.com/3-minute-test/mysql>\nTerraria\n**Assessments**\n\n<https://www.bigshyft.com/assessments/rd9pl>\n\n<https://www.bigshyft.com/assessments/x9q4g>\n\n## System Design**\n-   Airbnb: <https://lnkd.in/dAPjjaA3>\n-   Amazon: <https://lnkd.in/dyp43Yqp>\n-   Asana: <https://lnkd.in/dWqZxf6Y>\n-   Atlassian: <https://lnkd.in/d-i34bUQ>\n-   Bittorrent: <https://lnkd.in/dfZPa6Ma>\n-   Cloudera: <https://blog.cloudera.com>\n-   Docker: <https://blog.docker.com>\n-   Dropbox: <https://lnkd.in/dUQJTxac>\n-   eBay: <https://lnkd.in/dnmca2uT>\n-   Facebook: <https://lnkd.in/dbwkUDjN>\n-   GitHub: <https://lnkd.in/dSC9StzD>\n-   Google: <https://lnkd.in/ddPVy6Zj>\n-   Groupon: <https://lnkd.in/dsyGvUWF>\n-   Highscalability: <http://highscalability.com>\n-   Instacart: <https://tech.instacart.com>\n-   Instagram: <https://lnkd.in/dEs6FyGn>\n-   Linkedin: <https://lnkd.in/d_yQe9g6>\n-   Mixpanel: <https://mixpanel.com/blog>\n-   Netflix: <https://lnkd.in/dKhbQqxd>\n-   Nextdoor: <https://lnkd.in/dDdGPQgR>\n-   PayPal: <https://lnkd.in/d9YkeE_h>\n-   Pinterest: <https://lnkd.in/duz8a8vq>\n-   Quora: <https://lnkd.in/d-iuzYZq>\n-   Reddit: <https://redditblog.com>\n-   Salesforce: <https://lnkd.in/dV9unb47>\n-   Shopify: <https://lnkd.in/dQtK4TME>\n-   Slack: <https://slack.engineering>\n-   Soundcloud: <https://lnkd.in/dgWK_v4h>\n-   Spotify: <https://labs.spotify.com>\n-   Stripe: <https://lnkd.in/dm-WBTgr>\n-   System design primer: <https://lnkd.in/dnUnsQE9>\n-   Twitter: <https://lnkd.in/d9tmm5wj>\n-   Thumbtack: <https://lnkd.in/d6QTWF_p>\n-   Uber: <http://eng.uber.com>\n-   Yahoo: <https://lnkd.in/dKgyhbNE>\n-   Yelp: <https://lnkd.in/d_6hhMS4>\n-   Zoom: <https://lnkd.in/dquH3cKY>\n"},{"fields":{"slug":"/Computer-Science/Interview-Question/Coding-Interview-Questions/","title":"Coding Interview Questions"},"frontmatter":{"draft":false},"rawBody":"# Coding Interview Questions\n\nCreated: 2018-06-23 23:39:22 +0500\n\nModified: 2022-05-24 23:10:04 +0500\n\n---\n\nPlatforms\n\n[https://coderpad.io](https://coderpad.io/)\n\n[https://codeshare.io](https://codeshare.io/)\n\n<https://foobar.withgoogle.com>-   print - given n*m print incrementally in clockwise direction\n\n1 2 3\n\n8 9 4\n\n7 6 5-   Divide a number into perfect grid with minimum waste where number of column is fixed.\n\nEx - 19 should return 4/5 instead of 3/7\n**Shortest range in k-sorted lists**\n\n[SHORTEST RANGE IN K SORTED LISTS - CODING INTERVIEW QUESTION AT GOOGLE, APPLE OR FACEBOOK](https://www.youtube.com/watch?v=zplklOy7ENo)\n![SHORTEST RANGE K ORTED LISTS ](media/Coding-Interview-Questions-image1.jpg)\n**Permutations of a given string**\n\n**# 1 Python**\n\ndef toString(List):\n\nreturn ''.join(List)\n\ndef permute(a, l, r):\n\nif l==r:\n\nprint(toString(a))\n\nelse:\n\nfor i in range(l,r+1):\n\na[l], a[i] = a[i], a[l]\n\npermute(a, l+1, r)\n\na[l], a[i] = a[i], a[l]\n\n# Driver program to test the above function\n\nstring = \"ABC\"\n\nn = len(string)\n\na = list(string)\n\npermute(a, 0, n-1)\n\n## 2 Java\n\nvoid func(String str) {\n\nfunc(str, \"\");\n\n}\nvoid func (String str, String prefix) {\n\nif (str.length() == 0) {\n\nSystem.out.println(prefix);\n\n} else {\n\nfor (int i = 0; i < str.length(); i++) {\n\nString rem = str.substring(0, i) + str.substring(i + 1);\n\nfunc(rem, prefix + str.charAt( i ));\n\n}\n\n}\n\n}\n\n## 3 Python library functions\n```\n    from itertools import permutations\n\n    len(list(permutations('abcde', 5)))\n```\n\n## Others\n-   <https://www.geeksforgeeks.org/split-the-given-array-into-k-sub-arrays-such-that-maximum-sum-of-all-sub-arrays-is-minimum>\n-   <https://practice.geeksforgeeks.org/problems/lru-cache/1>-   **Interview (medium difficulty) - <https://practice.geeksforgeeks.org/problems/largest-even-number3821/1#_=_>**\n-   **Easy (5-10 mins max) - [https://practice.geeksforgeeks.org/problems/leaders-in-an-array-1587115620/1#](https://practice.geeksforgeeks.org/problems/leaders-in-an-array-1587115620/1)**\n-   **Easy (5-10 mins max) - <https://practice.geeksforgeeks.org/problems/non-repeating-element3958/1>**\n-   [**https://practice.geeksforgeeks.org/problems/trapping-rain-water-1587115621/1**](https://practice.geeksforgeeks.org/problems/trapping-rain-water-1587115621/1)\n-   [**https://practice.geeksforgeeks.org/problems/sort-an-array-of-0s-1s-and-2s4231/1**](https://practice.geeksforgeeks.org/problems/sort-an-array-of-0s-1s-and-2s4231/1)-   <https://www.geeksforgeeks.org/find-a-tour-that-visits-all-stations>\n-   <https://www.geeksforgeeks.org/find-local-minima-array>\n-   <https://www.geeksforgeeks.org/equilibrium-index-of-an-array>\n-   Given k sorted arrays of size n each, merge them and print the sorted output.\n-   <https://www.hackerrank.com/challenges/qheap1/problem>\n-   [Circular Array Rotation](https://www.hackerrank.com/challenges/circular-array-rotation/problem)\n-   <https://practice.geeksforgeeks.org/problems/sort-in-specific-order2422/1>\n-   <https://www.geeksforgeeks.org/print-nodes-at-k-distance-from-root>\n-   <https://www.geeksforgeeks.org/tree-isomorphism-problem>\n-   <https://practice.geeksforgeeks.org/problems/determine-if-two-trees-are-identical/1>\n-   <https://practice.geeksforgeeks.org/problems/check-for-balanced-tree/1>-   **Medium (O(n)/O(nlogn) - 10 mins, corner cases) - <https://www.hackerrank.com/challenges/flatland-space-stations/problem>**\n-   **Medium - <https://www.hackerrank.com/challenges/bigger-is-greater/problem>**\n-   **Medium - <https://www.hackerrank.com/challenges/beautiful-triplets/problem>**\n-   **Medium - <https://www.hackerrank.com/challenges/kaprekar-numbers/problem>**-   <https://practice.geeksforgeeks.org/problems/coin-change2448/1>\n<https://www.geeksforgeeks.org/must-coding-questions-company-wise>\n\n<https://www.geeksforgeeks.org/practice-for-cracking-any-coding-interview>\n\n## Interview**\n-   How do you find the missing number in a given integer array of 1 to 100?\n-   How do you find the third node from the end in a singly linked list?\n-   How do you find all pairs of an integer array whose sum is equal to a given number?\n-   How do you find the middle element of a singly linked list ?\n-   How do you reverse a singly linked list without recursion?\n-   How do you print the first non-repeated character from a string?\n-   How is a radix sort algorithm implemented?\n-   Write Algorithms to Check if Two String are Anagram\n**Interview Questions**\n\n**Question -** Write a function to return first n number of elements of Fibonacci series (code must be clean and all corners cases handled)\n\n**Example -** For n = 6 return [0,1,1,2,3,5]\n**Question -** Write a function to find factorial of a number (code must be clean and all corners cases handled)\n\n**Example -** For n = 5 return 120\n\n**Driver function -**\n\nn = 10000000\n\nfor i in range(0, n):\n\nprint ( fact (random(0, n) )-   Write a recursive based solution\n-   Can you convert this recursion into non-recursive code (using for/while)\n-   How can you improve on the current solution (Hint: DP)\n**Question -** Design a data structure that supports insert, delete, search and getRandom in constant time\n\n**insert(x):** Inserts an item x to the data structure if not already present.\n\n**delete(x):** Removes an item x from the data structure if present.\n\n**search(x):** Searches an item x in the data structure.\n\n**getRandom():** Returns a random element from current set of elements\n**Question**\n\nGiven an array of integers where every integer occurs three times except for one integer,\n\nwhich only occurs once, find and return the non-duplicated integer.\n\nFor example, given [6, 1, 3, 3, 3, 6, 6], return 1. Given [13, 19, 13, 13], return 19.\n\nDo this in O(N) time and O(1) space.\n**Question**\n\nGiven an array of strictly the characters 'R', 'G', and 'B',\n\nsegregate the values of the array so that all the Rs come first,\n\nthe Gs come second, and the Bs come last. You can only swap elements of the array.\nDo this in linear time and in-place.\nFor example, given the array ['G', 'B', 'R', 'R', 'B', 'R', 'G'],\n\nit should become ['R', 'R', 'R', 'G', 'G', 'B', 'B'].\n**Question**\n\nGiven a sorted list of integers, square the elements and give the output in sorted order.\n\nFor example, given [-9, -2, 0, 2, 3], return [0, 4, 4, 9, 81].\n**Question**\n\nWrite a function that prints the least integer that is not present in a given list and cannot be represented by the summation of the sub-elements of the list.\nE.g. Fora = [1,2,5,7]the least integer not represented by the list or a slice of the list is 4, and ifa = [1,2,2,5,7]then the least non-representable integer is 18.\n**Question**\n\nHow would you optimally calculate p^k, where k is a non-negative integer? What is the complexity of the solution?\nFirst, let's mention that the trivial solution has the complexity of O(k). The problem can be solved by squaring and multiplying.\nWe know that p^k = p^x * p^y if x+y=k. We also know that p^k = (p^a)^b if a*b=k.\nFor an even value of k, choosing a = 2 and b = k/2, thus having p^k = (p^2)^(k/2), will reduce the number of required multiplications almost in half. For an odd value of k, choosing x = 1 and y=k-1 will result in y being even, and we can then simply repeat the same process as for the even case. This allows us to define a recursive function:\n\nFUNCTION pow(base, exponent)\nIF exponent == 0\nRETURN 1\nELSE IF exponent is even\nRETURN pow(base * base, exponent / 2)\nELSE\nRETURN base * pow(base * base, (exponent - 1) / 2)\nEND IF\n\nThis solution results in a complexity of O(log k).\n**Others**\n\nTopological sort\n\nDag resolving (DAG)\n\nTraversals (BFS, DFS, Pre, Post, in)\n\nLongest Consecutive Sequence\n\nDAG\n\nLP Solver\n\nSubset sum\n-   N-Queen Problem\n-   Word Ladder Problem\n-   Knight's Tour Problem\nFind all subsets of a given set (recursion + dp)\n\nFind a sum in a subset\n\nPython | Get all substrings of given string\n\n<https://www.geeksforgeeks.org/python-get-all-substrings-of-given-string>\n\nEgg drop - <https://www.geeksforgeeks.org/egg-dropping-puzzle-dp-11>\n\nFind number of duplets in a positive integer array without using space in O(n) time complexity (save the information in the same array using index)\n\nDjkstra's algo\n\nSudoku solver\n\nChess solver\nB+TREE\n\nFinding number of duplicates in an array\n\nPython collision resolution\n\nImplementing column oriented database on disk\n\nImplementing hash table\n\nImplementing LRU\n\nImplementing min stack (where minimum element can be fetched in o(1))\n\nImplementing queue using two stack\n\nImplementing stack using two queue\nFind the subsequence with largest sum of elements in an array\n\n<https://stackoverflow.com/questions/3733251/find-the-subsequence-with-largest-sum-of-elements-in-an-array>\n\nMaximum product subarray\n\nBalanced parenthesis problem\n**Writing programming interview questions hasn't made me rich yet ... so I might give up and start trading Apple stocks all day instead.**\n\nFirst, I wanna know how much money Icould havemade yesterday if I'd been trading Apple stocks all day.\nSo I grabbed Apple's stock prices from yesterday and put them ina listcalledstock_prices, where:\n-   Theindicesare the time (in minutes) past trade opening time, which was 9:30am local time.\n-   Thevaluesare the price (in US dollars) of one share of Apple stock at that time.\nSo if the stock cost $500 at 10:30am, that meansstock_prices[60] = 500.\nWrite an efficientfunctionthat takesstock_pricesand returnsthe best profit I could have made from one purchase and one sale of one share of Apple stock yesterday.\n\nFor example:\n\nstock_prices = [10, 7, 5, 8, 11, 9]\n\nget_max_profit(stock_prices)\n# Returns 6 (buying for $5 and selling for $11)\n\nNo \"shorting\"---you need to buy before you can sell. Also, you can't buyandsell in the same time step---at least 1 minute has to pass.\n**Gotchas**\n-   You can't just take the difference between the highest price and the lowest price, because the highest price might comebeforethe lowest price. And you have to buy before you can sell.\n-   What if the pricegoes down all day? In that case, the best profit will benegative.\n-   You can do this inO(n)O(n)time andO(1)O(1)space!\n**Solution -**\n\ndef best_profit_from_stock(stock_prices):\n\nprofit = 0\n\nif stock_prices:\n\nmin_stock = stock_prices[0]\n\nfor i in range(1, len(stock_prices)):\n\ncurr_profit = stock_prices[i] - min_stock\n\nif curr_profit > profit:\n\nprofit = curr_profit\n\nelse:\n\nmin_stock = stock_prices[i]\nreturn profit\nassert best_profit_from_stock([10, 7, 5, 8, 11, 9, 2, 10, 1]) == 8\n\nassert best_profit_from_stock([10, 5, 7, 8]) == 3\n\nassert best_profit_from_stock([3,2,1]) == 0\n\nassert best_profit_from_stock([100,2]) == 0\n\nassert best_profit_from_stock([]) == 0\n\nassert best_profit_from_stock([1]) == 0\n\nassert best_profit_from_stock([1,2]) == 1\n\nassert best_profit_from_stock([1,2,3]) == 2\n\nassert best_profit_from_stock([1,2,4]) == 3\n\nassert best_profit_from_stock([1,2,4,100]) == 99\n\nassert best_profit_from_stock([2,100]) == 98\n\nassert best_profit_from_stock([-1]) == 0\n\nassert best_profit_from_stock([-1,-2]) == 0\n<https://www.interviewcake.com/question/java/stock-price>\n<https://www.freecodecamp.org/news/10-common-coding-interview-problems-solved>\n\n"},{"fields":{"slug":"/Computer-Science/Interview-Question/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-06-24 06:18:49 +0500\n\nModified: 2021-05-31 17:13:01 +0500\n\n---\n\n<https://dev.to/fahimulhaq/top-10-system-design-interview-questions-for-software-engineers>\n\n<https://www.linkedin.com/pulse/six-steps-tackle-any-system-design-interview-question-varun-hasija>\n\n<https://leetcode.com/discuss/career/229177/My-System-Design-Template>\n\n## Uber Big Data Platform**\n\n<https://eng.uber.com/uber-big-data-platform>\n\n## Resources**\n\n<https://www.pramp.com/#>\n\n<https://codesignal.com>\n"},{"fields":{"slug":"/Computer-Science/Interview-Question/System-Design---Autocomplete-or-TypeAhead/","title":"System Design - Autocomplete or TypeAhead"},"frontmatter":{"draft":false},"rawBody":"# System Design - Autocomplete or TypeAhead\n\nCreated: 2018-04-24 19:12:46 +0500\n\nModified: 2021-12-04 17:36:03 +0500\n\n---\n\n[System design : Design Autocomplete or Typeahead Suggestions for Google search](https://www.youtube.com/watch?v=us0qySiUsGU)\nUse Case\n\n1.  Suggestions for Google Search\nModules\n\n1.  Autocorrect for misspelled words\n\n2.  Location based results\n\n3.  Personalized results\n\n4.  More than one Language\nData Structure\n\n1.  Distributed Trie\nImportant Concepts\n\n1.  Request Flow\n    -   Get a prefix, look into distributed trie and return results\n    -   Store top k prefixes on each node\n\n2.  Data Collection Flow\n    -   Getting list of strings from a background process, aggregate them and update trie\n    -   Aggregators\n    -   Appliers\nRequest Flow\n\n1.  User types query in search bar\n\n2.  Query goes to Load Balancer\n\n3.  Load balancer routes the request to one of several nodes (using some algorithm like round robin algorithm)\n\n4.  Selected node looks into distributed cache (redis/memcached)\n\n5.  If not found, looks into zookeeper instance for finding the Trie that is reponsible for the given prefix.\n\n6.  Get the prefixes from Trie, populate the cache for future use and return the suggestions.\nOptimizations\n\n1.  Cache in CDN (Content Delivery Networks)\n\n2.  Cache results in Local Machine\n![ii9 ](media/System-Design---Autocomplete-or-TypeAhead-image1.png)\n![DO 0 ](media/System-Design---Autocomplete-or-TypeAhead-image2.png)\n![Neo ãƒ— ãƒ£ ](media/System-Design---Autocomplete-or-TypeAhead-image3.png)\n"},{"fields":{"slug":"/Computer-Science/Interview-Question/System-Design---Google-Search/","title":"System Design - Google Search"},"frontmatter":{"draft":false},"rawBody":"# System Design - Google Search\n\nCreated: 2018-05-07 19:43:06 +0500\n\nModified: 2022-05-30 21:41:09 +0500\n\n---\n\n[How does Google Search work?](https://www.youtube.com/watch?v=KyCYyoGusqs)-   Can crawl according to page ranks (high reputation, links from other sites to yours)\n-   2002 - Update fretz\n    -   Crawl top sites often\n    -   Have supplemental index that are crawled after long periods of time\n-   Indexing is done in words consisting of documents order\n-   All the indexes are parallelized with each consisting some part of the web\n\n## What happens when you type google.com into your browser's address box and press enter?\n-   [The \"g\" key is pressed](https://github.com/alex/what-happens-when/blob/master/README.rst#the-g-key-is-pressed)\n-   [The \"enter\" key bottoms out](https://github.com/alex/what-happens-when/blob/master/README.rst#the-enter-key-bottoms-out)\n-   [Interrupt fires [NOT for USB keyboards]](https://github.com/alex/what-happens-when/blob/master/README.rst#interrupt-fires-not-for-usb-keyboards)\n-   [(On Windows) AWM_KEYDOWNmessage is sent to the app](https://github.com/alex/what-happens-when/blob/master/README.rst#on-windows-a-wm-keydown-message-is-sent-to-the-app)\n-   [(On OS X) AKeyDownNSEvent is sent to the app](https://github.com/alex/what-happens-when/blob/master/README.rst#on-os-x-a-keydown-nsevent-is-sent-to-the-app)\n-   [(On GNU/Linux) the Xorg server listens for keycodes](https://github.com/alex/what-happens-when/blob/master/README.rst#on-gnu-linux-the-xorg-server-listens-for-keycodes)\n-   [Parse URL](https://github.com/alex/what-happens-when/blob/master/README.rst#parse-url)\n-   [Is it a URL or a search term?](https://github.com/alex/what-happens-when/blob/master/README.rst#is-it-a-url-or-a-search-term)\n-   [Convert non-ASCII Unicode characters in hostname](https://github.com/alex/what-happens-when/blob/master/README.rst#convert-non-ascii-unicode-characters-in-hostname)\n-   [Check HSTS list](https://github.com/alex/what-happens-when/blob/master/README.rst#check-hsts-list)\n-   [DNS lookup](https://github.com/alex/what-happens-when/blob/master/README.rst#dns-lookup)\n-   [ARP process](https://github.com/alex/what-happens-when/blob/master/README.rst#arp-process)\n-   [Opening of a socket](https://github.com/alex/what-happens-when/blob/master/README.rst#opening-of-a-socket)\n-   [TLS handshake](https://github.com/alex/what-happens-when/blob/master/README.rst#tls-handshake)\n-   [HTTP protocol](https://github.com/alex/what-happens-when/blob/master/README.rst#http-protocol)\n-   [HTTP Server Request Handle](https://github.com/alex/what-happens-when/blob/master/README.rst#http-server-request-handle)\n-   [Behind the scenes of the Browser](https://github.com/alex/what-happens-when/blob/master/README.rst#behind-the-scenes-of-the-browser)\n-   [Browser](https://github.com/alex/what-happens-when/blob/master/README.rst#browser)\n-   [HTML parsing](https://github.com/alex/what-happens-when/blob/master/README.rst#html-parsing)\n-   [CSS interpretation](https://github.com/alex/what-happens-when/blob/master/README.rst#css-interpretation)\n-   [Page Rendering](https://github.com/alex/what-happens-when/blob/master/README.rst#page-rendering)\n-   [GPU Rendering](https://github.com/alex/what-happens-when/blob/master/README.rst#gpu-rendering)\n-   [Window Server](https://github.com/alex/what-happens-when/blob/master/README.rst#window-server)\n-   [Post-rendering and user-induced execution](https://github.com/alex/what-happens-when/blob/master/README.rst#post-rendering-and-user-induced-execution)\n\n## Browser\nOnce the server supplies the resources (HTML, CSS, JS, images, etc.) to the browser it undergoes the below process:\n-   Parsing - HTML, CSS, JS\n-   Rendering - Construct DOM Tree â†’ Render Tree â†’ Layout of Render Tree â†’ Painting the render tree\n\n## Browser High Level Structure\nThe components of the browsers are:\n-   User interface:The user interface includes the address bar, back/forward button, bookmarking menu, etc. Every part of the browser display except the window where you see the requested page.\n-   Browser engine:The browser engine marshals actions between the UI and the rendering engine.\n-   Rendering engine:The rendering engine is responsible for displaying requested content. For example if the requested content is HTML, the rendering engine parses HTML and CSS, and displays the parsed content on the screen.\n-   Networking:The networking handles network calls such as HTTP requests, using different implementations for different platforms behind a platform-independent interface.\n-   UI backend:The UI backend is used for drawing basic widgets like combo boxes and windows. This backend exposes a generic interface that is not platform specific. Underneath it uses operating system user interface methods.\n-   JavaScript engine:The JavaScript engine is used to parse and execute JavaScript code.\n-   Data storage:The data storage is a persistence layer. The browser may need to save all sorts of data locally, such as cookies. Browsers also support storage mechanisms such as localStorage, IndexedDB, WebSQL and FileSystem.\n\n## HTML parsing\nThe rendering engine starts getting the contents of the requested document from the networking layer. This will usually be done in 8kB chunks.\nThe primary job of HTML parser is to parse the HTML markup into a parse tree.\nThe output tree (the \"parse tree\") is a tree of DOM element and attribute nodes. DOM is short for Document Object Model. It is the object presentation of the HTML document and the interface of HTML elements to the outside world like JavaScript. The root of the tree is the \"Document\" object. Prior of any manipulation via scripting, the DOM has an almost one-to-one relation to the markup.\n\n### The parsing algorithm\nHTML cannot be parsed using the regular top-down or bottom-up parsers.\nThe reasons are:\n-   The forgiving nature of the language.\n-   The fact that browsers have traditional error tolerance to support well known cases of invalid HTML.\n-   The parsing process is reentrant. For other languages, the source doesn't change during parsing, but in HTML, dynamic code (such as script elements containing document.write() calls) can add extra tokens, so the parsing process actually modifies the input.\nUnable to use the regular parsing techniques, the browser utilizes a custom parser for parsing HTML. The parsing algorithm is described in detail by the HTML5 specification.\nThe algorithm consists of two stages: tokenization and tree construction.\n\n### Actions when the parsing is finished\nThe browser begins fetching external resources linked to the page (CSS, images, JavaScript files, etc.).\nAt this stage the browser marks the document as interactive and starts parsing scripts that are in \"deferred\" mode: those that should be executed after the document is parsed. The document state is set to \"complete\" and a \"load\" event is fired.\nNote there is never an \"Invalid Syntax\" error on an HTML page. Browsers fix any invalid content and go on.\n\n### CSS interpretation\n-   Parse CSS files, tag contents, and style attribute values using [\"CSS lexical and syntax grammar\"](http://www.w3.org/TR/CSS2/grammar.html)\n-   Each CSS file is parsed into a StyleSheet object, where each object contains CSS rules with selectors and objects corresponding CSS grammar.\n-   A CSS parser can be top-down or bottom-up when a specific parser generator is used.\n\n### Page Rendering\n-   Create a 'Frame Tree' or 'Render Tree' by traversing the DOM nodes, and calculating the CSS style values for each node.\n-   Calculate the preferred width of each node in the 'Frame Tree' bottom up by summing the preferred width of the child nodes and the node's horizontal margins, borders, and padding.\n-   Calculate the actual width of each node top-down by allocating each node's available width to its children.\n-   Calculate the height of each node bottom-up by applying text wrapping and summing the child node heights and the node's margins, borders, and padding.\n-   Calculate the coordinates of each node using the information calculated above.\n-   More complicated steps are taken when elements are floated, positioned absolutely or relatively, or other complex features are used. See <http://dev.w3.org/csswg/css2> and <http://www.w3.org/Style/CSS/current-work> for more details.\n-   Create layers to describe which parts of the page can be animated as a group without being re-rasterized. Each frame/render object is assigned to a layer.\n-   Textures are allocated for each layer of the page.\n-   The frame/render objects for each layer are traversed and drawing commands are executed for their respective layer. This may be rasterized by the CPU or drawn on the GPU directly using D2D/SkiaGL.\n-   All of the above steps may reuse calculated values from the last time the webpage was rendered, so that incremental changes require less work.\n-   The page layers are sent to the compositing process where they are combined with layers for other visible content like the browser chrome, iframes and addon panels.\n-   Final layer positions are computed and the composite commands are issued via Direct3D/OpenGL. The GPU command buffer(s) are flushed to the GPU for asynchronous rendering and the frame is sent to the window server.\n\n### GPU Rendering\n-   During the rendering process the graphical computing layers can use general purposeCPUor the graphical processorGPUas well.\n-   When usingGPUfor graphical rendering computations the graphical software layers split the task into multiple pieces, so it can take advantage ofGPUmassive parallelism for float point calculations required for the rendering process.\n\n### Post-rendering and user-induced execution\n\nAfter rendering has completed, the browser executes JavaScript code as a result of some timing mechanism (such as a Google Doodle animation) or user interaction (typing a query into the search box and receiving suggestions). Plugins such as Flash or Java may execute as well, although not at this time on the Google homepage. Scripts can cause additional network requests to be performed, as well as modify the page or its layout, causing another round of page rendering and painting.\n<https://github.com/alex/what-happens-when>\n\n**Problems**\n-   What is the use case of the hook in Godrej lock 6086 (only 2 page results)\n"},{"fields":{"slug":"/Computer-Science/Interview-Question/System-Design---MMOG---Game/","title":"System Design - MMOG - Game"},"frontmatter":{"draft":false},"rawBody":"# System Design - MMOG - Game\n\nCreated: 2020-08-10 15:54:07 +0500\n\nModified: 2021-12-04 17:52:01 +0500\n\n---\n\n[System design: Design Multiplayer game](https://www.youtube.com/playlist?list=PLkQkbY7JNJuCoOw3epgKcNIU6rFri4iQk)\n**Game Loop**\n\n![31 Events Input Display physiCS Game Loop Draw ](media/System-Design---MMOG---Game-image1.png)\n\nwhile (true) {\n\ncheck_input()\n\nupdate_game_state()\n\nrender_screen()\n\n}\n**Physics Engines**\n-   Unity\n-   Unreal Engine\n![Audio Graphics n p u t user Input Game Logc Andoid Framework Phone o p u t ](media/System-Design---MMOG---Game-image2.png)\n1.  Strategic games\n\n2.  Slow turn games\n\n3.  First person games\n**Authoritative Server**\n-   Holds all the state information and validates each state information with it's own state\n-   No one can hack the client and run mod commands\n-   Therefore can't have peer to peer connection between players\n**Networking**\n-   **Deterministic Lock Step**\n-   Sends only input only and not the new state of game character\n-   Encode each bit for each input thereby saving more bandwidth\n-   Using this other side can simulate their own environment\n-   Sampling size - 60 / sec\n1.  TCP (Head of line blocking), therefore can't use TCP\n\n2.  Playout Buffer (keep 5 sec in buffer like in youtube, netflix, that keeps preloaded some data beforehand, so intermittent network can be smoothened)\n\n3.  Can have a buffer space (park space) of 50 ms so the overall experience is smooth\n\n4.  Therefore instead of TCP use UDP,\n\n![](media/System-Design---MMOG---Game-image3.png)\n-   Send previous inputs in all packets\n-   Add timestamps for replay-   **State Synchronization**\n    -   Sync state of the environments object\n        -   Object State\n            -   Position\n            -   Orientation\n            -   Linear Velocity\n            -   Angular Velocity\n-   **Jitters and lagging**\n    -   **Prediction**\n        -   Linear Interpolation\n        -   Polar Interpolation\n        -   Hermite Interpolation\n**Handling states, corner cases, rules for objects**\n-   Via if-else statement\n    -   Not maintainable\n    -   Not scalable\n    -   No design pattern\n\n![R AS ?ReSS å ](media/System-Design---MMOG---Game-image4.png)\n![// Interface class PlayerState { handlelnput ( player , update (player) { input ) ](media/System-Design---MMOG---Game-image5.png)\n![class StandingState implements PlayerState { handlelnput (player, input) { // Code update (player) { // Code ](media/System-Design---MMOG---Game-image6.png)\n![Online ames S stem desi n backend Tube TechDummies CDN LOAD B LANCERS LOGIN SERVER PATCH SERVER PROXY / CONNECTION SERVER WORLD GAME CLIENT Hadoop ELK DB SERVER CACHE AREA/GAME SERVER 0 0:23/ 33:22 ](media/System-Design---MMOG---Game-image7.png)\n-   World Server\n-   Game /Area Server\n-   Data Structure - Map Template\n![Player crossing Area/Area server Inter server data transfe Area server 1 Area server Serialize deserialize criti al save data Worid server DataBase oad ](media/System-Design---MMOG---Game-image8.png)\n**Patch Servers**\n-   Everyone should be in same version in MMOG\n-   Real time updates\n**Game State Backup**\n\n1.  Important bits\n\n2.  Only if changed\n\n3.  Individual\n\n4.  Async\n**CDN**\n\nGame Sprites\n**Database**\n-   SQL\n-   Sharded SQL\n-   NoSQL"},{"fields":{"slug":"/Computer-Science/Interview-Question/System-Design---Messenger---WhatsApp/","title":"System Design - Messenger / WhatsApp"},"frontmatter":{"draft":false},"rawBody":"# System Design - Messenger / WhatsApp\n\nCreated: 2018-04-06 22:30:44 +0500\n\nModified: 2021-08-27 19:59:58 +0500\n\n---\n\n[System Design : Design messaging/chat service like Facebook Messenger or Whatsapp](https://www.youtube.com/watch?v=zKPNUMkwOJE)\n\n[System Design: Messenger service like Whatsapp or WeChat - Interview Question](https://www.youtube.com/watch?v=5m0L0k8ZtEs)\n**WhatsApp tech stack**\n-   variant of XMPP for signaling.\n-   Opus voice codec, but in 16Khz\n-   **Opus/SILK audio codec**and**NAT (Network Address Translation)**techniques. The STUN server and Peer to Peer connection are the key elements included to boost and maintain authentication to the users.\n-   WhatsApp is using the[PJSIP library](http://www.pjsip.org/)to implement Voice over IP (VoIP) functionality. The captures shows no signs of DTLS, which suggests the use of SDES encryption (see[here](https://webrtchacks.com/webrtc-must-implement-dtls-srtp-but-must-not-implement-sdes/)for Victor's past post on this). Even though[STUN](https://webrtchacks.com/stun-helps-webrtc-traverse-nats/)is used, the binding requests do not contain ICE-specific attributes. RTP and RTCP are multiplexed on the same port.\n<https://webrtchacks.com/whats-up-with-whatsapp-and-webrtc>\n\n## Design a Messaging Service**\n-   **Messaging**\n\n    a.  One to one message\n\n    b.  Group message\n\n    c.  Broadcast message-   Timeline\n-   Sharing\n\n    a.  text / stickers\n\n    b.  image / gifs\n\n    c.  video\n\n    d.  status (text / image / video)-   **Delivery**\n\n    a.  Sent notification\n\n    b.  Delivered notification\n\n    c.  Read notification (can be sent like a regular message [like an acknowledgement])\n-   **Push notifications**\n\n    a.  Only delivering, because there cannot be interaction with the message\n\n    b.  Uses GCM-   Backup\n\n    a.  Local\n\n    b.  Cloud\n-   Storage\n\n    a.  Store messages\n\n    b.  Store analytics\n\n    c.  Send and delete\n-   Network\n\n    a.  **Ephermeral**\n    b.  TCP - Handshake\n\n    c.  Load balancing (layers of load balancing and caching)\n-   Scaling\n\n    a.  Horizontally\n\n    b.  Messages can be delivered out of order because different messages can be used to handle message requests.\n-   Caching\n-   Database\n\n    a.  storing logs\n\n    b.  analytics data\n\n    c.  history\n-   Database replication\n-   Database sharding\n-   Queuing\n\n    a.  Messages Queue\n-   Security\n\n    a.  End to end encryption\n-   How to monetize the application\n\n    a.  sell stickers, emojis\n\n    b.  permium services\n**Two Types of Chatting**\n\nFacebook -\n-   Keep all the messages\n-   Can read the message if end to end encryption is not turned on\n\nWhatsApp / Signal -\n-   only keeps the messages till the receiver doesn't receive the message, then delete it\n-   End to end encryption enabled, cannot read messages\n**Facebook Messengers**\n\nFeatures -\n-   One to one chatting\n-   Online / Sent / Read\n-   Sending pictures or other files\n-   Database\n-   Security\nUser talks to Load Balancer (which can operate at Level-3, Level-4 or Level-7)\n\nLoad Balancer then talks to one of the node servers\n\nNetworking - HTTP, WebSockets\n![](media/System-Design---Messenger---WhatsApp-image1.png)-   User will login using username and password at that time server will know that user is online\n-   User A will send request to a load balancer, Load balancer will redirect the request to one of the hosts using FIFO or number of connections or load average of these hosts.\n-   Bidirectional connection is needed for heartbeat\n-   We store all the data of heartbeat in in-memory cache i.e. redis\n-   Using the last heartbeat user can know when a user was last online (like 45 mins ago)\n-   All the messages will be stored in Cassandra DB\n-   If user is not online then text message will be stored in unread table\n![](media/System-Design---Messenger---WhatsApp-image2.png)\n**Sending messages when user is offline**\n\n![VS e I - CD ](media/System-Design---Messenger---WhatsApp-image3.png)\n**Sending Images**\n-   Use thumbnail to send image\n-   Save into blob storage and pass down url\n\n![BE s l.)VWeed ](media/System-Design---Messenger---WhatsApp-image4.png)\n**Optimizations**\n-   Persisting messages\n-   Convert old data messages to blob structure and save it in blob storage\n-   Search everytime someone searches\n-   Since, search is very expensive and done so rarely\n-   Group Table for group chat\n\n"},{"fields":{"slug":"/Computer-Science/Interview-Question/System-Design---Others/","title":"System Design - Others"},"frontmatter":{"draft":false},"rawBody":"# System Design - Others\n\nCreated: 2020-01-03 23:44:06 +0500\n\nModified: 2022-07-15 17:39:32 +0500\n\n---\n\n**CricInfo**\n\n[CRICINFO system design | CRICBUZZ System desi](http://youtube.com/watch?v=exSwQtMxGd4)gn\n![](media/System-Design---Others-image1.jpg)\n![F AST CAST 2/3 ](media/System-Design---Others-image2.png)\n**Netflix**\n\n1.  AWS\n\n2.  OpenConnect (Netflix CDN)\n1.  OC\n\n2.  Backend\n\n3.  Client\n![NETFLIXS stem desi n software architecture for netflix NET FL IX / TochDummios Narondra L NAREN.LGOgmail.oorn OpenConnect 0 4:20/ 51:25 Mecno scnvâ€¢ces EVENT ](media/System-Design---Others-image3.png)\n<https://www.linkedin.com/pulse/system-design-netflix-narendra-l>\n\n<https://netflixtechblog.com/netflixs-viewing-data-how-we-know-where-you-are-in-house-of-cards-608dd61077da>\n\n[NETFLIX System design | software architecture for netflix](https://www.youtube.com/watch?v=psQzyFfsUGU)\n**Gitlab**\n\n![](media/System-Design---Others-image4.png)\n\n**S3 system design (distributed cloud storage)**\n\n**Cluster Manager**\n\n1.  Allocates account\n\n2.  Disaster recovery\n\n3.  Resource track\n\n4.  Holds policies Authentication + Authorization\n\n5.  Manages Cluster-   **Hash(UUID) - Partition Map Table**\n-   **Consistent Hashing**\n-   **1 hit wonder (don't cache any request till a threshold)**\n    -   Maybe the request is only once/twice for a day\n**Streaming layer (Stream Manager)**\n\n1.  Append only\n\n2.  Seal\n\n3.  Garbage collection\n\n4.  Replication\n\n5.  Health of FS\n\n6.  Store offsets\n![ã€Œ ãƒ» ãª 0 ? 0 ã— ) å° ã® ã‚¤ çš„ \" ã‚Š ã ã€ å· (s ãƒ¡ ãƒ‚ ). ãŸ ã„ ã® ã® ã‚“ ã€ ãƒ« å· ã„ ã® 5 ã‚“ ãƒ ãƒ¬ u? ãƒ¬ è¨€ 0 â†’ ãƒ» Ou å” ãƒ‰ \" ) ãƒ­ ã„ ã‚´ ã‚¤ ã€ ã„ ãƒ„ d ãƒ¨ ^ 815 ãƒ¼ ãƒ» ãƒ» ãƒ» â†’ 52 0 ](media/System-Design---Others-image5.png)\n<https://www.youtube.com/watch?v=UmWtcgC96X8>\n\n## Web Crawlers**\n\n**Use Case**\n-   Search engine\n-   Copywrite violation detection\n-   Keyword based finding\n    -   New analysis (share market)\n-   Web malware detection\n-   Web analytics\n-   Data science data crawlers\n**Features**\n-   Politeness / Crawl rate\n-   DNS query\n-   Distributed crawling\n-   Priority crawling\n-   Duplicate detection\n    -   Bruteforce\n    -   Hashing (MD5-SHA1)\n    -   MinHash\n    -   SimHash (Google uses this)\n    -   Fuzzy search\n    -   Latent semantic indexing\n    -   Standard boolean model\n[System Design distributed web crawler to crawl Billions of web pages | web crawler system design](https://www.youtube.com/watch?v=BKZxZwUgL3Y)\n**Financial System**\n\n![](media/System-Design---Others-image6.png)\n\n**TikTok**\n\n[System Design Interview: TikTok architecture with @sudoCODE](https://www.youtube.com/watch?v=07BVxmVFDGY&ab_channel=GauravSen)\n-   Intagram Reels\n-   Short TikTok Videos\n**Challenges**\n\na.  Ingestion\n\nb.  Storage\n\nc.  Distribution\nFunctional requirements\n-   Upload videos\n-   Feed\n-   Profiles\n\nNon-functional requirements\n-   Scalability\n-   Availability\n-   Fault tolerant\n-   Performance\n**Others**\n\nHTTP + json is not good (because contract is not upheld from developers while sending apis)\n"},{"fields":{"slug":"/Computer-Science/Interview-Question/System-Design---Parking-Lot/","title":"System Design - Parking Lot"},"frontmatter":{"draft":false},"rawBody":"# System Design - Parking Lot\n\nCreated: 2018-04-07 12:47:36 +0500\n\nModified: 2021-08-27 19:59:20 +0500\n\n---\n\n[System Design Interview Question: DESIGN A PARKING LOT - asked at Google, Facebook](https://www.youtube.com/watch?v=DSGsa0pu8-k)\n\n1.  System Design\n\n2.  Algorithmic Problem Solution\n\n3.  Object Oriented Design Question\nProblem Statement - Design a parking lot, Wants a system to manage thousands of cars.\n1.  Handling Ambiguity - recognize the breadth of the question\n\n2.  Clarify question like\n\n    a.  do you want a system design\n\n    b.  do you want class hierarchy\n\n3.  Systematic Approach (take a step back and don't rush)\n\n    a.  How is the parking lot designed?\n\n        i.  Building\n\n        ii. Open space\n\n        iii. Assessibility\n\n    b.  How many spots\n\n        i.  10 spots\n\n        ii. 1000 spots multiple buildings\n\n    c.  Multiple levels?\n\n        i.  depedencies\n\n        ii. optimize to fill up certain areas first\n    d.  **Multiple entrances**\n\n        i.  **Concurrency issue**\n\n        ii. **race condition** (When system tries to access same spot at same time)\n    e.  Pricing strategy\n\n        i.  Premium\n\n        ii. Fair pricing strategy if the required size parking spot not available\n\n        iii. Parking spot for disability (near to entrance)\n\n        iv. Economical parking spot\n\n        v.  Dynamic pricing (Higher pricing at high rush hour)\nDesign a system for 4 sizes -\n\n1.  Small (motorcycle)\n\n2.  Medium (car)\n\n3.  Large (Bus)\n\n4.  Extra Large (Truck)\nAssumption\n\n1.  Can put small car to bigger spot\nClass Hierarchy -\n\n**Abstract Vehicle**\n-   String licensePlate\n-   enum color\nImplementation of vehicle (Create 4 classes that inherit from the vehicle)\n\na.  MotorCycle (S)\n\nb.  Car (M)\n\nc.  Bus (L)\n\nd.  Truck (XL)\nclass ParkingLot ( zipCode:int )\n\nplaceVehicle ( vehicle:Vehicle )\n\nreturn Spot\nclass Spot ( id:Long, size:enum )\nDatabase for storing spots, vehicles etc.\nFind free spot (can have an array of spot, but it will be a linear operation to find a free spot)\n\nWe can create a stack which have free spots.\n4 stacks, each for storing free spots of each size.\nplaceVehicle - O(1), constant time operation + put in HashMap (Fast Lookup)\nSpot : removeVehicle ( vehicle : Vehicle )\n\nLook up Hashmap\n"},{"fields":{"slug":"/Computer-Science/Interview-Question/System-Design---TinyURL/","title":"System Design - TinyURL"},"frontmatter":{"draft":false},"rawBody":"# System Design - TinyURL\n\nCreated: 2018-04-17 20:09:23 +0500\n\nModified: 2021-10-01 22:15:00 +0500\n\n---\n\n[System Design : Design a service like TinyUrl](https://www.youtube.com/watch?v=fMZMm_0ZhK4)\n**Problem Statement -** Given a long url return a short url, and given a short url return the corresponding long url.\n-   How to generate 7-8 Characters long and is unique\n-   How to design the persistance layer (where you would store the short url and long url)\n**API**\n\ncreateTiny(longUrl) -> tinyUrl\n\ngetLong(tinyUrl) -> longUrl\n-   Add Expiration time\n**Application layer**\n\n![](media/System-Design---TinyURL-image1.png)\n**Load Balancer**\n\nDelegate requests to one of the worker threads\n-   Software\n-   Software + Hardware\n**Worker URL**\n\n1.  Take longer url generate tiny url and store it in persistance layer and return tiny url\n\n2.  Take shorter url fetch it's corresponding long url and return back long url.\n**Cache**\n-   memcached\n-   redis\n**How to generate a tinyURL**\n-   Characters - a-zA-Z0-9 (Use a BASE 62 encoding since A-Za-z0-9 total are 62 characters.)\n-   Length of generated tinyURL - 7 characters long\n-   Total combinations - 62^7 - 3.5 trillion (million requests/second - 40 days, 1000 requests/second - 110 years exhaust)\n-   Any number from 0 to 3.5 trillion can be represented by 43 bits\n![ã‚¢ ãƒ¤ ã£ ä¸€ ã” æ–° ã« Pick ã€Œ ä¸€ ã“ ä¸» ãƒ¬ ç¿’ ä¸€ ãƒ£ ã„ - ã‚€ ä¸€ ã€‚ ã¾ ã« ](media/System-Design---TinyURL-image2.png)\n**Database**\n-   Key as tiny url\n-   value as longer url\n1.  Generate random tiny url & check db\n\n    1.  Technique 1\n\nget a random tiny url and put tiny url with long url in database.\n\nProblem - Other worker thread can do the same\n\n2.  Technique 2\n\nPut if absent\n\n3.  Technique 3\n\nput (tiny, long)\n\nget(tiny) - if both are same than service worker is done\n\nOtherwise again do put(tiny, long) and repeat\n\nProblem - For every put you have to do atleast one get.\n2.  Pick first 43 bits of MD5\n\nWe calculate the MD5 of longer url then take the first 43 bits of that md5 and use that to generate the tiny url\n\nMD5 is a hashing function that generate 128 bit long hash.\n\nProbability of collision - smaller if more bits are taken, but than the length of tiny url gets longer.\nUse md5 to generate tinyurl and than check database for collision.\nAdvantage of random generation vs MD5\n\nSave some space in database\n\nIf two users want to generate two tiny urls for same long string, so in 1st technique there will be two tiny urls with 2 records, but in 2nd technique both will have same MD5 with same first 43 bits so we can have some **deduping (to remove duplicate entries from a list or database),** so we have to store only one row instead of two row in the database.\nConvert 43 bits to 7 characters long url\n\n1.  Convert 43 bits binray to decimal\n\n2.  Convert decimal to base 62 (divide by 62 and mod by 62) - will get number from 0 to 61\n\n3.  Map 0 - 61 to characters\n3.  Counter\n\n    i.  Single Host\n        -   a single maintainer is responsible to maintain the counter\n        -   Can be a database or a zookeeper instance\n        -   Every worker thread requests a counter maintainer to give it a counter, and using that counter worker creates the tiny url\n        -   Problem - Single point of contact, single point of failure, single point of bottleneck\n\n    ii. All Host\n        -   Every worker will be responsible to generate a unique number according to there worker id, timestamp and some uniqueness\n        -   Problem - Can cause collision if requests increases\n\n    iii. Range Based\n         -   We divide the total 3.5 trillion combinations into ranges, every worker will be alloted each range. Ranges can be divided into billions which will also be divided into millions\n         -   These all allotments of ranges is maintained by zookeeper\n         -   It guarantees that there are no collisions.\n         -   Also can add some random bits to increase security.\n![WA Coc å­“ 3e3 å†– D,&ea ](media/System-Design---TinyURL-image3.png)\nGet requests can be cached using CDN\n[Paste bin system design | Software architecture for paste bin](https://www.youtube.com/watch?v=josjRSBqEBI)\n\n**Non Functional**\n-   Durability\n-   HA\n-   Low latency\n**Functional**\n-   Paste + Size max 10 MB\n-   Custom URL Path\n-   Paste expiry\n-   User login / Anonymous\n**Capacity Estimation**\n\nAssumptions\n-   Pastes: 100K / day\n-   Reads: 10X per paste\nTraffic\n\nHits: 100000/24 = 4166.6667 / hour\n\n100000/(24*3600) = 1.1574 / sec\n\nRead: 1000000/(24*3600) = 11.5741 / sec\n\nBuffer: 30% more = 20 rps\nData Storage\n\n10 MB * 100K = 1000 GB / day (worst)\n\n100 KB * 100K = 10 GB / day (avg)\n\n365 * 1000GB = 365 TB / year\n**Cache**\n\nSave key, s3 location, along with some amount of data, that can quickly show some data to user while full data is fetched from s3\nPaste\n-   pasteid\n-   content\n-   s3_link\n-   created_at\n-   expire_at\nUser\n-   id\n-   name\n-   created\n-   metadata\n![pool < ](media/System-Design---TinyURL-image4.png)\nDKGS - Distributed Key Generation Service\n\n8 byte/8char\n-   Millisec timestamp - 41 bits\n-   Node ID - 10 bits\n-   Local counter in each node - 12 bits\n-   = 64 bit random id for each key\nAsync cleanup service\n-   Every hour run through whole database to get expired keys and delete the same from s3/blob storage\n\n"},{"fields":{"slug":"/Computer-Science/Interview-Question/System-Design---Twitter/","title":"System Design - Twitter"},"frontmatter":{"draft":false},"rawBody":"# System Design - Twitter\n\nCreated: 2018-04-25 18:50:48 +0500\n\nModified: 2022-03-01 17:10:45 +0500\n\n---\n\n[System Design: How to design Twitter? Interview question at Facebook, Google, Microsoft](https://www.youtube.com/watch?v=KmAyPUv9gOY)\n\n[Twitter system design | twitter Software architecture | twitter interview questions](https://www.youtube.com/watch?v=wYk0xPP_P_8)\n![DB SYSTEM DESIGN YOIC 'TECH DUMMIES NAREN.LG@GMALCOM Balancer CLIENTS DB Writer Trends data Apache storm} update cache Entries for followers FANOUT (Async) zookeeper for maintaining redis cluster REDIS cluster Home and User Timeline Timeline Search service PUSH WEBSOCK persistant connection Earlybird (Based on Lucene) ](media/System-Design---Twitter-image1.jpg)\n**Core Features -**\n\n1.  Tweeting\n\n2.  Timeline\n    -   User (Own tweets in profile)\n    -   Home (All tweets from people you follow)\n\n3.  Following\n\n4.  Full Text Search\n\n5.  HashTags\n\n6.  Push Notifications\n\n7.  Text Notifications\n\n8.  How to incorporate Advertisments\n**Database -**\n\n1.  Tweets database\n\n2.  Users database\n\nProblem with this structure is that to get a tweet corresponding to user, if would take a lot of time because there would be a big select query. (Every time we open twitter home statement)\n-   This is not feasible\n**Characteristics**\n-   Twitter is ready heavy (showing home profile for a user should be super fast)\n-   Prefer availability over consitency. (Eventual Consistency - Consistency over a time frame)\n**Optimized Solution**\n-   When we tweet we do a Put / Post request\n-   Request goes to a Load Balancer\n    -   Does Fan Out - put tweet on every follower home timeline (in-memory DB / Redis)\n-   Twitter has redis cluster\n    -   Every user has a redis database\n    -   Every redis database for each user is replicated 3 times\n    -   Every redis database is in-memory\n    -   Can store the redis database for a period of time tied to user login activity\n    -   If a user has 100 followers than 3*100 = 300 redis databases will be updated\n    -   Problem - If a celebrity tweet there would be million of updates.\n    -   Solution - Can use a mixed approach, where sql can be used for very large updates.\n    -   In-memory + Syncronous calls\n-   Followers\n    -   Followers table will give all the redis database hashes where we have to update the home profile\n-   Tradeoffs\n    -   Read heavy\n    -   Eventual consistency\n    -   Space\n-   Accessing Timeline\n    -   Get Request\n    -   Load Balancer\n        -   Very fast hash lookup for getting redis databases\n    -   One of the three redis databases reponds with bob's timeline\n**TwitterBot / Twitter Bot**\n\n<https://www.labnol.org/twitter-bots-tutorial-4796>\n\n<https://www.labnol.org/twitter-search-examples-203>\n\n"},{"fields":{"slug":"/Computer-Science/Interview-Question/System-Design---Uber-Lyft-ride-sharing-services/","title":"System Design - Uber Lyft ride sharing services"},"frontmatter":{"draft":false},"rawBody":"# System Design - Uber Lyft ride sharing services\n\nCreated: 2018-04-05 11:53:32 +0500\n\nModified: 2021-08-27 20:00:12 +0500\n\n---\n\n[System Design: Uber Lyft ride sharing services - Interview question](https://www.youtube.com/watch?v=J3DY3Te3A_A)\n**Core Features**\n\n1.  Customer - Driver matching\n\n2.  Mapping\nBottom up (What do we need to store?)\n**Storage**\n\n1.  Trip Storage (needed immediately)\n\n2.  Analytics data-   Cloud providers\n-   In house storage-   Close data center\n-   Backup data center\nWhen close data center is down, data can be fetched from backup data center. Providing high availability, Low latency\nUber uses mix of sql and nosql databases, fork of mysql which operates schemaless\n\nAlso use cassandra for low latency\nperodically store data on data-warehouses, where they can run big expensive queries to find out trends in their services.\n-   Where people are using uber\n-   Where drivers are not enough drivers\n-   trends regarding time and day\n\nUber uses Hadoop as a Data Warehouse (for business analytics)\nWhenever we have a service that needs to be highly available and serves million of customers, we need to think about some sort of caching layer, layer where data can be stored closer to the customers and probably in some sort of in-memory database, which is much faster than storing data on speeding disks.\nCan be closer to customers, We can prewarm them before the customer queries it.\nPosition of driver, or live data is not feasible to store in cache, so it can be saved in an in-memory database like redis and then sent to customer.\nCache - map data, event data\n**Logging (logging layer)**\n\nLegal perspective\n\nAs real time as possible\n\nEndpoints - driver app, customer app\n\nKafka (ingest logging message), it is a service that can ingest a lot of messages in real time. (guarantee that it's not lost). Lot of servers in cluster and synchronize with each other.\n\nPeriodically dumps data to Data Warehouse (Hadoop)\n**SOA (Service Oriented Architecture)**\n\nThere are a lot of small machines distributed so one if goes down doesn't effect others. Service lives in many different machines and in many different regions\n**Provisioning (Getting apps into machines)**\n\nGetting all the software (libraries, code, applications) into a newly started machine or container.\nRepository of all applications and libraries is used to provision everything to individual systems\n**Teraform** is used to provision systems.\n\n**Containers (Shielded runtime environments) - Docker**\n\n**Apache Mesos (distributed systems kernel, manage distributed computer clusters) -** manages collaboration and interfaces between services in distributed systems.\n**Network Routing (Route requests to backend) -** needs to know state of each application.\n\n**Stateless -** there is no state for each communication, therefore if one request is to one server, then another request can be routed to any other server.\n**Testing**\n\nUnit Tests\n\nSuite of Integration tests\n\nResiliency testing - Case of failure testing\n\n**Uber - Hailstorm** - goes to a random set of services or hosts and turns them down.\n\nUse Shadow fleets, group of hosts that have some applications and gets all the data but doen't serve any customer. Get request, calls all the same services as in production.**Mapping (Graph problem)**\n\nTravelling Salesman Problem (TSP - NP Complete Problem)\n\nOptimization problem (no efficient solution)\n\nDijkstra's Algorithm\n\nA* Search Algorithm\n\nDirected Weighted Graph (Weights are speed limit/Traffic)\n**Uber**\n\nGetting very precise ETA (driver to customer)\n\nInclude Traffic Congestion / Include Speed limits for roads\n\nPrecompute the data for maps\n\nUse **historical data** for ETA calculation (pretty accurate)\n\nSplit the city into smaller blocks and then re-calculate the ETA's in real time. (Splitting the graph algorithm)\n\n![2 6 ](media/System-Design---Uber-Lyft-ride-sharing-services-image1.png)\n![ã¤ ã® ã€‚ ) ãƒ• ã³ ã¤ ã‚„ ](media/System-Design---Uber-Lyft-ride-sharing-services-image2.png)**Summary -**\n\n1.  Core features\n\n    a.  Driver Customer matching\n\n    b.  Mapping\n## Driver Customer Matching\n\n1.  Storage\n\n    a.  Trip storage\n\n        i.  Close Data Center\n\n        ii. Backup Data Center\n\n        iii. Cloud platforms\n\n        iv. In-house data stores\n\n        v.  Mix of SQL and NoSQL databases\n\n        vi. Low Latency - Cassendra\n\n    b.  Caching\n\n        i.  In-memory database - redis\n\n        ii. Map data\n\n        iii. Event data\n\n    c.  Analytics\n\n        i.  Data Warehouse - Hadoop\n\n    d.  Logging\n\n        i.  Kafka - ingest logging message\n\n2.  SOA (Service Oriented Architecture)\n\n3.  Provisioning\n\n    a.  Teraform (for installing apps from central repositories to distributed clusters)\n\n    b.  Docker (Containerization)\n\n    c.  Apache mesos (manage distributed system clusters)\n\n4.  Network Routing\n\n    a.  Stateless servers\n\n5.  Testing\n\n    a.  Unit Tests\n\n    b.  Suite of integration tests\n\n    c.  Resiliency testing\n\n        i.  HailStorm\n\n        ii. Shadow fleets\n## Mapping\n\n1.  Algorithms\n\n    a.  Dijkstra's Algorithm\n\n    b.  A* Search Algorithm\n\n    c.  Travelling Salesman Problem (TSP)\n\n    d.  Optimization Problem\n\n2.  Directed Weighted Graph\n\n    a.  Vertices - Intersections\n\n    b.  Roads - Edges\n\n    c.  Speed limit / Traffic / Blockage - Weights\n\n3.  Uber\n\n    a.  Precompute the data for maps\n\n    b.  Use historical data\n\n    c.  Segment the city and do real time ETA calculation"},{"fields":{"slug":"/Computer-Science/Interview-Question/TopTal/","title":"TopTal"},"frontmatter":{"draft":false},"rawBody":"# TopTal\n\nCreated: 2017-11-19 17:57:20 +0500\n\nModified: 2020-01-17 19:39:00 +0500\n\n---\n\nAlgorithm and Coding Round (Online - Codility)\n![Task 1 Task 2 Task 3 6 Bob the Adventurer is one step away from solving the mystery of an ancient Mayan tomb. He just approched the secret chamber where the secret Mayan scriptures are locked in a chest. There are N ancient statues in the room. After long thought, Bob figured out that in order to open the treasure chest he needs to stand in the middle of the room and hit every statue with a laser ray at the same time. Bob is a highly experienced adventurer, so setting multiple laser rays at the same time is not a problem for him. Moreover, every ray that he creates is perfectly straight and never changes direction at all. The middle of the room, where Bob is standing, has coordinates (0, 0). Every statue is located at some point with coordinates (x, y). Each statue is made of pure glass, so that if any ray hits it, it does not stop, but goes through the statue and continues beyond in the same, unchanged direction. Bob wonders how he can hit every ancient statue in the room using the fewest rays possible. Assume that the following declarations are given: class Point2D { public int x; public int y; Write a function class Solution { public int solution (Point2D[] A); that, given an array of points A, representing the locations of the statues, returns the minimal number of rays that Bob must set in order to hit every statue in the room. Enr ovnrnnlo rliuon nn nrrnu A ](media/TopTal-image1.png)\n![Task 1 For example, given an array A Task 2 ALO]. x All]. x 1 Task 3 AL 2]. x 2 AL 3]. x 2 your function should return 4. 2 4 2 ( statue ( statue ( statue ( statue ( statue 0) 1) 2) 3) 4) As is shown in the image, it is possible to create four rays in such a way that: â€¢ the first will hit statue 0; â€¢ the second will hit statues 1 and 2; â€¢ the third will hit statue 3; â€¢ the fourth will hit statue 4. ](media/TopTal-image2.png)\n![Task 1 Task 2 Task 3 As is shown in the image, it is possible to create four rays in such a way that: â€¢ the first will hit statue 0; â€¢ the second will hit statues 1 and 2; â€¢ the third will hit statue 3; â€¢ the fourth will hit statue 4. Assume that: N is an integer within the range [1 .. 100,000]; â€¢ the coordinates of each point in array A are integers within the range [-1 â€¢ the elements of A are all distinct; Array A does not contain point (0,0). Complexity: â€¢ expected worst-case time complexity is O(N); â€¢ expected worst-case space complexity is O(N*log(N)), beyond input storage (not counting the storage required for input arguments). Elements of input arrays can be modified. Copyright 2009-2017 by Codility Limited. All Rights Reserved. Unauthorized copying, publication or disclosure prohibited. Custom test cases 0/10 ](media/TopTal-image3.png)\n![Task 1 Task 2 Task 3 6 A non-empty zero-indexed array A consisting of N integers is given. This array contains a decimal representation of a number V, i.e. element A[K] contains the K-th least significant digit of the decimal representation of V. For example, array A such that: ALO] All] represents the number V = 153. Write a function: class Solution { public int solution( int [ ] A); that, given an array A consisting of N integers specifying a decimal representation of a number V, returns the sum of the digits in the decimal representation of the number 17*V. For example, given array A such that: ALO] All] the function should return 9, because: â€¢ array A represents the number 1 53, â€¢ 153 = 2601, â€¢ the sum of the digits in the decimal representation of 2601 is 2+6+0+1=9. Assume that: N is an integer within the range [1 .. 1,000,000]; â€¢ each element of array A is an integer within the range [0..9]. Custom test cases 0/10 ](media/TopTal-image4.png)\n![Task 1 Task 2 Task 3 represents the number V = 153. Write a function: class Solution { public int solution( int [ ] A); that, given an array A consisting of N integers specifying a decimal representation of a number V, returns the sum of the digits in the decimal representation of the number 17*V. For example, given array A such that: ALO] All] the function should return 9, because: â€¢ array A represents the number 1 53, â€¢ 153 = 2601, â€¢ the sum of the digits in the decimal representation of 2601 is 2+6+0+1=9. Assume that: N is an integer within the range [1 .. 1,000,000]; â€¢ each element of array A is an integer within the range [0..9]. Complexity: â€¢ expected worst-case time complexity is O(N); â€¢ expected worst-case space complexity is 0(1), beyond input storage (not counting the storage required for input arguments). Elements of input arrays can be modified. Copyright 2009-2017 by Codility Limited. All Rights Reserved. Unauthorized copying, publication or disclosure prohibited. Custom test cases 0/10 ](media/TopTal-image5.png)\n![Task 1 Task 2 Task 3 There are N giraffes in your zoo. The zoo is famous for the distinct heights of its giraffes. This means that there are no two giraffes of equal height in the zoo. The giraffes live in displays located along the main path. Every giraffe has its own display. For aesthetic reasons, you want the giraffes to be arranged in ascending order of height. Reordering all the giraffes at once, however, would cause the animals a lot of stress, so you want to split the displays into as many smaller groups as possible and reorder all the groups separately. A group should contain one or more consecutive displays. After reordering all the groups, the giraffes should be arranged in ascending order of height. Write a function: class Solution { public int solution( int [ ] A); that, given a zero-indexed array A containing N integers, where All] denotes the height of the giraffe in the I-th display, returns the maximum number of groups that can be reordered independently in order to sort all the giraffes in ascending order of height. For example, given A = [1, 5, 4, 9, 8, 7, 12, 13, 14], the function should return 6. Displays can be split into six groups: [1], [5, 4], [9, 8, 7], [1 2], [13] and [14]. Then, after reordering each group, the whole sequence of giraffes will be sorted into ascending order. Note that group [1 2, 13, 14] was already ordered correctly, so it could be split into three groups, each containing one giraffe. Given A = [4, 3, 2, 6, 1], the function should return 1. Displays cannot be split into smaller groups; the giraffes have to be sorted all at once. Assume that: ](media/TopTal-image6.png)\n![Task 1 Task 2 Task 3 Write a function: class Solution { public int solution( int [ ] A); that, given a zero-indexed array A containing N integers, where All] denotes the height of the giraffe in the I-th display, returns the maximum number of groups that can be reordered independently in order to sort all the giraffes in ascending order of height. For example, given A = [1, 5, 4, 9, 8, 7, 12, 13, 14], the function should return 6. Displays can be split into six groups: [1], [5, 4], [9, 8, 7], [12], [13] and [14]. Then, after reordering each group, the whole sequence of giraffes will be sorted into ascending order. Note that group [1 2, 13, 14] was already ordered correctly, so it could be split into three groups, each containing one giraffe. Given A = [4, 3, 2, 6, 1], the function should return 1. Displays cannot be split into smaller groups; the giraffes have to be sorted all at once. Assume that: N is an integer within the range [1 .. 100,000]; â€¢ each element of array A is an integer within the range â€¢ the elements of A are all distinct. Complexity: â€¢ expected worst-case time complexity is O(N*log(N)); â€¢ expected worst-case space complexity is O(N), beyond input storage (not counting the storage required for input arguments). Elements of input arrays can be modified. Copyright 2009-2017 by Codility Limited. All Rights Reserved. Unauthorized copying, publication or disclosure prohibited. Custom test cases 0/10 ](media/TopTal-image7.png)\n\n"},{"fields":{"slug":"/Computer-Science/IoT/Device-Management/","title":"Device Management"},"frontmatter":{"draft":false},"rawBody":"# Device Management\n\nCreated: 2018-11-22 17:16:31 +0500\n\nModified: 2018-11-22 17:16:43 +0500\n\n---\n\n![Image result for device management lwm2m](media/Device-Management-image1.jpg)\n\n"},{"fields":{"slug":"/Computer-Science/IoT/Edge-Computing/","title":"Edge Computing"},"frontmatter":{"draft":false},"rawBody":"# Edge Computing\n\nCreated: 2018-04-13 14:00:24 +0500\n\nModified: 2020-04-10 22:53:52 +0500\n\n---\n\nEdge computingis a method of optimizing[cloud computing](https://en.wikipedia.org/wiki/Cloud_computing)systems by performing data processing at the edge of the network, near the source of the data.\nThis reduces the communications bandwidth needed between sensors and the central datacenter by performing analytics and knowledge generation at or near the source of the data.\nThis approach requires leveraging resources that may not be continuously connected to a network such as laptops, smartphones, tablets and sensors.Edge computing covers a wide range of technologies including[wireless sensor networks](https://en.wikipedia.org/wiki/Wireless_sensor_network), mobile data acquisition, mobile signature analysis, cooperative distributed[peer-to-peer](https://en.wikipedia.org/wiki/Peer-to-peer)[ad hoc networking](https://en.wikipedia.org/wiki/Ad_hoc_networking)and processing also classifiable as local cloud/[fog computing](https://en.wikipedia.org/wiki/Fog_computing)and[grid/mesh computing](https://en.wikipedia.org/wiki/Grid_computing), dew computing,[mobile edge computing](https://en.wikipedia.org/wiki/Mobile_edge_computing),[cloudlet](https://en.wikipedia.org/wiki/Cloudlet),[distributed data storage](https://en.wikipedia.org/wiki/Distributed_data_store)and retrieval, autonomic[self-healing networks](https://en.wikipedia.org/wiki/Self-healing_ring), remote cloud services,[augmented reality](https://en.wikipedia.org/wiki/Augmented_reality), and more.\n**Possible advantages of edge computing are:**\n\n1.  Edge application services significantly decrease the volumes of data that must be moved, the consequent traffic, and the distance the data must travel, thereby reducing transmission costs, shrinking latency, and improving[quality of service](https://en.wikipedia.org/wiki/Quality_of_service)(QoS).\n\n2.  Edge computing eliminates, or at least de-emphasizes, the core computing environment, limiting or removing a major bottleneck and a potential point of failure.\n\n3.  The ability to \"virtualize\" (i.e., logically group CPU capabilities on an as-needed, real-time basis) extends[scalability](https://en.wikipedia.org/wiki/Scalability). The edge-computing market generally operates basically on a \"charge for network services\" model, and it could be arguedthat typical customers for edge services are organizations desiring linear scale of business application performance to the growth of, e.g., a subscriber base.\n**LFEdge**\n\nLF Edge is an umbrella organization that aims to establish an open, interoperable framework for edge computing independent of hardware, silicon, cloud, or operating system. By bringing together industry leaders, LF Edge will create a common framework for hardware and software standards and best practices critical to sustaining current and future generations of IoT and edge devices.\n[https://www.lfedge.org/#](https://www.lfedge.org/)\n**EVE (Edge Virtualization Engine)**\n\nProject EVE develops the open source Edge Virtualization Engine (EVE). EVE leverages a type-1 hypervisor, so it deploys on bare metal device hardware. It also provides system and orchestration services, and a container runtime. This means developers can enjoy consistent behavior on any supported platform. EVE can run both legacy and newer cloud-native apps in edge computing devices.\nEdge devices based on EVE gain the following capabilities:\n-   Higher efficiency and usage of device resources\n-   \"Secure by default\" deployment profile\n-   Host any operating system deployable in a virtual machine\n-   Host many apps in virtual machines and containers\n-   Serverless capability via unikernels\n-   Scalable, centralized management for many devices over large distances, and hosting many apps\n-   Remote updates of entire software stack\n-   Remote control of all device resources including CPU, memory, networking, and device ports\n-   Automated patching for security updates\n-   Automated connectivity to one or more public clouds\n-   Built-in mesh networking capabilities for edge-to-edge data flow\n-   Built-in cloud networking using standard VPN technologies available in public clouds\n<https://www.lfedge.org/projects/eve>\n\n## Others**\n\n<https://www.lfedge.org/projects/akraino>\n"},{"fields":{"slug":"/Computer-Science/IoT/EdgeXFoundary/","title":"EdgeXFoundary"},"frontmatter":{"draft":false},"rawBody":"# EdgeXFoundary\n\nCreated: 2019-02-07 19:43:15 +0500\n\nModified: 2019-02-07 19:48:58 +0500\n\n---\n\nFounded in 2017, EdgeX Foundry acts as a vendor-neutral interoperabilityframework. It is hostedin a hardware and OS agnostic referenceplatform and seeks toenablean ecosystem of plug-and-playcomponents, unitingthe marketplace and acceleratingIoT deployment. The project wants to enable collaborators tofreely work onopen and interoperable IoT solutionswith existing and self-created connectivity standards.\nThe EdgeX Foundry is a collection of open source microservices that span from the edge of the physical realm on the Device Services Layer, with the Core Services Layer at the center. These services communicate through a common API, allowing them to be augmented or replaced by custom implementations.\nBy bringing this much-needed interoperability, EdgeX makes it easier to monitor physical world items, send instructions to them, collect data from them, move the data across the fog up to the cloud where it may be stored, aggregated, analyzed, and turned into information, actuated, and acted upon. So EdgeX enables data to travel northwards towards the Cloud and also laterally to other gateways, or back to devices, sensors, and actuators.\nThe initiative is aligned around a common goal: the simplification and standardization of the foundation for tiered edge computing architectures in the industrial IoT market while still enabling the ecosystem to provide significant value-added differentiation.\n\n![EDGE X FOUNDRY Platform Architecture \"NORTHBOUND\" INFRASTRUCTURE AND APPLICATIONS LOOSELY-COUPLED MICROSERVICES FRAMEWORK EXPORT SERVICES CLIENT REGISTRATION SUPPORTING SERVICES RULES ENGINE CORE SERVICES CORE DATA SCHEDULING CHOICE OF PROTOCOL DISTRIBUTION ALERTS & NOTIFICATIONS LOGGING REQUIRED INTEROPERABILITY FOUNDATION REPLACEABLE REFERENCE SERVICES ADDITIONAL SERVICES ADDITIONAL SERVICES 3 ALL MICROSERVICES INTERCOMMUNICATE VIA APIs - COMMAND -o -< Z O Z z O m z z DEVICE SERVICES (ANY COMBINATION OF STANDARD OR PROPRIETARY PROTOCOLS VIA SDK) REST OPC-UA MODBUS BACNET ULE BLE MQTT o ENOCEAN o \"SOUTHBOUND\" DEVICES, SENSORS AND ACTUATORS ADD'L VIRTUAL DEVICE SERVICES SDK ](media/EdgeXFoundary-image1.jpeg)\n\n**South Side:**All IoT objects, withinthe physical realm, and the edge of the network that communicates directly with those devices, sensors, actuators, and other IoT objects, and collects the data from them, is known collectively as the \"South Side.\"\n\n**North Side:**The Cloud (or Enterprise system) where data iscollected, stored, aggregated, analyzed, and turned into information, and the part of the network that communicates with the Cloud, is referred to as the \"north side\" of the network.\n\nEdgeX enables data to be sent \"north,\" \"south,\" or laterally as needed and as directed.\n**References**\n\n<https://en.wikipedia.org/wiki/Linux_Foundation#EdgeX_Foundry>\n\n<https://www.edgexfoundry.org/get-started>\n\n<https://wiki.edgexfoundry.org/display/FA/Introduction+to+EdgeX+Foundry>\n\n"},{"fields":{"slug":"/Computer-Science/IoT/Industrial-IoT-(IIoT)/","title":"Industrial IoT (IIoT)"},"frontmatter":{"draft":false},"rawBody":"# Industrial IoT (IIoT)\n\nCreated: 2018-11-20 15:12:57 +0500\n\nModified: 2018-11-20 15:19:13 +0500\n\n---\n\n**Reference Architecure**\n\n![page1image65052384](media/Industrial-IoT-(IIoT)-image1.png)\n**Sensors**\n-   Pressure\n-   Temperature\n-   Moisure\n-   Air flow\n-   Acceleration\n-   Position/Velocity\n-   Proximity\n**Actuators**\n**Control Systems**\n\nIndustrial Control Systems (ICSs) are used to monitor and control the processes and interactions between sensors and actuators.\n<table>\n<colgroup>\n<col style=\"width: 11%\" />\n<col style=\"width: 88%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th><strong>PLC</strong></th>\n<th><p><strong>Programmable Logic Controller</strong></p>\n<p>An Industrial computer used for automation of (often electromechanical) processes.</p>\n<p>Often programmed with IEC 61131-3 languages such as Ladder Logic and Structured Text.</p>\n<p>Continuously monitors input and determines necessary output based on programmed logic.</p>\n<p>Similar to Programmable Automation Controllers (which can be programmed in languages like C, in addition to ladder logic).</p></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><strong>SCADA</strong></td>\n<td><p><strong>Supervisory Control and Data Acquisition</strong></p>\n<p>Provides supervisory-level control over a larger-scale â€œsystem of systemsâ€ (e.g. systems that span over multiple areas of the plant rather than one local set of processes).</p>\n<p>Often includes HMI/SCADA systems, remote terminal units, and local operator interfaces.</p>\n<p>Evolved with the Industrial Internet of Things to allow for near-real-time state reports and increased security over more standard protocols such as OPC UA.</p></td>\n</tr>\n<tr class=\"even\">\n<td><strong>DCS</strong></td>\n<td><p><strong>Distributed Control System</strong></p>\n<p>Distributes elements of control across the system itself, rather than centralizing these through a single controller.</p>\n<p>Generally used to control continuous plant processes (e.g. chemical production).</p>\n<p>Increased Human-Machine Interface accessibility could simplify access, but could increase security concerns as well.</p></td>\n</tr>\n</tbody>\n</table>\n\n## Protocols**\n\n<table>\n<colgroup>\n<col style=\"width: 14%\" />\n<col style=\"width: 85%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th><strong>MQTT</strong></th>\n<th><p>A publish-subscribe protocol used over TCP/IP.</p>\n<p>Lightweight, low code footprint, minimal bandwidth.</p></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><strong>CoAP</strong></td>\n<td><p>Constrained Application Protocol</p>\n<p>Application layer protocol used for constrained (low-power, low-memory, etc.) nodes and networks.</p></td>\n</tr>\n<tr class=\"even\">\n<td><strong>AMQP</strong></td>\n<td><p>Advanced Message Queuing Protocol</p>\n<p>Application layer, wire-level protocol that supports a variety of messaging patterns.</p></td>\n</tr>\n<tr class=\"odd\">\n<td><strong>HTTP/2</strong></td>\n<td><p>Updated version of Hypertext Transfer Protocol</p>\n<p>Built with HTTP 1.1 compatibility and performance enhancement in mind.</p></td>\n</tr>\n<tr class=\"even\">\n<td><strong>IPv6</strong></td>\n<td><p>Internet Protocol Version 6</p>\n<p>Updated version of the Internet Protocol Version</p>\n<p>4, necessary for assigning unique addresses to the rapidly growing number of machines connected to the Internet (due partially to the increase of Things and M2M connections).</p></td>\n</tr>\n<tr class=\"odd\">\n<td><strong>6LoWPAN</strong></td>\n<td><p>IPv6 over Low power Wireless Personal Area Networks</p>\n<p>The 6LoWPAN group has defined encapsulation and header compression mechanisms that allow IPv6 packets to be sent and received over IEEE 802.15.4 based networks.</p></td>\n</tr>\n</tbody>\n</table>\n\n## References**\n\nIndustrial Internet - DZone Guide\n\nIndustrial Internet Reference Architecture\n\n"},{"fields":{"slug":"/Computer-Science/IoT/IoT-Intro/","title":"IoT Intro"},"frontmatter":{"draft":false},"rawBody":"# IoT Intro\n\nCreated: 2018-04-24 11:03:42 +0500\n\nModified: 2021-07-04 22:16:04 +0500\n\n---\n\n**Internet of Things (IoT)**\n\nIoT world is all about communication between devices, gateways and the cloud; messages are exchanged between all these parties in order to provide a comprehensive end-to-end solution.\n**Shadow (Device Shadow) -**\n\nA json file maintained on the cloud where all the updates are performed. These is synced with the devices on fields.\n**Digital Twin**\n\n<https://dzone.com/articles/apache-kafka-as-digital-twin-in-industrial-iot-iio>\n\n## TPM - Trusted Platform Module**\n\nIs an[international standard](https://en.wikipedia.org/wiki/International_standard)for a[secure cryptoprocessor](https://en.wikipedia.org/wiki/Secure_cryptoprocessor), a dedicated[microcontroller](https://en.wikipedia.org/wiki/Microcontroller)designed to secure hardware through integrated[cryptographic keys](https://en.wikipedia.org/wiki/Cryptographic_keys).\n**Four basic architectural components**\n-   **Devices**\n\nDevices are the physical hardware elements that collect sensor data and might perform actuation.\n-   **Gateway**\n\nGateways collect, preprocess, and transfer sensor data from devices and might deliver actuation requests from the cloud to devices.\n-   **Cloud platform**\n\nThe cloud platform---usually offered as a software-as-a-service solution--- has a number of important roles, including data acquisition, data analytics, and device management and actuation.\n-   **Applications**\n\nApplications range from simple web-based data visualization dashboards to highly domain-specific mobile apps.\nOn a high level, the software architecture choices for IoT client devices fall into the following seven categories, ranging from simple to more complex:\n-   no-OS architectures\n-   RTOS (real-time OS) architectures\n-   language-runtime architectures\n-   full-OS architectures\n-   app-OS architectures\n-   server-OS architectures\n-   container-OS architectures\n# Software architecture options for IoT devices\n\n| Feature                         |                                                                |                                               |                                                         | Architecture option                       |                                                                               |                                                                                                                    |\n|------------|----------|----------|-----------|-----------|----------|----------|\n|                                | No OS or RTOS                                                   | Language runtime                               | Full OS                                                  | App OS                                    | Server OS                                                                      | Container OS                                                                                                        |\n| Typical devices                 | Simple sensor devices, heartbeat sensors, lightbulbs, and so on | Feature watches, more advanced sensing devices | \"Maker\" devices, generic sensing solutions               | High-end smartwatches                     | Solutions benefiting from a portable webserver and edge-computing capabilities | Solutions benefiting from fully isomorphic apps---that is, code that can be migrated between the cloud and the edge |\n| Minimum required RAM            | Tens of kilobytes                                               | Hundreds of kilobytes                          | A few megabytes                                          | Hundreds of megabytes                     | Tens of megabytes                                                              | Gigabytes                                                                                                           |\n| Typical communication protocols | Constrained (MQTT, LWM2M, CoAP)                                 | Constrained (MQTT, LWM2M, CoAP)                | Standard Internet protocols (HTTP, HTTPS)                | Standard Internet protocols (HTTP, HTTPS) | Standard Internet protocols (HTTP, HTTPS)                                      | Standard Internet protocols (HTTP, HTTPS)                                                                           |\n| Typical development language    | C or assembly                                                   | Java, JavaScript, Python                       | C or C11                                                 | Java, ObjectiveC, Swift                   | JavaScript                                                                     | Various                                                                                                             |\n| Libraries                       | None or system- specific                                        | Language- specific generic libraries           | OS libraries, generic UI libraries                       | Platform libraries                        | Node.js npm modules                                                            | Various                                                                                                             |\n| Dynamic software updates        | Firmware updates only                                           | Yes                                            | Yes                                                      | Yes                                       | Yes                                                                            | Yes                                                                                                                 |\n| Third-party apps supported      | No                                                              | Yes                                            | Yes                                                      | Yes                                       | Yes                                                                            | Yes                                                                                                                 |\n| Isomorphic apps possible        | No                                                              | Yes                                            | Only if the hardware architectures are binary compatible | Yes                                       | Yes                                                                            | Yes                                                                                                                 |\n**Key points -**\n-   Windowing for streaming data\n**Messaging mechanisms**\n\n1.  Store and forward mechanism (using queues, topic/subscriptions in a broker)\n\n2.  Direct messaging (receiver needs to be online in order to allow the devices to send data)\n**Messaging Patterns**\n\n1.  competing consumers\n\nA messaging pattern in which more consumers get messages from a common source (i.e. queue) but each message is delivered to only one consumer.\n\n2.  request/reply\n\n3.  pub/sub\n**Communication patterns**\n\n1.  **Telemetry**\n    -   **Data flows in one direction from the device to other systems for conveying status changes in the device itself (i.e. sensors reading, ...)**\n    -   **Direct Messaging mechanism can be used if data is not to be stored.**\n\n2.  **Inquires**\n\nRequests from the device looking to gather required information or asking to initiate activities\n-   Requests and response\n\n3.  **Commands**\n    -   **Commands from other systems sent to a device (or a group of devices) to perform specific activities expecting a result from the command execution, or at least a status for that**\n    -   **Mainly store and forward mechanism is used, sometimes a TTL (Time To Live) on the command message is useful in order to avoid the possibility that an offline device will execute an \"old\" message that is not useful at the time the device comes back online.**\n\n4.  **Notifications**\n\nInformation flows in one direction from other systems to a device (or a group of devices) for conveying status changes\n![IOT Communication Patterns Telemetry Inquiries a Cloud Commands a Notifications ](media/IoT-Intro-image1.png)\n**Classifying Device States**\n\n<table>\n<colgroup>\n<col style=\"width: 15%\" />\n<col style=\"width: 84%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th><strong>Classifier</strong></th>\n<th><strong>Description</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>Assumed State</td>\n<td><p>We are unable to get the state of the device. Best we can do is to assume the state based on our last command.</p>\n<p>Example - WattMan Lite which we deployed previously, we were controlling ACs based on infrared signal - during that time we used to work on \"Assumed State\".</p></td>\n</tr>\n<tr class=\"even\">\n<td>Cloud Polling</td>\n<td>Integration of this device happens via the cloud and requires an active internet connection. Polling the state means that an update might be noticed later.</td>\n</tr>\n<tr class=\"odd\">\n<td>Cloud Push</td>\n<td>Integration of this device happens via the cloud and requires an active internet connection. Home Assistant will be notified as soon as a new state is available.</td>\n</tr>\n<tr class=\"even\">\n<td>Local Polling</td>\n<td><p>Offers direct communication with device. Polling the state means that an update might be noticed later.</p>\n<p>Example - For relay boards where we communicate over modbus protocol, we work on Local Polling. Periodically, we read the current status of values in relay board and ensure that it's the same state that we had last set.</p></td>\n</tr>\n<tr class=\"odd\">\n<td>Local Push</td>\n<td><p>Offers direct communication with device. Home Assistant will be notified as soon as a new state is available.</p>\n<p>Example - For the smart plug that we are currently interfacing, device does \"Local Push\" on the mqtt server on Rpi. So it reports it's state on a mqtt topic in a periodic basis.</p></td>\n</tr>\n</tbody>\n</table>\n\n## Tools**\n\n1.  **Node-RED**\n    -   **Flow-based programming for the Internet of Things**\n    -   **Node-RED is a programming tool for wiring together hardware devices, APIs and online services in new and interesting ways.**\n    -   **It provides a browser-based editor that makes it easy to wire together flows using the wide range of nodes in the palette that can be deployed to its runtime in a single-click.**\n**IoT Protocols**\n\n1.  Infrastructure(ex: 6LowPAN, IPv4/IPv6, RPL)\n\n2.  Identification(ex: EPC, uCode, IPv6, URIs)\n\n3.  Comms / Transport(ex: Wifi, Bluetooth, LPWAN)\n\n4.  Discovery(ex: Physical Web, mDNS, DNS-SD)\n\n5.  Data Protocols(ex: MQTT, CoAP, AMQP, Websocket, Node)\n\n6.  Device Management(ex: TR-069, OMA-DM)\n\n7.  Semantic(ex: JSON-LD, Web Thing Model)\n\n8.  Multi-layer Frameworks(ex: Alljoyn, IoTivity, Weave, Homekit)\n<https://www.postscapes.com/internet-of-things-protocols>\n\n## References -**\n\n<https://cloudplatform.googleblog.com/2018/04/implementing-an-event-driven-architecture-on-serverless-the-Smart-Parking-story.html?m=1>\n\nIoT Applications, Protocols, and Best Practices - DZone Research Guides\n\n<https://www.home-assistant.io/blog/2016/02/12/classifying-the-internet-of-things>\nEnd to end overview - <https://cloud.google.com/solutions/iot-overview>-   <https://www.edx.org/course/iot-system-architecture-design-and-evaluation>\n-   <https://www.edx.org/microsoft-professional-program-certificate-in-iot>\n"},{"fields":{"slug":"/Computer-Science/IoT/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-03-13 20:43:49 +0500\n\nModified: 2022-10-30 23:34:37 +0500\n\n---\n\n**CE - Accurate Indoor Location for the IoT (Jan 19)**\n-   802.11b - Received Signal Strength Indication (RSSI)\n    -   Due to multipath fading, the RSSI fall-off from the antenna does not follow a uniform inverse square law and is distorted by the building materials that it travels through.\n    -   Large errors of 10 to 20 meters\n    -   ![Phone Time distance = c x ((t4-t1) FTM request FTM Ping Ack Pong FTM: ti, t4 - (t3-t2)) / 2 t4 Figure 2. The 802.11mc Fine Timing Measurement (F TM) Protocol. ](media/Others-image1.png)\n-   Bluetooth low energy beacons\n-   Low Power Wide Area Network (LPWAN)\n-   LoRa Network Protocol (Long Range)\n\n<https://www.postscapes.com/long-range-wireless-iot-protocol-lora>\n\n## P2PLoc for Group-Motion Applications**\n\nP2PLoc (peer-to-peer localization) envisions wearable IoT devices on users' arms or wrists that exchange wireless messages to ultimately compute the relative positions of each group member. The outcome is a topology or configuration of mo- bile users that can be tracked in real time.\nUsing the time wireless signals take to travel between two devices as a measure of the distance between them. The precision of this time measurement directly correlates with the bandwidth of the wireless signal used. Therefore, we use ultra-wideband (UWB) radios with a 1 GHz bandwidth. When used with a packet-handshake protocol called two-way ranging (TWR), today's UWB platforms can estimate the distance between two devices with about 10 cm precision without clock synchronization.\nP2PLoc: Peer-to-Peer Localization of Fast-Moving Entities - Computing Edge, Feb 2019\n**Battery Powered Sensors**\n\nManaging Energy Consumption as an Architectural Quality Attribute -- Computing Edge, Feb 2019\n-   If used protocol buffers for sending data - 42% energy savings compared to plaintext\n-   Less polling time and sending buffered data\n**IoT with Kafka**\n\n<https://dzone.com/articles/iot-live-demo-100000-connected-cars-with-kubernete>\n\n## Others**\n\n<https://github.com/home-assistant/core>\n\n<https://www.home-assistant.io>\n\n<https://developers.home.google.com/matter>\n\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Basic-Computer-Organization/","title":"Basic Computer Organization"},"frontmatter":{"draft":false},"rawBody":"# Basic Computer Organization\n\nCreated: 2019-08-04 17:06:01 +0500\n\nModified: 2022-03-02 20:32:11 +0500\n\n---\n\n**Instruction Execution Cycle**\n\n1.  Instruction Fetch - Obtain instruction from program store\n\n2.  Instruction Decode - Determine required actions and instruction size\n\n3.  Operand Fetch - Locate and obtain operand data\n\n4.  Execute - Compute result value or status\n\n5.  Result Store - Deposit results in storage for later use\n\n6.  Next Instruction - Determine successor instruction\n**Processor Memory Interaction**\n\n![Memory fetch the next instruction execute the instruction Processor Control unit ALU Registers, A, MAR, MOR, PC, SP Processor 1.2 decode the instruction Main Memory gcc gcc gcc ](media/Basic-Computer-Organization-image1.png)\n**Inside the CPU**\n\n![memory address register instruction register program counter RAM ADDRESS s MAR IR Control control lines CPU BUS DATA BUS MOR ACC ALU clock ystem clock memory data register accumulator (work register) arithmetic logic unit ](media/Basic-Computer-Organization-image3.png)**Instruction Execution Cycle**\n\n1.  Address of the next instruction is transferred from PC to MAR\n\n2.  The instruction is located in memory\n\n3.  Instruction is copied from memory to MDR\n\n4.  Instruction is transferred to and decoded in the IR\n\n5.  Control unit sends signals to appropriate devices to cause execution of the instruction\n**Address Decoder is used to decode memory addresses**\n\n![Ao Do DI Address input D2 D3 Row Selects Address Data bus Data ditecuon control Read/write ](media/Basic-Computer-Organization-image4.png)\n**Byte Ordering**\n\n![Little Endian CD 34 AB 12 w 12 AB 34 CD Register OAOBOCOD Memory Big-endian + Big Endian vs Little Endian Big Endian vs. Little Endian Big Endian 12 AB 34 CD LSB 27+ xOOOOOlC2 Little endian Regster OAOBOCOD Ln.enÃ¥&l 1 1000010 00000001 00000000 00000000 Memory oc 0+2: 0B 0+3: OA 01 00 oo lower address higher MSB Big endian 00000000 00000000 00000001 1 1000010 oo oo 01 ](media/Basic-Computer-Organization-image5.png)\n**Byte/Word Alignment**\n\n![ata data a data Buffer ata data aligned to 1 byte aligned to 4 bytes ](media/Basic-Computer-Organization-image6.png)\n**Types of Processor Operations**\n\n1.  Arithmetic and Logical Operations\n\n    a.  integer arithmetic\n\n    b.  comparing two quantities\n\n    c.  shifting, rotating bits in a quantity\n\n    d.  testing, comparing, and converting bits\n\n2.  Data Movement Operations\n\n    a.  moving data from memory to cpu\n\n    b.  moving data from memory to memory\n\n    c.  input and output\n\n3.  Program Control Operations\n\n    a.  starting a program\n\n    b.  halting a program\n\n    c.  skipping to other instructions\n\n    d.  testing data to decide whether to skip over some instructions\n**Instruction Set Architecture (ISA)**\n\nAn**ISA** is an abstract model of a[computer](https://en.wikipedia.org/wiki/Computer). It is also referred to as**architecture**or**computer architecture**. A realization of an ISA, such as a[central processing unit](https://en.wikipedia.org/wiki/Central_processing_unit)(CPU), is called an*implementation*.\n<https://en.wikipedia.org/wiki/Instruction_set_architecture>\nBased on where opcode and operand are\n\na.  Stack architecture\n\nb.  Accumulator architecture\n\nc.  Register-Memory architecture\n\nd.  Register-Register / Load Store architecture\n![(a) Stack Memory Stack Push A Push B Add Pop C (b) Accumulator Accumulator Load A Add B Store C (c) Register-memory Register (register-memory) Load RI, A Add Store R3,C (d) Register-registernoad-store Register (load-store) Load RI, A Load R2,B Add Store R3,C The code sequence for C = A + B for four classes of instruction sets. ](media/Basic-Computer-Organization-image7.png)\n![Stack machine Push D Push B Push C Add Mul Push E Sub POP A Accumulator machine Load B + Add C Sub E Store A Load Store machine Load RI D Load R2 B â€¢:â€¢ Load R3 C + Add R4, R2, R3 + Mul R5, RI, R4 + Load R6,E + Sub R7, R5, R6 + Store R7, A ](media/Basic-Computer-Organization-image8.png)\n**Addressing Modes**\n\n![+ The way by which an operand is specified in an instruction. Register Immediate Direct Register indirect Displacement Indexed Scaled Memory indirect Auto-increment Auto-decrement add add add add add add add add add add 100 (r2) (r2+r3) (r2+r3*4) rl, -(r2) rl+r2 rl+5 rl+M rl+M [ r2] rl+M [r2+100] rl+M rl+M [ r2+r3*4 rl <- rl+M[r2] ](media/Basic-Computer-Organization-image9.png)-   **Immediate Addressing**\n\nThe simplest addressing mode is**Immediate addressing**where you write the data value into the instruction.\n\nIn Pentium assembler this is the default and:\n\nMOV EAX,04\n-   **Register Indirect Addressing**\n-   **Indexed Addressing**\n<https://www.i-programmer.info/babbages-bag/150.html>\n\n## Branch Predictor**\n\nIn[computer architecture](https://en.wikipedia.org/wiki/Computer_architecture), a**branch predictor**^[[1]](https://en.wikipedia.org/wiki/Branch_predictor#cite_note-dbp-class-report-1)[[2]](https://en.wikipedia.org/wiki/Branch_predictor#cite_note-schemes-and-performances-2)[[3]](https://en.wikipedia.org/wiki/Branch_predictor#cite_note-3)[[4]](https://en.wikipedia.org/wiki/Branch_predictor#cite_note-4)[[5]](https://en.wikipedia.org/wiki/Branch_predictor#cite_note-5)^is a[digital circuit](https://en.wikipedia.org/wiki/Digital_electronics)that tries to guess which way a[branch](https://en.wikipedia.org/wiki/Branch_(computer_science))(e.g. an[if--then--else structure](https://en.wikipedia.org/wiki/Conditional_(programming))) will go before this is known definitively. The purpose of the branch predictor is to improve the flow in the[instruction pipeline](https://en.wikipedia.org/wiki/Instruction_pipeline). Branch predictors play a critical role in achieving high effective[performance](https://en.wikipedia.org/wiki/Computer_performance)in many modern[pipelined](https://en.wikipedia.org/wiki/Pipeline_(computing))[microprocessor](https://en.wikipedia.org/wiki/Microprocessor)architectures[^[6]^](https://en.wikipedia.org/wiki/Branch_predictor#cite_note-BPdynSurvey-6)such as[x86](https://en.wikipedia.org/wiki/X86).\n\n![Clock cycle Waiting instructions Completed in structions ](media/Basic-Computer-Organization-image10.png)\n\nExample of 4-stage pipeline. The colored boxes represent instructions independent of each other.\nTwo-way branching is usually implemented with a[conditional jump](https://en.wikipedia.org/wiki/Branch_(computer_science))instruction. A conditional jump can either be \"not taken\" and continue execution with the first branch of code which follows immediately after the conditional jump, or it can be \"taken\" and jump to a different place in program memory where the second branch of code is stored. It is not known for certain whether a conditional jump will be taken or not taken until the condition has been calculated and the conditional jump has passed the execution stage in the instruction pipeline (see fig. 1).\nWithout branch prediction, the processor would have to wait until the conditional jump instruction has passed the execute stage before the next instruction can enter the fetch stage in the pipeline. The branch predictor attempts to avoid this waste of time by trying to guess whether the conditional jump is most likely to be taken or not taken. The branch that is guessed to be the most likely is then fetched and[speculatively executed](https://en.wikipedia.org/wiki/Speculative_execution). If it is later detected that the guess was wrong then the speculatively executed or partially executed instructions are discarded and the pipeline starts over with the correct branch, incurring a delay.\nThe time that is wasted in case of a**branch misprediction**is equal to the number of stages in the pipeline from the fetch stage to the execute stage. Modern microprocessors tend to have quite long pipelines so that the misprediction delay is between 10 and 20[clock cycles](https://en.wikipedia.org/wiki/Clock_cycle). As a result, making a pipeline longer increases the need for a more advanced branch predictor.\nThe first time a conditional jump instruction is encountered, there is not much information to base a prediction on. But the branch predictor keeps records of whether branches are taken or not taken. When it encounters a conditional jump that has been seen several times before then it can base the prediction on the history. The branch predictor may, for example, recognize that the conditional jump is taken more often than not, or that it is taken every second time.\nBranch prediction is not the same as[branch target prediction](https://en.wikipedia.org/wiki/Branch_target_predictor). Branch prediction attempts to guess whether a conditional jump will be taken or not. Branch target prediction attempts to guess the target of a taken conditional or unconditional jump before it is computed by decoding and executing the instruction itself. Branch prediction and branch target prediction are often combined into the same circuitry.\n[Implementation](https://en.wikipedia.org/wiki/Branch_predictor#Implementation)\n-   [1.1 Static branch prediction](https://en.wikipedia.org/wiki/Branch_predictor#Static_branch_prediction)\n-   [1.2 Dynamic branch prediction](https://en.wikipedia.org/wiki/Branch_predictor#Dynamic_branch_prediction)\n-   [1.3 Random branch prediction](https://en.wikipedia.org/wiki/Branch_predictor#Random_branch_prediction)\n-   [1.4 Next line prediction](https://en.wikipedia.org/wiki/Branch_predictor#Next_line_prediction)\n-   [1.5 One-level branch prediction](https://en.wikipedia.org/wiki/Branch_predictor#One-level_branch_prediction)\n    -   [1.5.1 Saturating counter](https://en.wikipedia.org/wiki/Branch_predictor#Saturating_counter)\n-   [1.6 Two-level predictor](https://en.wikipedia.org/wiki/Branch_predictor#Two-level_predictor)\n    -   [1.6.1 Two-level adaptive predictor](https://en.wikipedia.org/wiki/Branch_predictor#Two-level_adaptive_predictor)\n-   [1.7 Local branch prediction](https://en.wikipedia.org/wiki/Branch_predictor#Local_branch_prediction)\n-   [1.8 Global branch prediction](https://en.wikipedia.org/wiki/Branch_predictor#Global_branch_prediction)\n-   [1.9 Alloyed branch prediction](https://en.wikipedia.org/wiki/Branch_predictor#Alloyed_branch_prediction)\n-   [1.10 Agree predictor](https://en.wikipedia.org/wiki/Branch_predictor#Agree_predictor)\n-   [1.11 Hybrid predictor](https://en.wikipedia.org/wiki/Branch_predictor#Hybrid_predictor)\n-   [1.12 Loop predictor](https://en.wikipedia.org/wiki/Branch_predictor#Loop_predictor)\n-   [1.13 Indirect branch predictor](https://en.wikipedia.org/wiki/Branch_predictor#Indirect_branch_predictor)\n-   [1.14 Prediction of function returns](https://en.wikipedia.org/wiki/Branch_predictor#Prediction_of_function_returns)\n-   [1.15 Overriding branch prediction](https://en.wikipedia.org/wiki/Branch_predictor#Overriding_branch_prediction)\n-   [1.16 Neural branch prediction](https://en.wikipedia.org/wiki/Branch_predictor#Neural_branch_prediction)\n<https://en.wikipedia.org/wiki/Branch_predictor>\n\n## Application Binary Interface (ABI)**\n\nIn[computer software](https://en.wikipedia.org/wiki/Computer_software), anapplication binary interface(ABI) is an[interface](https://en.wikipedia.org/wiki/Interface_(computing))between two binary program modules; often, one of these modules is a[library](https://en.wikipedia.org/wiki/Library_(computing))or[operating system](https://en.wikipedia.org/wiki/Operating_system)facility, and the other is a program that is being run by a user.\nAnABIdefines how data structures or computational routines are accessed in[machine code](https://en.wikipedia.org/wiki/Machine_code), which is a low-level, hardware-dependent format; in contrast, an[API](https://en.wikipedia.org/wiki/Application_programming_interface)defines this access in[source code](https://en.wikipedia.org/wiki/Source_code), which is a relatively high-level, hardware-independent, often[human-readable](https://en.wikipedia.org/wiki/Human-readable)format. A common aspect of an ABI is the[calling convention](https://en.wikipedia.org/wiki/Calling_convention), which determines how data is provided as input to or read as output from computational routines; examples are the[x86 calling conventions](https://en.wikipedia.org/wiki/X86_calling_conventions).\nAdhering to an ABI (which may or may not be officially standardized) is usually the job of a[compiler](https://en.wikipedia.org/wiki/Compiler), operating system, or library author; however, an application programmer may have to deal with an ABI directly when writing a program in a mix of programming languages, which can be achieved by using[foreign function calls](https://en.wikipedia.org/wiki/Foreign_function_call).\n<https://en.wikipedia.org/wiki/Application_binary_interface>\n\n## ELF (Executable and Linkable Format)**\n\nIn[computing](https://en.wikipedia.org/wiki/Computing), theExecutable and Linkable Format(ELF, formerly namedExtensible Linking Format), is a common standard[file format](https://en.wikipedia.org/wiki/File_format)for[executable](https://en.wikipedia.org/wiki/Executable)files,[object code](https://en.wikipedia.org/wiki/Object_code),[shared libraries](https://en.wikipedia.org/wiki/Library_(computing)), and[core dumps](https://en.wikipedia.org/wiki/Core_dump). First published in the specification for the[application binary interface](https://en.wikipedia.org/wiki/Application_binary_interface)(ABI) of the[Unix](https://en.wikipedia.org/wiki/Unix) operating system version named[System V Release 4](https://en.wikipedia.org/wiki/System_V_Release_4) (SVR4), and later in the Tool Interface Standard,it was quickly accepted among different vendors of[Unix](https://en.wikipedia.org/wiki/Unix)systems. In 1999, it was chosen as the standard binary file format for Unix and[Unix-like](https://en.wikipedia.org/wiki/Unix-like)systems on[x86](https://en.wikipedia.org/wiki/X86)processors by the[86open](https://en.wikipedia.org/wiki/Executable_and_Linkable_Format#86open) project.\nBy design, the ELF format is flexible, extensible, and[cross-platform](https://en.wikipedia.org/wiki/Cross-platform). For instance it supports different endiannesses and address sizes so it does not exclude any particular [central processing unit](https://en.wikipedia.org/wiki/Central_processing_unit) (CPU) or[instruction set architecture](https://en.wikipedia.org/wiki/Instruction_set_architecture). This has allowed it to be adopted by many different[operating systems](https://en.wikipedia.org/wiki/Operating_system)on many different hardware [platforms](https://en.wikipedia.org/wiki/Computing_platform)\nELF is used as standard file format for object files on Linux. Prior to this, the a.out file format was being used as a standard but lately ELF took over the charge as a standard.\nELF supports :\n-   Different processors\n-   Different data encoding\n-   Different classes of machines\n**File Layout**\n\nEach ELF file is made up of one ELF header, followed by file data. The data can include:\n-   Program header table, describing zero or more[memory segments](https://en.wikipedia.org/wiki/Memory_segmentation)\n-   Section header table, describing zero or more sections\n-   Data referred to by entries in the program header table or section header table\n\n![Linux executable walPthrough HEXADECMAL ANGE ALBERTINI CORKAMI.COM DUtQ DISSECTED ao ee 28 oe 80 34 28 ee 83 eo ee 08 E3 E2 '01 E3 78 74 61 ee ao ee ao ee 00 ee ee ee 80 eB ee ee ee ee ee ee ee ee 08 ee ee ao ee ee ee ee ee ee ee ee 80 oe ee 80 ee ao ee ee ee FILE 1/2 HEADER TECHNCAL FOR DENTFICATON EXECUTION SECTIONS CONTENTS OF EXECUTABLE 2/2 HEADER TECHNICAL DETAILS FOR FOR EXECUTION) 2 MAPPING 7F 84 45 46 01 01 el ea ea .ELF... ---$ uname -m armv71 ---$ ./simple.ARM Helto World! SIMPLE.ARM LOADING PROCESS 'l HEADER THE ELF HEADER IS PARSED ELF HEADER Câ‚¬mTFY AS PROGRAM HEADER TABLE ident e El \"RG El CLASS, El El VERSION e_tgpe nachine e version e e-phoff e snoff e ehstze e phentslze e -Phnun e shentsize e shnuâ€¢ e shstrndx 2 p_type p_offset p_vacdr P---paddr p_fi I p_fl ags ARM ASSEMBLY CODE DATA SECTIONS NAMES el ee ee ee oe ea 9B ee ee ee 9e ee 28 E3 '14 1B eo 80 EF ee rl, re, r z:ata_. 48 6 C 2E 73 68 ea 2E 72 01 72 Re E3 'e: 74 72 61 ee 02 ae 62 ee 88 ae ee 64 ee ee ee ee ee ee ee 08 08 ae 21 ae ae ae e: ee ee 74 2B ee 88 19 oe ee ee Be ee ee ee oe ee ee 78 E3 65 oe ee ee oe ae ee oe oe oe Hello. korld!. shstrtab.. text .. rodÃ¤ta. nov add nov nov svc nov nov svc 1 2 .13 pc, â€¢2B\"' STRT \"Hello korl SECTION I SECTION HEADER TABLE lcccÂ«c Tm, PROGRAM@ECTSI 3 EXECUTION ee ee ee 80 ee ee ee ee ee ee ee 73 64 oe oe oe 74 61 ea ea ea ea ee 74 ee ee ee ee . shrtrtab . text . rodata . shrtrtab 1 1 3 THE PROGRAM HEADER IS PARSED THE FILE IS MAPPED IN MEMORY ACCORDING TO ITS SEGMENT(S) ENTRY IS CALLED SYSCALLS ARE ACCESSED VIA: TRIVIA THE ELF WAS FIRS ](media/Basic-Computer-Organization-image11.png)\n<https://en.wikipedia.org/wiki/Executable_and_Linkable_Format>\n\n<https://www.thegeekstuff.com/2012/07/elf-object-file-format>\n\n## Von Neumann Architecture**\n\nThevon Neumann architecture---also known as thevon Neumann modelorPrinceton architecture---is a[computer architecture](https://en.wikipedia.org/wiki/Computer_architecture)based on a 1945 description by[John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann)and others in the[First Draft of a Report on the EDVAC](https://en.wikipedia.org/wiki/First_Draft_of_a_Report_on_the_EDVAC). That document describes a design architecture for an electronic[digital computer](https://en.wikipedia.org/wiki/Digital_computer)with these components:\n-   A[processing unit](https://en.wikipedia.org/wiki/Central_processing_unit)that contains an[arithmetic logic unit](https://en.wikipedia.org/wiki/Arithmetic_logic_unit)and[processor registers](https://en.wikipedia.org/wiki/Processor_register)\n-   A[control unit](https://en.wikipedia.org/wiki/Control_unit)that contains an[instruction register](https://en.wikipedia.org/wiki/Instruction_register)and[program counter](https://en.wikipedia.org/wiki/Program_counter)\n-   [Memory](https://en.wikipedia.org/wiki/Computer_memory)that stores[data](https://en.wikipedia.org/wiki/Data_(computing))and[instructions](https://en.wikipedia.org/wiki/Instruction_set)\n-   External[mass storage](https://en.wikipedia.org/wiki/Mass_storage)\n-   [Input and output](https://en.wikipedia.org/wiki/Input_and_output)mechanisms\n![Input Device Central Processing Unit Control Unit Arithmetic/Logic Unit Memory Unit Output Device ](media/Basic-Computer-Organization-image12.jpeg)\nThe term \"von Neumann architecture\" has evolved to mean any[stored-program computer](https://en.wikipedia.org/wiki/Stored-program_computer)in which an[instruction fetch](https://en.wikipedia.org/wiki/Instruction_fetch)and a data operation cannot occur at the same time because they share a common[bus](https://en.wikipedia.org/wiki/Bus_(computing)). This is referred to as the[von Neumann bottleneck](https://en.wikipedia.org/wiki/Von_Neumann_architecture#Von_Neumann_bottleneck), and often limits the performance of the system.\nThe design of a von Neumann architecture machine is simpler than a[Harvard architecture](https://en.wikipedia.org/wiki/Harvard_architecture)machine---which is also a stored-program system but has one dedicated set of address and data buses for reading and writing to memory, and another set of address and[data buses](https://en.wikipedia.org/wiki/Memory_bus)to fetch[instructions](https://en.wikipedia.org/wiki/Instruction_fetch).\nA stored-program digital computer keeps both[program instructions](https://en.wikipedia.org/wiki/Computer_program)and data in[read--write](https://en.wikipedia.org/wiki/Read%E2%80%93write_memory),[random-access memory](https://en.wikipedia.org/wiki/Random-access_memory)(RAM). Stored-program computers were an advancement over the program-controlled computers of the 1940s, such as the[Colossus](https://en.wikipedia.org/wiki/Colossus_computer)and the[ENIAC](https://en.wikipedia.org/wiki/ENIAC). Those were programmed by setting switches and inserting patch cables to route data and control signals between various functional units. The vast majority of modern computers use the same memory for both data and program instructions, but have[caches](https://en.wikipedia.org/wiki/CPU_cache)between the CPU and memory, and, for the caches closest to the CPU, have separate caches for instructions and data, so that most instruction and data fetches use separate buses ([split cache architecture](https://en.wikipedia.org/wiki/Modified_Harvard_architecture#Split-cache_(or_almost-von-Neumann)_architecture)).\nHistorically there have been 2 types of Computers:\n\n1.  **Fixed Program Computers --**Their function is very specific and they couldn't be programmed, e.g. Calculators.\n\n2.  **Stored Program Computers --**These can be programmed to carry out many different tasks, applications are stored on them, hence the name.\n**Main Memory Unit (Registers)**\n\n1.  **Accumulator:**Stores the results of calculations made by ALU\n\n2.  **Program Counter (PC):**Keeps track of the memory location of the next instructions to be dealt with. The PC then passes this next address to Memory Address Register (MAR)\n\n3.  **Memory Address Register (MAR):**It stores the memory locations of instructions that need to be fetched from memory or stored into memory\n\n4.  **Memory Data Register (MDR):**It stores instructions fetched from memory or any data that is to be transferred to, and stored in, memory\n\n5.  **Current Instruction Register (CIR):**It stores the most recently fetched instructions while it is waiting to be coded and executed\n\n6.  **Instruction Buffer Register (IBR):**The instruction that is not to be executed immediately is placed in the instruction buffer register IBR\n<https://en.wikipedia.org/wiki/Von_Neumann_architecture>\n\n<https://www.geeksforgeeks.org/computer-organization-von-neumann-architecture>\n"},{"fields":{"slug":"/Computer-Science/Operating-System/CPU---GPU---TPU/","title":"CPU | GPU | TPU"},"frontmatter":{"draft":false},"rawBody":"# CPU | GPU | TPU\n\nCreated: 2018-10-07 14:50:15 +0500\n\nModified: 2022-08-10 21:55:49 +0500\n\n---\n\nMAC - Multiplier, Adder, Accumulator\n\nTensor - n-dimensional array\n\nSpecifically for matrix operations\n**CPU / GPU**\n-   A CPU is a scalar machine, which means it processes instructions one step at a time.\n-   A GPU is composed of hundreds of cores that can handle thousands of threads simultaneously.\n    -   Thats because GPUs were designed for 3d game rendering, which often involves parallel operations -The ability of a GPU with 100+ cores to process thousands of threads can accelerate some software by 100x over a CPU alone.\n    -   What's more, the GPU achieves this acceleration while being more power- and cost-efficient than a CPU.\n    -   So when neural networks run on GPUs, they run much faster than on CPUs\n-   ![alt text](media/CPU---GPU---TPU-image1.jpg)\n-   A GPU is a vector machine. You can give it a long list of dataâ€Š---â€Ša 1D vectorâ€Š---â€Šand run computations on the entire list at the same time.\n-   This way, we can perform more computations per second, but we have to perform the same computation on a vector of data in parallel.\n-   GPUs are general purpose chips. They don't just perform matrix operations, they can really do any kind of computation.\n-   GPUs are optimized for taking huge batches of data and performing the same operation over and over very quickly\n**TPU**\n-   TPU hardware is comprised of four independent chips.\n-   Each chip consists of two compute cores called Tensor Cores.\n-   A Tensor Core consists of scalar, vector and matrix units (MXU).\n-   In addition, 8 GB of on-chip memory (HBM) is associated with each Tensor Core.\n-   The bulk of the compute horsepower in a Cloud TPU is provided by the MXU.\n-   Each MXU is capable of performing 16K multiply-accumulate operations in each cycle.\n-   While the MXU's inputs and outputs are 32-bit floating point values, the MXU performs multiplies at reduced bfloat16 precision.\n-   Bfloat16 is a 16-bit floating point representation that provides better training and model accuracy than the IEEE half-precision representation. -From a software perspective, each of the 8 cores on a Cloud TPU can execute user computations (XLA ops) independently.\n-   High-bandwidth interconnects allow the chips to communicate directly with each other.-   ![alt text](media/CPU---GPU---TPU-image2.png)\n\n**The Systolic Array**\n-   The way to achieve that matrix performance is through a piece of architecture called a systolic array.\n-   This is the interesting bit, and it's why a TPU is performant.\n-   A systolic array is a kind of hardware algorithm, and it describes a pattern of cells on a chip that computes matrix multiplication.\n-   \"Systolic\" describes how data moves in waves across the chip, like the beating of a human heart.\n\n![alt text](media/CPU---GPU---TPU-image3.png)\n**Use Cases**\n\nCPUs:\n-   Quick prototyping that requires maximum flexibility\n-   Simple models that do not take long to train\n-   Small models with small effective batch sizes\n-   Models that are dominated by custom TensorFlow operations written in C++\n-   Models that are limited by available I/O or the networking bandwidth of the host system\n\nGPUs:\n-   Models that are not written in TensorFlow or cannot be written in TensorFlow\n-   Models for which source does not exist or is too onerous to change\n-   Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\n-   Models with TensorFlow ops that are not available on Cloud TPU (see the list of available TensorFlow ops)\n-   Medium-to-large models with larger effective batch sizes\n\nTPUs:\n-   Models dominated by matrix computations\n-   Models with no custom TensorFlow operations inside the main training loop\n-   Models that train for weeks or months\n-   Larger and very large models with very large effective batch sizes\n**CPU Time**\n\n<https://dzone.com/articles/nice-cpu-time-ni-time-in-top>\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Caches---Caching/","title":"Caches / Caching"},"frontmatter":{"draft":false},"rawBody":"# Caches / Caching\n\nCreated: 2019-01-09 12:33:37 +0500\n\nModified: 2021-11-07 17:15:50 +0500\n\n---\n\n**Points to remeber about cache**\n\na.  Cache data cannot be the source of truth\n\nb.  Cache data has to be pretty small because cache tends to keep all the data in-memory\n\nc.  Consider Eviction policies (Page replacement policies)\nIn[computing](https://en.wikipedia.org/wiki/Computing), acacheis a hardware or software component that stores data so that future requests for that data can be served faster; the data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere. Acache hitoccurs when the requested data can be found in a cache, while acache missoccurs when it cannot. Cache hits are served by reading data from the cache, which is faster than recomputing a result or reading from a slower data store; thus, the more requests that can be served from the cache, the faster the system performs.\nTo be cost-effective and to enable efficient use of data, caches must be relatively small. Nevertheless, caches have proven themselves in many areas of computing, because typical[computer applications](https://en.wikipedia.org/wiki/Application_software)access data with a high degree of[locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference). Such access patterns exhibit temporal locality, where data is requested that has been recently requested already, and[spatial](https://en.wikipedia.org/wiki/Memory_address)locality, where data is requested that is stored physically close to data that has already been requested.\nA cache hit happens whenever data is already available in the cache and can be returned without any other operation, otherwise the cache responds with a cache miss or, if available, can transparently retrieve the value from another underlying backend system and cache it before returning it to the requestor.\nCaches are designed to respond to cache requests in near real-time and therefore are implemented as simple key-value stores. The underlying data structure, however, can still be different and depends on the storage backend. In addition, caches can most often be used by multiple front end consumers such as web applications.\nCaches can have multiple levels, so-called tiered storages, which are ordered by their speed factor. Frequently or recently used data are typically held in memory, whereas other data (depending on the tiered storage implementations) can be written to SSD, slower spinning disk systems and later on other even slower systems, or can be evicted completely - if reproducible.\nTypical use cases are in-memory caches for database, slowly retrievable disk-based data, data stored remotely behind slow network connections or results of previous calculations.\n-   Caching first approach\n**Benefits of caching**\n-   Improved responsiveness\n-   Decreased network costs\n-   Improved performance with same hardware\n-   Availability of content with interruption on network or backend resources\n**WHAT TO CACHE?**\n\nA good starting point to find out what to cache in your application is to imagine everything where multiple executions of some request result in the same outcome. This can be database queries, HTML fragments, complete web pages or an output of a heavy computation.\nIt also makes sense to store any kind of language related data geographically local to the content consumer. Common elements of this type of data is translations, user data for people living in a given region. Only one rule applies: data should not change too often or fast but should be read very frequently.\n**WHAT NOT TO CACHE?**\n\nA common misconception is if you cache everything, you'll automatically benefit from it. What often works in the first place delivers another problem during high data peaks.\nData that changes often is generally not very good for caches. Whenever data changes, the cache must be invalidated and, depending on the chosen caching strategy, this can be a costly operation. Imagine a caching system that is located around the world and your data change with a rate of more than 100 changes per second. The benefit of having those data cached will be nullified by the fact that all caches need to invalidate and maybe re-retrieve that changed data record for every single change.\nAnother point to think about is to not cache data that is fast to retrieve anyways. Caching those elements will introduce an additional round-trip while filling the cache, requiring additional memory. The benefit might not show the expected results or be worth the overhead of bringing in another layer into the architecture.\n**Writing Policies (cache access patterns)**\n\nWhen a system writes data to cache, it must at some point write that data to the backing store as well. The timing of this write is controlled by what is known as thewrite policy.\n-   **Write-through**\n\n![1. Data written to the Cache Cache & Database 2. Data written to the both are always in Database sync ](media/Caches---Caching-image1.png)\nwrite is done synchronously both to the cache and to the backing store. The significance here is not the order in which it happens or whether it happens in parallel. The significance is that I/O completion is only confirmed once the data has been written to both places.\n-   **Advantage:**Ensures fast retrieval while making sure the data is in the backing store and is not lost in case the cache is disrupted.\n-   **Disadvantage:**Writing data will experience latency as you have to write to two places every time.\n-   **What is it good for?**\n\nThe write-through policy is good for applications that has more reads than writes. This will result in slightly higher write latency but low read latency. So, it's ok to spend a bit longer writing once, but then benefit from reading frequently with low latency.-   **Write-back(also calledwrite-behind)**\n\n![1. Data written to the Cache 2. Asynchronous replication between Cache & Database ](media/Caches---Caching-image2.png)\nInitially, writing is done only to the cache. The write to the backing store is postponed until the modified content is about to be replaced by another cache block.\nUsing the write-back policy, data is written to the cache and immediately I/O completion is confirmed. The data is then typically also written to the backing store in the background but the completion confirmation is not blocked on that.-   **Advantage:**Low latency and high throughput for write-intensive applications.\n-   **Disadvantage:**There is data availability risk because the cache could fail (and so suffer from data loss) before the data is persisted to the backing store. This result in the data being lost.\n-   **What is it good for?**\n\nThe write-back policy is the best performer for mixed workloads as both read and write I/O have similar response time levels. In reality, you can add resiliency (e.g. by duplicating writes) to reduce the likelihood of data loss.-   **Write-around**\n\n![Cache queries database for missing entries Data written to the database ](media/Caches---Caching-image3.png)\nUsing the write-around policy, data is written only to the backing store without writing to the cache. So, I/O completion is confirmed as soon as the data is written to the backing store.\n-   **Advantage:**Good for not flooding the cache with data that may not subsequently be re-read.\n-   **Disadvantage:**Reading recently written data will result in a cache miss (and so a higher latency) because the data can only be read from the slower backing store.\n-   **What is it good for?**\n\nThe write-around policy is good for applications that don't frequently re-read recently written data. This will result in lower write latency but higher read latency which is a acceptable trade-off for these scenarios.\n**Cache Types**\n\n1.  **HTTP Cache**\n\nA HTTP Cache is mostly used in browsers. This kind of cache keeps information about the last modification date of a resource or a content hash to identify changes to its content. Web servers are expected to deliver useful information about the state of an element to prevent retrieval of an already cached element from the server.\nThis kind of cache is used to reduce network traffic, minimize cost and offer the user an instant experience for multiple visits.\n2.  **Fragment Cache**\n\nA Fragment Cache caches parts of a response or result. This could be a database query outcome or a part of an HTML page. Whatever it is, it should not change often.\nA common use case for a Fragment Cache is a web page known to contain user specific and user unspecific content. The user-independent content can be cached as a fragment and augmented with user specific content on retrieval. This process is called **Content Enrichment.**\nThis caching type is used to reduce operation cost and hardware by providing the same throughput with less computational overhead.\n3.  **Object Cache**\n\nAn Object Cache stores any sort of objects that otherwise need to be read from other data representations. A cache of this type can be used in front of a database to speed up queries and store the resulting objects (e.g. Object Relational Mapping, ORM), or store un-marshalled results of XML, JSON or other general data representations transformed into objects.\nThese caches often act as a proxy between some external resource, like a database or webservice, and they speed up transformation processes or prevent additional network round-trips between the consumer and producer systems.\n**Caching Strategies**\n\n1.  Cooperative/Distributed caching\n\nCache data is distributed between nodes\n\nIn Cooperative Caching, also known as Distributed Caching, multiple distinct systems (normally referred to as cluster-nodes) work together to build a huge, shared cache.\n\n2.  Partial caching\n\nPartial Caching describes a type of caching where not all data is stored inside the cache. Depending on certain criteria, responses might not be cacheable or are not expected to be cached (like temporary failures).\n\nA typical example for data where not everything is cacheable is websites. Some pages are \"static\" and\n\nonly change if some manual or regular action happens. Those pages can easily be cached and invalidated whenever this particular action happened. Apart from that, other pages consist of mostly dynamic content or frequently updated content (like stock market tickers) and shouldn't be cached at all.\n\n3.  Geographical caching\n\nGeographical Caches are located in strategically chosen locations to optimize latency on requests, therefore this kind of cache will mostly be used for website content. It is also known as CDN (Content Delivery Network).\n\n4.  Preemptive caching\n\nA Preemptive Cache itself is not a caching type like the others above but is mostly used in conjunction with a Geographical Cache.\n\nUsing a warm-up engine a Preemptive Cache is populated on startup and tries to update itself based on rules or events. The idea behind this cache addition is to reload data from any backend service or central cluster even before a requestor wants to retrieve the element. This keeps access time to the cached elements constant and prevents accesses to single elements from becoming unexpectedly long.\n\nBuilding a Preemptive Cache is not easy and requires a lot of knowledge of the cached domain and the update workflows.\n\n5.  Latency SLA caching\n\nA Latency SLA Cache is able to maintain latency SLAs even if the cache is slow or overloaded. This type of cache can be build in two different ways.\n\nThe first option is to have a timeout to exceed before the system either requests the potentially cached element from the original source (in parallel to the already running cache request) or provides a simple default answer, and uses whatever returns first.\n\nThe other option is to always fire both requests in parallel and take whatever returns first. This option is not the preferred way of implementation since it mostly works against the idea of caching and won't reduce load on the backend system. This option might still make sense if multiple caching layers are available, like always try the first and second nearest caches in parallel.\n**Caching Topologies**\n\n1.  **IN-PROCESS CACHES**\n\nThe In-Process Cache is most oftenly used in non-distributed systems. The cache is kept inside the application's memory space itself and offers the fastest possible access speed.\n\nThis type of cache is used for caching database entities but can also be used as some kind of an object pool, for instance pooling most recently used network connections to be reused at a later point.\n\nAdvantages:\n-   Highest access speed\n-   Data available locally\n-   Easy to maintain\n\nDisadvantages:\n-   Data duplication if multiple applications\n-   High memory consumption on a single node\n-   Data cached inside the applications memory\n-   Seems easy to build but has a lot of hidden challenges\n2.  **EMBEDDED NODE CACHES**\n\nUsing an Embedded Node Cache the application itself will be part of the cluster. This caching topology is a kind of combination between an In-Process Cache and the Cooperative Caching and it can either use partitioning or full dataset replication.\n\nUsing full replication the application will get all the benefits of an In-Process Cache since all data is available locally (highest access speed) but for the sake of memory consumption and heap size.\n\nBy using data partitioning the application knows about the owner of a requested record and will ask directly using an existing data stream. Speed is lower than locally available data but still accessible quickly.\n\nAdvantages:\n-   Data can be replicated for highest access speed\n-   Data can be partitioned to create bigger datasets\n-   Cached data might be used a shared state lookup between applications U Possible to scale out the application itself\n\nDisadvantages:\n-   High duplication rate on replication\n-   Application and cache cannot be scaled independently U Data cached inside the applications memory\n3.  **CLIENT-SERVER CACHES**\n\nA Client-Server Cache is one of the most typical setups these days (next to a pure In-Process Cache). In general these systems tend to be Cooperative Caches by having a multi-server architecture to scale out and have the same feature set as the Embedded Node Caches but with the client layer on top.\n\nThis architecture keeps separate clusters of the applications using the cached data and the data itself, offering the possibility to scale the application cluster and the caching cluster independently. Instead of a caching cluster it is also possible to have a single caching server however this situation slowly losing traction.\n\nHaving a Client-Server Cache architecture is quite similar to the common usage patterns of an external relational database or other network-connected backend resources.\n\nAdvantages:\n-   Data can be replicated for highest access speed\n-   Data can be partitioned to create bigger datasets\n-   Cached data might be used a shared state lookup between applications U Applications and cache can be scaled out independently\n-   Applications can be restarted without losing data\n\nDisadvantages:\n-   High duplication rate on replication\n-   Always an additional network round trip (fast network)\n**Caching places**\n-   Client Side Caching\n-   Server Side Caching\n-   Network Caching\n**Cache Coherence**\n\nIn[computer architecture](https://en.wikipedia.org/wiki/Computer_architecture),cache coherenceis the uniformity of shared resource data that ends up stored in multiple[local caches](https://en.wikipedia.org/wiki/Cache_(computing)). When clients in a system maintain[caches](https://en.wikipedia.org/wiki/CPU_cache)of a common memory resource, problems may arise with incoherent data, which is particularly the case with[CPUs](https://en.wikipedia.org/wiki/Central_processing_unit)in a[multiprocessing](https://en.wikipedia.org/wiki/Multiprocessing)system.\nIn the illustration, consider both the clients have a cached copy of a particular memory block from a previous read. Suppose the client on the bottom updates/changes that memory block, the client on the top could be left with an invalid cache of memory without any notification of the change. Cache coherence is intended to manage such conflicts by maintaining a coherent view of the data values in multiple caches.\n![](media/Caches---Caching-image4.png)\n**Requirements for cache coherence**\n\n**Write Propagation**\n\nChanges to the data in any cache must be propagated to other copies (of that cache line) in the peer caches.\n**Transaction Serialization**\n\nReads/Writes to a single memory location must be seen by all processors in the same order.\n**Coherence Mechanisms**\n\n**Snooping**\n\nMain article:[Bus snooping](https://en.wikipedia.org/wiki/Bus_snooping)\n\nFirst introduced in 1983,snooping is a process where the individual caches monitor address lines for accesses to memory locations that they have cached.Thewrite-invalidate protocolsandwrite-update protocolsmake use of this mechanism.\nFor the snooping mechanism, a snoop filter reduces the snooping traffic by maintaining a plurality of entries, each representing a cache line that may be owned by one or more nodes. When replacement of one of the entries is required, the snoop filter selects for the replacement the entry representing the cache line or lines owned by the fewest nodes, as determined from a presence vector in each of the entries. A temporal or other type of algorithm is used to refine the selection if more than one cache line is owned by the fewest nodes.\n**Directory-based**\n\nMain article:[Directory-based cache coherence](https://en.wikipedia.org/wiki/Directory-based_cache_coherence)\n\nIn a directory-based system, the data being shared is placed in a common directory that maintains the coherence between caches. The directory acts as a filter through which the processor must ask permission to load an entry from the primary memory to its cache. When an entry is changed, the directory either updates or invalidates the other caches with that entry.\n<https://en.wikipedia.org/wiki/Cache_coherence>\n\n## Cache Invalidation**\n\nCache invalidationis a process in a[computer system](https://en.wikipedia.org/wiki/Computer_system)whereby entries in a[cache](https://en.wikipedia.org/wiki/Cache_(computing))are replaced or removed.\nIt can be done explicitly, as part of a[cache coherence](https://en.wikipedia.org/wiki/Cache_coherence)protocol. In such a case, a processor changes a memory location and then invalidates the cached[values](https://en.wikipedia.org/wiki/Value_(computer_science))of that memory location across the rest of the computer system.\n**Explicit invalidation**\n\nCache invalidationcan be used to[push](https://en.wikipedia.org/wiki/Push_technology)new content to a[client](https://en.wikipedia.org/wiki/Client_(computing)). This method functions as an alternative to other methods of displaying new content to connected clients.Invalidationis carried out by changing the application data, which in turn marks the information received by the client as out-of-date. After the cache is invalidated, if the client requests the cache, they are delivered a new version.\n**Methods**\n\nThere are three specific methods to invalidate a cache, but not all[caching proxies](https://en.wikipedia.org/wiki/Caching_proxy)support these methods.\n\n1.  **Purge**\n\nRemoves content from caching proxy immediately. When the client requests the data again, it is[fetched](https://en.wikipedia.org/wiki/Instruction_cycle)from the application and stored in the caching proxy. This method removes all variants of the cached content.\n\n2.  **Refresh**\n\nFetches requested content from the application, even if cached content is available. The content previously stored in the cache is replaced with a new version from the application. This method affects only one variant of the cached content.\n\n3.  **Ban**\n\nA reference to the cached content is added to a[blacklist](https://en.wikipedia.org/wiki/Blacklist_(computing))(or ban list). Client requests are then checked against this blacklist, and if a request matches, new content is fetched from the application, returned to the client, and added to the cache.\nThis method, unlike purge, does not immediately remove cached content from the caching proxy. Instead, the cached content is updated after a client requests that specific information.\n**Alternatives**\n\nThere are a few alternatives to cache invalidation that still deliver updated content to the client. One alternative is to expire the cached content quickly by reducing the[time-to-live (TTL)](https://en.wikipedia.org/wiki/Time_to_live)to a very low value. Another alternative is to validate the cached content at each request. A third option is to not cache content requested by the client. These alternatives can cause issues, as they create high load on the application due to more frequent requests for information.\n**Disadvantages**\n\nUsing invalidation to transfer new content can be difficult when invalidating multiple objects. Invalidating multiple representations adds a level of complexity to the application. Cache invalidation must be carried out through a caching proxy; these requests can impact performance of the caching proxy, causing information to be transferred at a slower rate to clients.\n<https://en.wikipedia.org/wiki/Cache_invalidation>\n\n## Locality of reference / Cache locality / Principle of locality**\n\nIt is the tendency of a processor to access the same set of memory locations repetitively over a short period of time.\nLocality is a type of[predictable](https://en.wikipedia.org/wiki/Predictability)behavior that occurs in computer systems. Systems that exhibit stronglocality of referenceare great candidates for performance optimization through the use of techniques such as the[caching](https://en.wikipedia.org/wiki/CPU_cache),[prefetching](https://en.wikipedia.org/wiki/Prefetch_instruction)for memory and advanced[branch predictors](https://en.wikipedia.org/wiki/Branch_predictor)at the[pipelining](https://en.wikipedia.org/wiki/Pipeline_(computing))stage of a processor core.\n**Types of locality**\n-   **Temporal locality**\n\nIf at one point a particular memory location is referenced, then it is likely that the same location will be referenced again in the near future. There is a temporal proximity between the adjacent references to the same memory location. In this case it is common to make efforts to store a copy of the referenced data in faster memory storage, to reduce the latency of subsequent references. Temporal locality is a special case of spatial locality (see below), namely when the prospective location is identical to the present location.\n-   **Spatial locality**\n\nIf a particular storage location is referenced at a particular time, then it is likely that nearby memory locations will be referenced in the near future. In this case it is common to attempt to guess the size and shape of the area around the current reference for which it is worthwhile to prepare faster access for subsequent reference.\n-   **Memory locality**\n\nSpatial locality explicitly relating to[memory](https://en.wikipedia.org/wiki/Computer_memory).\n-   **[Branch](https://en.wikipedia.org/wiki/Branch_(computer_science))locality**\n\nIf there are only a few possible alternatives for the prospective part of the path in the spatial-temporal coordinate space. This is the case when an instruction loop has a simple structure, or the possible outcome of a small system of conditional branching instructions is restricted to a small set of possibilities. Branch locality is typically not a spatial locality since the few possibilities can be located far away from each other.\n-   **Equidistant locality**\n\nIt is halfway between the spatial locality and the branch locality. Consider a loop accessing locations in an equidistant pattern, i.e., the path in the spatial-temporal coordinate space is a dotted line. In this case, a simple linear function can predict which location will be accessed in the near future.\nIn order to benefit from the very frequently occurring temporal and spatial locality, most of the information storage systems are[hierarchical](https://en.wikipedia.org/wiki/Computer_data_storage#Hierarchy_of_storage). The equidistant locality is usually supported by the diverse nontrivial increment instructions of the processors. For branch locality, the contemporary processors have sophisticated branch predictors, and on the basis of this prediction the memory manager of the processor tries to collect and preprocess the data of the plausible alternatives.\n<https://en.wikipedia.org/wiki/Locality_of_reference>\n\n<https://www.geeksforgeeks.org/locality-of-reference-and-cache-operation-in-cache-memory>Æ’\n\n<https://www.geeksforgeeks.org/computer-organization-locality-and-cache-friendly-code>\n\n## Cache-oblivious algorithm**\n\nIn[computing](https://en.wikipedia.org/wiki/Computing), acache-oblivious algorithm(or cache-transcendent algorithm) is an[algorithm](https://en.wikipedia.org/wiki/Algorithm)designed to take advantage of a[CPU cache](https://en.wikipedia.org/wiki/CPU_cache)without having the size of the cache (or the length of the[cache lines](https://en.wikipedia.org/wiki/Cache_line), etc.) as an explicit parameter. An**optimal cache-oblivious algorithm**is a cache-oblivious algorithm that uses the cache optimally (in an[asymptotic](https://en.wikipedia.org/wiki/Asymptotic_notation)sense, ignoring constant factors). Thus, a cache-oblivious algorithm is designed to perform well, without modification, on multiple machines with different cache sizes, or for a[memory hierarchy](https://en.wikipedia.org/wiki/Memory_hierarchy)with different levels of cache having different sizes. Cache-oblivious algorithms are contrasted with explicit[blocking](https://en.wikipedia.org/wiki/Loop_tiling),as in[loop nest optimization](https://en.wikipedia.org/wiki/Loop_nest_optimization), which explicitly breaks a problem into blocks that are optimally sized for a given cache.\nOptimal cache-oblivious algorithms are known for[matrix multiplication](https://en.wikipedia.org/wiki/Cache-oblivious_matrix_multiplication),[matrix transposition](https://en.wikipedia.org/wiki/Matrix_transposition),[sorting](https://en.wikipedia.org/wiki/Funnelsort), and several other problems. Some more general algorithms, such as[Cooley--Tukey FFT](https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm), are optimally cache-oblivious under certain choices of parameters. Because these algorithms are only optimal in an asymptotic sense (ignoring constant factors), further machine-specific tuning may be required to obtain nearly optimal performance in an absolute sense. The goal of cache-oblivious algorithms is to reduce the amount of such tuning that is required.\nTypically, a cache-oblivious algorithm works by a[recursive](https://en.wikipedia.org/wiki/Recursion)[divide and conquer algorithm](https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm), where the problem is divided into smaller and smaller subproblems. Eventually, one reaches a subproblem size that fits into cache, regardless of the cache size. For example, an optimal cache-oblivious matrix multiplication is obtained by recursively dividing each matrix into four sub-matrices to be multiplied, multiplying the submatrices in a[depth-first](https://en.wikipedia.org/wiki/Depth-first)fashion. In tuning for a specific machine, one may use a[hybrid algorithm](https://en.wikipedia.org/wiki/Hybrid_algorithm)which uses blocking tuned for the specific cache sizes at the bottom level, but otherwise uses the cache-oblivious algorithm.\n<https://en.wikipedia.org/wiki/Cache-oblivious_algorithm>\n\n## HTTP ETag (Entity tag)**\n\nThe**ETag**or**entity tag**is part of[HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol), the protocol for the[World Wide Web](https://en.wikipedia.org/wiki/World_Wide_Web). It is one of several mechanisms that HTTP provides for[Web cache](https://en.wikipedia.org/wiki/Web_cache)validation, which allows a client to make conditional requests. This allows caches to be more efficient and saves bandwidth, as a Web server does not need to send a full response if the content has not changed. ETags can also be used for[optimistic concurrency control](https://en.wikipedia.org/wiki/Optimistic_concurrency_control),as a way to help prevent simultaneous updates of a resource from overwriting each other.\nAn ETag is an opaque identifier assigned by a Web server to a specific version of a resource found at a[URL](https://en.wikipedia.org/wiki/Uniform_Resource_Locator). If the resource representation at that URL ever changes, a new and different ETag is assigned. Used in this manner, ETags are similar to[fingerprints](https://en.wikipedia.org/wiki/Fingerprint_(computing))and can quickly be compared to determine whether two representations of a resource are the same.\n<https://en.wikipedia.org/wiki/HTTP_ETag>\n\n## Others**\n-   set-associative (SA) caches\n-   log-structured (LS) caches\n-   Kangaroo - <https://engineering.fb.com/2021/10/26/core-data/kangaroo>\n\n## References**\n\n[https://en.wikipedia.org/wiki/Cache_(computing)](https://en.wikipedia.org/wiki/Cache_(computing)#WRITE-BACK)\n\n<https://www.freecodecamp.org/news/what-is-cached-data>\n\n<https://engineering.fb.com/2021/09/02/open-source/cachelib>\n\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Compilers/","title":"Compilers"},"frontmatter":{"draft":false},"rawBody":"# Compilers\n\nCreated: 2018-05-24 11:08:23 +0500\n\nModified: 2022-01-16 20:10:22 +0500\n\n---\n\nLLVM - The**LLVM**compiler infrastructure project is a \"collection of modular and reusable[compiler](https://en.wikipedia.org/wiki/Compiler)and[toolchain](https://en.wikipedia.org/wiki/Toolchain)technologies\" used to develop compiler[front ends](https://en.wikipedia.org/wiki/Compiler#Front_end)and[back ends](https://en.wikipedia.org/wiki/Compiler#Back_end).\nLLVM is written in[C++](https://en.wikipedia.org/wiki/C%2B%2B)and is designed for[compile-time](https://en.wikipedia.org/wiki/Compile-time),[link-time](https://en.wikipedia.org/wiki/Link-time),[run-time](https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)), and \"idle-time\" optimization of programs written in arbitrary[programming languages](https://en.wikipedia.org/wiki/Programming_language).\n1.  LLVM Intermediate Representation (IR)\n\n2.  LLVM debugger\n\n3.  LLVM representation of the C++ standard library\n**Computer voting can never be safe - Medium**\n-   Trusting trust attack\n\nOccurs when an attacker attempts to disseminate a compiler executable that produces corrupted executables, at least one of those produced corrupted executables is a corrupted compiler, and the attacker attempts to make this situation self-perpetuating.-   Diverse double-compiling\n\nUsing 4 compilers of different languages and sources to check each other's work\n**References**\n-   I'm a Computer Scientist. Here's why you should never trust a Computer (Especially when it comes to voting) - Medium blog by Ryan North\n-   Fully Countering Trusting Trust through Diverse Double-Compiling by David A. Wheeler (MS, George Mason University)\n**Compiler**\n-   Lexers (Tokenization)\n-   AST (Abstract Syntax Tree)\n**PEG Parsers (Parsing Expression Grammer)**\n\nIn[computer science](https://en.wikipedia.org/wiki/Computer_science), aparsing expression grammar, orPEG, is a type of[analytic](https://en.wikipedia.org/wiki/Formal_grammar#Analytic_grammars)[formal grammar](https://en.wikipedia.org/wiki/Formal_grammar), i.e. it describes a[formal language](https://en.wikipedia.org/wiki/Formal_language)in terms of a set of rules for recognizing[strings](https://en.wikipedia.org/wiki/String_(computer_science))in the language. The formalism was introduced by Bryan Ford in 2004and is closely related to the family of[top-down parsing languages](https://en.wikipedia.org/wiki/Top-down_parsing_language)introduced in the early 1970s. Syntactically, PEGs also look similar to[context-free grammars](https://en.wikipedia.org/wiki/Context-free_grammar)(CFGs), but they have a different interpretation: the choice operator selects the first match in PEG, while it is ambiguous in CFG. This is closer to how string recognition tends to be done in practice, e.g. by a[recursive descent parser](https://en.wikipedia.org/wiki/Recursive_descent_parser).\nUnlike CFGs, PEGs cannot be[ambiguous](https://en.wikipedia.org/wiki/Ambiguous_grammar); if a string parses, it has exactly one valid[parse tree](https://en.wikipedia.org/wiki/Parse_tree). It is conjectured that there exist context-free languages that cannot be recognized by a PEG, but this is not yet proven.PEGs are well-suited to parsing computer languages (and artificial human languages such as[Lojban](https://en.wikipedia.org/wiki/Lojban)), but not[natural languages](https://en.wikipedia.org/wiki/Natural_language)where the performance of PEG algorithms is comparable to general CFG algorithms such as the[Earley algorithm](https://en.wikipedia.org/wiki/Earley_algorithm).\n<https://en.wikipedia.org/wiki/Parsing_expression_grammar>\n\n<https://medium.com/@gvanrossum_83706/peg-parsing-series-de5d41b2ed60>\n\n## Compilation JIT vs AOT**\n\n**Just-In-time (JIT)**compilation is the process of translating code written in a programming language to machine code at runtime (during a program or application's execution). At runtime, certain dynamic information is available, such as type identification. A JIT compiler monitors to detect functions or loops of code that are run multiple times. These pieces of code are then compiled. If they're quite commonly executed, JIT will optimize them and also store the optimized, compiled code for execution. Browsers use JIT compilation to run JavaScript.\n**Ahead-Of-Time (AOT)**compilation is the process of translating code written in a programming language to machine code before execution (as opposed to at runtime). Doing so reduces runtime overhead and compiles all files together rather than separately.\nSome benefits of AOT:\n-   Better security because evaluation is already done.\n-   Templates and styles are inlined with JS so fewer asynchronous calls.\n-   Small download size, because the compiler need not to be download as the app is precompiled.\n-   AOT also enables tree shaking.\n<https://github.com/jamiebuilds/the-super-tiny-compiler/blob/master/the-super-tiny-compiler.js>\n\n<https://www.toptal.com/scala/writing-an-interpreter>\n\n[Back to Basics: Compiling and Linking](https://youtu.be/tjDfP8tQDyY)\n\n[Machine Code Instructions](https://youtu.be/Mv2XQgpbTNE)\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Concepts/","title":"Concepts"},"frontmatter":{"draft":false},"rawBody":"# Concepts\n\nCreated: 2020-06-02 07:02:32 +0500\n\nModified: 2020-06-25 22:44:10 +0500\n\n---\n\n**Processes and Process Management**\n\nA process is basically a program in execution. The execution of a process must progress in a sequential fashion. To put it in simple terms, we write our computer programs in a text file, and when we execute this program, it becomes a process which performs all the tasks mentioned in the program.\n\nWhen a program is loaded into the memory and it becomes a process, it can be divided into four sections â”€ stack, heap, text, and data.\n\n![1 ](media/Concepts-image1.jpg)\n-   Stack:The process Stack contains the temporary data, such as method/function parameters, return address, and local variables.\n-   Heap:This is dynamically allocated memory to a process during its run time.\n-   Text:This includes the current activity represented by the value of Program Counter and the contents of the processor's registers.\n-   Data:This section contains the global and static variables.\nWhen a process executes, it passes through different states. These stages may differ in different operating systems, and the names of these states are also not standardized. In general, a process can have one of the following five states at a time:\n\n![Start Ready Running Wait Terminated ](media/Concepts-image2.jpg)\n-   Start:The initial state when a process is first started/created.\n-   Ready:The process is waiting to be assigned to a processor. Ready processes are waiting to have the processor allocated to them by the operating system so that they can run. A process may come into this state after theStartstate, or while running it by but getting interrupted by the scheduler to assign CPU to some other process.\n-   Running:Once the process has been assigned to a processor by the OS scheduler, the process state is set to running and the processor executes its instructions.\n-   Waiting:the process moves into the waiting state if it needs to wait for a resource, such as waiting for user input, or waiting for a file to become available.\n-   Terminated or Exit:Once the process finishes its execution, or it is terminated by the operating system, it is moved to the terminated state where it waits to be removed from main memory.\nA Process Control Block is a data structure maintained by the Operating System for every process. The PCB is identified by an integer process ID (PID). A PCB keeps all the information needed to keep track of a process as listed below:\n\n![Process ID State Pointer Priority Program counter CPU registers I/O information Accounting information etc.... ](media/Concepts-image3.jpg)\n-   Process State:The current state of the process --- whether it is ready, running, waiting, or whatever.\n-   Process Privileges:This is required to allow/disallow access to system resources.\n-   Process ID:Unique identification for each of the processes in the operating system.\n-   Pointer:A pointer to the parent process.\n-   Program Counter:Program Counter is a pointer to the address of the next instruction to be executed for this process.\n-   CPU Registers:Various CPU registers where processes need to be stored for execution for running state.\n-   CPU Scheduling Information:Process priority and other scheduling information which is required to schedule the process.\n-   Memory Management Information:This includes the information of page table, memory limits, and segment table, depending on the memory used by the operating system.\n-   Accounting Information:This includes the amount of CPU used for process execution, time limits, execution ID, and so on.\n-   IO Status Information:This includes a list of I/O devices allocated to the process.\n**Inter Process Communication (IPC)**\n\nThere are two types of processes: **Independent and Cooperating**. An independent process is not affected by the execution of other processes, while a cooperating process can be affected by other executing processes.\nYou might think that those processes, which are running independently, would execute very efficiently. But in reality, there are many situations when a process' cooperative nature can be utilized for increasing computational speed, convenience, and modularity.Inter-process communication (IPC)is a mechanism which allows processes to communicate with each other and synchronize their actions. The communication between these processes can be seen as a method of cooperation between them.\n\nProcesses can communicate with each other in two ways: Shared Memory and Message Parsing.\n**Shared Memory Method**\n\nLet's say there are two processes: the Producer and the Consumer. The producer produces some item and the Consumer consumes that item. The two processes shares a common space or memory location known as the \"buffer,\" where the item produced by the Producer is stored and from where the Consumer consumes the item if needed.\nThere are two versions of this problem: the first one is known as the unbounded buffer problem, in which the Producer can keep on producing items and there is no limit on the size of the buffer. The second one is known as the bounded buffer problem, in which the Producer can produce up to a certain number of items, and after that it starts waiting for the Consumer to consume them.\n\n![stack heap data shared memory (mapped) text Process PI shared memory Shared Memory stack heap data shared memory (mapped) text Process P2 ](media/Concepts-image4.png)\n\nIn the bounded buffer problem, the Producer and the Consumer will share some common memory. Then the Producer will start producing items. If the total number of produced items is equal to the size of buffer, the Producer will wait until they're consumed by the Consumer.\nSimilarly, the Consumer first checks for the availability of the item, and if no item is available, the Consumer will wait for the Producer to produce it. If there are items available, the Consumer will consume them.\n**Message Parsing Method**\n\nIn this method, processes communicate with each other without using any kind of of shared memory. If two processes p1 and p2 want to communicate with each other, they proceed as follows:\n-   Establish a communication link (if a link already exists, no need to establish it again.)\n-   Start exchanging messages using basic primitives. We need at least two primitives: send(message, destination) orsend(message) andreceive(message, host) or receive(message)\n\n![Big Picture Ooit sleep and wakeup oute*es semaphores shared memory message passing 0 communication? ](media/Concepts-image5.png)\n\nThe message size can be fixed or variable. If it is a fixed size, it is easy for the OS designer but complicated for the programmer. If it is a variable size, then it is easy for the programmer but complicated for the OS designer. A standard message has two parts: aheaderand abody.\nTheheaderis used for storing the Message type, destination id, source id, message length, and control information. The control information contains information like what to do if it runs out of buffer space, the sequence number, and its priority. Generally, the message is sent using the FIFO style.\n**Input/Output Management**\n\nOne of the important jobs of an Operating System is to manage various input/output (I/O) devices, including the mouse, keyboards, touch pad, disk drives, display adapters, USB devices, Bit-mapped screen, LED, Analog-to-digital converter, On/off switch, network connections, audio I/O, printers, and so on.\nAnI/O systemis required to take an application I/O request and send it to the physical device, then take whatever response comes back from the device and send it to the application. I/O devices can be divided into two categories:\n-   **Block devices:**A block device is one with which the driver communicates by sending entire blocks of data. For example, hard disks, USB cameras, Disk-On-Key, and so on.\n-   **Character Devices:**A character device is one with which the driver communicates by sending and receiving single characters (bytes, octets). For example, serial ports, parallel ports, sounds cards, and so on.\n\n![Memory Memory CPU Controller Monitor Video Controller keyboard Keyboard Controller USB Drive USB Controller Disk Drive Disk Controller ](media/Concepts-image6.jpg)\nThe CPU must have a way to pass information to and from an I/O device. There are three approaches available to communicate with the CPU and Device.\n1.  **Special Instruction I/O**\n\nThis uses CPU instructions that are specifically made for controlling I/O devices. These instructions typically allow data to be sent to an I/O device or be read from an I/O device.\n2.  **Memory-mapped I/O**\n\nWhen using memory-mapped I/O, the same address space is shared by memory and I/O devices. The device is connected directly to certain main memory locations so that the I/O device can transfer block of data to/from the memory without going through the CPU.\n\n![CPU Data I/O Commands 1/0 Device Data Memory ](media/Concepts-image7.jpg)\n\nWhile using memory mapped I/O, the OS allocates buffer in the memory and informs the I/O device to use that buffer to send data to the CPU. The I/O device operates asynchronously with the CPU, and interrupts the CPU when finished.\nThe advantage to this method is that every instruction which can access memory can be used to manipulate an I/O device. Memory-mapped I/O is used for most high-speed I/O devices like disks and communication interfaces.\n3.  **Direct memory access (DMA)**\n\nSlow devices like keyboards will generate an interruption to the main CPU after each byte is transferred. If a fast device, such as a disk, generated an interruption for each byte, the operating system would spend most of its time handling these interruptions. So a typical computer uses direct memory access (DMA) hardware to reduce this overhead.\nDirect Memory Access (DMA) means the CPU grants the I/O module authority to read from or write to memory without involvement. The DMA module itself controls the exchange of data between the main memory and the I/O device. The CPU is only involved at the beginning and end of the transfer and interrupted only after the entire block has been transferred.\nDirect Memory Access needs special hardware called a DMA controller (DMAC) that manages the data transfers and arbitrates access to the system bus. The controllers are programmed with source and destination pointers (where to read/write the data), counters to track the number of transferred bytes, and various settings. These include the I/O and memory types and the interruptions and states for the CPU cycles.\n**Virtualization**\n\nVirtualization is technology that allows you to create multiple simulated environments or dedicated resources from a single, physical hardware system.\nSoftware called a**hypervisor**connects directly to that hardware and allows you to split one system into separate, distinct, and secure environments known as**virtual machines(VMs).** These VMs rely on the hypervisor's ability to separate the machine's resources from the hardware and distribute them appropriately.\nThe original, physical machine equipped with the hypervisor is calledthe **host**, while the many VMs that use its resources are called**guests**. These guests treat computing resources --- like CPU, memory, and storage --- as a hangar of resources that can easily be relocated. Operators can control virtual instances of CPU, memory, storage, and other resources so that guests receive the resources they need when they need them.\nIdeally, all related VMs are managed through a single web-based virtualization management console, which speeds things up. Virtualization lets you dictate how much processing power, storage, and memory to give to VMs, and environments are better protected since VMs are separated from their supporting hardware and each other.\nSimply put, virtualization creates the environments and resources you need from underused hardware.\n**Types of Virtualization:**\n\n1.  **Data Virtualization:**Data that's spread all over can be consolidated into a single source. Data virtualization allows companies to treat data as a dynamic supply --- providing processing capabilities that can bring together data from multiple sources, easily accommodate new data sources, and transform data according to user needs. Data virtualization tools sit in front of multiple data sources and allow them to be treated as single source. They deliver the needed data --- in the required form --- at the right time to any application or user.\n2.  **DesktopVirtualization:**Easily confused with operating system virtualization --- which allows you to deploy multiple operating systems on a single machine --- desktop virtualization allows a central administrator (or automated administration tool) to deploy simulated desktop environments to hundreds of physical machines at once. Unlike traditional desktop environments that are physically installed, configured, and updated on each machine, desktop virtualization allows admins to perform mass configurations, updates, and security checks on all virtual desktops.\n3.  **Server Virtualization:**Servers are computers designed to process a high volume of specific tasks really well so other computers --- like laptops and desktops --- can do a variety of other tasks. Virtualizing a server lets it to do more of those specific functions and involves partitioning it so that the components can be used to serve multiple functions.\n4.  **Operating System Virtualization:**Operating system virtualization happens at the kernel --- the central task managers of operating systems. It's a useful way to run Linux and Windows environments side-by-side. Enterprises can also push virtual operating systems to computers, which: (1) Reduces bulk hardware costs, since the computers don't require such high out-of-the-box capabilities, (2) Increases security, since all virtual instances can be monitored and isolated, and (3) Limits time spent on IT services like software updates.\n5.  **Network Functions Virtualization:**Network functions virtualization (NFV) separates a network's key functions (like directory services, file sharing, and IP configuration) so they can be distributed among environments. Once software functions are independent of the physical machines they once lived on, specific functions can be packaged together into a new network and assigned to an environment. Virtualizing networks reduces the number of physical components --- like switches, routers, servers, cables, and hubs --- that are needed to create multiple, independent networks, and it's particularly popular in the telecommunications industry.\n<https://www.redhat.com/en/topics/virtualization/what-is-nfv>\n\n## Distributed File Systems**\n\nA distributed file system is a client/server-based application that allows clients to access and process data stored on the server as if it were on their own computer. When a user accesses a file on the server, the server sends the user a copy of the file, which is cached on the user's computer while the data is being processed and is then returned to the server.\nIdeally, a distributed file system organizes file and directory services of individual servers into a global directory in such a way that remote data access is not location-specific but is identical from any client. All files are accessible to all users of the global file system and organization is hierarchical and directory-based.\n![Node 1 DataNode Process Local File System Node 2 DataNode Process Local File System Master Node NameNode Process File System Metadata Node 3 DataNode Process Local File System Node N DataNode Process Local File Sustem HDFS Data Blocks HDFS ](media/Concepts-image8.png)\n\nSince more than one client may access the same data simultaneously, the server must have a mechanism in place (such as maintaining information about the times of access) to organize updates so that the client always receives the most current version of data and that data conflicts do not arise. Distributed file systems typically use file or database replication (distributing copies of data on multiple servers) to protect against data access failures.\nSun Microsystems' Network File System ([NFS](http://searchenterprisedesktop.techtarget.com/definition/Network-File-System)), Novell[NetWare](http://searchnetworking.techtarget.com/definition/NetWare), Microsoft's Distributed File System, and IBM's DFS are some examples of distributed file systems.\n**Distributed Shared Memory**\n\nDistributed Shared Memory (DSM) is a resource management component of a distributed operating system that implements the shared memory model in distributed systems, which have no physically shared memory. The shared memory provides a virtual address space that is shared among all computers in a distributed system.\nIn DSM, data is accessed from a shared space similar to the way that virtual memory is accessed. Data moves between secondary and main memory, as well as, between the distributed main memories of different nodes. Ownership of pages in memory starts out in some pre-defined state but changes during the course of normal operation. Ownership changes take place when data moves from one node to another due to an access by a particular process.\n\n![Distributed shared memory: goal Distributed in- memory state 2 read/write Ranks Graph IStrl Ute Storage 3 ](media/Concepts-image9.jpg)\n\n**Advantages of Distributed Shared Memory:**\n-   Hide data movement and provide a simpler abstraction for sharing data. Programmers don't need to worry about memory transfers between machines like when using the message passing model.\n-   Allows the passing of complex structures by reference, simplifying algorithm development for distributed applications.\n-   Takes advantage of \"locality of reference\" by moving the entire page containing the data referenced rather than just the piece of data.\n-   Cheaper to build than multiprocessor systems. Ideas can be implemented using normal hardware and do not require anything complex to connect the shared memory to the processors.\n-   Larger memory sizes are available to programs, by combining all physical memory of all nodes. This large memory will not incur disk latency due to swapping like in traditional distributed systems.\n-   Unlimited number of nodes can be used. Unlike multiprocessor systems where main memory is accessed via a common bus, thus limiting the size of the multiprocessor system.\n-   Programs written for shared memory multiprocessors can be run on DSM systems.\nThere are two different ways that nodes can be informed of who owns what page: invalidation and broadcast. Invalidation is a method that invalidates a page when some process asks for write access to that page and becomes its new owner. This way the next time some other process tries to read or write to a copy of the page it thought it had, the page will not be available and the process will have to re-request access to that page. Broadcasting will automatically update all copies of a memory page when a process writes to it. This is also called write-update. This method is a lot less efficient more difficult to implement because a new value has to sent instead of an invalidation message.\n<https://medium.com/cracking-the-data-science-interview/how-operating-systems-work-10-concepts-you-should-know-as-a-developer-8d63bb38331f>\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Concurrency---Threading/","title":"Concurrency / Threading"},"frontmatter":{"draft":false},"rawBody":"# Concurrency / Threading\n\nCreated: 2018-10-12 21:08:49 +0500\n\nModified: 2020-07-01 13:41:18 +0500\n\n---\n\n**Concurrency**\n-   Composition of independently executing functions/processes\n-   Parallelism is the simultaneous execution of multiple things, possible related and possibly not.\n-   Concurrency is about dealing with a lot of things at once while parallelism is about doing a lot of things at once\n-   Concurrent - Mouse, keyboard, display and disk drivers\n-   Parallel - Vector dot product\n-   Adding some design can increase the speed of program execution (Analogy is of grofers working together to load books into the incinerator)\n**Process**\n\nIn general, most processes can be described as either**[I/O-bound](https://en.wikipedia.org/wiki/I/O-bound)or[CPU-bound](https://en.wikipedia.org/wiki/CPU-bound)**. An I/O-bound process is one that spends more of its time doing I/O than it spends doing computations. A CPU-bound process, in contrast, generates I/O requests infrequently, using more of its time doing computations. It is important that a long-term scheduler selects a good process mix of I/O-bound and CPU-bound processes. If all processes are I/O-bound, the ready queue will almost always be empty, and the short-term scheduler will have little to do. On the other hand, if all processes are CPU-bound, the I/O waiting queue will almost always be empty, devices will go unused, and again the system will be unbalanced. The system with the best performance will thus have a combination of CPU-bound and I/O-bound processes. In modern operating systems, this is used to make sure that real-time processes get enough CPU time to finish their tasks.\n**Threads**\n\nA thread is just a sequence of instructions that can be executed independently by a processor. Threads are lighter than the process and so you can spawn a lot of them.\nA real life application would be a web server.\nA webserver typically is designed to handle multiple requests at the same time. And these requests normally don't depend on each other.\nSo a Thread can be created (or taken from a Thread pool) and requests can be delegated, to achieve concurrency.\nModern processors can executed multiple threads at once (multi-threading) and also switch between threads to achieve parallelism.A thread is a flow of execution through the process code. It has its own program counter that keeps track of which instruction to execute next. It also has system registers which hold its current working variables, and a stack which contains the execution history.\nA thread shares with its peer threads various information like code segment, data segment, and open files. When one thread alters a code segment memory item, all other threads see that.\nA thread is also called a**lightweight process**. Threads provide a way to improve application performance through parallelism. Threads represent a software approach to improving the performance of operating systems by reducing the overhead. A thread is equivalent to a classical process.\nEach thread belongs to exactly one process, and no thread can exist outside a process. Each thread represents a separate flow of control. Threads have been successfully used in implementing network servers and web servers. They also provide a suitable foundation for parallel execution of applications on shared memory multiprocessors.\n![](media/Concurrency---Threading-image1.jpg)\n**Advantages of threads:**\n-   They minimize the context switching time.\n-   Using them provides concurrency within a process.\n-   They provide efficient communication.\n-   It is more economical to create and context switch threads.\n-   Threads allow utilization of multiprocessor architectures to a greater scale and efficiency.\n**Threads are implemented in the following two ways:**\n-   User Level Threads:User-managed threads.\n-   Kernel Level Threads:Operating System-managed threads acting on a kernel, an operating system core.\n**User Level Threads**\n\nIn this case, the thread management kernel is not aware of the existence of threads. The thread library contains code for creating and destroying threads, for passing messages and data between threads, for scheduling thread execution, and for saving and restoring thread contexts. The application starts with a single thread.\n\n![](media/Concurrency---Threading-image2.png)\n**Advantages:**\n-   Thread switching does not require Kernel mode privileges.\n-   User level thread can run on any operating system.\n-   Scheduling can be application-specific in the user level thread.\n-   User level threads are fast to create and manage.\n**Disadvantages:**\n-   In a typical operating system, most system calls are blocking.\n-   Multithreaded application cannot take advantage of multiprocessing.\n**Kernel Level Threads**\n\nIn this case, thread management is done by the Kernel. There is no thread management code in the application area. Kernel threads are supported directly by the operating system. Any application can be programmed to be multithreaded. All of the threads within an application are supported within a single process.\nThe Kernel maintains context information for the process as a whole and for individuals threads within the process. Scheduling by the Kernel is done on a thread basis. The Kernel performs thread creation, scheduling, and management in Kernel space. Kernel threads are generally slower to create and manage than the user threads.\n\n![](media/Concurrency---Threading-image3.png)\n**Advantages**\n-   The Kernel can simultaneously schedule multiple threads from the same process on multiple processes.\n-   If one thread in a process is blocked, the Kernel can schedule another thread of the same process.\n-   Kernel routines themselves can be multithreaded.\n**Disadvantages**\n-   Kernel threads are generally slower to create and manage than the user threads.\n-   Transfer of control from one thread to another within the same process requires a mode switch to the Kernel.\n**Threads vs Processes**\n-   Threads **uses shared state**, so there is an ease of communication between multiple threads, but the disadvantage of shared state is that there can be race condition, multiple threads can race with each other to get access to resources.\n-   Processes **doesn't have shared state**, they are fully independent from each other. The weakness of processes is lack of communication (hence the need for IPC and object pickling and other overhead)-   Threads are more lightweight and have lower overhead compared to processes. Spawning processes is a bit slower than spawning threads.\n| **Bottleneck** | **Example**                        | **Optimize with** |\n|----------------|------------------------------------|-------------------|\n| IO             | Network connection, file operation | Multithreading    |\n| CPU            | Complex math problem, search       | Multiprocessing   |\n<https://zacs.site/blog/linear-python.html>\n\n## Are Threads Lighter than Processes**\n\nYes and No.\nIn concept,\n\n1.  Threads share memory and don't need to create a new virtual memory space when they are created and thus don't require a MMU (memory management unit) context switch\n\n2.  Communication between threads is simpler as they have a shared memory while processes requires various modes of IPC (Inter-Process Communications) like semaphores, message queues, pipes etc.\nThat being said, this doesn't always guarantee a better performance than processes in this multi-core processor world.\ne.g. Linux doesn't distinguish between threads and processes and both are called tasks. Each task can have a minimum to maximum level of sharing when cloned.\nWhen you callfork(), a new task is created with no shared file descriptors, PIDs and memory space. When you callpthread_create(), a new task is created with all of the above shared.\nAlso, synchronising data in[shared memory](https://users.cs.cf.ac.uk/Dave.Marshall/C/node27.html)as well as in[L1 cache](https://www.quora.com/What-is-the-L1-L2-and-L3-cache-of-a-microprocessor-and-how-does-it-affect-the-performance-of-it-For-example-I-have-a-laptop-with-an-Intel-4700MQ-microprocessor-with-a-6MB-L3-cache-What-does-this-value-indicate)of tasks running on multiple cores takes a bigger toll than running different processes on isolated memory.\nLinux developers have tried to minimise the cost between task switch and have succeeded at it. Creating a new task is still a bigger overhead than a new thread but switching is not.\n**What can be improved in Threads?**\n\nThere are three things which make threads slow:\n\n1.  Threads consume a lot of memory due to their large stack size (â‰¥ 1MB). So creating 1000s of thread means you already need 1GB of memory.\n\n2.  Threads need to restore a lot of registers some of which include AVX( Advanced vector extension), SSE (Streaming SIMD Ext.), Floating Point registers, Program Counter (PC), Stack Pointer (SP) which hurts the application performance.\n\n3.  Threads setup and teardown requires call to OS for resources (such as memory) which is slow.\n**Threads vs Async**\n\n**Threads**\n\nThreads switch preemptively. This is convenient because you don't need to add explicit code to cause a task switch.\n\nThe cost of this convenience is that you have to assume a switch can happen at any time. Accordingly, critical sections have to be guarded with locks. Dinning Philosophers Problem.\n\nThe limit on threads is total CPU power minus the cost of task switches and synchronization overhead.\n**Async**\n\nAsync switches cooperatively, so you do need to add explicit code \"yield\" or \"await\" to cause a task switch.\n\nNow you control when task switches occur, so locks and other synchronization are no longer needed.\n\nAlso, the cost task switches is very low. Calling a pure Python function has more overhead than restarting a generator or awaitable.\n\nThis means that async is very cheap.\n\nIn return, you'll need a non-blocking version of just about everything you do. Accordingly, the async world has a huge ecosystem of support tools. This increases the learning curve.\n**Comparison**\n-   Async maximizes CPU utilization because it has less overhead than threads.\n-   Threading typically works with existing code and tools as long as locks are added around critical sections.\n-   For complex systems, async ismucheasier to get right than threads with locks.\n-   Threads require very little tooling (locks and queues).\n-   Async needs a great deal of tooling (futures, event loops, and non-blocking versions of just about everything).\n-   In a threaded system the decision to suspend one thread and execute another is largely outside of the programmer's control. Rather, it is under the control of the operating system, and the programmer must assume that a thread may be suspended and replaced with another at almost any time. In contrast, under the asynchronous model a task will continue to run until it explicitly relinquishes control to other tasks.-   The problem with locks is that it just a flag, and it should be checked to access the resources. If you don't check then there would be problems.\n**Considerations**\n-   Threading\n-   Multiprocessing\n-   Async\n**Amdahl's Law**\n\n**Amdahl's law is often used in parallel computing to predict the theoretical speedup when using multiple processors.** For example, if a program needs 20 hours using a single processor core, and a particular part of the program which takes one hour to execute cannot be parallelized, while the remaining 19 hours (p = 0.95) of execution time can be parallelized, then regardless of how many processors are devoted to a parallelized execution of this program, the minimum execution time cannot be less than that critical one hour. Hence, the theoretical speedup is limited to at most 20 times(1/(1âˆ’p)=20). For this reason parallel computing is relevant only for a low number of processors and very parallelizable programs.\n**Process Control Block**\n\n1.  Process State\n\n2.  Process Number\n\n3.  Program Counter\n\n4.  Registers\n\n5.  Memory Limits\n\n6.  List of open Files\n\n7.  Signal Mask\n\n8.  CPU Scheduling info\n**Synchronization**\n\nSynchronizationrefers to one of two distinct but related concepts\n\n1.  **Process Synchronization**\n\nProcess synchronizationrefers to the idea that multiple processes are to join up or[handshake](https://en.wikipedia.org/wiki/Handshaking)at a certain point, in order to reach an agreement or commit to a certain sequence of action.\n\n2.  **Data Synchronization**\n\n[Data synchronization](https://en.wikipedia.org/wiki/Data_synchronization)refers to the idea of keeping multiple copies of a dataset in coherence with one another, or to maintain[data integrity](https://en.wikipedia.org/wiki/Data_integrity)\nProcess synchronization primitives are commonly used to implement data synchronization.\n**Need for synchronization**\n\nThe need for synchronization does not arise merely in multi-processor systems but for any kind of concurrent processes; even in single processor systems. Mentioned below are some of the main needs for synchronization:\n-   [Forks and Joins](https://en.wikipedia.org/wiki/Fork-join_model):When a job arrives at a fork point, it is split into N sub-jobs which are then serviced by n tasks. After being serviced, each sub-job waits until all other sub-jobs are done processing. Then, they are joined again and leave the system. Thus, in parallel programming, we require synchronization as all the parallel processes wait for several other processes to occur.\n-   [Producer-Consumer:](https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem)In a producer-consumer relationship, the consumer process is dependent on the producer process till the necessary data has been produced.\n-   Exclusive use resources:When multiple processes are dependent on a resource and they need to access it at the same time the operating system needs to ensure that only one processor accesses it at a given point in time.This reduces concurrency.\n**Implementation of Synchronization**\n-   **Spinlock**\n\nAnother effective way of implementing synchronization is by using spinlocks. Before accessing any shared resource or piece of code, every processor checks a flag. If the flag is reset, then the processor sets the flag and continues executing the thread. But, if the flag is set (locked), the threads would keep spinning in a loop and keep checking if the flag is set or not. But, spinlocks are effective only if the flag is reset for lower cycles otherwise it can lead to performance issues as it wastes many processor cycles waiting.-   **Barriers**\n\nBarriers are simple to implement and provide good responsiveness. They are based on the concept of implementing wait cycles to provide synchronization. Consider three threads running simultaneously, starting from barrier 1. After time t, thread1 reaches barrier 2 but it still has to wait for thread 2 and 3 to reach barrier2 as it does not have the correct data. Once all the threads reach barrier 2 they all start again. After time t, thread 1 reaches barrier3 but it will have to wait for threads 2 and 3 and the correct data again.\nThus, in barrier synchronization of multiple threads there will always be a few threads that will end up waiting for other threads as in the above example thread 1 keeps waiting for thread 2 and 3. This results in severe degradation of the process performance.\nIn[parallel computing](https://en.wikipedia.org/wiki/Parallel_computing), abarrieris a type of[synchronization](https://en.wikipedia.org/wiki/Synchronization_(computer_science))method. A barrier for a group of threads or processes in the source code means any thread/process must stop at this point and cannot proceed until all other threads/processes reach this barrier.\nMany collective routines and directive-based parallel languages impose implicit barriers. For example, a paralleldoloop in[Fortran](https://en.wikipedia.org/wiki/Fortran)with[OpenMP](https://en.wikipedia.org/wiki/OpenMP)will not be allowed to continue on any thread until the last iteration is completed. This is in case the program relies on the result of the loop immediately after its completion. In[message passing](https://en.wikipedia.org/wiki/Message_passing), any global communication (such as reduction or scatter) may imply a barrier.\n<https://en.wikipedia.org/wiki/Barrier_(computer_science)>-   **Semaphores**\n\nSemaphores are signalling mechanisms which can allow one or more threads/processors to access a section. A Semaphore has a flag which has a certain fixed value associated with it and each time a thread wishes to access the section, it decrements the flag. Similarly, when the thread leaves the section, the flag is incremented. If the flag is zero, the thread cannot access the section and gets blocked if it chooses to wait.\n\nSome semaphores would allow only one thread or process in the code section. Such Semaphores are called binary semaphore and are very similar to Mutex. Here, if the value of semaphore is 1, the thread is allowed to access and if the value is 0, the access is denied.\n<https://en.wikipedia.org/wiki/Synchronization_(computer_science)>\n\n## Hyper - Threading Technology (HTT)**\n\nHyper-threading(officially calledHyper-Threading TechnologyorHT Technologyand abbreviated asHTTorHT) is[Intel's](https://en.wikipedia.org/wiki/Intel)[proprietary](https://en.wikipedia.org/wiki/Proprietary_hardware)[simultaneous multithreading](https://en.wikipedia.org/wiki/Simultaneous_multithreading)(SMT) implementation used to improve[parallelization](https://en.wikipedia.org/wiki/Parallel_computation)of computations (doing multiple tasks at once) performed on[x86](https://en.wikipedia.org/wiki/X86)microprocessors. It first appeared in February 2002 on[Xeon](https://en.wikipedia.org/wiki/Xeon)server[processors](https://en.wikipedia.org/wiki/Central_processing_unit)and in November 2002 on[Pentium4](https://en.wikipedia.org/wiki/Pentium_4)desktop CPUs.Later, Intel included this technology in[Itanium](https://en.wikipedia.org/wiki/Itanium),[Atom](https://en.wikipedia.org/wiki/Intel_Atom), and[Core 'i' Series](https://en.wikipedia.org/wiki/Intel_Core)CPUs, among others.\nFor each[processor core](https://en.wikipedia.org/wiki/Processor_core)that is physically present, the[operating system](https://en.wikipedia.org/wiki/Operating_system)addresses two virtual (logical) cores and shares the workload between them when possible. The main function of hyper-threading is to increase the number of independent instructions in the pipeline; it takes advantage of[superscalar](https://en.wikipedia.org/wiki/Superscalar_processor)architecture, in which multiple instructions operate on separate data[in parallel](https://en.wikipedia.org/wiki/Parallel_computing). With HTT, one physical core appears as two processors to the operating system, allowing[concurrent](https://en.wikipedia.org/wiki/Concurrent_computing)scheduling of two processes per core. In addition, two or more processes can use the same resources: If resources for one process are not available, then another process can continue if its resources are available.\nIn addition to requiring simultaneous multithreading (SMT) support in the operating system, hyper-threading can be properly utilized only with an operating system specifically optimized for it.Furthermore, Intel recommends HTT to be disabled when using operating systems unaware of this hardware feature.\n<https://en.wikipedia.org/wiki/Hyper-threading>\nMultithreading, concurrency, locks, synchronization\n**Compare and Swap (CAS)**\n\nCompare and Swap is an atomic structure used in multithreading to achieve synchronization. It compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value. This is done as a single atomic operation. The atomicity guarantees that the new value is calculated based on up-to-date information; if the value had been updated by another thread in the meantime, the write would fail. The result of the operation must indicate whether it performed the substitution; this can be done either with a simple[boolean](https://en.wikipedia.org/wiki/Boolean_logic)response (this variant is often called**compare-and-set**), or by returning the value read from the memory location (*not*the value written to it).\nAtomic instruction that compares contents of a memory location M to a given value V\n-   If values are equal, installs new given value V' in M\n-   Otherwise operation fails\n__sync_bool_compare_and_swap(&M, 20, 30)\n\n__sync_bool_compare_and_swap(Address, Compare Value, New Value)\n<https://en.wikipedia.org/wiki/Compare-and-swap>\n\n## See also -**\n\nPython > Advanced > Concurrency\n**References**\n\n[https://schneems.com/2017/10/23/wtf-is-a-thread/#](https://schneems.com/2017/10/23/wtf-is-a-thread/)\nDijkstra's Guarded Commands - <https://en.wikipedia.org/wiki/Guarded_Command_Language>\nCommunicating sequential processes **- C.A.R. Hoare**\n[Rob Pike - 'Concurrency Is Not Parallelism'](https://vimeo.com/49718712)\n![](media/Concurrency---Threading-image4.jpg)\n\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Concurrency-Models---Async/","title":"Concurrency Models - Async"},"frontmatter":{"draft":false},"rawBody":"# Concurrency Models - Async\n\nCreated: 2018-10-13 10:42:25 +0500\n\nModified: 2018-12-19 00:10:10 +0500\n\n---\n\n**Single-threaded synchronous model**\n\n![Figure 1: the synchronous model](media/Concurrency-Models---Async-image1.png)\nEach task is performed one at a time, with one finishing completely before another is started. And if the tasks are always performed in a definite order, the implementation of a later task can assume that all earlier tasks have finished without errors, with all their output available for use.\n**Threaded model**\n\n![Figure 2: the threaded model](media/Concurrency-Models---Async-image2.png)\n\nIn this model, each task is performed in a separate thread of control. The threads are managed by the operating system and may, on a system with multiple processors or multiple cores, run truly concurrently, or may be interleaved together on a single processor. The point is, in the threaded model the details of execution are handled by the OS and the programmer simply thinks in terms of independent instruction streams which may run simultaneously.\n**Asynchronous model**\n\n![Figure 3: the asynchronous model](media/Concurrency-Models---Async-image3.png)\n\nThe tasks are interleaved with one another, but in a single thread of control.\n**Async benefits**\n\nThere is a condition under which an asynchronous system will simply outperform a synchronous one.\n\n![Figure 4: blocking in a synchronous program](media/Concurrency-Models---Async-image4.png)\n\n*fig - blocking in an asynchronous program*\nThe fundamental idea behind the asynchronous model is that an asynchronous program, when faced with a task that would normally block in a synchronous program, will instead execute some other task that can still make progress.\nCompared to the synchronous model, the asynchronous model performs best when:\n\n1.  There are a large number of tasks so there is likely always at least one task that can make progress.\n\n2.  The tasks perform lots of I/O, causing a synchronous program to waste lots of time blocking when other tasks could be running.\n\n3.  The tasks are largely independent from one another so there is little need for inter-task communication (and thus for one task to wait upon another).\n**References**\n\n<http://krondo.com/in-which-we-begin-at-the-beginning>\n\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Concurrency-Problems/","title":"Concurrency Problems"},"frontmatter":{"draft":false},"rawBody":"# Concurrency Problems\n\nCreated: 2018-05-24 00:37:05 +0500\n\nModified: 2020-06-01 16:41:09 +0500\n\n---\n\n# Problems\n-   Deadlock\n\n[deadlock](https://en.wikipedia.org/wiki/Deadlock), which occurs when many processes are waiting for a shared resource (critical section) which is being held by some other process. In this case, the processes just keep waiting and execute no further\n-   Distributed Deadlock\n-   Resource Starvation\n\n[starvation](https://en.wikipedia.org/wiki/Resource_starvation), which occurs when a process is waiting to enter the critical section, but other processes monopolize the critical section, and the first process is forced to wait indefinitely;\n-   Priority Inversion\n\n[priority inversion](https://en.wikipedia.org/wiki/Priority_inversion), which occurs when a high-priority process is in the critical section, and it is interrupted by a medium-priority process. This violation of priority rules can happen under certain circumstances and may lead to serious consequences in real-time systems\n\nHappened in Mars Pathfinder in 1997\n-   Busy Waiting\n\n[busy waiting](https://en.wikipedia.org/wiki/Busy_waiting), which occurs when a process frequently polls to determine if it has access to a critical section. This frequent polling robs processing time from other processes.\n-   Livelock\n\nAlivelockis similar to a deadlock, except that the states of the processes involved in the livelock constantly change with regard to one another, none progressing. Its a special case of resource starvation\n-   Race Condition\n\nSince threads have a shared memory space, they can have access to shared variables. A race condition occurs when multiple threads try to change the same variable simultaneously. The thread scheduler can arbitrarily swap between threads, so we have no way of knowing the order in which the threads will try to change the data. This can result in incorrect behavior in either of the threads, particularly if the threads decide to do something based on the value of the variable. To prevent this from happening, a mutual exclusion (or mutex)lockcan be placed around the piece of the code that modifies the variable so that only one thread can write to the variable at a time.\n# Producer Consumer Problem\n\nAlso known as Bounded-buffer problem is a classic example of a multi-process synchronizationproblem.\nThe problem describes two processes, the producer and the consumer, who share a common, fixed-size[buffer](https://en.wikipedia.org/wiki/Buffer_(computer_science))used as a[queue](https://en.wikipedia.org/wiki/Queue_(data_structure)).\nThe producer's job is to generate data, put it into the buffer, and start again. At the same time, the consumer is consuming the data (i.e., removing it from the buffer), one piece at a time.\nThe problem is to make sure that the producer won't try to add data into the buffer if it's full and that the consumer won't try to remove data from an empty buffer.\nThe solution for the producer is to either go to sleep or discard data if the buffer is full. The next time the consumer removes an item from the buffer, it notifies the producer, who starts to fill the buffer again. In the same way, the consumer can go to sleep if it finds the buffer empty. The next time the producer puts data into the buffer, it wakes up the sleeping consumer. The solution can be reached by means of[inter-process communication](https://en.wikipedia.org/wiki/Inter-process_communication), typically using[semaphores](https://en.wikipedia.org/wiki/Semaphore_(programming)). An inadequate solution could result in a[deadlock](https://en.wikipedia.org/wiki/Deadlock)where both processes are waiting to be awakened. The problem can also be generalized to have multiple producers and consumers.\n# Reader-Writer Problem\n\nIn[computer science](https://en.wikipedia.org/wiki/Computer_science), the**readers-writers problems**are examples of a common computing problem in[concurrency](https://en.wikipedia.org/wiki/Concurrency_(computer_science)). There are at least three variations of the problems, which deal with situations in which many[threads](https://en.wikipedia.org/wiki/Thread_(computer_science))try to access the same shared resource at one time. Some threads may read and some may write, with the constraint that no process may access the shared resource for either reading or writing while another process is in the act of writing to it. (In particular, it*is*allowed for two or more readers to access the share at the same time.) A[readers-writer lock](https://en.wikipedia.org/wiki/Readers-writer_lock)is a[data structure](https://en.wikipedia.org/wiki/Data_structure)that solves one or more of the readers-writers problems.\n# Sleeping Barber Problem\n\nThe**sleeping barber problem**is a classic inter-process communication and [synchronization](https://en.wikipedia.org/wiki/Synchronization) problem between multiple [operating system](https://en.wikipedia.org/wiki/Operating_system)[processes](https://en.wikipedia.org/wiki/Process_(computing)).\n# Dining Philosophers Problem\n\nThe**dining philosophers problem**is an example problem often used in[concurrent](https://en.wikipedia.org/wiki/Concurrency_(computer_science))algorithm design to illustrate[synchronization](https://en.wikipedia.org/wiki/Synchronization_(computer_science))issues and techniques for resolving them.\n**Problem Statement**\n\nFive silent[philosophers](https://en.wikipedia.org/wiki/Philosopher)sit at a round table with bowls of[spaghetti](https://en.wikipedia.org/wiki/Spaghetti). Forks are placed between each pair of adjacent philosophers.\n\nEach philosopher must alternately think and eat. However, a philosopher can only eat spaghetti when they have both left and right forks. Each fork can be held by only one philosopher and so a philosopher can use the fork only if it is not being used by another philosopher. After an individual philosopher finishes eating, they need to put down both forks so that the forks become available to others. A philosopher can take the fork on their right or the one on their left as they become available, but cannot start eating before getting both forks.\n\nEating is not limited by the remaining amounts of spaghetti or stomach space; an infinite supply and an infinite demand are assumed.\n\nThe problem is how to design a discipline of behavior (a[concurrent](https://en.wikipedia.org/wiki/Concurrency_(computer_science))[algorithm](https://en.wikipedia.org/wiki/Algorithm)) such that no philosopher will starve; i.e., each can forever continue to alternate between eating and thinking, assuming that no philosopher can know when others may want to eat or think.\n**Solutions**\n-   Resource hierarchy solution\n-   Arbitrator solution (waiter / mutex)\n-   Chandy/Misra solution\n**References**\n\n<https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem>\n\n<https://en.wikipedia.org/wiki/Readers%E2%80%93writers_problem>\n\n<https://en.wikipedia.org/wiki/Sleeping_barber_problem>\n\n<https://en.wikipedia.org/wiki/Cigarette_smokers_problem>\n\n<https://en.wikipedia.org/wiki/Dining_philosophers_problem>\n\n<https://www.geeksforgeeks.org/operating-system-dining-philosopher-problem-using-semaphores>\n\n<https://www.geeksforgeeks.org/dining-philosophers-solution-using-monitors>\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Coroutines/","title":"Coroutines"},"frontmatter":{"draft":false},"rawBody":"# Coroutines\n\nCreated: 2019-04-21 20:42:57 +0500\n\nModified: 2019-04-21 23:20:12 +0500\n\n---\n\n[Coroutines](https://www.geeksforgeeks.org/coroutine-in-python/)are general control structures where flow control is cooperatively passed between two different routines without returning.\nCoroutinesare[computer program](https://en.wikipedia.org/wiki/Computer_program)components that generalize[subroutines](https://en.wikipedia.org/wiki/Subroutine)for[non-preemptive multitasking](https://en.wikipedia.org/wiki/Non-preemptive_multitasking), by allowing execution to be suspended and resumed. Coroutines are well-suited for implementing familiar program components such as[cooperative tasks](https://en.wikipedia.org/wiki/Cooperative_multitasking),[exceptions](https://en.wikipedia.org/wiki/Exception_handling),[event loops](https://en.wikipedia.org/wiki/Event_loop),[iterators](https://en.wikipedia.org/wiki/Iterator),[infinite lists](https://en.wikipedia.org/wiki/Lazy_evaluation)and[pipes](https://en.wikipedia.org/wiki/Pipeline_(software)).\n**Comparision with subroutines**\n\nSubroutines are special cases of coroutines.When subroutines are invoked, execution begins at the start, and once a subroutine exits, it is finished; an instance of a subroutine only returns once, and does not hold state between invocations. By contrast, coroutines can exit by calling other coroutines, which may later return to the point where they were invoked in the original coroutine; from the coroutine's point of view, it is not exiting but calling another coroutine.Thus, a coroutine instance holds state, and varies between invocations; there can be multiple instances of a given coroutine at once. The difference between calling another coroutine by means of[\"yielding\"](https://en.wikipedia.org/wiki/Yield_(multithreading))to it and simply calling another routine (which then, also, would return to the original point), is that the relationship between two coroutines which yield to each other is not that of caller-callee, but instead symmetric.\nAny subroutine can be translated to a coroutine which does not callyield.\n**Comparision with threads**\n\nCoroutines are very similar to[threads](https://en.wikipedia.org/wiki/Thread_(computing)). However, coroutines are[cooperatively](https://en.wikipedia.org/wiki/Cooperative_multitasking)multitasked, whereas threads are typically[preemptively](https://en.wikipedia.org/wiki/Preemptive_multitasking)[multitasked](https://en.wikipedia.org/wiki/Multitasking). This means that coroutines provide[concurrency](https://en.wikipedia.org/wiki/Concurrency_(computer_science))but not[parallelism](https://en.wikipedia.org/wiki/Parallel_computing). The advantages of coroutines over threads are that they may be used in a[hard-realtime](https://en.wikipedia.org/wiki/Hard_realtime)context ([switching](https://en.wikipedia.org/wiki/Context_switch)between coroutines need not involve any[system calls](https://en.wikipedia.org/wiki/System_calls)or any[blocking](https://en.wikipedia.org/wiki/Blocking_(computing))calls whatsoever), there is no need for synchronisation primitives such as[mutexes](https://en.wikipedia.org/wiki/Mutex), semaphores, etc. in order to guard[critical sections](https://en.wikipedia.org/wiki/Critical_sections), and there is no need for support from the operating system.\nIt is possible to implement coroutines using preemptively-scheduled threads, in a way that will be transparent to the calling code, but some of the advantages (particularly the suitability for hard-realtime operation and relative cheapness of switching between them) will be lost.\n**Comparision with generators**\n\n[Generators](https://en.wikipedia.org/wiki/Generator_(computer_science)), also known as semicoroutines,are a subset of coroutines. Specifically, while both can yield multiple times, suspending their execution and allowing re-entry at multiple entry points, they differ in coroutines' ability to control where execution continues immediately after they yield, while generators cannot, instead transferring control back to the generator's caller.That is, since generators are primarily used to simplify the writing of[iterators](https://en.wikipedia.org/wiki/Iterator), theyieldstatement in a generator does not specify a coroutine to jump to, but rather passes a value back to a parent routine.\n**Comparision with mutual recursion**\n\nUsing coroutines for state machines or concurrency is similar to using[mutual recursion](https://en.wikipedia.org/wiki/Mutual_recursion)with[tail calls](https://en.wikipedia.org/wiki/Tail_call), as in both cases the control changes to a different one of a set of routines. However, coroutines are more flexible and generally more efficient. Since coroutines yield rather than return, and then resume execution rather than restarting from the beginning, they are able to hold state, both variables (as in a closure) and execution point, and yields are not limited to being in tail position; mutually recursive subroutines must either use shared variables or pass state as parameters. Further, each mutually recursive call of a subroutine requires a new stack frame (unless[tail call elimination](https://en.wikipedia.org/wiki/Tail_call_elimination)is implemented), while passing control between coroutines uses the existing contexts and can be implemented simply by a jump.\n**Implementations**\n\n1.  Goroutine in Golang\n\n2.  lightweight threads / suspending functions in Kotlin\n**References**\n\n<https://en.wikipedia.org/wiki/Coroutine>\n"},{"fields":{"slug":"/Computer-Science/Operating-System/DRAM/","title":"DRAM"},"frontmatter":{"draft":false},"rawBody":"# DRAM\n\nCreated: 2020-02-11 17:32:54 +0500\n\nModified: 2020-02-11 17:40:43 +0500\n\n---\n\nDynamic random-access memory(DRAM) is a type of[random access](https://en.wikipedia.org/wiki/Random-access_memory)[semiconductor memory](https://en.wikipedia.org/wiki/Semiconductor_memory)that stores each[bit](https://en.wikipedia.org/wiki/Bit)of data in a[memory cell](https://en.wikipedia.org/wiki/Memory_cell_(computing))consisting of a tiny[capacitor](https://en.wikipedia.org/wiki/Capacitor)and a[transistor](https://en.wikipedia.org/wiki/Transistor), both typically based on[metal-oxide-semiconductor](https://en.wikipedia.org/wiki/Metal-oxide-semiconductor)(MOS) technology. The capacitor can either be charged or discharged; these two states are taken to represent the two values of a bit, conventionally called 0 and 1. The[electric charge](https://en.wikipedia.org/wiki/Electric_charge)on the capacitors slowly leaks off, so without intervention the data on the chip would soon be lost. To prevent this, DRAM requires an external[memory refresh](https://en.wikipedia.org/wiki/Memory_refresh)circuit which periodically rewrites the data in the capacitors, restoring them to their original charge. This refresh process is the defining characteristic of dynamic random-access memory, in contrast to[static random-access memory](https://en.wikipedia.org/wiki/Static_random-access_memory)(SRAM) which does not require data to be refreshed. Unlike[flash memory](https://en.wikipedia.org/wiki/Flash_memory), DRAM is[volatile memory](https://en.wikipedia.org/wiki/Volatile_memory)(vs.[non-volatile memory](https://en.wikipedia.org/wiki/Non-volatile_memory)), since it loses its data quickly when power is removed. However, DRAM does exhibit limited[data remanence](https://en.wikipedia.org/wiki/Data_remanence).\nDRAM typically takes the form of an[integrated circuit](https://en.wikipedia.org/wiki/Integrated_circuit)chip, which can consist of dozens to billions of DRAM memory cells. DRAM chips are widely used in[digital electronics](https://en.wikipedia.org/wiki/Digital_electronics)where low-cost and high-capacity[computer memory](https://en.wikipedia.org/wiki/Computer_memory)is required. One of the largest applications for DRAM is the[main memory](https://en.wikipedia.org/wiki/Main_memory)(colloquially called the \"RAM\") in modern[computers](https://en.wikipedia.org/wiki/Computer)and[graphics cards](https://en.wikipedia.org/wiki/Graphics_card)(where the \"main memory\" is called thegraphics memory). It is also used in many portable devices and[video game](https://en.wikipedia.org/wiki/Video_game)consoles. In contrast, SRAM, which is faster and more expensive than DRAM, is typically used where speed is of greater concern than cost and size, such as the[cache memories](https://en.wikipedia.org/wiki/CPU_cache)in[processors](https://en.wikipedia.org/wiki/Central_processing_unit).\nDue to its need of a system to perform refreshing, DRAM has more complicated circuitry and timing requirements than SRAM, but it is much more widely used. The advantage of DRAM is the structural simplicity of its[memory cells](https://en.wikipedia.org/wiki/Memory_cell_(computing)): only one[transistor](https://en.wikipedia.org/wiki/Transistor)and a capacitor are required per bit, compared to four or six transistors in SRAM. This allows DRAM to reach very high[densities](https://en.wikipedia.org/wiki/Computer_storage_density), making DRAM much cheaper per bit. The transistors and capacitors used are extremely small; billions can fit on a single memory chip. Due to the dynamic nature of its[memory cells](https://en.wikipedia.org/wiki/Memory_cell_(computing)), DRAM consumes relatively large amounts of power, with different ways for managing the power consumption.\nDRAM had a 47% increase in the price-per-bit in 2017, the largest jump in 30 years since the 45% percent jump in 1988, while in recent years the price has been going down.\n<https://en.wikipedia.org/wiki/Dynamic_random-access_memory>\nIn[dynamic RAM](https://en.wikipedia.org/wiki/Dynamic_RAM)(DRAM), each[bit](https://en.wikipedia.org/wiki/Bit)of stored data occupies a separate memory cell that is electrically implemented with one[capacitor](https://en.wikipedia.org/wiki/Capacitor)and one[transistor](https://en.wikipedia.org/wiki/Transistor). The charge state of a capacitor (charged or discharged) is what determines whether a DRAM cell stores \"1\" or \"0\" as a[binary value](https://en.wikipedia.org/wiki/Binary_value). Huge numbers of DRAM memory cells are packed into[integrated circuits](https://en.wikipedia.org/wiki/Integrated_circuit), together with some additional logic that organises the cells for the purposes of reading, writing, and[refreshing](https://en.wikipedia.org/wiki/Memory_refresh)the data.\nMemory cells (blue squares in the illustration) are further organised into[matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics))and addressed through rows and columns. A memory address applied to a matrix is broken into the row address and column address, which are processed by the row and column[address decoders](https://en.wikipedia.org/wiki/Address_decoder)(in the illustration, vertical and horizontal green rectangles, respectively). After a row address selects the row for a read operation (the selection is also known as[row activation](https://en.wikipedia.org/wiki/Row_activation)), bits from all cells in the row are transferred into the[sense amplifiers](https://en.wikipedia.org/wiki/Sense_amplifier)that form the row buffer (red squares in the illustration), from which the exact bit is selected using the column address. Consequently, read operations are of a destructive nature because the design of DRAM requires memory cells to be rewritten after their values have been read by transferring the cell charges into the row buffer. Write operations decode the addresses in a similar way, but as a result of the design entire rows must be rewritten for the value of a single bit to be changed.\nAs a result of storing data bits using capacitors that have a natural discharge rate, DRAM memory cells lose their state over time and require periodic[rewriting](https://en.wikipedia.org/wiki/Memory_refresh)of all memory cells, which is a process known as refreshing.As another result of the design, DRAM memory is susceptible to random changes in stored data, which are known as[soft memory errors](https://en.wikipedia.org/wiki/Soft_error)and attributed to[cosmic rays](https://en.wikipedia.org/wiki/Cosmic_ray#Effect_on_electronics)and other causes. There are different techniques that counteract soft memory errors and improve the reliability of DRAM, of which[error-correcting code (ECC) memory](https://en.wikipedia.org/wiki/ECC_memory)and its advanced variants (such as[lockstep memory](https://en.wikipedia.org/wiki/Lockstep_memory)) are most commonly used.\n![A RAS Dat CAS ](media/DRAM-image1.png)\n\nA high-level illustration of DRAM organization, which includes[memory cells](https://en.wikipedia.org/wiki/Memory_cell_(computing))(blue squares),[address decoders](https://en.wikipedia.org/wiki/Address_decoder)(green rectangles), and[sense amplifiers](https://en.wikipedia.org/wiki/Sense_amplifier)(red squares)\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Disk-IO/","title":"Disk IO"},"frontmatter":{"draft":false},"rawBody":"# Disk IO\n\nCreated: 2019-06-23 12:36:56 +0500\n\nModified: 2020-02-25 17:07:49 +0500\n\n---\n\n**IO**\n-   Syscalls:[open](http://man7.org/linux/man-pages/man2/open.2.html),[write](http://man7.org/linux/man-pages/man2/write.2.html),[read](http://man7.org/linux/man-pages/man2/read.2.html),[fsync](http://man7.org/linux/man-pages/man2/fsync.2.html),[sync](http://man7.org/linux/man-pages/man2/sync.2.html),[close](http://man7.org/linux/man-pages/man2/close.2.html)\n-   Standard IO:[fopen](https://linux.die.net/man/3/fopen),[fwrite](https://linux.die.net/man/3/fwrite),[fread](https://linux.die.net/man/3/fread),[fflush](https://linux.die.net/man/3/fflush),[fclose](https://linux.die.net/man/3/fclose)\n-   Vectored IO:[writev](https://linux.die.net/man/2/writev),[readv](https://linux.die.net/man/2/readv)\n-   Memory mapped IO:[open](http://man7.org/linux/man-pages/man2/open.2.html),[mmap](http://man7.org/linux/man-pages/man2/mmap.2.html),[msync](http://man7.org/linux/man-pages/man2/msync.2.html),[munmap](http://man7.org/linux/man-pages/man2/munmap.2.html)\n**Buffered IO**\n\nsetvbuf\n\nThe three types of buffering available are unbuffered, block buffered, and line buffered. When an output stream is unbuffered, information appears on the destination file or terminal as soon as written; when it is block buffered many characters are saved up and written as a block; when it is line buffered characters are saved up until a newline is output or input is read from any stream attached to a terminal device (typicallystdin)\n**Sector/Block/Page**\n\n*Block Device*is a special file type providing buffered access to hardware devices such as HDDs or SSDs. Block Devices work act upon*sectors*,group of adjacent bytes. Most disk devices have a sector size of 512 bytes. Sector is the smallest unit of data transfer for block device, it is not possible to transfer less than one sector worth of data. However, often it is possible to fetch multiple adjacent segments at a time. The smallest addressable unit ofFile Systemis*block*.Block is a group of multiple adjacent sectors requested by a device driver. Typical block sizes are 512, 1024, 2048 and 4096 bytes. Usually IO is done through the *Virtual Memory*, which caches requested filesystem blocks in memory and serves as a buffer for intermediate operations. Virtual Memory works with pages, which map to filesystem blocks. Typical page size is 4096 bytes.\n**In summary, Virtual Memory*pages*map to Filesystem*blocks*, which map to Block Device*sectors*.**\n**Standard IO**\n\nStandard IO usesread()andwrite()syscalls for performing IO operations. When reading the data, Page Cache is addressed first. If the data is absent, thePage Faultis triggered and contents are paged in. This means that reads, performed against the currently unmapped area will take longer, as caching layer is transparent to user.\nDuring writes, buffer contents are first written to Page Cache. This means that data does not reach the disk right away. The actual hardware write is done when Kernel decides it's time to perform awritebackof thedirty page.\n\n![Application read ( ) Page Cache 4K Page write ( ) ](media/Disk-IO-image1.png)\n\nStandard IO takes a user space buffer and then copies it's content to the page cache. When the O_DIRECT flag is used, the buffer is written directly to the blockdevice.\n**Page Cache**\n\n[Page Cache](https://github.com/torvalds/linux/blob/master/include/linux/buffer_head.h)stores the recently accessed fragments of files that are more likely to be accessed in the nearest time. When working with disk files,read()andwrite()calls do not initiate disk accesses directly and go through Page Cache instead.\n![Appl i cat i on Kernel Page Cache 4K Page Block Device 612B Block ](media/Disk-IO-image2.png)\nHow Buffered IO works: Applications perform reads and writes through the Kernel Page Cache, which allows sharing pages processes, serving reads from cache and throttling writes to reduceIO.\nWhen thereadoperation is performed, the Page Cache is consulted first. If the data is already loaded in the Page Cache, it is simply copied out for the user: no disk access is performed and read is served entirely from memory. Otherwise file contents are loaded in Page Cache and then returned to the user. If Page Cache is full, least recently used pages are flushed on disk and evicted from cache to free space for new pages.\n\nwrite()call simply copies user-space buffer to kernel Page Cache, marking the written page asdirty. Later, kernel writes modifications on disk in a process calledflushorwriteback. Actual IO normally does not happen immediately. Meanwhile,read()will supply data from the Page Cache instead of reading (now outdated) disk contents. As you can see, Page Cache is loaded both on reads and writes.\nPages markeddirtywill beflushedto disk as since their cached representation is now different from the one on disk. This process is called*writeback*. writeback might have potential drawbacks, such as queuing up IO requests, so it's worth understanding thresholds and ratios that used for writeback when it's in use and check queue depths to make sure you can avoid throttling and high latencies. You can find more information on tuning Virtual Memory in[Linux Kernel Documentation](https://www.kernel.org/doc/Documentation/sysctl/vm.txt).\nLogic behind Page Cache is explained by**Temporal localityprinciple**, that states that recently accessed pages will be accessed again at some point in nearest future.\nAnother principle,**Spatial Locality**, implies that the elements physically located nearby have a good chance of being located close to each other. This principle is used in a process called \"prefetch\" that loads file contents ahead of time anticipating their access and amortizing some of the IO costs.\nPage Cache also improves IO performance by delaying writes and coalescing adjacent reads.\nDisambiguation: Buffer Cache and Page Cache: previously entirely separate concepts,[got unified in 2.4 Linux kernel](https://lwn.net/Articles/712467/). Right now it's mostly referred to as Page Cache, but some people people still use term Buffer Cache, which became synonymous.\nPage Cache, depending on the access pattern, holds file chunks that were recently accessed or may be accessed soon (prefetched or marked with[fadvise](https://medium.com/@ifesdjeen/on-disk-io-part-2-more-flavours-of-io-c945db3edb13)). Since all IO operations are happening through Page Cache, operations sequences such asread-write-readcan be served from memory, without subsequent disk accesses.\n**Delaying Errors**\n\nWhen performing a write that's backed by the kernel and/or a library buffer, it is important to make sure that the data actually reaches the disk, since it might be buffered or cached somewhere. The errors will appear when the data is flushed to disk, which can be whilefsyncing or closing the file.\n**Direct IO**\n\nThere are situations when it's undesirable to use the Kernel Page Cache to perform IO. In such cases, one can use[O_DIRECT](https://ext4.wiki.kernel.org/index.php/Clarifying_Direct_IO%27s_Semantics)flag when opening a file. It instructs the Operating Systems to bypass thePage Cache, avoid storing extra copy of data and perform IO operations directly against the block device. This means that buffers are flushed directly on disk, without copying their contents to the corresponding cached page first and waiting for the Kernel to trigger a writeback.\nFor a \"traditional\" application using Direct IO will most likely cause a performance degradation rather than the speedup, but in the right hands it can help to gain a fine-grained control over IO operations and improve performance. Usually applications using this type of IO implement their own application-specific caching layer.\n![App1ication Ğ’1Ğ¾ÑĞš Device 512B Ğ’1Ğ¾ÑĞš ](media/Disk-IO-image3.png)\n\nHow Direct IO works: Application bypasses the Page Cache, so the writes are made towards the hardware storage right away. This might result into performance degradation, since the Kernel buffers and caches the writes, sharing the cache contents between application. When used well, can result into major performance gains and improved memoryusage.\nUsing Direct IO is often frowned upon by the Kernel developers. It goes so far, that Linux man page quotes Linus Torwalds: \"[The thing that has always disturbed me about O_DIRECT is that the whole interface is just stupid](http://yarchive.net/comp/linux/o_direct.html)\".\nHowever, databases such as[PostgreSQL](https://www.postgresql.org/message-id/529F7D58.1060301%40agliodbs.com)and[MySQL](https://dev.mysql.com/doc/refman/5.5/en/optimizing-innodb-diskio.html)use Direct IO for a reason. Developers can ensure fine-grained control over the data access, possibly using a custom IO Scheduler and an application-specific Buffer Cache. For example, PostgreSQL uses Direct IO for[WAL](https://www.postgresql.org/docs/9.5/static/runtime-config-wal.html)(write-ahead-log), since they have to perform writes as fast as possible while ensuring its durability and can use this optimization since they know for sure that the data won't be immediately reused, so writing it bypassing Page Cache won't cause performance degradation.\n\nIt is discouraged to open the same file with Direct IO and Page Cache simultaneously, since direct operations will be performed against disk device even if the data is in Page Cache, which may lead to undesired results.\n**Block Alignment**\n\nBecause Direct IO involves direct access to backing store, bypassing intermediate buffers in Page Cache, it is required that all operations are aligned to sector boundary.\n\n![512B Block ](media/Disk-IO-image4.png)\n\nExamples of unaligned writes (highlighted). Left to right: the write neither starts, nor ends on the block boundary; the write starts on the block boundary, but the write size isn't a multiple of the block size; the write doesn't start on the block boundary.\nIn other words, every operation has to have a starting offset of a multiple of 512 and a buffer size has to be a multiple of 512 as well. When using Page Cache, because writes first go to memory, alignment is not important: when actual block device write is performed, Kernel will make sure to split the page into parts of the right size and perform aligned writes towards hardware.\n![612B Block ](media/Disk-IO-image5.png)\n\nExamples of aligned writes (highlighted). Left to right: the write starts and ends on the block boundary and is exactly the size of the block; the write starts and ends on the block boundary and has a size that is a multiple of the blocksize.\nFor example, RocksDB is making sure that the operations are block-aligned[by checking it upfront](https://github.com/facebook/rocksdb/blob/master/env/io_posix.cc#L312-L316)(older versions were allowing unaligned access by aligning in the background).\nWhether or not O_DIRECT flag is used, it is always a good idea to make sure your reads and writes are block aligned. Crossing segment boundary will cause multiple sectors to be loaded from (or written back on) disk as shown on images above. Using the block size or a value that fits neatly inside of a block guarantees block-aligned I/O requests, and prevents extraneous work inside the kernel.\n**Nonblocking Filesystem IO**\n\nI'm adding this part here since I very often hear \"nonblocking\" in the context of Filesystem IO. It's quite normal, since most of the programming interface for Network and Filesystem IO is the same. But it's worth mentioning that there's[no true \"nonblocking\" Filesystem IO](https://www.remlab.net/op/nonblock.shtml), which can be understood in the same sense.\nO_NONBLOCK is generally ignored for regular files, because block device operations are considered non-blocking (unlike socket operations, for example). Filesystem IO delays are not taken into account by the system. Possibly this decision was made because there's a more or less hard time bound on operation completion.\nFor same reason, something you would usually use in Network context, likeselectandepoll, does not allow monitoring and/or checking status of regular files.\n**I/O buffering**\n\nIn order to program for data integrity, it is crucial to have an understanding of the overall system architecture. Data can travel through several layers before it finally reaches stable storage, as seen below:\n\n![[Data flow diagram]](media/Disk-IO-image6.png)\n\nAt the top is the running application which has data that it needs to save to stable storage. That data starts out as one or more blocks of memory, or buffers, in the application itself. Those buffers can also be handed to a library, which may perform its own buffering. Regardless of whether data is buffered in application buffers or by a library, the data lives in the application's address space. The next layer that the data goes through is the kernel, which keeps its own version of a write-back cache called the page cache. Dirty pages can live in the page cache for an indeterminate amount of time, depending on overall system load and I/O patterns. When dirty data is finally evicted from the kernel's page cache, it is written to a storage device (such as a hard disk). The storage device may further buffer the data in a volatile write-back cache. If power is lost while data is in this cache, the data will be lost. Finally, at the very bottom of the stack is the non-volatile storage. When the data hits this layer, it is considered to be \"safe.\"\n<https://lwn.net/Articles/457667>\n\n## Access Patterns**\n\nAccess patterns are patterns with which a program reads and writes the data. In general, we distinguish between the r**andom and sequential access patterns**. But, of course, nothing is absolute. Having fully sequential reads is not possible for ad-hoc queries, since the data has to be located first, but as soon as it is located, it can be read sequentially.\nBysequentialaccess we usually mean reads monotonically going from lower offsets to the higher ones and the higher offsets are immediately following the lower ones.\nRandomaccess is reading non-contiguous chunks of data. It usually involves disk seeks, skipping portions of the file in order to locate the data. Hop size is often hard to predict and spans many pages (for example, when traversing a B-Tree on disk, we have to skip entire levels in order to continue the search). In summary, sequential access implies reading contiguous blocks monotonically and random access is pretty much anything else.\nSequentialaccess is often preferred because of it's predictability. In[one of previous posts](https://medium.com/@ifesdjeen/on-disk-io-part-2-more-flavours-of-io-c945db3edb13)we've discussed the fact that avoiding Page Faults allows for a better performance, since reads are served from RAM rather than disk. When reading data sequentially, Kernel may load the pages ahead of time in the process calledprefetching: speculative reading from disk based on some prediction of future requests. In addition, sequential reads avoid additional seeks.\nOptimising for sequential reads and for sequential writes are orthogonal problems. Records written sequentially are not always read together (for example, point queries in sequentially written LSM Tree are still random). Similarly, data read together wasn't necessarily put on disk in a sequential manner (for example, a sequential read of the level in a B-Tree, which might have been updated randomly).\n**Random Reads onSSDs**\n\nOnHDDs, sequential access is preferred to random because of their physical organisation and the way they work. Read/write head is attached to the mechanical arm that has to travel across the disk in order to read the blocks of data; disk has to rotate in to position the track sector under read/write head. This all involves a non-trivial amount of movement. Operating System tries to amortise the costs by caching, buffering and scheduling operations optimally.\nSSDs are made of electronics and do not have any moving components. In this regard, SSDs are inherently different from HDDs and there's no performance degradation caused by where data is stored on disk physically. However, current SSD technology suffers from the performance degradation caused bywrite amplification. Lack of moving parts allows for several other characteristics, such as parallelism, but we won't be discussing them in this article.\nMinimal read unit on SSD ispage. Reads and writes are performed in pages. Deleting a page worth of data does not immediately remove data physically. Instead, a page is marked as stale and will wait for Garbage Collection to reclaim free space.\nBecause writes are performed in pages, even if a single byte has to be updated, the whole page will be written anyway. At the same time, because of the specifics of NAND storage, pages can not be updated in place, so writes can be performed only into the empty pages. These two properties attribute for thewrite amplificationon SSDs.\nAfter an extensive amount of random writes, an FTL (Flash Transportation Layer) runs out of free pages and has to performGarbage Collection: a process that reads, consolidates then and writes active pages in free blocks, freeing blocks, occupied by stale pages and reclaiming disk space.\nSome SSDs implement background Garbage Collection, which takes advantage of idle time in order to consolidate blocks and reclaim stale pages before new data has to be written, which ensures that future foreground write processes have enough free pages available. But given enough write pressure, Garbage Collection process may not keep up with the amount of work, negatively impacting write performance.\nA key goal of log-structured systems is sequentialising writes. However, if the FTL is shared by two log- structured applications (or even a single application with multiple append streams), the incoming data into the FTL is likely to look random or disjoint. You can read more about \"stacking\" log operations in[this paper](https://www.usenix.org/system/files/conference/inflow14/inflow14-yang.pdf).\nWe've discussed multiple things one has to take into consideration when working with SSDs.Writing complete pagesis better than writing data smaller than the page size, since the smallest SSD unit storage is a page. Because updating page will effectively allocate a new page and invalidate the previous one, updates may result into Garbage Collection. It's better tokeep the write operations page-alignedin order to avoid additional write multiplication. And last,keeping the data with similar lifecycle together(e.g. the data that would be both written and discarded at the same time) will be beneficial for performance. Most of these points are points speak favour of immutable LSM-like Storage, rather than systems that allows in-place updates: writes are batched and SSTables are written sequentially, files are immutable and, when deleted, the whole file is invalidated at once.\n<https://medium.com/databasss/on-disk-io-part-1-flavours-of-io-8e1ace1de017>\n\n<https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f>\n\n<https://medium.com/databasss/on-disk-storage-part-4-b-trees-30791060741>\n\n<https://medium.com/databasss/on-disk-io-access-patterns-in-lsm-trees-2ba8dffc05f9>\n\n## IOPS**\n\nInput/output operations per second(IOPS, pronouncedeye-ops) is an [input/output](https://en.wikipedia.org/wiki/Input/output) performance measurement used to characterize[computer storage](https://en.wikipedia.org/wiki/Data_storage_device)devices like[hard disk drives](https://en.wikipedia.org/wiki/Hard_disk_drive)(HDD),[solid state drives](https://en.wikipedia.org/wiki/Solid_state_drives)(SSD), and[storage area networks](https://en.wikipedia.org/wiki/Storage_area_network)(SAN). Like[benchmarks](https://en.wikipedia.org/wiki/Benchmark), IOPS numbers published by storage device manufacturers do not directly relate to real-world application performance.\n<https://en.wikipedia.org/wiki/IOPS>\n\n## Wear Leveling**\n\n**Wear leveling**(also written as**wear levelling**) is a technique[^[1]^](https://en.wikipedia.org/wiki/Wear_leveling#cite_note-Fundamental_Patent-1)for prolonging the[service life](https://en.wikipedia.org/wiki/Service_life)of some kinds of erasable[computer storage](https://en.wikipedia.org/wiki/Computer_storage)media, such as[flash memory](https://en.wikipedia.org/wiki/Flash_memory), which is used in[solid-state drives](https://en.wikipedia.org/wiki/Solid-state_drive)(SSDs) and[USB flash drives](https://en.wikipedia.org/wiki/USB_flash_drive), and[phase-change memory](https://en.wikipedia.org/wiki/Phase-change_memory). There are several wear leveling mechanisms that provide varying levels of longevity enhancement in such memory systems.\n**Monitoring**\n-   Writes/sec-- write operations rate.\n-   Reads/sec-- read operations rate.\n-   Busy time-- the % of the elapsed time when your particular disk drive was busy in servicing write or read requests.\n-   Queue length-- the number of requests on the disk that are in the queue.\n**P/E Cycles**\n\nA solid-state-storage program-erase cycle is a sequence of events in which data is written to[solid-state](https://searchstorage.techtarget.com/definition/solid-state-storage)[NAND flash memory](https://searchstorage.techtarget.com/definition/NAND-flash-memory)cell (such as the type found in a so-called flash or thumb drive), then erased, and then rewritten. Program-erase (PE) cycles can serve as a criterion for quantifying the endurance of a[flash storage](https://searchstorage.techtarget.com/definition/flash-storage)device.\n[Flash memory](https://searchstorage.techtarget.com/definition/flash-memory)devices are capable of a limited number of PE cycles because each cycle causes a small amount of physical damage to the medium. This damage accumulates over time, eventually rendering the device unusable. The number of PE cycles that a given device can sustain before problems become prohibitive varies with the type of technology. The least reliable technology is called multi-level cell (MLC). Enterprise-grade MLC (or E-MLC) offers an improvement over MLC; the most reliable technology is known as single-level cell (SLC).\nSome disagreement exists in the literature as to the maximum number of PE cycles that each type of technology can execute while maintaining satisfactory performance. For MLC, typical maximum PE-cycle-per-[block](https://searchsqlserver.techtarget.com/definition/block)numbers range from 1500 to 10,000. For E-MLC, numbers range up to approximately 30,000 PE cycles per block. For SLC, devices can execute up to roughly 100,000 PE cycles per block.\n<https://www.mydigitaldiscount.com/everything-you-need-to-know-about-slc-mlc-and-tlc-nand-flash.html>\n\n## Distributed Asynchronous Object Storage (DAOS)**\n\nTheDistributedAsynchronousObjectStorage (DAOS) is an open-source software-defined object store designed from the ground up for massively distributed Non Volatile Memory (NVM). DAOS takes advantage of next generation NVM technology like Storage Class Memory (SCM) and NVM express (NVMe) while presenting a key-value storage interface and providing features such as transactional non-blocking I/O, advanced data protection with self healing on top of commodity hardware, end-to-end data integrity, fine grained data control and elastic storage to optimize performance and cost.\n<https://github.com/daos-stack/daos>\n\n<https://www.sigarch.org/from-flops-to-iops-the-new-bottlenecks-of-scientific-computing>\n\n## Anatomy of SSD**\n\n<https://www.techspot.com/amp/article/1985-anatomy-ssd>\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2020-06-01 16:02:27 +0500\n\nModified: 2021-09-06 23:49:34 +0500\n\n---\n\n**There are three key elements of an operating system**\n\n1.  Abstractions(process, thread, file, socket, memory)\n\n2.  Mechanisms(create, schedule, open, write, allocate)\n\n3.  Policies(LRU, EDF)\n**There are two operating system design principles**\n\n1.  Separation of mechanism and policyby implementing flexible mechanisms to support policies\n\n2.  Optimization for common case:Where will the OS be used? What will the user want to execute on that machine? What are the workload requirements?\n**There are three types of Operating Systems commonly used nowadays**\n\n1.  Monolithic OS, where the entire OS is working in kernel space and is alone in supervisor mode\n\n2.  Modular OS, in which some part of the system core will be located in independent files called modules that can be added to the system at run time\n\n3.  Micro OS, where the kernel is broken down into separate processes, known as servers. Some of the servers run in kernel space and some run in user-space\n<https://www.freecodecamp.org/news/what-is-an-os-operating-system-definition-for-beginners>\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Journaling-File-System/","title":"Journaling File System"},"frontmatter":{"draft":false},"rawBody":"# Journaling File System\n\nCreated: 2019-12-03 21:36:20 +0500\n\nModified: 2019-12-05 17:13:23 +0500\n\n---\n\nAjournaling file systemis a[file system](https://en.wikipedia.org/wiki/File_system)that keeps track of changes not yet committed to the file system's main part by recording the intentions of such changes in a data structure known as a \"[journal](https://en.wikipedia.org/wiki/Journal_(computing))\", which is usually a[circular log](https://en.wikipedia.org/wiki/Circular_log). In the event of a system crash or power failure, such file systems can be brought back online more quickly with a lower likelihood of becoming corrupted.\nDepending on the actual implementation, a journaling file system may only keep track of stored[metadata](https://en.wikipedia.org/wiki/Metadata), resulting in improved performance at the expense of increased possibility for data corruption. Alternatively, a journaling file system may track both stored data and related metadata, while some implementations allow selectable behavior in this regard.\n**Rationale**\n\nUpdating file systems to reflect changes to files and directories usually requires many separate write operations. This makes it possible for an interruption (like a power failure or system[crash](https://en.wikipedia.org/wiki/Crash_(computing))) between writes to leave data structures in an invalid intermediate state.\nFor example, deleting a file on a Unix file system involves three steps:\n\n1.  Removing its directory entry.\n\n2.  Releasing the[inode](https://en.wikipedia.org/wiki/Inode)to the pool of free inodes.\n\n3.  Returning all disk blocks to the pool of free disk blocks.\nIf a crash occurs after step 1 and before step 2, there will be an orphaned inode and hence a[storage leak](https://en.wikipedia.org/wiki/Storage_leak); if a crash occurs between steps 2 and 3, then the blocks previously used by the file cannot be used for new files, effectively decreasing the storage capacity of the file system. Re-arranging the steps does not help, either. If step 3 preceded step 1, a crash between them could allow the file's blocks to be reused for a new file, meaning the partially deleted file would contain part of the contents of another file, and modifications to either file would show up in both. On the other hand, if step 2 preceded step 1, a crash between them would cause the file to be inaccessible, despite appearing to exist.\nDetecting and recovering from such inconsistencies normally requires a complete[walk](https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms#walk)of its data structures, for example by a tool such as[fsck](https://en.wikipedia.org/wiki/Fsck)(the file system checker).This must typically be done before the file system is next mounted for read-write access. If the file system is large and if there is relatively little I/O bandwidth, this can take a long time and result in longer downtimes if it blocks the rest of the system from coming back online.\nTo prevent this, a journaled file system allocates a special area---the journal---in which it records the changes it will make ahead of time. After a crash, recovery simply involves reading the journal from the file system and replaying changes from this journal until the file system is consistent again. The changes are thus said to be[atomic](https://en.wikipedia.org/wiki/Atomicity_(database_systems))(not divisible) in that they either succeed (succeeded originally or are replayed completely during recovery), or are not replayed at all (are skipped because they had not yet been completely written to the journal before the crash occurred).\n**Techniques**\n\nSome file systems allow the journal to grow, shrink and be re-allocated just as a regular file, while others put the journal in a contiguous area or a hidden file that is guaranteed not to move or change size while the file system is mounted. Some file systems may also allowexternal journalson a separate device, such as a[solid-state drive](https://en.wikipedia.org/wiki/Solid-state_drive)or battery-backed non-volatile RAM. Changes to the journal may themselves be journaled for additional redundancy, or the journal may be distributed across multiple physical volumes to protect against device failure.\nThe internal format of the journal must guard against crashes while the journal itself is being written to. Many journal implementations (such as the JBD2 layer in[ext4](https://en.wikipedia.org/wiki/Ext4)) bracket every change logged with a checksum, on the understanding that a crash would leave a partially written change with a missing (or mismatched) checksum that can simply be ignored when replaying the journal at next remount.\n**Physical journals**\n\nAphysical journallogs an advance copy of every block that will later be written to the main file system. If there is a crash when the main file system is being written to, the write can simply be replayed to completion when the file system is next mounted. If there is a crash when the write is being logged to the journal, the partial write will have a missing or mismatched checksum and can be ignored at next mount.\nPhysical journals impose a significant performance penalty because every changed block must be committedtwiceto storage, but may be acceptable when absolute fault protection is required.\n**Logical journals**\n\nAlogical journalstores only changes to file[metadata](https://en.wikipedia.org/wiki/Metadata)in the journal, and trades fault tolerance for substantially better write performance.[[7]](https://en.wikipedia.org/wiki/Journaling_file_system#cite_note-7)A file system with a logical journal still recovers quickly after a crash, but may allow unjournaled file data and journaled metadata to fall out of sync with each other, causing data corruption.\nFor example, appending to a file may involve three separate writes to:\n\n1.  The file's[inode](https://en.wikipedia.org/wiki/Inode), to note in the file's metadata that its size has increased.\n\n2.  The free space map, to mark out an allocation of space for the to-be-appended data.\n\n3.  The newly allocated space, to actually write the appended data.\nIn a metadata-only journal, step 3 would not be logged. If step 3 was not done, but steps 1 and 2 are replayed during recovery, the file will be appended with garbage.\n**Write hazards**\n\nThe write cache in most operating systems sorts its writes (using the[elevator algorithm](https://en.wikipedia.org/wiki/Elevator_algorithm)or some similar scheme) to maximize throughput. To avoid an out-of-order write hazard with a metadata-only journal, writes for file data must be sorted so that they are committed to storage before their associated metadata. This can be tricky to implement because it requires coordination within the operating system kernel between the file system driver and write cache. An out-of-order write hazard can also occur if a device cannot write blocks immediately to its underlying storage, that is, it cannot flush its write-cache to disk due to deferred write being enabled.\nTo complicate matters, many mass storage devices have their own write caches, in which they may aggressively reorder writes for better performance. (This is particularly common on magnetic hard drives, which have large seek latencies that can be minimized with elevator sorting.) Some journaling file systems conservatively assume such write-reordering always takes place, and sacrifice performance for correctness by forcing the device to flush its cache at certain points in the journal (called barriers in[ext3](https://en.wikipedia.org/wiki/Ext3)and[ext4](https://en.wikipedia.org/wiki/Ext4)).\n**Alternatives**\n\n**Soft updates**\n\nSome[UFS](https://en.wikipedia.org/wiki/Unix_File_System)implementations avoid journaling and instead implement[soft updates](https://en.wikipedia.org/wiki/Soft_updates): they order their writes in such a way that the on-disk file system is never inconsistent, or that the only inconsistency that can be created in the event of a crash is a storage leak. To recover from these leaks, the free space map is reconciled against a full walk of the file system at next mount. This[garbage collection](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science))is usually done in the background.\n**Log-structured file systems**\n\nIn[log-structured file systems](https://en.wikipedia.org/wiki/Log-structured_file_system), the write-twice penalty does not apply because the journal itselfisthe file system: it occupies the entire storage device and is structured so that it can be traversed as would a normal file system.\n**Copy-on-write file systems**\n\nFull[copy-on-write](https://en.wikipedia.org/wiki/Copy-on-write)file systems (such as[ZFS](https://en.wikipedia.org/wiki/ZFS)and[Btrfs](https://en.wikipedia.org/wiki/Btrfs)) avoid in-place changes to file data by writing out the data in newly allocated blocks, followed by updated metadata that would point to the new data and disown the old, followed by metadata pointing to that, and so on up to the superblock, or the root of the file system hierarchy. This has the same correctness-preserving properties as a journal, without the write-twice overhead.\n<https://en.wikipedia.org/wiki/Journaling_file_system>\n\n## ZFS**\n\n**ZFS**is a combined[file system](https://en.wikipedia.org/wiki/File_system)and[logical volume manager](https://en.wikipedia.org/wiki/Logical_volume_management)designed by[Sun Microsystems](https://en.wikipedia.org/wiki/Sun_Microsystems). ZFS is scalable, and includes extensive protection against[data corruption](https://en.wikipedia.org/wiki/Data_corruption), support for high storage capacities, efficient data compression, integration of the concepts of filesystem and[volume management](https://en.wikipedia.org/wiki/Volume_(computing)),[snapshots](https://en.wikipedia.org/wiki/Snapshot_(computer_storage))and[copy-on-write](https://en.wikipedia.org/wiki/Copy-on-write)clones, continuous integrity checking and automatic repair,[RAID-Z](https://en.wikipedia.org/wiki/ZFS#RAID-Z), native[NFSv4](https://en.wikipedia.org/wiki/NFSv4)[ACLs](https://en.wikipedia.org/wiki/Access_control_lists), and can be very precisely configured. The two main implementations, by[Oracle](https://en.wikipedia.org/wiki/Oracle_Corporation)and by the[OpenZFS](https://en.wikipedia.org/wiki/OpenZFS)project, are extremely similar, making ZFS widely available within[Unix-like](https://en.wikipedia.org/wiki/Unix-like)systems.\n<https://en.wikipedia.org/wiki/ZFS>\n\n## Btrfs**\n\nBtrfs, an abbreviation for[b-tree](https://en.wikipedia.org/wiki/B-tree)[file system](https://en.wikipedia.org/wiki/File_system), (pronounced as \"butter fuss\",\"better F S\",\"butter F S\",\"b-tree F S\",or simply by spelling it out) is a file system based on the[copy-on-write](https://en.wikipedia.org/wiki/Copy-on-write#In_computer_storage)(COW) principle, initially designed at[Oracle Corporation](https://en.wikipedia.org/wiki/Oracle_Corporation)for use in[Linux](https://en.wikipedia.org/wiki/Linux). The development of Btrfs began in 2007, and since November 2013 the file system's on-disk format has been declared stable in the Linux kernel.\nBtrfs is intended to address the lack of[pooling](https://en.wikipedia.org/wiki/Pool_(computer_science)),[snapshots](https://en.wikipedia.org/wiki/Snapshot_(computer_storage)),[checksums](https://en.wikipedia.org/wiki/Checksum), and integral multi-device spanning in[Linux file systems](https://en.wikipedia.org/wiki/Linux_file_systems).Chris Mason, the principal Btrfs author, has stated that its goal was \"to let Linux scale for the storage that will be available. Scaling is not just about addressing the storage but also means being able to administer and to manage it with a clean interface that lets people see what's being used and makes it more reliable\".\n<https://en.wikipedia.org/wiki/Btrfs>\n\n## ext4**\n\nThe**ext4 journaling file system**or**fourth extended filesystem**is a[journaling file system](https://en.wikipedia.org/wiki/Journaling_file_system)for[Linux](https://en.wikipedia.org/wiki/Linux), developed as the successor to[ext3](https://en.wikipedia.org/wiki/Ext3).\n**Features**\n-   Large file system\n-   Extents\n-   Backward compatibility\n-   Persistent pre-allocation\n-   Delayed allocation\n-   Unlimited number of subdirectories\n-   Journal checksums\n-   Metadata checksumming\n-   Faster file-system checking\n-   Multiblock allocator\n-   Improved timestamps\n-   Project quotas\n-   Transparent encryption\n-   Lazy initialization\n-   Write barriers\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Memory-Allocation/","title":"Memory Allocation"},"frontmatter":{"draft":false},"rawBody":"# Memory Allocation\n\nCreated: 2019-12-02 13:29:36 +0500\n\nModified: 2019-12-02 13:32:14 +0500\n\n---\n\n**Dynamic memory allocation**\n\nIs memory allocated at runtime usingcalloc(),malloc()and friends. It is sometimes also referred to as 'heap' memory, although it has nothing to do with the heap data-structure[ref](http://www.quora.com/Why-is-dynamic-memory-allocation-called-heap-memory-allocation).\n\nint * a = malloc(sizeof(int));\nHeap memory is persistent untilfree()is called. In other words, you control the lifetime of the variable.\n**Automatic memory allocation**\n\nThis is what is commonly known as 'stack' memory, and is allocated when you enter a new scope (usually when a new function is pushed on the call stack). Once you move out of the scope, the values of automatic memory addresses are undefined, and it is an[error to access them](https://stackoverflow.com/a/6445794/140264).\n\nint a = 43;\nNote that scope does not necessarily mean function. Scopes can nest within a function, and the variable will be in-scope only within the block in which it was declared. Note also that where this memory is allocated is not specified. (On asanesystem it will be on the stack, or registers for optimisation)\n**Static memory allocation**\n\nIs allocated at compile time*, and the lifetime of a variable in static memory is the[lifetime of the program](http://en.wikipedia.org/wiki/Static_variable).\nIn C, static memory can be allocated using thestatickeyword. The scope is the compilation unit only.\nThings get more interesting[when theexternkeyword is considered](http://en.wikipedia.org/wiki/Extern_variable). When anexternvariable isdefinedthe compiler allocates memory for it. When anexternvariable isdeclared, the compiler requires that the variable bedefinedelsewhere. Failure to declare/defineexternvariables will cause linking problems, while failure to declare/definestaticvariables will cause compilation problems.\n\nin file scope, the static keyword is optional (outside of a function):\n\nint a = 32;\nBut not in function scope (inside of a function):\n\nstatic int a = 32;\nTechnically,externandstaticare two separate classes of variables in C.\n\nextern int a; /* Declaration */\nint a; /* Definition */\n***Notes on static memory allocation**\n\nIt's somewhat confusing to say that static memory is allocated at compile time, especially if we start considering that the compilation machine and the host machine might not be the same or might not even be on the same architecture.\n\nIt may be better to thinkthat the allocation of static memory is handled by the compilerrather thanallocated at compile time.\n\nFor example the compiler may create a largedatasection in the compiled binary and when the program is loaded in memory, the address within thedatasegment of the program will be used as the location of the allocated memory. This has the marked disadvantage of making the compiled binary very large if uses a lot of static memory. It's possible to write a multi-gigabytes binary generated from less than half a dozen lines of code. Another option is for the compiler to inject initialisation code that will allocate memory in some other way before the program is executed. This code will vary according to the target platform and OS. In practice, modern compilers use heuristics to decide which of these options to use. You can try this out yourself by writing a small C program that allocates a large static array of either 10k, 1m, 10m, 100m, 1G or 10G items. For many compilers, the binary size will keep growing linearly with the size of the array, and past a certain point, it will shrink again as the compiler uses another allocation strategy.\n**Register Memory**\n\nThe last memory class are 'register' variables. As expected, register variables should be allocated on a CPU's register, but the decision is actually left to the compiler. You may not turn a register variable into a reference by using address-of.\nregister int meaning = 42;\nprintf(\"%pn\",&meaning); /* this is wrong and will fail at compile time. */\nMost modern compilers are smarter than you at picking which variables should be put in registers.\n**References**\n\n<https://stackoverflow.com/questions/8385322/difference-between-static-memory-allocation-and-dynamic-memory-allocation>\n\n<https://www.memorymanagement.org/index.html>\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Memory-Layout/","title":"Memory Layout"},"frontmatter":{"draft":false},"rawBody":"# Memory Layout\n\nCreated: 2019-07-14 17:07:02 +0500\n\nModified: 2019-12-02 13:29:28 +0500\n\n---\n\nA typical memory representation of C program consists of following sections.\n\n1.  Text segment\n\n2.  Initialized data segment\n\n3.  Uninitialized data segment\n\n4.  Stack\n\n5.  Heap\n![high addres low address stack heap uninitialized data(bss) initialized data text command-line arguments and environment variables initialized to zero by exec read from program file by exec ](media/Memory-Layout-image1.jpg)\nA typical memory layout of a running process\n\n1.  **Text Segment:**\n\nA text segment , also known as a code segment or simply as text, is one of the sections of a program in an object file or in memory, which contains executable instructions.\nAs a memory region, a text segment may be placed below the heap or stack in order to prevent heaps and stack overflows from overwriting it.\nUsually, the text segment is sharable so that only a single copy needs to be in memory for frequently executed programs, such as text editors, the C compiler, the shells, and so on. Also, the text segment is often read-only, to prevent a program from accidentally modifying its instructions.\n2.  **Initialized Data Segment:**\n\nInitialized data segment, usually called simply the Data Segment. A data segment is a portion of virtual address space of a program, which contains the global variables and static variables that are initialized by the programmer.\nNote that, data segment is not read-only, since the values of the variables can be altered at run time.\nThis segment can be further classified into initialized read-only area and initialized read-write area.\nFor instance the global string defined by char s[] = \"hello world\" in C and a C statement like int debug=1 outside the main (i.e. global) would be stored in initialized read-write area. And a global C statement like const char* string = \"hello world\" makes the string literal \"hello world\" to be stored in initialized read-only area and the character pointer variable string in initialized read-write area.\nEx: static int i = 10 will be stored in data segment and global int i = 10 will also be stored in data segment\n3.  **Uninitialized Data Segment:**\n\nUninitialized data segment, often called the \"bss\" segment, named after an ancient assembler operator that stood for \"block started by symbol.\" Data in this segment is initialized by the kernel to arithmetic 0 before the program starts executing\nuninitialized data starts at the end of the data segment and contains all global variables and static variables that are initialized to zero or do not have explicit initialization in source code.\nFor instance a variable declared static int i; would be contained in the BSS segment.\nFor instance a global variable declared int j; would be contained in the BSS segment.\n4.  **Stack:**\n\nThe stack area traditionally adjoined the heap area and grew the opposite direction; when the stack pointer met the heap pointer, free memory was exhausted. (With modern large address spaces and virtual memory techniques they may be placed almost anywhere, but they still typically grow opposite directions.)\nThe stack area contains the program stack, a LIFO structure, typically located in the higher parts of memory. On the standard PC x86 computer architecture it grows toward address zero; on some other architectures it grows the opposite direction. A \"stack pointer\" register tracks the top of the stack; it is adjusted each time a value is \"pushed\" onto the stack. The set of values pushed for one function call is termed a \"stack frame\"; A stack frame consists at minimum of a return address.\nStack, where automatic variables are stored, along with information that is saved each time a function is called. Each time a function is called, the address of where to return to and certain information about the caller's environment, such as some of the machine registers, are saved on the stack. The newly called function then allocates room on the stack for its automatic and temporary variables. This is how recursive functions in C can work. Each time a recursive function calls itself, a new stack frame is used, so one set of variables doesn't interfere with the variables from another instance of the function.\n5.  **Heap:**\n\nHeap is the segment where dynamic memory allocation usually takes place.\nThe heap area begins at the end of the BSS segment and grows to larger addresses from there.The Heap area is managed by malloc, realloc, and free, which may use the brk and sbrk system calls to adjust its size (note that the use of brk/sbrk and a single \"heap area\" is not required to fulfill the contract of malloc/realloc/free; they may also be implemented using mmap to reserve potentially non-contiguous regions of virtual memory into the process' virtual address space). The Heap area is shared by all shared libraries and dynamically loaded modules in a process.\n<https://www.geeksforgeeks.org/memory-layout-of-c-program>\n\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Memory-Mapping-mmap/","title":"Memory Mapping mmap"},"frontmatter":{"draft":false},"rawBody":"# Memory Mapping mmap\n\nCreated: 2019-06-27 12:08:25 +0500\n\nModified: 2021-07-27 22:26:07 +0500\n\n---\n\n**Memory Mapping (mmap)**\n\nMemory mapping ([mmap](http://man7.org/linux/man-pages/man2/mmap.2.html)) allows you to access a file as if it was loaded in memory entirely.It simplies file access and is frequently used by database and application developers.\n\n![Application Mapped Buffer Page Cache 4K Page ](media/Memory-Mapping-mmap-image1.png)\n\nMemory mapping maps the process virtual pages directly to the Kernel Page Cache, avoiding additional copy from and to user-space buffer as it is done with StandardIO.\nWithmmapa file can be mapped to a memory segmentprivatelyor in shared mode. Private mapping allows reading from the file, but any write would trigger copy-on-write of the page in question in order to leave the original page intact and keep the changes private, so none of the changes will get reflected on the file itself. Insharedmode, the file mapping is shared with other processes so they can see updates to the mapped memory segment. Additionally, changes are carried through to the underlying file (precise control over which requires the use of[msync](http://man7.org/linux/man-pages/man2/msync.2.html)).\nUnless specified otherwise, file contents are not loaded into memory right away, but in a lazy manner. Space required for the memory mapping is reserved, but is not allocated right away. The first read or write operation results in a page-fault, triggering the allocation of the appropriate page. By passingMAP_POPULATEit is possible to pre-fault the mapped area and force a file read-ahead.\nMemory Mapping is done through the page cache, the same way as the Standard IO operations such asreadandwriteand is using a[demand paging](https://en.wikipedia.org/wiki/Demand_paging).\nDuring the first memory access, aPage Faultis issued, which signals the kernel that the requested page is currently not loaded into memory and will have to be loaded. The kernel identifies where which data has to be loaded from. Page Faults are transparent to the developer: The program flow will carry on as if nothing happened. Sometimes, Page Faults may have a negative impact on performance,\nIt's also possible to map a file into memory with protection flags (for example, in a read-only mode). If an operation on the mapped memory segment violates the requested protection, aSegmentation Faultis issued.\nmmapis a very useful tool for working with IO: It avoids creating an extraneous copy of the buffer in memory (unlike Standard IO, where the data has to be copied into the user-space buffers before the system call is made). Besides, it avoids a system call (and subsequent context switch) overhead for triggering actual IO operation, except when Page Faults occur. From a developers perspective, issuing a random read using anmmapped file looks just like a normal pointer operation and doesn't involve[lseek](http://man7.org/linux/man-pages/man2/lseek.2.html)calls.\nThe disadvantages ofmmapthat are mentioned most of the time are less relevant with modern hardware:\n-   mmap imposes overhead of the kernel data structures required for managing the memory mappings: In today's realities and memory sizes, this argument does not play a major role.\n-   Memory-mapped file size limit: Most of the time, the kernel code is much more memory-friendly anyways and 64 bit architectures allow mapping larger files.\nOf course, this doesn't imply that everything has to be done with the memory-mapped files.\nmmapis quite frequently used by database implementers. For example, the[MongoDB](https://docs.mongodb.com/manual/faq/storage/)default storage engine wasmmap-backed, and[SQLite](https://sqlite.org/mmap.html)is using memory mapping extensively.\n**Page Cache Optimizations**\n\nFrom what we discussed so far, it looks like using Standard IO simplifies many things and has some benefits, but at the cost of control loss: you're at the grace of the kernel and the page cache. This is true, but only to a certain extend. Usually, the kernel can do a much better job predicting when to perform a write-back and prefetch pages, using internal statistics. However, sometimes it's possible to help the kernel to manage the page cache in a way that would be beneficial for the application.\nOne of the ways of informing the kernel about your intentions is using[fadvise](https://linux.die.net/man/2/fadvise). Using the following flags, it is possible to instruct the kernel about your intentions and let it optimize the use of the page cache:\n-   FADV_SEQUENTIALspecifies that the file is read sequentially, from lower offsets to higher ones, so the kernel can make sure to fetch the pages in advance, before the actual read occurs.\n-   FADV_RANDOMdisables read-ahead, evicting pages that are unlikely to be accessed any time soon from the page cache.\n-   FADV_WILLNEEDnotifies the OS that the page will be needed by the process in the near future. This gives the kernel an opportunity to cache the page ahead of time and, when the read operation occurs, to serve it from the page cache instead of page-faulting.\n-   FADV_DONTNEEDadvises the kernel that it can free the cache for the corresponding pages (making sure that the data is synchronised with the disk beforehand).\n-   There's one more flag (FADV_NOREUSE), but on Linux it has no effect.\nJust as the name suggests,fadviseis only acting advisory. The kernel is not obligated to do exactly asfadvisesuggests.\nSince database developers often can predict accesses,fadvise issuch a useful tool. For example,[RocksDB uses](https://github.com/facebook/rocksdb/blob/master/env/io_posix.cc#L377-L401)it for notifying the kernel about access patterns, depending on the file type (SSTable or HintFile), mode (Random or Sequential) and operation (Write or Compaction).\n\nAnother useful call is[mlock](https://linux.die.net/man/2/mlock).It allows you to force pages to be held in memory. This means that once the page is loaded into memory, all subsequent operations will be served from the page cache. It has to be used with caution, since calling it on every page will simply exhaust the system resources.\n**AIO (Linux Asynchronous IO)**\n\nAIO is an interface allowing to initiate multiple IO operations and register callbacks that will be triggered on their completion. Operations will be performed asynchronously (e.g. the system call will return immediately). Using async IO helps the application to continue work on the main thread while the submitted IO job is processed.\nThe two main syscalls responsible for Linux AIO areio_submitandio_getevents.io_submitallows passing one or multiple commands, holding a buffer, offset and an operation that has to be performed. Completions can be queried by usingio_getevents, a call that allows to collect result events for corresponding commands. This allows for a fully asynchronous interface for handling IO, pipelining IO operations and freeing application threads, potentially reducing the amount of context switches and wake-ups.\nUnfortunately, Linux AIO has several shortcomings: the syscalls API isn't exposed by the glibc and requires a library for wiring them up[(libaio](https://pagure.io/libaio/commits/master)seems to be the most popular).[Despite several attempts to fix that](https://lwn.net/Articles/671649/), only file descriptors with O_DIRECT flag are supported, so buffered asynchronous operations won't work. Besides, some operations, such asstat,fsync,openand some others aren't fully asynchronous.\nIt's worth mentioning that Linux AIO shouldn't be confused with[Posix AIO](http://man7.org/linux/man-pages/man7/aio.7.html), which is a different thing altogether. The Posix AIO implementation on Linux is implemented completely in user space and does not use this Linux-specific AIO subsystem at all.\n**Vectored IO**\n\nOne, possibly less popular, method of performing IO operations is Vectored IO (also known as Scatter/Gather). It is called this way because it operates on a vector of buffers and allows reading and writing data to/from disk using multiple buffers per system call.\nWhen performing a vectored read, bytes will be read from the source into the buffer first (up to first buffers's length offset). Then, bytes from the source starting at the first buffer's length and up to the second buffer's length offset will be read into the second buffer and so on,as ifthe source was filling up buffers one after another (although operation order and parallelism are not deterministic). A vectored write works in a similar manner: Buffers will be written as if they were concatenated before the write.\n![Vectored ä¸€ 0 Buffers æ—¥ ç›® æ—¥ æ—¥ ç›® ç›® å›— ç›® å›— ç›® ç›® å›— å›— ç›® ç›® ](media/Memory-Mapping-mmap-image2.png)\n\nVectored IO example: userspace buffers of different size are mapped to the contiguous file region, allowing filling up and flushing multiple buffers with a singlesyscall.\nSuch an approach can help by allowing reading smaller chunks (therefore avoiding allocation of large memory areas for contiguous blocks) and, at the same time, reducing the amount of system calls required to fill up all these buffers with data from disk. Another advantage is that both reads and writes are atomic: The kernel prevents other processes from performing IO on the same descriptor during read and write operations, guaranteeing data integrity.\nFrom the development perspective, if data is laid out in a certain way in the file (say, it's split out into a fixed-size header and multiple fixed size blocks), it is possible to issue a single call that will fill up separate buffers allocated for these parts.\nThis sounds rather useful but, somehow, just a few databases use the Vectored IO. This might be because general purpose databases work with a bunch of files simultaneously, trying to guarantee liveness for each running operation and reduce their latencies, so and data is accessed and cached block-wise. Vectored IO is more useful for analytics workloads and/or columnar databases, where the data is stored on disk contiguously, and its processing can be done in parallel in sparse blocks. One of the examples is[Apache Arrow](https://github.com/apache/arrow/blob/master/java/memory/src/main/java/io/netty/buffer/ArrowBuf.java#L26-L27).\n**Demand Paging**\n\nIn[computer](https://en.wikipedia.org/wiki/Computer)[operating systems](https://en.wikipedia.org/wiki/Operating_systems),**demand paging**(as opposed to[anticipatory](https://en.wikipedia.org/wiki/Paging#Page_replacement_techniques)paging) is a method of[virtual memory](https://en.wikipedia.org/wiki/Virtual_memory)management. In a system that uses demand paging, the operating system copies a disk[page](https://en.wikipedia.org/wiki/Paging)into physical memory only if an attempt is made to access it and that page is not already in memory (*i.e.*, if a[page fault](https://en.wikipedia.org/wiki/Page_fault)occurs). It follows that a[process](https://en.wikipedia.org/wiki/Process_(computing))begins execution with none of its pages in physical memory, and many page faults will occur until most of a process's[working set](https://en.wikipedia.org/wiki/Working_set)of pages are located in physical memory. This is an example of a[lazy loading](https://en.wikipedia.org/wiki/Lazy_loading)technique.\n<https://en.wikipedia.org/wiki/Demand_paging>\n\n## References**\n\n<https://medium.com/databasss/on-disk-io-part-2-more-flavours-of-io-c945db3edb13>\n\n<https://www.youtube.com/watch?v=2RYT_ZfrYFk>\nWhen you mmap() a 2GB file to load it into memory, does 2GB of RAM get used right away?\n-   no, the exiting thing about mmap is that you can load a file into memory, but it only actually get reads into memory lazily when you access the memory. So no memory gets used right away."},{"fields":{"slug":"/Computer-Science/Operating-System/Memory/","title":"Memory"},"frontmatter":{"draft":false},"rawBody":"# Memory\n\nCreated: 2019-07-01 15:44:42 +0500\n\nModified: 2020-12-25 18:05:27 +0500\n\n---\n\n**Stack Allocation**\n\nThe allocation happens on contiguous blocks of memory. We call it stack memory allocation because the allocation happens in function call stack. The size of memory to be allocated is known to compiler and whenever a function is called, its variables get memory allocated on the stack. And whenever the function call is over, the memory for the variables is deallocated. This all happens using some predefined routines in compiler. Programmer does not have to worry about memory allocation and deallocation of stack variables.\n**Heap Allocation**\n\nThe memory is allocated during execution of instructions written by programmers. Note that the name heap has nothing to do with heap data structure. It is called heap because it is a pile of memory space available to programmers to allocated and de-allocate. If a programmer does not handle this memory well,[memory leak](https://www.geeksforgeeks.org/what-is-memory-leak-how-can-we-avoid/)can happen in the program.\n**Heap**\n\nHeapis the portion of memory wheredynamically allocated memory resides (i.e. memory allocated via**malloc**). Memory allocated from the heap will remain allocated until one of the following occurs:\n\n1.  The memory isfree'd\n\n2.  The program terminates\nIf all references to allocated memory are lost (e.g. you don't store a pointer to it anymore), you have what is called amemory leak. This is where the memory has still been allocated, but you have no easy way of accessing it anymore. Leaked memory cannot be reclaimed for future memory allocations, but when the program ends the memory will be free'd up by the operating system.\nContrast this withstackmemory which is where local variables (those defined within a method) live. Memory allocated on the stack generally only lives until the function returns (there are some exceptions to this, e.g. static local variables).\n**Key Differences Between Stack and Heap Allocations**\n\n1.  In a stack, the allocation and deallocation is automatically done by whereas, in heap, it needs to be done by the programmer manually.\n\n2.  Handling of Heap frame is costlier than handling of stack frame.\n\n3.  Memory shortage problem is more likely to happen in stack whereas the main issue in heap memory is fragmentation.\n\n4.  Stack frame access is easier than the heap frame as the stack have small region of memory and is cache friendly, but in case of heap frames which are dispersed throughout the memory so it cause more cache misses.\n\n5.  Stack is not flexible, the memory size allotted cannot be changed whereas a heap is flexible, and the allotted memory can be altered.\n\n6.  Accessing time of heap takes is more than a stack.\n**Comparison Chart:**\n\n| PARAMETER                   | STACK                                      | HEAP                                     |\n|--------------------|---------------------------|--------------------------|\n| Basic                       | Memory is allocated in a contiguous block. | Memory is allocated in any random order. |\n| Allocation and Deallocation | Automatic by compiler instructions.        | Manual by programmer.                    |\n| Cost                        | Less                                       | More                                     |\n| Implementation              | Hard                                       | Easy                                     |\n| Access time                 | Faster                                     | Slower                                   |\n| Main Issue                  | Shortage of memory                         | Memory fragmentation                     |\n| Locality of reference       | Excellent                                  | Adequate                                 |\n| Flexibility                 | Fixed size                                 | Resizing is possible                     |\n**Virtual Memory Size(VSZ)**\n\nIs all memory that the process can access, including memory that is swapped out, memory that is allocated, but not used, and memory that is from shared libraries.\n\n**Address space allocated** - this has addresses allocated in the process's memory map, but there isn't necessarily any actual memory behind it all right now\n**Resident Set Size(RSS)**\n\nIs number of memory pages the process has in real memory multiplied by pagesize. This excludes swapped out memory pages.\n\n**Physically resident memory** - this is currently occupying space in the machine's physical memory\n**TOP command**\n\n**%MEM**is directly related to RES, it's the percentage use of total physicalmemory by the process.\n\n**VIRT**is the total memory that this process has access to shared memory, mapped pages, swapped out pages, etc.\n\n**RES**is the total physicalmemory used shared or private that the process has access to.\n\n**SHR**is the total physical shared memory that the process has access to.\n\n**DATA**is the total private memory mapped to process physical or not.\n\n**CODE**also known as \"textresident set\" is total physical memory used to load application code.\nSo, to sum up,**RES**is most close to the memory used by the process in memory, excluding what's swapped out. but keep in mind that includes the**SHR**(shared physical memory) which mean it could have been used by some other process as well.\n**Utilities**\n\ncat /proc/meminfo\n\nfree\n\ntop\n**Memory Management**\n\nAs computers typically are running multiple tasks, reading and writing directly from/to physical memory is a bad idea. Imagine how easy is to write a program, which reads all the stuff (including your passwords) out of memory or a program, which would write at different program's memory addresses.\nSo, instead of doing things with Physical Memory we have a concept ofVirtual Memory. When your program runs, it only sees it's memory and it thinks that it's the only one in here. Also, not all of your program's stored memory bytes could be in RAM. If you don't access specific memory block often enough, Operating System can put some block of memory into slower storage (like disk) saving precious RAM. And OS won't even admit to your application that OS did it. But we all know that OS did it.\n\n![](media/Memory-image1.png)\n\nVirtual memory can be implemented usingSegmentationorPage tablesbased on your CPU architecture and OS. Page tables are way more common than segmentation.\nInPaged Virtual Memory, we divide virtual memory into blocks, calledPages. Pages can vary in size based on hardware, but usually pages are4-64 KB, often with the capability to use huge pages from2 MB to 1 GB. The division into blocks is useful as it would require a lot more memory to manage each memory slot individually and would slow down performance of your computer.\nIn order to implement Paged Virtual Memory, there is a chip calledMemory Management Unit (MMU), which sits between CPU and your memory. MMU holds mapping from virtual address to physical address in a table (which it stores in memory) calledPage Table, containing onePage Table Entry (PTE)per page. Also MMU has a physical cache calledTranslation Lookaside Buffer (TLB), which store recent translations from Virtual Memory to Physical. Schematically it looks like this:\n\n![TLB bus Physical Memory 1011 1000 cpu 1011 1011 Virtual Address 0010 0010 MMLJ 1000 1011 0100 1001 ](media/Memory-image2.png)\n\nSo let's say OS decides to put some virtual memory page into disk and your program tries to access it. This process looks like this:\n\n1.  CPU issues a command to access the virtual address, MMU check's it in it's Page Table and prohibits access, because no Physical RAM has been allocated to that virtual page.\n\n2.  Then MMU sends a Page Fault to the CPU.\n\n3.  The Operating System then handles the Page fault, by finding a spare memory block of RAM (called frame) and setting up new PTE (Page Table Entry) to map it.\n\n4.  If no RAM is free, it may choose an existing page, using some replacement algorithm, and save it to disk (this process is calledpaging).\n\n5.  With some Memory Management Units, there can also be a shortage of Page Table Entrys, in which case the OS will have to free one for the new mapping.\nOperating systems usually manages multiple applications(processes) so the whole memory management bit looks like this:\n\n![](media/Memory-image3.png)\n\nEach process has one linear virtual address space, with addresses running from0 to some huge maximum. Virtual address space doesn't need to be contiguous, so that not all of these virtual addresses are actually used to store data and they don't consume space in RAM or disk. What's really cool about this is that the same frame of real memory can back multiple virtual pages belonging to multiple processes.\n[https://povilasv.me/go-memory-management/#](https://povilasv.me/go-memory-management/)\nMemory management is the functionality of an operating system which handles or manages primary memory. It moves processes back and forth between the main memory and the disk during execution.\n\nMemory management keeps track of each and every memory location, regardless of whether it is allocated to some process or free. It checks how much memory is to be allocated to processes. It decides which process will get memory at what time. And it tracks whenever memory gets freed up or unallocated, and correspondingly updates the status.\nTheprocess address spaceis the set of logical addresses that a process references in its code. For example, when 32-bit addressing is in use, addresses can range from 0 to 0x7fffffff; that is, Â²Â³Â¹ possible numbers, for a total theoretical size of 2 gigabytes.\nThe operating system takes care of mapping the logical addresses to physical addresses at the time of memory allocation to the program. There are three types of addresses used in a program before and after memory is allocated:\n-   **Symbolic addresses:** The addresses used in a source code. The variable names, constants, and instruction labels are the basic elements of the symbolic address space.\n-   **Relative addresses:** At the time of compilation, a compiler converts symbolic addresses into relative addresses.\n-   **Physical addresses:** The loader generates these addresses at the time when a program is loaded into main memory.\nVirtual and physical addresses are the same in compile-time and load-time address binding schemes. Virtual and physical addresses differ in execution-time address-binding schemes.\nThe set of all logical addresses generated by a program is referred to as alogical address space. The set of all physical addresses corresponding to these logical addresses is referred to as aphysical address space.\n**Memory Management Unit (MMU)**\n\nAmemory management unit(MMU), sometimes calledpaged memory management unit(PMMU), is a[computer hardware](https://en.wikipedia.org/wiki/Computer_hardware)unit having all[memory](https://en.wikipedia.org/wiki/Computer_memory)references passed through itself, primarily performing the translation of[virtual memory addresses](https://en.wikipedia.org/wiki/Virtual_address)to[physical addresses](https://en.wikipedia.org/wiki/Physical_address).\nAn MMU effectively performs[virtual memory](https://en.wikipedia.org/wiki/Virtual_memory)management, handling at the same time[memory protection](https://en.wikipedia.org/wiki/Memory_protection),[cache](https://en.wikipedia.org/wiki/CPU_cache)control,[bus](https://en.wikipedia.org/wiki/Computer_bus)[arbitration](https://en.wikipedia.org/wiki/Arbiter_(electronics)) and, in simpler computer architectures (especially[8-bit](https://en.wikipedia.org/wiki/8-bit) systems), [bank switching](https://en.wikipedia.org/wiki/Bank_switching).\n**Memory management**is a form of[resource management](https://en.wikipedia.org/wiki/Resource_management_(computing))applied to[computer memory](https://en.wikipedia.org/wiki/Computer_memory). The essential requirement of memory management is to provide ways to dynamically allocate portions of memory to programs at their request, and free it for reuse when no longer needed. This is critical to any advanced computer system where more than a single[process](https://en.wikipedia.org/wiki/Process_(computing))might be underway at any time.[[1]](https://en.wikipedia.org/wiki/Memory_management#cite_note-1)\nSeveral methods have been devised that increase the effectiveness of memory management.[Virtual memory](https://en.wikipedia.org/wiki/Virtual_memory)systems separate the memory addresses used by a process from actual physical addresses, allowing separation of processes and increasing the size of the[virtual address space](https://en.wikipedia.org/wiki/Virtual_address_space)beyond the available amount of[RAM](https://en.wikipedia.org/wiki/Random-access_memory)using[paging](https://en.wikipedia.org/wiki/Paging)or swapping to[secondary storage](https://en.wikipedia.org/wiki/Secondary_storage). The quality of the virtual memory manager can have an extensive effect on overall system performance.\nApplication-level memory management is generally categorized as either automatic memory management, usually involving[garbage collection](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)), or[manual memory management](https://en.wikipedia.org/wiki/Manual_memory_management).\n[https://en.wikipedia.org/wiki/Memory_management](https://en.wikipedia.org/wiki/Memory_management#HEAP)\n\n<https://gceasy.io>\n\n## Memory Hierarchy**\n-   **[Processor registers](https://en.wikipedia.org/wiki/Processor_register)--** the fastest possible access (usually 1 CPU cycle). A few thousand bytes in size\n-   [**Cache**](https://en.wikipedia.org/wiki/CPU_cache)\n    -   Level 0 (L0)[Micro operations](https://en.wikipedia.org/wiki/Micro-operation)cache -- 6[KiB](https://en.wikipedia.org/wiki/KiB)in size\n    -   Level 1 (L1)[Instruction](https://en.wikipedia.org/wiki/Opcode)cache -- 128 KiB in size\n    -   Level 1 (L1) Data cache -- 128 KiB in size. Best access speed is around 700[GiB](https://en.wikipedia.org/wiki/GiB)/second\n    -   Level 2 (L2) Instruction and data (shared) -- 1[MiB](https://en.wikipedia.org/wiki/MiB)in size. Best access speed is around 200 GiB/second\n    -   Level 3 (L3) Shared cache -- 6 MiB in size. Best access speed is around 100 GB/second\n    -   Level 4 (L4) Shared cache -- 128 MiB in size. Best access speed is around 40 GB/second\n-   **[Main memory](https://en.wikipedia.org/wiki/Computer_memory)([Primary storage](https://en.wikipedia.org/wiki/Primary_storage)) --**[Gigabytes](https://en.wikipedia.org/wiki/GiB)in size. Best access speed is around 10 GB/second.In the case of a[NUMA](https://en.wikipedia.org/wiki/Non-Uniform_Memory_Access)machine, access times may not be uniform\n-   **[Disk storage](https://en.wikipedia.org/wiki/Disk_storage)([Secondary storage](https://en.wikipedia.org/wiki/Secondary_storage)) --**[Terabytes](https://en.wikipedia.org/wiki/TiB)in size. As of 2017, best access speed is from a consumer[solid state drive](https://en.wikipedia.org/wiki/Solid-state_drive)is about 2000 MB/second\n-   **[Nearline storage](https://en.wikipedia.org/wiki/Nearline_storage)([Tertiary storage](https://en.wikipedia.org/wiki/Tertiary_storage)) --** Up to[exabytes](https://en.wikipedia.org/wiki/Exabytes)in size. As of 2013, best access speed is about 160 MB/second\n-   [**Offline storage**](https://en.wikipedia.org/wiki/Offline_storage)\nThe lower levels of the hierarchy -- from disks downwards -- are also known as[tiered storage](https://en.wikipedia.org/wiki/Tiered_storage). The formal distinction between online, nearline, and offline storage is:\n-   Online storage is immediately available for I/O.\n-   Nearline storage is not immediately available, but can be made online quickly without human intervention\n-   Offline storage is not immediately available, and requires some human intervention to bring online.\n<https://en.wikipedia.org/wiki/Memory_hierarchy>\n\n<https://www.thegeekstuff.com/2012/02/linux-memory-swap-cache-shared-vm>\n\n## Transparent Huge Pages (THP)**\n\nTransparent Huge Pages (THP) is a Linux memory management system that reduces the overhead of Translation Lookaside Buffer (TLB) lookups on machines with large amounts of memory by using larger memory pages.\nA **compressible resource** can be throttled, but an impressible resource---not so much. For example, CPU is considered compressible and memory is incompressible.\n\nCPU is considered a \"compressible\" resource. If your app starts hitting your CPU limits, Kubernetes starts throttling your container. This means the CPU will be artificially restricted, giving your app potentially worse performance! However, it won't be terminated or evicted. You can use a[liveness health check](https://cloudplatform.googleblog.com/2018/05/Kubernetes-best-practices-Setting-up-health-checks-with-readiness-and-liveness-probes.html)to make sure performance has not been impacted.-   **[Disk Cache memory](http://en.wikipedia.org/wiki/Page_cache):** This are chunks of the physical memory, the RAM, used to store files. That way when a program needs to read the file, it's fetched from memory instead of the hard disk. This is done because memory is way faster.\n-   **[Swap](http://en.wikipedia.org/wiki/Paging):** This is a place on the hard disk (usually a dedicated partition) that is used to store programs or data that can't fit in memory, like when a program grows more than the available memory. SWAP is way slower than RAM, so when you hit swap the computer gets slower, but at least the program can work. In linux swap is also used to hibernate, or to move low used program out of memory to allow more space to the disk cache.\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Microprocessor/","title":"Microprocessor"},"frontmatter":{"draft":false},"rawBody":"# Microprocessor\n\nCreated: 2018-12-08 19:27:10 +0500\n\nModified: 2021-06-06 16:49:19 +0500\n\n---\n\n**Course - NPTEL - IIT-Guwahati, by Dr. John Jose**\n# Multilevel Machine Architecture\n\n![Level 5 Level 4 Level 3 Level 2 Level 1 Level O Problem-oriented language level Translation (compiler) Assembly language level Translation (assembler) Operating system machine level Partial interpretation (operating system) Instruction set architecture level Interpretation (microprogram) or direct execution Microarchitecture level Hardware Digital logic level Figure 1-2. A six-level computer. The support method for each level is indicat- ed below it (along with the name of the supporting program). ](media/Microprocessor-image1.png)\n# Data Path\n\n![MAR control registers To main memory C bus ALU control MOR Ã¼BR cpp TOS opc ALU Shifter Control signals Enable onto B bus Write C bus to register B bus Shifter control Figure 4-1. The data path of the example microarchitecture used in this chapter. ](media/Microprocessor-image2.png)\n# Data Path Timing\n\n![Shifter output stable Cycle 1 starts here Registers loaded instantaneously from C bus and memory on rising edge of clock Clock cycle 1 Clock cycle 2 Ay  Az ALU and shifter Propagation from shifter to registers New MPC used to MIR with next microinstruction here MPC available here Set up signals to drive data path Drive H and B bus Figure 4-3. Timing diagram of one data path cycle. ](media/Microprocessor-image3.png)\n# Microinstructions\n\n![Bits 9 NEXT ADDRESS Addr M c JAM 3 A PM M N A Z 8 1 8 N ALU N N N P c S 9 p L c S P C PM M D R A R T E 3 A Mem B bus registers o = MDR 2 = MBR 3 = MBRU 4 T B C bus 5=LV 6 = CPP 7 = TOS 8 = OPC 9-15 none Figure 4-5. The microinstruction format for the Mic-l. ](media/Microprocessor-image4.png)\n**Two types of CPU architecture**\n\n1.  RISC Architecture\n\n2.  CISC Architecture\n**RISC & CISC Comparison**\n\n![CISC It is prominent on Hardware It has high cycles per second It has transistors used for storing Instructions which are complex LOAD and STORE memory-to-memory is induced in instructions It has multi-clock RISC It is prominent on the Software It has low cycles per second More transistors are used for storing memory LOAD and STORE register-register are independent It has a single - clock ](media/Microprocessor-image5.jpg)\n**ARM - Advanced RISC Machine**\n**References**\n\n<https://www.edgefxkits.com/blog/what-is-risc-and-cisc-architecture>"},{"fields":{"slug":"/Computer-Science/Operating-System/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2018-07-13 02:47:32 +0500\n\nModified: 2021-09-10 21:41:23 +0500\n\n---\n\n**Interrupt Coelesing**\n**Bank Switching**\n\nBank switchingis a technique used in computer design to increase the amount of usable memory beyond the amount directly addressable by the[processor](https://en.wikipedia.org/wiki/Microprocessor).It can be used to configure a system differently at different times; for example, a[ROM](https://en.wikipedia.org/wiki/Read-only_memory)required to[start a system](https://en.wikipedia.org/wiki/Booting)from diskette could be switched out when no longer needed. In video game systems, bank switching allowed larger games to be developed for play on existing consoles.\nBank switching originated in[minicomputer](https://en.wikipedia.org/wiki/Minicomputer)systems.Many modern [microcontrollers](https://en.wikipedia.org/wiki/Microcontroller) and [microprocessors](https://en.wikipedia.org/wiki/Microprocessor) use bank switching to manage[random-access memory](https://en.wikipedia.org/wiki/Random-access_memory), non-volatile memory, input-output devices and system management registers in small[embedded systems](https://en.wikipedia.org/wiki/Embedded_system). The technique was common in[8-bit](https://en.wikipedia.org/wiki/8-bit)[microcomputer](https://en.wikipedia.org/wiki/Microcomputer) systems. Bank-switching may also be used to work around limitations in address bus width, where some hardware constraint prevents straightforward addition of more address lines. Some control-oriented microprocessors use a bank-switching technique to access internal I/O and control registers, which limits the number of register address bits that must be used in every instruction.\nUnlike memory management by[paging](https://en.wikipedia.org/wiki/Paging), data is not exchanged with a mass storage device like[disk storage](https://en.wikipedia.org/wiki/Disk_storage). Data remains in quiescent storage in a memory area that is not currently accessible to the processor (although it may be accessible to the video display,[DMA controller](https://en.wikipedia.org/wiki/Direct_memory_access), or other subsystems of the computer).\n**Copy on Write (CoW / COW)**\n\nCopy-on-write(CoWorCOW), sometimes referred to asimplicit sharingorshadowing,is a resource-management technique used in[computer programming](https://en.wikipedia.org/wiki/Computer_programming)to efficiently implement a \"duplicate\" or \"copy\" operation on modifiable resources.If a resource is duplicated but not modified, it is not necessary to create a new resource; the resource can be shared between the copy and the original. Modifications must still create a copy, hence the technique: the copy operation is deferred to the first write. By sharing resources in this way, it is possible to significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations.\n\n<https://en.wikipedia.org/wiki/Copy-on-write>\n\n## Virtual Hard Drive**\n\n**VHD(Virtual Hard Disk)** is a[file format](https://en.wikipedia.org/wiki/File_format)which represents a virtual[hard disk drive](https://en.wikipedia.org/wiki/Hard_disk_drive)(HDD). It may contain what is found on a physical HDD, such as[disk partitions](https://en.wikipedia.org/wiki/Disk_partition)and a[file system](https://en.wikipedia.org/wiki/File_system), which in turn can contain[files](https://en.wikipedia.org/wiki/Computer_file)and[folders](https://en.wikipedia.org/wiki/Folder_(computing)). It is typically used as the hard disk of a[virtual machine](https://en.wikipedia.org/wiki/Virtual_machine).\n**Wayland**\n\nWayland is intended as a simpler replacement for X, easier to develop and maintain. GNOME and KDE are expected to be ported to it.\nWayland is a protocol for a compositor to talk to its clients as well as a C library implementation of that protocol. The compositor can be a standalone display server running on Linux kernel modesetting and evdev input devices, an X application, or a wayland client itself. The clients can be traditional applications, X servers (rootless or fullscreen) or other display servers.\nPart of the Wayland project is also the Weston reference implementation of a Wayland compositor. Weston can run as an X client or under Linux KMS and ships with a few demo clients. The Weston compositor is a minimal and fast compositor and is suitable for many embedded and mobile use cases.\n[https://wayland.freedesktop.org](https://wayland.freedesktop.org/)\n**Weston**\n\nWeston is the reference implementation of a Wayland compositor also developed by the Wayland project. It is written in[C](https://en.wikipedia.org/wiki/C_(programming_language))and published under the[MIT License](https://en.wikipedia.org/wiki/MIT_License). Weston has official support for only the[Linux](https://en.wikipedia.org/wiki/Linux)operating system due to Weston's dependence on certain features of the[Linux kernel](https://en.wikipedia.org/wiki/Linux_kernel), such as[kernel mode-setting](https://en.wikipedia.org/wiki/KMS_driver),[Graphics Execution Manager](https://en.wikipedia.org/wiki/Graphics_Execution_Manager)(GEM), and[udev](https://en.wikipedia.org/wiki/Udev), which have not been implemented in other Unix-like operating systems.[[55]](https://en.wikipedia.org/wiki/Wayland_(display_server_protocol)#cite_note-wayland_README-55)When running on Linux, handling of the input hardware relies on[evdev](https://en.wikipedia.org/wiki/Evdev), while the handling of buffers relies on[Generic Buffer Management](https://en.wikipedia.org/wiki/Generic_Buffer_Management)(GBM). However, in 2013 a prototype port of Weston to[FreeBSD](https://en.wikipedia.org/wiki/FreeBSD)was announced.[[56]](https://en.wikipedia.org/wiki/Wayland_(display_server_protocol)#cite_note-Larabel_2013_freebsd-56)\n<https://en.wikipedia.org/wiki/Wayland_(display_server_protocol)#Weston>\n\n## Handle**\n\nIn[computer programming](https://en.wikipedia.org/wiki/Computer_programming), ahandleis an abstract[reference](https://en.wikipedia.org/wiki/Reference_(computer_science))to a[resource](https://en.wikipedia.org/wiki/System_resource). Handles are used when[application software](https://en.wikipedia.org/wiki/Application_software)references blocks of[memory](https://en.wikipedia.org/wiki/Memory_(computing))or objects managed by another system, such as a[database](https://en.wikipedia.org/wiki/Database)or an[operating system](https://en.wikipedia.org/wiki/Operating_system). A resource handle can be an[opaque](https://en.wikipedia.org/wiki/Opaque_data_type)[identifier](https://en.wikipedia.org/wiki/Identifier), in which case it is often an[integer](https://en.wikipedia.org/wiki/Integer)number (often an[array index](https://en.wikipedia.org/wiki/Array_index)in an array or \"table\" that is used to manage that type of resource), or it can be a[pointer](https://en.wikipedia.org/wiki/Pointer_(computer_programming))that allows access to further information.\nCommon resource handles are[file descriptors](https://en.wikipedia.org/wiki/File_descriptor),[network sockets](https://en.wikipedia.org/wiki/Network_socket),[database connections](https://en.wikipedia.org/wiki/Database_connection),[process identifiers](https://en.wikipedia.org/wiki/Process_identifier)(PIDs), and[job IDs](https://en.wikipedia.org/wiki/Job_ID). Process IDs and job IDs are explicitly visible integers, while file descriptors and sockets (which are often implemented as a form of file descriptor) are represented as integers, but are typically considered opaque. In traditional implementations, file descriptors are indices into a (per-process)[file descriptor table](https://en.wikipedia.org/wiki/File_descriptor_table), thence a (system-wide)[file table](https://en.wikipedia.org/wiki/File_table).\n<https://en.wikipedia.org/wiki/Handle_(computing)>\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Parallel-Processing/","title":"Parallel Processing"},"frontmatter":{"draft":false},"rawBody":"# Parallel Processing\n\nCreated: 2019-03-21 22:38:19 +0500\n\nModified: 2020-03-16 22:46:23 +0500\n\n---\n\n**Parallel Processing, MPP (Massive Parallel Processing)**\n\n**Symmetric Multi-Processing (SMP)** is a tightly coupled multiprocessor system where processors share resources -- single instances of the Operating System (OS), memory, I/O devices and connected using a common bus. SMP is the primary parallel architecture employed in servers and is depicted in the following image.\n\n![Cache CPU Cache CPU Cache BUS Shared Memory CPU Cache I/O ](media/Parallel-Processing-image1.jpg)\n\n**Massively Parallel Processing (MPP)** is the coordinated processing of a single task by multiple processors, each processor using its own OS and memory and communicating with each other using some form of messaging interface. MPP can be setup with a shared nothing or shared disk architecture.\n\nIn a shared nothing architecture, there is no single point of contention across the system and nodes do not share memory or disk storage. Data is horizontally partitioned across nodes, such that each node has a subset of rows from each table in the database. Each node then processes only the rows on its own disks. Systems based on this architecture can achieve massive scale as there is no single bottleneck to slow down the system. This is what Emma is looking for.\n\nMPP with shared-nothing architecture is depicted in the following image.\n\n![High-Speed Interconnect Node 2 Node 1 G...R s...z Node 3 ](media/Parallel-Processing-image2.jpg)\n\n**Message Passing Interface (MPI)**\n\nMessage Passing Interface(MPI) is a standardized and portable[message-passing](https://en.wikipedia.org/wiki/Message-passing)standard designed by a group of researchers from academia and industry to function on a wide variety of[parallel computing](https://en.wikipedia.org/wiki/Parallel_computing)architectures. The standard defines the syntax and semantics of a core of library routines useful to a wide range of users writing portable message-passing programs in[C](https://en.wikipedia.org/wiki/C_(programming_language)),[C++](https://en.wikipedia.org/wiki/C%2B%2B), and[Fortran](https://en.wikipedia.org/wiki/Fortran). There are several well-tested and efficient implementations of MPI, many of which are open-source or in the public domain. These fostered the development of a parallel software industry, and encouraged development of portable and scalable large-scale parallel applications.\n<https://en.wikipedia.org/wiki/Message_Passing_Interface>"},{"fields":{"slug":"/Computer-Science/Operating-System/RAID/","title":"RAID"},"frontmatter":{"draft":false},"rawBody":"# RAID\n\nCreated: 2019-12-05 16:42:17 +0500\n\nModified: 2019-12-08 16:28:41 +0500\n\n---\n\nRAID(Redundant Array of Inexpensive Disksor Drives, orRedundant Array of Independent Disks) is a data[storage virtualization](https://en.wikipedia.org/wiki/Storage_virtualization)technology that combines multiple physical[disk drive](https://en.wikipedia.org/wiki/Disk_drive)components into one or more logical units for the purposes of[data redundancy](https://en.wikipedia.org/wiki/Data_redundancy), performance improvement, or both. This was in contrast to the previous concept of highly reliable mainframe disk drives referred to as \"single large expensive disk\" (SLED).\nData is distributed across the drives in one of several ways, referred to as RAID levels, depending on the required level of[redundancy](https://en.wikipedia.org/wiki/Redundancy_(engineering))and performance. The different schemes, or data distribution layouts, are named by the word \"RAID\" followed by a number, for example RAID0 or RAID1. Each scheme, or RAID level, provides a different balance among the key goals:[reliability](https://en.wikipedia.org/wiki/Reliability_engineering),[availability](https://en.wikipedia.org/wiki/Availability),[performance](https://en.wikipedia.org/wiki/Computer_performance), and[capacity](https://en.wikipedia.org/wiki/Computer_data_storage#Capacity). RAID levels greater than RAID0 provide protection against unrecoverable[sector](https://en.wikipedia.org/wiki/Disk_sector)read errors, as well as against failures of whole physical drives.\nMany RAID levels employ an error protection scheme called \"[parity](https://en.wikipedia.org/wiki/Parity_bit)\", a widely used method in information technology to provide[fault tolerance](https://en.wikipedia.org/wiki/Fault_tolerance)in a given set of data. Most use simple[XOR](https://en.wikipedia.org/wiki/Exclusive_or), but RAID6 uses two separate parities based respectively on addition and multiplication in a particular[Galois field](https://en.wikipedia.org/wiki/Galois_field)or[Reed--Solomon error correction](https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction).\nRAID can also provide data security with[solid-state drives](https://en.wikipedia.org/wiki/Solid-state_drive)(SSDs) without the expense of an all-SSD system. For example, a fast SSD can be mirrored with a mechanical drive. For this configuration to provide a significant speed advantage an appropriate controller is needed that uses the fast SSD for all read operations.[Adaptec](https://en.wikipedia.org/wiki/Adaptec)calls this \"hybrid RAID\".\n**Standard levels**\n\nA number of standard schemes have evolved. These are calledlevels. Originally, there were five RAID levels, but many variations have evolved, notably several[nested levels](https://en.wikipedia.org/wiki/Nested_RAID_levels)and many[non-standard levels](https://en.wikipedia.org/wiki/Non-standard_RAID_levels)(mostly[proprietary](https://en.wikipedia.org/wiki/Proprietary_software)). RAID levels and their associated data formats are standardized by the[Storage Networking Industry Association](https://en.wikipedia.org/wiki/Storage_Networking_Industry_Association)(SNIA) in the Common RAID Disk Drive Format (DDF) standard:\n**RAID0**\n\n[RAID0](https://en.wikipedia.org/wiki/RAID_0)consists of[striping](https://en.wikipedia.org/wiki/Data_striping), but no[mirroring](https://en.wikipedia.org/wiki/Disk_mirroring)or[parity](https://en.wikipedia.org/wiki/Parity_bit). Compared to a[spanned volume](https://en.wikipedia.org/wiki/Spanned_volume), thecapacityof a RAID0 volume is the same; it is the sum of the capacities of the disks in the set. But because striping distributes the contents ofeachfile amongalldisks in the set, the failure of any disk causesallfiles, the entire RAID0 volume, to be lost. A broken spanned volume at least preserves the files on the unfailing disks. The benefit of RAID0 is that the[throughput](https://en.wikipedia.org/wiki/Throughput)of read and write operations to any file is multiplied by the number of disks because, unlike spanned volumes, reads and writes are done[concurrently](https://en.wikipedia.org/wiki/Concurrency_(computer_science)),and the cost is complete vulnerability to drive failures. Indeed, the average failure rate is worse than that of an equivalent single non-RAID drive.\n**RAID1**\n\n[RAID1](https://en.wikipedia.org/wiki/RAID_1)consists of data mirroring, without parity or striping. Data is written identically to two drives, thereby producing a \"mirrored set\" of drives. Thus, any read request can be serviced by any drive in the set. If a request is broadcast to every drive in the set, it can be serviced by the drive that accesses the data first (depending on its[seek time](https://en.wikipedia.org/wiki/Seek_time)and[rotational latency](https://en.wikipedia.org/wiki/Rotational_latency)), improving performance. Sustained read throughput, if the controller or software is optimized for it, approaches the sum of throughputs of every drive in the set, just as for RAID0. Actual read throughput of most RAID1 implementations is slower than the fastest drive. Write throughput is always slower because every drive must be updated, and the slowest drive limits the write performance. The array continues to operate as long as at least one drive is functioning.\n**RAID2**\n\n[RAID2](https://en.wikipedia.org/wiki/RAID_2)consists of bit-level striping with dedicated[Hamming-code](https://en.wikipedia.org/wiki/Hamming_code)parity. All disk spindle rotation is synchronized and data is[striped](https://en.wikipedia.org/wiki/Data_striping)such that each sequential[bit](https://en.wikipedia.org/wiki/Bit)is on a different drive. Hamming-code parity is calculated across corresponding bits and stored on at least one parity drive.This level is of historical significance only; although it was used on some early machines (for example, the[Thinking Machines](https://en.wikipedia.org/wiki/Thinking_Machines_Corporation)CM-2),as of 2014it is not used by any commercially available system.\n**RAID3**\n\n[RAID3](https://en.wikipedia.org/wiki/RAID_3)consists of byte-level striping with dedicated parity. All disk spindle rotation is synchronized and data is striped such that each sequential[byte](https://en.wikipedia.org/wiki/Byte)is on a different drive. Parity is calculated across corresponding bytes and stored on a dedicated parity drive.Although implementations exist,RAID3 is not commonly used in practice.\n**RAID4**\n\n[RAID4](https://en.wikipedia.org/wiki/RAID_4)consists of block-level striping with dedicated parity. This level was previously used by[NetApp](https://en.wikipedia.org/wiki/NetApp), but has now been largely replaced by a proprietary implementation of RAID4 with two parity disks, called[RAID-DP](https://en.wikipedia.org/wiki/RAID-DP). The main advantage of RAID4 over RAID2 and 3 is I/O parallelism: in RAID2 and 3, a single read I/O operation requires reading the whole group of data drives, while in RAID4 one I/O read operation does not have to spread across all data drives. As a result, more I/O operations can be executed in parallel, improving the performance of small transfers.\n**RAID5**\n\n[RAID5](https://en.wikipedia.org/wiki/RAID_5)consists of block-level striping with distributed parity. Unlike RAID4, parity information is distributed among the drives, requiring all drives but one to be present to operate. Upon failure of a single drive, subsequent reads can be calculated from the distributed parity such that no data is lost. RAID5 requires at least three disks.Like all single-parity concepts, large RAID5 implementations are susceptible to system failures because of trends regarding array rebuild time and the chance of drive failure during rebuild (see \"[Increasing rebuild time and failure probability](https://en.wikipedia.org/wiki/RAID#Increasing_rebuild_time_and_failure_probability)\" section, below).Rebuilding an array requires reading all data from all disks, opening a chance for a second drive failure and the loss of the entire array.\n**RAID6**\n\n[RAID6](https://en.wikipedia.org/wiki/RAID_6)consists of block-level striping with double distributed parity. Double parity provides fault tolerance up to two failed drives. This makes larger RAID groups more practical, especially for high-availability systems, as large-capacity drives take longer to restore. RAID6 requires a minimum of four disks. As with RAID5, a single drive failure results in reduced performance of the entire array until the failed drive has been replaced.With a RAID6 array, using drives from multiple sources and manufacturers, it is possible to mitigate most of the problems associated with RAID5. The larger the drive capacities and the larger the array size, the more important it becomes to choose RAID6 instead of RAID5.RAID10 also minimizes these problems.\n**Nested (hybrid) RAID**\n\nIn what was originally termedhybrid RAID,many storage controllers allow RAID levels to be nested. The elements of aRAIDmay be either individual drives or arrays themselves. Arrays are rarely nested more than one level deep.\nThe final array is known as the top array. When the top array is RAID0 (such as in RAID1+0 and RAID5+0), most vendors omit the \"+\" (yielding[RAID10](https://en.wikipedia.org/wiki/RAID_10)and RAID50, respectively).\n-   **RAID0+1:**creates two stripes and mirrors them. If a single drive failure occurs then one of the stripes has failed, at this point it is running effectively as RAID 0 with no redundancy. Significantly higher risk is introduced during a rebuild than RAID 1+0 as all the data from all the drives in the remaining stripe has to be read rather than just from one drive, increasing the chance of an unrecoverable read error (URE) and significantly extending the rebuild window.\n-   **RAID1+0:**(see:[RAID10](https://en.wikipedia.org/wiki/RAID_10)) creates a striped set from a series of mirrored drives. The array can sustain multiple drive losses so long as no mirror loses all its drives.\nRAID 10 is a nested RAID system created by combining RAID 1 and RAID 0. The combination is known as a stripe of mirrors.-   **[JBOD](https://en.wikipedia.org/wiki/JBOD)RAID N+N**:With **JBOD (just a bunch of disks)**, it is possible to concatenate disks, but also volumes such as RAID sets. With larger drive capacities, write delay and rebuilding time increase dramatically (especially, as described above, with RAID 5 and RAID 6). By splitting a larger RAID N set into smaller subsets and concatenating them with linear JBOD,[[clarification needed](https://en.wikipedia.org/wiki/Wikipedia:Please_clarify)]write and rebuilding time will be reduced. If a hardware RAID controller is not capable of nesting linear JBOD with RAID N, then linear JBOD can be achieved with OS-level software RAID in combination with separate RAID N subset volumes created within one, or more, hardware RAID controller(s). Besides a drastic speed increase, this also provides a substantial advantage: the possibility to start a linear JBOD with a small set of disks and to be able to expand the total set with disks of different size, later on (in time, disks of bigger size become available on the market). There is another advantage in the form of disaster recovery (if a RAID N subset happens to fail, then the data on the other RAID N subsets is not lost, reducing restore time).[[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)]\n**Non-standard levels**\n\nMany configurations other than the basic numbered RAID levels are possible, and many companies, organizations, and groups have created their own non-standard configurations, in many cases designed to meet the specialized needs of a small niche group. Such configurations include the following:\n-   [Linux MD RAID 10](https://en.wikipedia.org/wiki/Linux_MD_RAID_10)provides a general RAID driver that in its \"near\" layout defaults to a standard RAID1 with two drives, and a standard RAID1+0 with four drives; however, it can include any number of drives, including odd numbers. With its \"far\" layout, MD RAID10 can run both striped and mirrored, even with only two drives inf2layout; this runs mirroring with striped reads, giving the read performance of RAID0. Regular RAID1, as provided by[Linux software RAID](https://en.wikipedia.org/wiki/Linux_software_RAID), does not stripe reads, but can perform reads in parallel.\n-   [Hadoop](https://en.wikipedia.org/wiki/Hadoop)has a RAID system that generates a parity file by xor-ing a stripe of blocks in a single HDFS file.\n-   [BeeGFS](https://en.wikipedia.org/wiki/BeeGFS), the parallel file system, has internal striping (comparable to file-based RAID0) and replication (comparable to file-based RAID10) options to aggregate throughput and capacity of multiple servers and is typically based on top of an underlying RAID to make disk failures transparent.\n**Implementations**\n\n1.  Hardware based\n\n2.  Software based\n\n3.  Firmware and driver based\n**Integrity**\n\n[Data scrubbing](https://en.wikipedia.org/wiki/Data_scrubbing)(referred to in some environments aspatrol read) involves periodic reading and checking by the RAID controller of all the blocks in an array, including those not otherwise accessed. This detects bad blocks before use.Data scrubbing checks for bad blocks on each storage device in an array, but also uses the redundancy of the array to recover bad blocks on a single drive and to reassign the recovered data to spare blocks elsewhere on the drive.\n**Weaknesses**\n\n1.  Correlated failures\n\n2.  Unrecoverable read errors during rebuild\n\n3.  Increasing rebuild time and failure probability\n\n4.  Atomicity: including parity inconsistency due to system crashes\n\n5.  Write-cache reliability\n<https://en.wikipedia.org/wiki/RAID>\n\n## Striping**\n\nWe all know that,RAID is collection of multiple disk'sand in these disk predefined number of contiguously addressable disk blocks are defined which are called asstripsand collection of such strips in aligned in multiple disk is calledstripe.\n![RAID Striping](media/RAID-image1.png)\nSuppose you have[hard disk](https://www.storagetutorials.com/add-storage-vm-host-without-reboot/), which is a collection of multiple addressable block and these blocks are stacked together and called strip and you have multiple such hard disk, which are place parallel or serially. Then such combination of disk is calledstripe.\nNote:Without mirroring and parity, Striped RAID cannot protect data but striping may significantly improve I/O performance.\n**Mirroring**\n\nMirroringis very simple to understand and one of the most reliable way of data protection. In this technique, you just make amirror copy of diskwhich you want to protect and in this way you have two copies of data. In the time of failure, the controller use second disk to serve the data, thus making data availability continuous.\n![RAID Mirroring](media/RAID-image2.png)\nWhen the failed disk is replaced with a new disk, the controller copies the data from the surviving disk of themirrored pair. Data is simultaneously recorded on both the disk. Though this type of RAID gives you highest availability of data but it is costly as it requires double amount of disk space and thus increasing the cost.\n**Parity**\n\nAs explained above, mirroring involves high cost, so to protect the data new technique is used with striping called parity. This is reliable andlow cost solution for data protection. In this method and additional HDD or disk is added to the stripe width to hold parity bit.\nParity is a redundancy check that ensures full protection of data without maintaining a full set of duplicate data.\n![RAID Parity](media/RAID-image3.png)\nThe parity bits are used to re-create the data at the time of failure. Parity information can be stored on separate,dedicated HDDsor distributed across all the drives in a RAID set. In the above image, parity is stored on a separate disk.\nThe first three disks, labeled D, contain the data. The fourth disk, labeled P, stores the parity information, which in this case is the sum of the elements in each row. Now, if one of the Disks (D) fails, the missing value can be calculated by subtracting the sum of the rest of the elements from the parity value.\n<https://www.storagetutorials.com/understanding-concept-striping-mirroring-parity>\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Scheduling/","title":"Scheduling"},"frontmatter":{"draft":false},"rawBody":"# Scheduling\n\nCreated: 2019-10-02 12:21:43 +0500\n\nModified: 2020-06-01 16:34:20 +0500\n\n---\n\nIn[computing](https://en.wikipedia.org/wiki/Computing),schedulingis the method by which work is assigned to resources that complete the work. The work may be virtual computation elements such as[threads](https://en.wikipedia.org/wiki/Thread_(computer_science)),[processes](https://en.wikipedia.org/wiki/Process_(computing))or data[flows](https://en.wikipedia.org/wiki/Flow_(computer_networking)), which are in turn scheduled onto hardware resources such as[processors](https://en.wikipedia.org/wiki/Central_processing_unit),[network links](https://en.wikipedia.org/wiki/Telecommunications_link)or[expansion cards](https://en.wikipedia.org/wiki/Expansion_card).\nA scheduler is what carries out the scheduling activity. Schedulers are often implemented so they keep all computer resources busy (as in[load balancing](https://en.wikipedia.org/wiki/Load_balancing_(computing))), allow multiple users to share system resources effectively, or to achieve a target[quality of service](https://en.wikipedia.org/wiki/Quality_of_service). Scheduling is fundamental to computation itself, and an intrinsic part of the[execution model](https://en.wikipedia.org/wiki/Execution_model)of a computer system; the concept of scheduling makes it possible to have[computer multitasking](https://en.wikipedia.org/wiki/Computer_multitasking)with a single[central processing unit](https://en.wikipedia.org/wiki/Central_processing_unit)(CPU).\nThe process of scheduling is the responsibility of the process manager that handles the removal of the running process from the CPU and the selection of another process on the basis of a particular strategy.\n\nProcess scheduling is an essential part of a Multiprogramming operating system. These operating systems allow more than one process to be loaded into the executable memory at a time, and the loaded process shares the CPU using time multiplexing.\nThe OS maintains all Process Control Blocks (PCBs) in**Process Scheduling Queues.** The OS maintains a separate queue for each of the process states, and PCBs of all processes in the same execution state are placed in the same queue. When the state of a process is changed, its PCB is unlinked from its current queue and moved to its new state queue.\nThe Operating System maintains the following important process scheduling queues:\n-   Job queue:This queue keeps all the processes in the system.\n-   Ready queue:This queue keeps a set of all processes residing in the main memory, ready and waiting to execute. A new process is always put in this queue.\n-   Device queues:The processes which are blocked due to unavailability of an I/O device constitute this queue.\n\n![](media/Scheduling-image1.jpg)\n\nThe OS can use different policies to manage each queue (FIFO, Round Robin, Priority, etc.). The OS scheduler determines how to move processes between the ready and run queues which can only have one entry per processor core on the system. In the above diagram, it has been merged with the CPU.\nTwo-state process models refer to running and non-running states:\n-   Running:When a new process is created, it enters into the system in the running state.\n-   Not Running:Processes that are not running are kept in queue, waiting for their turn to execute. Each entry in the queue is a pointer to a particular process. Queue is implemented by using a linked list. The use of dispatcher is as follows: when a process is interrupted, that process is transferred in the waiting queue. If the process has completed or aborted, the process is discarded. In either case, the dispatcher then selects a process from the queue to execute.\n**Context Switch**\n\nA**context switch**is the mechanism that stores and restores the state or context of a CPU in the Process Control block. It allows a process execution to be resumed from the same point at a later time. Using this technique, a context switcher enables multiple processes to share a single CPU. Context switching is an essential feature of a multitasking operating system.\nWhen the scheduler switches the CPU from executing one process to another, the state from the current running process is stored into the process control block. After this, the state for the next process is loaded from its own PCB and used to set the PC, registers, etc. At that point, the second process can start executing.\nContext switches are computationally intensive, since register and memory state must be saved and restored. To avoid the amount of context switching time, some hardware systems employ two or more sets of processor registers.\nWhen the process is switched, the following information is stored for later use: Program Counter, Scheduling Information, Base and Limit Register Value, Currently Used Register, Changed State, I/O State Information, and Accounting Information.\n**Goals**\n\nA scheduler may aim at one or more of many goals, for example: maximizing [throughput](https://en.wikipedia.org/wiki/Throughput) (the total amount of work completed per time unit); minimizing[wait time](https://en.wikipedia.org/wiki/Computer_performance#Response_time)(time from work becoming ready until the first point it begins execution); minimizing[latency](https://en.wikipedia.org/wiki/Latency_(engineering))or[response time](https://en.wikipedia.org/wiki/Response_time_(technology))(time from work becoming ready until it is finished in case of batch activity,or until the system responds and hands the first output to the user in case of interactive activity); or maximizingfairness(equal CPU time to each process, or more generally appropriate times according to the priority and workload of each process). In practice, these goals often conflict (e.g. throughput versus latency), thus a scheduler will implement a suitable compromise. Preference is measured by any one of the concerns mentioned above, depending upon the user's needs and objectives.\nIn[real-time](https://en.wikipedia.org/wiki/Real-time_computing)environments, such as[embedded systems](https://en.wikipedia.org/wiki/Embedded_system)for[automatic control](https://en.wikipedia.org/wiki/Automatic_control)in industry (for example[robotics](https://en.wikipedia.org/wiki/Robotics)), the scheduler also must ensure that processes can meet[deadlines](https://en.wikipedia.org/wiki/Time_limit); this is crucial for keeping the system stable. Scheduled tasks can also be distributed to remote devices across a network and[managed](https://en.wikipedia.org/wiki/Device_Management)through an administrative back end.\n**Types of schedulers**\n\n1.  **Process schedulers**\n\nThe process scheduler is a part of the operating system that decides which process runs at a certain point in time. It usually has the ability to pause a running process, move it to the back of the running queue and start a new process; such a scheduler is known as*[preemptive](https://en.wikipedia.org/wiki/Preemption_(computing))scheduler*, otherwise it is a*[cooperative](https://en.wikipedia.org/wiki/Nonpreemptive_multitasking)scheduler*\n2.  **Network schedulers**\n\n3.  **I/O schedulers**\n\n4.  **Job schedulers**\n**Scheduling**\n\n1.  Long term scheduling / Admission scheduler / High level scheduler\n\nThelong-term scheduler, oradmission scheduler, decides which jobs or processes are to be admitted to the ready queue (in main memory); that is, when an attempt is made to execute a program, its admission to the set of currently executing processes is either authorized or delayed by the long-term scheduler. Thus, this scheduler dictates what processes are to run on a system, and the degree of concurrency to be supported at any one time-- whether many or few processes are to be executed concurrently, and how the split between I/O-intensive and CPU-intensive processes is to be handled. The long-term scheduler is responsible for controlling the degree of multiprogramming.\nIn general, most processes can be described as either**[I/O-bound](https://en.wikipedia.org/wiki/I/O-bound)or[CPU-bound](https://en.wikipedia.org/wiki/CPU-bound)**. An I/O-bound process is one that spends more of its time doing I/O than it spends doing computations. A CPU-bound process, in contrast, generates I/O requests infrequently, using more of its time doing computations. It is important that a long-term scheduler selects a good process mix of I/O-bound and CPU-bound processes. If all processes are I/O-bound, the ready queue will almost always be empty, and the short-term scheduler will have little to do. On the other hand, if all processes are CPU-bound, the I/O waiting queue will almost always be empty, devices will go unused, and again the system will be unbalanced. The system with the best performance will thus have a combination of CPU-bound and I/O-bound processes. In modern operating systems, this is used to make sure that real-time processes get enough CPU time to finish their tasks.\n2.  Medium term scheduling\n\nThe*medium-term scheduler*temporarily removes processes from main memory and places them in secondary memory (such as a[hard disk drive](https://en.wikipedia.org/wiki/Hard_disk_drive)) or vice versa, which is commonly referred to as \"swapping out\" or \"swapping in\" (also incorrectly as \"[paging](https://en.wikipedia.org/wiki/Paging)out\" or \"paging in\"). The medium-term scheduler may decide to swap out a process which has not been active for some time, or a process which has a low priority, or a process which is[page faulting](https://en.wikipedia.org/wiki/Page_fault)frequently, or a process which is taking up a large amount of memory in order to free up main memory for other processes, swapping the process back in later when more memory is available, or when the process has been unblocked and is no longer waiting for a resource.\n3.  Short term scheduling\n\nTheshort-term scheduler(also known as theCPU scheduler) decides which of the ready, in-memory processes is to be executed (allocated a CPU) after a clock[interrupt](https://en.wikipedia.org/wiki/Interrupt), an I/O interrupt, an operating[system call](https://en.wikipedia.org/wiki/System_call)or another form of[signal](https://en.wikipedia.org/wiki/Signal_programming). Thus the short-term scheduler makes scheduling decisions much more frequently than the long-term or mid-term schedulers-- a scheduling decision will at a minimum have to be made after every time slice, and these are very short. This scheduler can be[preemptive](https://en.wikipedia.org/wiki/Preemption_(computing)), implying that it is capable of forcibly removing processes from a CPU when it decides to allocate that CPU to another process, or non-preemptive (also known as \"voluntary\" or \"co-operative\"), in which case the scheduler is unable to \"force\" processes off the CPU.\nA preemptive scheduler relies upon a[programmable interval timer](https://en.wikipedia.org/wiki/Programmable_interval_timer)which invokes an[interrupt handler](https://en.wikipedia.org/wiki/Interrupt_handler)that runs in[kernel mode](https://en.wikipedia.org/wiki/Kernel_mode)and implements the scheduling function.\n4.  Dispatcher\n\nAnother component that is involved in the CPU-scheduling function is the dispatcher, which is the module that gives control of the CPU to the process selected by the short-term scheduler. It receives control in kernel mode as the result of an interrupt or system call. The functions of a dispatcher mop the following:\n-   [Context switches](https://en.wikipedia.org/wiki/Context_switch), in which the dispatcher saves the[state](https://en.wikipedia.org/wiki/State_(computer_science))(also known as[context](https://en.wikipedia.org/wiki/Context_(computing))) of the[process](https://en.wikipedia.org/wiki/Process_(computing))or[thread](https://en.wikipedia.org/wiki/Thread_(computing))that was previously running; the dispatcher then loads the initial or previously saved state of the new process.\n-   Switching to user mode.\n-   Jumping to the proper location in the user program to restart that program indicated by its new state.\nThe dispatcher should be as fast as possible, since it is invoked during every process switch. During the context switches, the processor is virtually idle for a fraction of time, thus unnecessary context switches should be avoided. The time it takes for the dispatcher to stop one process and start another is known as thedispatch latency.\n**Scheduling Disciplines**\n\nScheduling disciplines are algorithms used for distributing resources among parties which simultaneously and asynchronously request them. Scheduling disciplines are used in[routers](https://en.wikipedia.org/wiki/Router_(computing))(to handle packet traffic) as well as in[operating systems](https://en.wikipedia.org/wiki/Operating_system)(to share[CPU time](https://en.wikipedia.org/wiki/CPU_time)among both[threads](https://en.wikipedia.org/wiki/Thread_(computer_science))and[processes](https://en.wikipedia.org/wiki/Process_(computing))), disk drives ([I/O scheduling](https://en.wikipedia.org/wiki/I/O_scheduling)), printers ([print spooler](https://en.wikipedia.org/wiki/Print_spooler)), most embedded systems, etc.\nThe main purposes of scheduling algorithms are to minimize[resource starvation](https://en.wikipedia.org/wiki/Resource_starvation)and to ensure fairness amongst the parties utilizing the resources. Scheduling deals with the problem of deciding which of the outstanding requests is to be allocated resources. There are many different scheduling algorithms. In this section, we introduce several of them.\n\nIn[packet-switched](https://en.wikipedia.org/wiki/Packet-switched)[computer networks](https://en.wikipedia.org/wiki/Computer_networks)and other[statistical multiplexing](https://en.wikipedia.org/wiki/Statistical_multiplexing), the notion of ascheduling algorithmis used as an alternative to[first-come first-served](https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics))queuing of data packets.\nThe simplest best-effort scheduling algorithms are[round-robin](https://en.wikipedia.org/wiki/Round-robin_scheduling),[fair queuing](https://en.wikipedia.org/wiki/Fair_queuing)(a[max-min fair](https://en.wikipedia.org/wiki/Max-min_fair)scheduling algorithm),[proportionally fair](https://en.wikipedia.org/wiki/Proportionally_fair)scheduling and[maximum throughput](https://en.wikipedia.org/wiki/Maximum_throughput_scheduling). If differentiated or guaranteed[quality of service](https://en.wikipedia.org/wiki/Quality_of_service)is offered, as opposed to best-effort communication,[weighted fair queuing](https://en.wikipedia.org/wiki/Weighted_fair_queuing)may be utilized.\n1.  First come, first served\n\n2.  Priority scheduling\n    -   Earliest Deadline First (EDF) / least time to go\n\n<https://en.wikipedia.org/wiki/Earliest_deadline_first_scheduling>\n-   Deadline-monotonic scheduling\n\n3.  Shortest remaining time first\n\n4.  Fixed priority pre-emptive scheduling\n\n5.  Round-robin scheduling\n\n6.  Multilevel queue scheduling\n\n7.  Work-conserving schedulers\n\n8.  Manual scheduling\n\nA very common method in embedded systems is to schedule jobs manually. This can for example be done in a time-multiplexed fashion. Sometimes the kernel is divided in three or more parts: Manual scheduling, preemptive and interrupt level. Exact methods for scheduling jobs are often proprietary.\n-   No resource starvation problems\n-   Very high predictability; allows implementation of hard real-time systems\n-   Almost no overhead\n-   May not be optimal for all applications\n-   Effectiveness is completely dependent on the implementation\n**Scheduling Optimization Problems**\n\nThere are several scheduling problems in which the goal is to decide which job goes to which station at what time, such that the total[makespan](https://en.wikipedia.org/wiki/Makespan)is minimized:\n-   [Job shop scheduling](https://en.wikipedia.org/wiki/Job_shop_scheduling)-- there arenjobs andmidentical stations. Each job should be executed on a single machine. This is usually regarded as an online problem.\n-   [Open-shop scheduling](https://en.wikipedia.org/wiki/Open-shop_scheduling)-- there arenjobs andmdifferent stations. Each job should spend some time at each station, in a free order.\n-   [Flow shop scheduling](https://en.wikipedia.org/wiki/Flow_shop_scheduling)-- there arenjobs andmdifferent stations. Each job should spend some time at each station, in a pre-determined order.\n**Schedulers**\n\n**SCHED_DEADLINE**is a CPU[scheduler](https://en.wikipedia.org/wiki/Scheduling_(computing))available in the[Linux kernel](https://en.wikipedia.org/wiki/Linux_kernel)since version 3.14,^[[1]](https://en.wikipedia.org/wiki/SCHED_DEADLINE#cite_note-lwn01-1)[[2]](https://en.wikipedia.org/wiki/SCHED_DEADLINE#cite_note-osnews01-2)^based on the[Earliest Deadline First (EDF)](https://en.wikipedia.org/wiki/Earliest_deadline_first_scheduling)and Constant Bandwidth Server (CBS)[^[3]^](https://en.wikipedia.org/wiki/SCHED_DEADLINE#cite_note-cbs-3)algorithms, supporting resource reservations: each task scheduled under such policy is associated with a*budget*Q (aka*runtime*), and a*period*P, corresponding to a declaration to the kernel that Q time units are required by that task every P time units, on any processor. This makesSCHED_DEADLINEparticularly suitable for[real-time](https://en.wikipedia.org/wiki/Real-time_computing)applications, like multimedia or industrial control, where P corresponds to the minimum time elapsing between subsequent activations of the task, and Q corresponds to the worst-case execution time needed by each activation of the task.\n<https://en.wikipedia.org/wiki/SCHED_DEADLINE>\n<https://en.wikipedia.org/wiki/Scheduling_(computing)>\n\n## Generic Cell Rate Algorithm**\n\nThegeneric cell rate algorithm(GCRA) is a[leaky bucket](https://en.wikipedia.org/wiki/Leaky_bucket)-type[scheduling algorithm](https://en.wikipedia.org/wiki/Scheduling_algorithm)for the[network scheduler](https://en.wikipedia.org/wiki/Network_scheduler)that is used in[Asynchronous Transfer Mode](https://en.wikipedia.org/wiki/Asynchronous_Transfer_Mode)(ATM) networks.It is used to measure the timing of[cells](https://en.wikipedia.org/wiki/Asynchronous_Transfer_Mode#The_structure_of_an_ATM_cell)on[virtual channels](https://en.wikipedia.org/wiki/Virtual_circuit)(VCs) and or[Virtual Paths](https://en.wikipedia.org/w/index.php?title=Virtual_Paths&action=edit&redlink=1)(VPs) against[bandwidth](https://en.wikipedia.org/wiki/Bandwidth_(signal_processing))and[jitter](https://en.wikipedia.org/wiki/Jitter)limits contained in a[traffic contract](https://en.wikipedia.org/wiki/Traffic_contract)for the VC or VP to which the cells belong. Cells that do not conform to the limits given by the traffic contract may then be re-timed (delayed) in[traffic shaping](https://en.wikipedia.org/wiki/Traffic_shaping), or may be dropped (discarded) or reduced in priority (demoted) in[traffic policing](https://en.wikipedia.org/wiki/Traffic_policing_(communications)). Nonconforming cells that are reduced in priority may then be dropped, in preference to higher priority cells, by downstream components in the network that are experiencing congestion. Alternatively they may reach their destination (VC or VP termination) if there is enough capacity for them, despite them being excess cells as far as the contract is concerned: see[priority control](https://en.wikipedia.org/w/index.php?title=Priority_control&action=edit&redlink=1).\nThe GCRA is given as the reference for checking the traffic on connections in the network, i.e.[usage/network parameter control](https://en.wikipedia.org/wiki/UPC_and_NPC)(UPC/NPC) at[user--network interfaces](https://en.wikipedia.org/wiki/User%E2%80%93network_interface)(UNI) or[inter-network interfaces or network-network interfaces](https://en.wikipedia.org/wiki/Network-to-network_interface)(INI/NNI).It is also given as the reference for the timing of cells transmitted (ATM PDU Data_Requests) onto an ATM network by a[network interface card](https://en.wikipedia.org/wiki/Network_interface_card)(NIC) in a host, i.e. on the user side of the UNI.This ensures that cells are not then discarded by UPC/NCP in the network, i.e. on the network side of the UNI. However, as the GCRA is only given as a reference, the network providers and users may use any other algorithm that gives the same result.\n<https://en.wikipedia.org/wiki/Generic_cell_rate_algorithm>\n\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Swap---Paging---Caching/","title":"Swap / Paging / Caching"},"frontmatter":{"draft":false},"rawBody":"# Swap / Paging / Caching\n\nCreated: 2018-03-29 13:05:32 +0500\n\nModified: 2021-09-16 23:37:45 +0500\n\n---\n\n**Paging**\n\nPaging is a memory management scheme that eliminates the need for contiguous allocation of physical memory. This scheme permits the physical address space of a process to be non-contiguous.\n-   Logical Address or Virtual Address (represented in bits): An address generated by the CPU\n-   Logical Address Space or Virtual Address Space( represented in words or bytes): The set of all logical addresses generated by a program\n-   Physical Address (represented in bits): An address actually available on memory unit\n-   Physical Address Space (represented in words or bytes): The set of all physical addresses corresponding to the logical addresses\nExample:\n-   If Logical Address = 31 bit, then Logical Address Space = 231words = 2 G words (1 G = 230)\n-   If Logical Address Space = 128 M words = 27* 220words, then Logical Address = log2227= 27 bits\n-   If Physical Address = 22 bit, then Physical Address Space = 222words = 4 M words (1 M = 220)\n-   If Physical Address Space = 16 M words = 24* 220words, then Physical Address = log2224= 24 bits\nThe mapping from virtual to physical address is done by the memory management unit (MMU) which is a hardware device and this mapping is known as paging technique.\n-   The Physical Address Space is conceptually divided into a number of fixed-size blocks, calledframes.\n-   The Logical address Space is also splitted into fixed-size blocks, calledpages.\n-   Page Size = Frame Size\nLet us consider an example:\n-   Physical Address = 12 bits, then Physical Address Space = 4 K words\n-   Logical Address = 13 bits, then Logical Address Space = 8 K words\n-   Page size = frame size = 1 K words (assumption)\n\n![size](media/Swap---Paging---Caching-image1.jpg)\nAddress generated by CPU is divided into\n-   Page number(p):Number of bits required to represent the pages in Logical Address Space or Page number\n-   Page offset(d):Number of bits required to represent particular word in a page or page size of Logical Address Space or word number of a page or page offset.\nPhysical Address is divided into\n-   Frame number(f):Number of bits required to represent the frame of Physical Address Space or Frame number.\n-   Frame offset(d):Number of bits required to represent particular word in a frame or frame size of Physical Address Space or word number of a frame or frame offset.\nThe hardware implementation of page table can be done by using dedicated registers. But the usage of register for the page table is satisfactory only if page table is small. If page table contain large number of entries then we can use TLB(translation Look-aside buffer), a special, small, fast look up hardware cache.\n-   The TLB is associative, high speed memory.\n-   Each entry in TLB consists of two parts: a tag and a value.\n-   When this memory is used, then an item is compared with all tags simultaneously.If the item is found, then corresponding value is returned.\n\n![Size1](media/Swap---Paging---Caching-image2.jpg)Main memory access time = m\nIf page table are kept in main memory,\nEffective access time = m(for page table) + m(for particular page in page table)\n\n![save2](media/Swap---Paging---Caching-image3.jpg)\n\n<https://www.geeksforgeeks.org/operating-system-paging>\n\n<https://en.wikipedia.org/wiki/Page_table>\n\n## Translation Lookaside Buffer (TLB)**\n\nAtranslation lookaside buffer(TLB) is a memory[cache](https://en.wikipedia.org/wiki/CPU_cache)that is used to reduce the time taken to access a user memory location.It is a part of the chip's[memory-management unit](https://en.wikipedia.org/wiki/Memory_management_unit)(MMU). The TLB stores the recent translations of[virtual memory](https://en.wikipedia.org/wiki/Virtual_memory)to[physical memory](https://en.wikipedia.org/wiki/Physical_memory)and can be called an address-translation cache. A TLB may reside between the[CPU](https://en.wikipedia.org/wiki/Central_processing_unit)and the[CPU cache](https://en.wikipedia.org/wiki/CPU_cache), between CPU cache and the main memory or between the different levels of the multi-level cache. The majority of desktop, laptop, and server processors include one or more TLBs in the memory-management hardware, and it is nearly always present in any processor that utilizes[paged](https://en.wikipedia.org/wiki/Paging)or[segmented](https://en.wikipedia.org/wiki/Memory_segmentation)[virtual memory](https://en.wikipedia.org/wiki/Virtual_memory).\nThe TLB is sometimes implemented as[content-addressable memory](https://en.wikipedia.org/wiki/Content-addressable_memory)(CAM). The CAM search key is the virtual address, and the search result is a[physical address](https://en.wikipedia.org/wiki/Physical_address). If the requested address is present in the TLB, the CAM search yields a match quickly and the retrieved physical address can be used to access memory. This is called a TLB hit. If the requested address is not in the TLB, it is a miss, and the translation proceeds by looking up the[page table](https://en.wikipedia.org/wiki/Page_table)in a process called apage walk. The page walk is time-consuming when compared to the processor speed, as it involves reading the contents of multiple memory locations and using them to compute the physical address. After the physical address is determined by the page walk, the virtual address to physical address mapping is entered into the TLB. The[PowerPC 604](https://en.wikipedia.org/wiki/PowerPC_604), for example, has a two-way[set-associative](https://en.wikipedia.org/wiki/Set-associative)TLB for data loads and stores.Some processors have different instruction and data address TLBs.\n<https://en.wikipedia.org/wiki/Translation_lookaside_buffer>\n\n## Page Table Entries**\n\nPage table has page table entries where each page table entry stores a frame number and optional status (like protection) bits. Many of status bits used in the virtual memory system. The mostimportantthing in PTE isframe Number.\nPage table entry has the following information\n\n![Frame Number Optional Information Present/Absent Protection Reference Caching Dirty PAGE TABLE ENTRY ](media/Swap---Paging---Caching-image4.png)\n\n1.  **Frame Number --**It gives the frame number in which the current page you are looking for is present. The number of bits required depends on the number of frames.Frame bit is also known as address translation bit.\n    Number of bits for frame = Size of physical memory/frame size\n\n2.  **Present/Absent bit --**Present or absent bit says whether a particular page you are looking for is present or absent. In case if it is not present, that is called Page Fault. It is set to 0 if the corresponding page is not in memory. Used to control page fault by the operating system to support virtual memory. Sometimes this bit is also known asvalid/invalidbits.\n\n3.  **Protection bit --**Protection bit says that what kind of protection you want on that page. So, these bit for the protection of the page frame (read, write etc).\n\n4.  **Referenced bit --**Referenced bit will say whether this page has been referred in the last clock cycle or not. It is set to 1 by hardware when the page is accessed.\n\n5.  **Caching enabled/disabled --**Some times we need the fresh data. Let us say the user is typing some information from the keyboard and your program should run according to the input given by the user. In that case, the information will come into the main memory. Therefore main memory contains the latest information which is typed by the user. Now if you try to put that page in the cache, that cache will show the old information. So whenever freshness is required, we don't want to go for caching or many levels of the memory.The information present in the closest level to the CPU and the information present in the closest level to the user might be different. So we want the information has to be consistency, which means whatever information user has given, CPU should be able to see it as first as possible. That is the reason we want to disable caching. So, this bitenables or disablecaching of the page.\n\n6.  **Modified bit --**Modified bit says whether the page has been modified or not. Modified means sometimes you might try to write something on to the page. If a page is modified, then whenever you should replace that page with some other page, then the modified information should be kept on the hard disk or it has to be written back or it has to be saved back. It is set to 1 by hardware on write-access to page which is used to avoid writing when swapped out. Sometimes this modified bit is also called as theDirty bit.\n<https://www.geeksforgeeks.org/operating-system-page-table-entries>\n\n## Swap**\n\nA swap file (or swap space or, in Windows NT, a pagefile) is a space on a[hard disk](http://searchstorage.techtarget.com/definition/hard-disk)used as the[virtual memory](http://searchstorage.techtarget.com/definition/virtual-memory)extension of a computer's real memory ([RAM](http://searchstorage.techtarget.com/definition/RAM-random-access-memory)). Having a swap file allows your computer's[operating system](http://whatis.techtarget.com/definition/operating-system-OS)to pretend that you have more RAM than you actually do. The least recently used files in RAM can be \"swapped out\" to your hard disk until they are needed later so that new files can be \"swapped in\" to RAM. In larger operating systems (such as IBM's[OS/390](http://searchdatacenter.techtarget.com/definition/OS-390)), the units that are moved are calledpages and the swapping is calledpaging.\nOne advantage of a swap file is that it can be organized as a single contiguous space so that fewer I/O operations are required to read or write a complete file.\nIn general,[Windows](http://searchwindowsserver.techtarget.com/definition/Windows)and[UNIX](http://searchdatacenter.techtarget.com/definition/Unix)-based operating systems provide a default swap file of a certain size that the user or a system administrator can usually change.\n**Page replacement policies**\n\nWell Known Deterministic online paging algorithms -\n\n1.  FIFO (First-In First-Out) - Evict the page that has been in fast memory longest.\n\n2.  LRU (Least Recently Used) - On a fault, evict the page in fast memory that was requested least recently.\n\n3.  LFU (Least Frequently Used) - Evict the page that has been requested least frequently\n\n4.  MIN - On a fault, evict the page whose next request occurs furthest in the future.\n\n5.  ARC (Adaptive Replacement Cache)\n\nAdaptive Replacement Cache(ARC) is a[page replacement algorithm](https://en.wikipedia.org/wiki/Page_replacement_algorithm)with better performance than [LRU](https://en.wikipedia.org/wiki/Cache_algorithms) (least recently used). This is accomplished by keeping track of both frequently used and recently used pages plus a recent eviction history for both.\n<https://en.wikipedia.org/wiki/Adaptive_replacement_cache>\n\n## EC2 instances and Linux doesn't have swap**\n\n| **Amount of physical RAM**                | **Recommended swap space**                     |\n|----------------------------------|--------------------------------------|\n| 2 GB of RAM or less                       | 2x the amount of RAM but never less than 32 MB |\n| More than 2 GB of RAM but less than 32 GB | 4 GB + (RAM -- 2 GB)                           |\n| 32 GB of RAM or more                      | 1x the amount of RAM                           |\n**References**\n\n<http://searchwindowsserver.techtarget.com/definition/swap-file-swap-space-or-pagefile>\n\n<https://www.redhat.com/en/blog/do-we-really-need-swap-modern-systems>\n\n<https://www.kernel.org/doc/gorman/html/understand/understand016.html>\n\n<https://www.kernel.org/doc/gorman/html/understand/understand014.html>\n\n<https://chrisdown.name/2018/01/02/in-defence-of-swap.html>\n\n## What's difference between CPU Cache and TLB?**\n\nBoth CPU Cache and TLB are hardware used in microprocessors but what's the difference, especially when someone says that TLB is also a type of Cache?\n**CPU Cache**is a fast memory which is used to improve latency of fetching information from Main memory (RAM) to CPU registers. So CPU Cache sits between Main memory and CPU. And this cache stores information temporarily so that the next access to the same information is faster. A CPU cache which used to store executable instructions, it's called Instruction Cache (I-Cache). A CPU cache which is used to store data, it's called Data Cache (D-Cache). So I-Cache and D-Cache speeds up fetching time for instructions and data respectively. A modern processor contains both I-Cache and D-Cache. For completeness, let us discuss about D-cache hierarchy as well. D-Cache is typically organized in a hierarchy i.e. Level 1 data cache, Level 2 data cache etc.. It should be noted that L1 D-Cache is faster/smaller/costlier as compared to L2 D-Cache. But the basic idea of 'CPU cache' is to speed up instruction/data fetch time from Main memory to CPU.\n**Translation Lookaside Buffer (i.e. TLB)**is required only if Virtual Memory is used by a processor. In short, TLB speeds up translation of virtual address to physical address by storing page-table in a faster memory. In fact, TLB also sits between CPU and Main memory. Precisely speaking, TLB is used by MMU when physical address needs to be translated to virtual address. By keeping this mapping of virtual-physical addresses in a fast memory, access to page-table improves. It should be noted that page-table (which itself is stored in RAM) keeps track of where virtual pages are stored in the physical memory. In that sense, TLB also can be considered as a cache of the page-table.\nBut the scope of operation forTLBandCPU Cache is different. TLB is about 'speeding up address translation for Virtual memory' so that page-table needn't to be accessed for every address. CPU Cache is about 'speeding up main memory access latency' so that RAM isn't accessed always by CPU. TLB operation comes at the time of address translation by MMU while CPU cache operation comes at the time of memory access by CPU. In fact, any modern processor deploys all I-Cache, L1 & L2 D-Cache and TLB.\n\n![virtual physical address address TLB CPU Lookup miss Translation miss Main Memory Cache data ](media/Swap---Paging---Caching-image5.png)\n<https://www.geeksforgeeks.org/whats-difference-between-cpu-cache-and-tlb>"},{"fields":{"slug":"/Computer-Science/Operating-System/Unix---Linux-File-System/","title":"Unix / Linux File System"},"frontmatter":{"draft":false},"rawBody":"# Unix / Linux File System\n\nCreated: 2018-05-20 12:10:58 +0500\n\nModified: 2022-01-20 20:49:58 +0500\n\n---\n\nCommand - man hier (layout of filesystems)\n**/**\n\nThis is the root directory. This is where whole tree starts\n**/bin**\n\nThis directory contains basic commands and programs that are needed to achieve a minimal working environment upon booting. These are kept separate from some of the other programs on the system to allow you to boot the system for maintenance even if other parts of the filesystem may be damaged or unavailable.\n\nIf you search this directory, you will find that bothlsandpwdreside here. Thecdcommand is actually built into the shell we are using (bash), which is in this directory too.\n**/boot**\n\nThis directory contains the actual files, images, and kernels necessary to boot the system. While/bincontains basic, essential utilities,/bootcontains the core components that actually allow the system to boot.\n\nIf you need to modify the bootloader on your system, or if you would like to see the actual kernel files and initial ramdisk (initrd), you can find them here. This directory must be accessible to the system very early on.\n**/dev**\n\nThis directory houses the files that represent devices on your system. Every hard drive, terminal device, input or output device available to the system is represented by a file here. Depending on the device, you can operate on the devices in different ways.\nFor instance, for a device that represents a hard drive, like/dev/sda, you can mount it to the filesystem to access it. On the other hand, if you have a file that represents a line printer like/dev/lpr, you can write directly to it to send the information to the printer.\n-   dev/sda-- The 1st SCSI disk or simply Hard Disk..\n-   dev/sdb-- The 2nd SCSI disk.\n-   dev/sdc --The 3rd SCSI disk.\n-   dev/hda-- The master disk on IDE primary controller.\n-   dev/hdb-- The slave disk on IDE primary controller.\n\nsda5 = 5th partition on the first HD ('a' is first HD)\n\nsdc8 = 8th partition on the third HD ('c' is third of three active HDs)\n\nsdb3 = 3rd partition on the second HD ('b' is second of two or more active HDs)\n**/dev/shm**\n\n**/dev/shm**is nothing but implementation of traditional**shared memory**concept. It is an efficient means of passing data between programs. One program will create a memory portion, which other processes (if permitted) can access. This will result into speeding up things on Linux.\nshm / shmfs is also known as tmpfs, which is a common name for a temporary file storage facility on many Unix-like operating systems. It is intended to appear as a mounted file system, but one which uses virtual memory instead of a persistent storage device.\nYou can use /dev/shm to improve the performance of application software such as Oracle or overall Linux system performance. On heavily loaded system, it can make tons of difference. For example VMware workstation/server can be optimized to improve your Linux host's performance (i.e. improve the performance of your virtual machines).\n<https://www.cyberciti.biz/tips/what-is-devshm-and-its-practical-usage.html>\n\n## /etc**\n\nThis is one area of the filesystem where you will spend a lot of time if you are working as a system administrator. This directory is basically a configuration directory for various system-wide services.\n\nBy default, this directory contains many files and subdirectories. It contains the configuration files for most of the activities on the system, regardless of their function. In cases where multiple configuration files are needed, many times a application-specific subdirectory is created to hold these files. If you are attempting to configure a service or program for the entire system, this is a great place to look.\n\n**Sources**\n\n/etc/apt/sources.list\n\n**User information - /etc/passwd**\n\nLocal user information is stored in the/etc/passwdfile. Each line in this file represents login information for one user.\n\n**cat /etc/passwd**\n\nroot:*:0:0:System Administrator:/var/root:/bin/sh\n-   User name\n-   Encrypted password (xmeans that the password is stored in the/etc/shadowfile)\n-   User ID number (UID)\n-   User's group ID number (GID)\n-   Full name of the user (GECOS)\n-   User home directory\n-   Login shell (defaults to/bin/bash)\n**cat /etc/group**\n\n**cat /etc/shadow**\n\n**cat /etc/logrotate.conf**\n\n**cat /etc/hostname**\n\n**cat /etc/host.conf**\n**Hosts file - /etc/hosts**\n\nThe[computer file](https://en.wikipedia.org/wiki/Computer_file)**hosts**is an operating system file that maps[hostnames](https://en.wikipedia.org/wiki/Hostname)to[IP addresses](https://en.wikipedia.org/wiki/IP_address). It is a[plain text](https://en.wikipedia.org/wiki/Plain_text)file. Originally a file named HOSTS.TXT was manually maintained and made available via file sharing by[Stanford Research Institute](https://en.wikipedia.org/wiki/Stanford_Research_Institute)for the[ARPANET](https://en.wikipedia.org/wiki/ARPANET)membership, containing the hostnames and address of hosts as contributed for inclusion by member organizations. The[Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System), first described in 1983 and implemented in 1984,automated the publication process and provided instantaneous and dynamic hostname resolution in the rapidly growing network. In modern operating systems, the hosts file remains an alternative name resolution mechanism, configurable often as part of facilities such as the[Name Service Switch](https://en.wikipedia.org/wiki/Name_Service_Switch)as either the primary method or as a fallback method.z\n<https://en.wikipedia.org/wiki/Hosts_(file)>\n\n<https://bencane.com/2013/10/29/managing-dns-locally-with-etchosts>\n\n## /home**\n\nThis location contains the home directories of all of the users on the system (except for the administrative user, root). If you have created other users, a directory matching their username will typically be created under this directory.\n\nInside each home directory, the associated user has write access. Typically, regular users only have write access to their own home directory. This helps keep the filesystem clean and ensures that not just anyone can change important configuration files.\n\nWithin the home directory, that are often hidden files and directories (represented by a starting dot) that allow for user-specific configuration of tools. You can often set system defaults in the/etcdirectory, and then each user can override them as necessary in their own home directory.\n**/lib**\n\nThis directory is used for all of the shared system libraries that are required by the/binand/sbin directories. These files basically provide functionality to the other programs on the system. This is one of the directories that you will not have to access often.\n**/lost+found**\n\nThis is a special directory that contains files recovered by/fsck, the Linux filesystem repair program. If the filesystem is damaged and recovery is undertaken, sometimes files are found but the reference to their location is lost. In this case, the system will place them in this directory.\n\nIn most cases, this directory will remain empty. If you experience corruption or any similar problems and are forced to perform recovery operations, it's always a good idea to check this location when you are finished.\n**/media**\n\nThis directory is typically empty at boot. Its real purpose is simply to provide a location to mount removable media (like cds). In a server environment, this won't be used in most circumstances. But if your Linux operating system ever mounts a media disk and you are unsure of where it placed it, this is a safe bet.\n**/mnt**\n\nThis directory is similar to the/mediadirectory in that it exists only to serve as a organization mount point for devices. In this case, this location is usually used to mount filesystems like external hard drives, etc.\n\nThis directory is often used in a VPS environment for mounting network accessible drives. If you have a filesystem on a remote system that you would like to mount on your server, this is a good place to do that.\n**/opt**\n\nThis directory's usage is rather ambiguous. It is used by some distributions, but ignored by others. Typically, it is used to store optional packages. In the Linux distribution world, this usually means packages and applications that were not installed from the repositories.\n\nFor instance, if your distribution typically provides the packages through a package manager, but you installed program X from source, then this directory would be a good location for that software. Another popular option for software of this nature is in the/usr/localdirectory.\n**/proc**\n\nThe/procdirectory is actually more than just a regular directory. It is actually a pseudo-filesystem of its own that is mounted to that directory. The proc filesystem does not contain real files, but is instead dynamically generated to reflect the internal state of the Linux kernel.\n\nThis means that we can check and modify different information from the kernel itself in real time. For instance, you can get detailed information about the memory usage by typingcat /proc/meminfo.\n**/root**\n\nThis is the home directory of the administrative user (called \"root\"). It functions exactly like the normal home directories, but is housed here instead.\n**/run**\n\nThis directory is for the operating system to write temporary runtime information during the early stages of the boot process. In general, you should not have to worry about much of the information in this directory.\n**/sbin**\n\nThis directory is much like the/bindirectory in that it contains programs deemed essential for using the operating system. The distinction is usually that/sbincontains commands that are available to the system administrator, while the other directory contains programs for all of the users of the system.\n**/selinux**\n\nThis directory contains information involving security enhanced Linux. This is a kernel module that is used to provide access control to the operating system. For the most part, you can ignore this.\n**/srv**\n\nThis directory is used to contain data files for services provided by the computer. In most cases, this directory is not used too much because its functionality can be implemented elsewhere in the filesystem.\n**/tmp**\n\nThis is a directory that is used to store temporary files on the system. It is writable by anyone on the computer and does not persist upon reboot. This means that any files that you need just for a little bit can be put here. They will be automatically deleted once the system shuts down.\n**/usr**\n\nThis directory is one of the largest directories on the system. It basically includes a set of folders that look similar to those in the root/directory, such as/usr/binand/usr/lib. This location is basically used to store all non-essential programs, their documentation, libraries, and other data that is not required for the most minimal usage of the system.\n\nThis is where most of the files on the system will be stored. Some important subdirectories are/usr/local, which is an alternative to the/optdirectory for storing locally compiled programs. Another interesting thing to check out is the/usr/sharedirectory, which contains documentation, configuration files, and other useful files.\n**/var**\n\nThis directory is supposed to contain variable data. In practice, this means it is used to contain information or directories that you expect to grow as the system is used.\n\nFor example, system logs and backups are housed here. Another popular use of this directory is to store web content if you are operating a web server.**utmp**,**wtmp**,**btmp**and variants such as**utmpx**,**wtmpx**and**btmpx**are files on[Unix-like](https://en.wikipedia.org/wiki/Unix-like)systems that keep track of all[logins](https://en.wikipedia.org/wiki/Logging_(computer_security))and[logouts](https://en.wikipedia.org/wiki/Logout)to the system.\n\nThe utmp file allows one to discover information about who is currently using the system. There may be more users currently using the system, because not all programs use utmp logging.\n\ncat /var/run/utmp\n**Xinetd**\n\nxinetd, the eXtended InterNET Daemon, is an open-source daemon which runs on many Linux and Unix systems and manages Internet-based connectivity. It offers a more secure extension to or version of inetd, the Internet daemon.\nxinetd performs the same function as inetd: it starts programs that provide Internet services. Instead of having such servers started at system initialization time, and be dormant until a connection request arrives, xinetd is he only daemon process started and it listens on all service ports for the services listed in its configuration file. When a request comes in, xinetd starts the appropriate server. Because of the way it operates, xinetd (as well as inetd) is also referred to as a super-server.\n<https://www.cyberciti.biz/faq/linux-how-do-i-configure-xinetd-service>\n\n## File Descriptor**\n\nIn[Unix](https://en.wikipedia.org/wiki/Unix)and[related](https://en.wikipedia.org/wiki/Unix-like)computer operating systems, afile descriptor(FD, less frequently fildes) is an abstract indicator ([handle](https://en.wikipedia.org/wiki/Handle_(computing))) used to access a[file](https://en.wikipedia.org/wiki/File_(computing))or other [input/output](https://en.wikipedia.org/wiki/Input/output) [resource](https://en.wikipedia.org/wiki/System_resource), such as a[pipe](https://en.wikipedia.org/wiki/Pipe_(Unix))or[network socket](https://en.wikipedia.org/wiki/Network_socket). File descriptors form part of the [POSIX](https://en.wikipedia.org/wiki/POSIX) [application programming interface](https://en.wikipedia.org/wiki/Application_programming_interface). A file descriptor is a non-negative[integer](https://en.wikipedia.org/wiki/Integer), generally represented in the[C](https://en.wikipedia.org/wiki/C_(programming_language))programming language as the typeint(negative values being reserved to indicate \"no value\" or an error condition).\nEach Unix[process](https://en.wikipedia.org/wiki/Process_(computing))(except perhaps a[daemon](https://en.wikipedia.org/wiki/Daemon_(computer_software))) should expect to have three standard POSIX file descriptors, corresponding to the three[standard streams](https://en.wikipedia.org/wiki/Standard_streams):\n\n| Integer value | Name                                                    | symbolic constant | file stream |\n|-------------|---------------|--------------------------|-------------------|\n| 0             | [Standard input](https://en.wikipedia.org/wiki/Stdin)   | STDIN_FILENO                                                                                                                               | stdin                                                                                                                              |\n| 1             | [Standard output](https://en.wikipedia.org/wiki/Stdout) | STDOUT_FILENO                                                                                                                              | stdout                                                                                                                             |\n| 2             | [Standard error](https://en.wikipedia.org/wiki/Stderr)  | STDERR_FILENO                                                                                                                              | stderr                                                                                                                             |\n/proc/113/fd -> 0,1,2 -> 2>&1\n<https://en.wikipedia.org/wiki/File_descriptor>\n\n## Ownership of Linux Files**\n\n**User**\n\nA user is the owner of the file. By default, the person who created a file becomes its owner. Hence, a user is also sometimes called an owner.\n**Group**\n\nA user-group can contain multiple users. All users belonging to a group will have the same access permissions to the file. Suppose you have a project where a number of people require access to a file. Instead of manually assigning permissions to each user, you could add all users to a group, and assign group permission to file such that only this group members and no one else can read or modify the files.\n**Other**\n\nAny other user who has access to a file. This person has neither created the file, nor he belongs to a usergroup who could own the file. Practically, it means everybody else. Hence, when you set the permission for others, it is also referred as set permissions for the world.\n**Permissions of Linux Files**\n-   **Read**\n\nThis permission give you the authority to open and read a file. Read permission on a directory gives you the ability to lists its content.-   **Write**\n\nThe write permission gives you the authority to modify the contents of a file. The write permission on a directory gives you the authority to add, remove and rename files stored in the directory. Consider a scenario where you have to write permission on file but do not have write permission on the directory where the file is stored. You will be able to modify the file contents. But you will not be able to rename, move or remove the file from the directory.-   **Execute**\n\nIn Windows, an executable program usually has an extension \".exe\" and which you can easily run. In Unix/Linux, you cannot run a program unless the execute permission is set. If the execute permission is not set, you might still be able to see/modify the program code (provided read & write permissions are set), but not run it.\n\n![File Permissions in Linux/Unix](media/Unix---Linux-File-System-image1.png)\n![File Permissions in Linux/Unix](media/Unix---Linux-File-System-image2.png)\nr= read permission\n\nw= write permission\n\nx= execute permission\n\n-= no permission\n![File Permissions in Linux/Unix](media/Unix---Linux-File-System-image3.png)\n**Changing file/directory permissions with 'chmod' command**\n\nThere are 2 ways to use the command\n\n1.  Absolute mode\n\n2.  Symbolic mode\n**Absolute(Numeric) Mode**\n\nIn this mode, filepermissions are not represented as characters but a three-digit octal number.\nThe table below gives numbers for all for permissions types.\n\n| Number | Permission Type       | Symbol |\n|--------|-----------------------|--------|\n| 0      | No Permission         | ---  |\n| 1      | Execute               | --x   |\n| 2      | Write                 | -w-    |\n| 3      | Execute + Write       | -wx    |\n| 4      | Read                  | r--   |\n| 5      | Read + Execute        | r-x    |\n| 6      | Read + Write          | rw-    |\n| 7      | Read + Write +Execute | rwx    |\nLet's see the chmod command in action.\n\n![File Permissions in Linux/Unix](media/Unix---Linux-File-System-image4.png)\n\nIn the above-given terminal window, we have changed the permissions of the file 'sample to '764'.\n\n![File Permissions in Linux/Unix](media/Unix---Linux-File-System-image5.png)\n\n'764' absolute code says the following:\n-   Owner can read, write and execute\n-   Usergroup can read and write\n-   World can only read\n\nThis is shown as '-rwxrw-r-\n\nThis is how you can change the permissions on file by assigning an absolute number.\n**Symbolic Mode**\n\nIn the Absolute mode, you change permissions for all 3 owners. In the symbolic mode, you can modify permissions of a specific owner. It makes use of mathematical symbols to modify the file permissions.\n\n| Operator | Description                                                    |\n|-------------|-----------------------------------------------------------|\n| +       | Adds a permission to a file or directory                       |\n| -       | Removes the permission                                         |\n| =        | Sets the permission and overrides the permissions set earlier. |\n\nThe various owners are represented as -\n\n| User Denotations |           |\n|------------------|------------|\n| u                | user/owner |\n| g                | group      |\n| o                | other      |\n| a                | all        |\n\nWe will not be using permissions in numbers like 755 but characters like rwx. Let's look into an example\n\n![File Permissions in Linux/Unix](media/Unix---Linux-File-System-image6.png)\n**Changing Ownership and Group**\n\nFor changing the ownership of a file/directory, you can use the following command:\n\n**chown user**\nIn case you want to change the user as well as group for a file or directory use the command\n\n**chown user:group filename**\nLet's see this in action\n\n![File Permissions in Linux/Unix](media/Unix---Linux-File-System-image7.png)\n\nIn case you want to change group-owner only, use the command\n\n**chgrp group_name filename**\n'chgrp'stands for change group.\n\n![File Permissions in Linux/Unix](media/Unix---Linux-File-System-image8.png)\n**Tip**\n-   The file /etc/group contains all the groups defined in the system\n-   You can use the command \"groups\" to find all the groups you are a member of\n\n![File Permissions in Linux/Unix](media/Unix---Linux-File-System-image9.png)\n-   You can use the command newgrp to work as a member a group other than your default group\n\n![File Permissions in Linux/Unix](media/Unix---Linux-File-System-image10.png)\n-   You cannot have 2 groups owning the same file.\n-   You do not have nested groups in Linux. One group cannot be sub-group of other\n-   x- eXecuting a directory means Being allowed to \"enter\" a dir and gain possible access to sub-dirs\n-   There are other permissions that you can set on Files and Directories which will be covered in a later advanced tutorial\n-   What does x - eXecuting a directory mean? A: Being allowed to \"enter\" a dir and gain possible access to sub-dirs.\n**Commands**\n\n**Change current directory permissions for user**\n\nchown -R test:test .\n**Change current file permission**\n\nsudo chmod +r /var/log/electric_meter.log\n**Give sudo access to user**\n\nsudo usermod -a -G adm telegraf\n\nsudo usermod -a -G root telegraf\n<https://www.guru99.com/file-permissions.html>\n\n<https://www.freecodecamp.org/news/file-systems-architecture-explained>\n\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Unix---Linux/","title":"Unix / Linux"},"frontmatter":{"draft":false},"rawBody":"# Unix / Linux\n\nCreated: 2018-05-09 18:04:32 +0500\n\nModified: 2022-12-10 00:48:32 +0500\n\n---\n\n**A UNIX OS is made up of three parts**\n-   **Kernel:** The kernel of UNIX is the hub of the OS: it allocates time and memory to programs and handles the filestore and communications in response to system calls,\n-   **Shell:** The shell acts as an interface between the user and the kernel. The shell is a command line interpreter (CLI). It interprets the commands the user types in and arranges for them to be carried out,\n-   **Programs:** The commands are themselves also programs: when they terminate, the shell gives the user another prompt.-   Everything in UNIX is either a file or a process.\n\nUnder the hood, everything is actually a file. A text file is a file, a directory is a file, your keyboard is a file (one that the system reads from only), your monitor is a file (one that the system writes to only) etc.\n-   A process is an executing program identified by a unique PID (process identifier).\n\nProcess identifiers (PIDs) are unique identifiers that the Linux kernel gives to each process. PIDs are namespaced, meaning that a container has its own set of PIDs that are mapped to PIDs on the host system. The first process launched when starting a Linux kernel has the PID 1. For a normal operating system, this process is the init system, for example, systemd or SysV. Similarly, the first process launched in a container gets PID 1. Docker and Kubernetes use signals to communicate with the processes inside containers, most notably to terminate them. Both Docker and Kubernetes can only send signals to the process that has PID 1 inside a container.\n**Linux is an extensionless system**\n\nFiles can have any extension they like or none at all.\n**Directory Structure**\n\n![usr its home scs pgl tmp staff var ee51vn ugl ma51ik docs pics report.doc ](media/Unix---Linux-image1.png)\n**Unix File Types**\n\nThere are 6 file types in Unix systems -\n\n1.  **Regular file**\n\nThis is the most common type of a file in Unix. Being a plain collection of bytes with arbitrary data. There's nothing mysterious about this type. Most of the files you will ever work with are regular.\nIn long-format output of ls, this type of file is specified by the \"-\" symbol.\n2.  **Directory**\n\nThis is a special type of a file in Unix, which only contains a list of other files (the contents of a directory). You don't work with directories directly, instead you manage them with standard commands provided with your OS. The whole directory structure of your Unix system is made of such special files with directory content in each of them.\nIn long-format output of ls, this type of file is specified by the \"d\" symbol:\n\n$ ls -ld *\n-rw-r--r-- 1 greys greys1024 Mar 29 06:31 text\ndrwxr-xr-x 2 greys greys4096 Aug 21 11:00 mydir\n3.  **Special Device file**\n\nThis type of files in Unix allows access to various devices known to your system. Literally, almost every device has a special file associated with it. This simplifies the way Unix interacts with different devices -- to the OS and most commands each device is still a file, so it can be read from and written to using various commands. Most special device files are owned by root, and regular users cannot create them,\n\nDepending on the way of accessing each device, its special device file can be either a character (shown as \"c\" in ls output) or a block (shown as \"b\") device. One device can have more than one device file associated, and it's perfectly normal to have both character and block device files for the same device.\n\nMost special device files are character ones, and devices referred by them are called raw devices. The simple reason behind such a name is that by accessing the device via its special device character file, you're accessing the raw data on the device in a form the device is ready to operate with. For terminal devices, it's one character at a time. For disk devices though, raw access means reading or writing in whole chunks of data -- blocks, which are native to your disk. The most important thing to remember about raw devices is that all the read/write operations to them are direct, immediate and not cached.\n\nBlock device file will provide similar access to the same device, only this time the interaction is going to be buffered by the kernel of your Unix OS. Grouping data into logical blocks and caching such blocks in memory allows the kernel to process most I/O requests much more efficiently. No longer does it have to physically access the disk every time a request happens. The data block is read once, and then all the operations to it happen in the cached version of it, with data being synced to the actual device in regular intervals by a special process running in your OS.\nHere's how the different types of special device files look in your ls output:\n\n$ ls -al /dev/loop0 /dev/ttys0\nbrw-rw---- 1 root disk 7, 0 Sep 7 05:03 /dev/loop0\ncrw-rw-rw- 1 root tty 3, 48 Sep 7 05:04 /dev/ttys0\n4.  **Named Pipe**\n\nPipes represent one of simpler forms of Unix interprocess communication. Their purpose is to connect I/O of two Unix processes accessing the pipe. One of the processes uses this pipe for output of data, while another process uses the very same named pipe file for input.\nIn long-format output of ls, named pipes are marked by the \"p\" symbol:\n\n$ ls -al /dev/xconsole\nprw-r----- 1 root adm 0 Sep 25 08:58 /dev/xconsole\n5.  **Symbolic Link**\n\nThis is yet another file type in Unix, used for referencing some other file of the filesystem. Symbolic link contains a text form of the path to the file it references. To an end user, symlink (sort for symbolic link) will appear to have its own name, but when you try reading or writing data to this file, it will instead reference these operations to the file it points to.\nIn long-format output of ls, symlinks are marked by the \"l\" symbol (that's a lower case L). It also show the path to the referenced file:\n\n$ ls -al hosts\nlrwxrwxrwx 1 greys www-data 10 Sep 25 09:06 hosts -> /etc/hosts\n6.  **Socket**\n\nAUnix socket(sometimes also calledIPC socket --inter-process communication socket) is a special file which allows for advanced inter-process communication. In essence, it is a stream of data, very similar to network stream (and network sockets), but all the transactions are local to the filesystem.\nIn long-format output of ls, Unix sockets are marked by \"s\" symbol:\n\n$ ls -al /dev/log\nsrw-rw-rw- 1 root root 0 Sep 7 05:04 /dev/log\n<https://www.unixtutorial.org/unix-file-types>\n\n## Linux Capabilities**\n\nNormally the root user (or any ID with UID of 0) gets a special treatment when running processes. The kernel and applications are usually programmed to skip the restriction of some activities when seeing this user ID. In other words, this user is allowed to do (almost) anything.\nLinux capabilities provide a subset of the available root privileges to a process. This effectively breaks up root privileges into smaller and distinctive units. Each of these units can then be independently be granted to processes. This way the full set of privileges is reduced and decreasing the risks of exploitation.\n<https://linux-audit.com/linux-capabilities-101>\n\n<https://blog.container-solutions.com/linux-capabilities-why-they-exist-and-how-they-work>\n\n## Linux kernel**\n\n![Linux kemel SCI (System Call Interface) Erminals Character devi ce drivers 1/0 subsystem Linux kemel Virtual File System Sockets Netfilter / Nftables Network protocols Linux kemel Packet Scheduler Network devi ce drivers IRQs File systems block layer Linux kemel 1/0 Scheduler Block drivers Memory management subsystem Virtual memory Paging page Page cache Dispatcher Process management subsystem Signal handling process/thread creation & termination Linux kemel Process Scheduler ](media/Unix---Linux-image2.png)\nThe kernel space is where we have system memory for low level applications on the kernel running. The user space is the environment where our user processes function and execute.\n![user Space process System Call Kernel Space RAM Disk ](media/Unix---Linux-image3.png)\n![user Programs Library/lnterpreter System Calls Kernel Space ](media/Unix---Linux-image4.png)\n\nWhenever an applications makes a request to a kernel level function, an interrupt is sent which tells the processor to stop whatever it is doing and attend to that particular request, you can think about it like context switching if it makes it easier to understand. Provided the user space application has relevant permission, there's a context switch to the kernel space, the user space application awaits a response back after the context switch has started and the required program/functionality in the kernel space is executed through the aid of the appropriate interrupt handler.\n**Syscalls**\n\nSystems Calls aka syscall is an API which allows a**small**part of kernel functionality to be exposed to user level applications. A small part is really stressed to inform whoever reading that syscalls are limited and are generic to serve a purpose. They are not the same across every operating system and do differ in both definition and mode of access.\n![System Call access() chdir() chmod() chown() kill() link() open() pause() stime() times() alarm() fork() chroot() exit() Description This checks if a calling process has access to the required file The chdir command changes the current directory of the system The mode of a file can be changed using this command This changes the ownership of a particular file This system call sends kill signal to one or more processes A new file name is linked to an existing file using link system call. This opens a file for the reading or writing process The pause call suspends a file until a particular signal occurs. This system call sets the correct time. Gets the parent and child process times The alarm system call sets the alarm clock of a process A new process is created using this command This changes the root directory of a file. The exit system call is used to exit a process. ](media/Unix---Linux-image5.png)\nSometimes, we have a group of syscalls that we want to group together, we do this using a linux kernel feature called Capabilities. These are predefined sets of privileges which a running program can have access to or be limited by.\nCapabilities further enhance syscalls by grouping related ones into defined privileges that can be granted or denied at once. This prevents even root level applications from exploiting restricted kernel spaces with reserved permissions.\n**Linux Namespaces**\n\n1.  CGroups\n\nBasically cgroups virtualize the view of process's cgroups in /proc/[pid]/cgroups. Whenever a process creates a new cgroup it enters in a new namespace in which all current directories become cgroup root directories of the new namespace. So we can say that it isolates cgroup root directory.\n\n2.  IPC (Interpolation Communication)\n\nThis namespace isolates interpolation communication. For example, In Linux, we have System V IPC (A communication mechanism) and Posfix(for messagequeues) which allows processes to exchange data in form of communication. So in simple words, we can say that IPC namespace isolates communication.\n\n3.  Network\n\nThis namespace isolates systems related to the network. For example, network devices, IP protocols, Firewall Rules (That's why we can use the single port with single service)\n\n4.  Mount\n\nThis namespace isolates mount points that can be seen by processes in each namespace. In simple words, you can take an example of filesystem mounting in which we can mount only one device or partition on a mount-point.\n\n5.  PID\n\nThis namespace isolates the PID. (In this child processes cannot see or trace the parent process but parent process can see or trace the child processes of the namespace. Processes in different PID namespace can have same PID.)\n\n6.  User\n\nThis namespace isolates security related identifier like group id and user id. In simple words, we can say that the process's group and user id has full privilege inside the namespace but not outside the namespace.\n\n7.  UTS\n\nThis namespace provides the isolation on hostname and domain name. It means processes has a separate copy of domain name or hostname so while changing hostname or domain name it will not affect the rest of the system.\n**Namespace Management**\n\nThis is the most advanced topic of Linux namespaces which should be done on kernel level. For the namespace management, you have to write aCprogram.\nFor management of namespace, we have these functions available in Linux\n-   clone()\n\nIf we use standalone clone() it will create a new process only, but if we pass one or more flags like CLONE_NEW*, then the new namespace will be created and child process will become the member of it.\n-   setns()\n\nThis allows joining existing namespace. The namespace is specified by the file descriptor referenced to process.\n-   unshare()\n\nThis allows calling process to disassociate from parts of current namespace. Basically, this function works on the processes that are being shared by other's namespace as well for ex:- mount namespace.\nWhat are the things a child process inherits from its parents?\n-   User + groups\n-   Environment variables\n-   Working directory\n-   Namespace & Capabilities\nName two pieces of information you can get from proc/$PID directory\n-   Environment variables\n-   Links to every open file\n-   Memory maps\n-   Current working directory\n-   Command line arguments\nWhen 2 programs use the same library (like openssl) are there 2 copies of that library in memory?\n-   No, if it's dynamically linked library (or shared library), then you can just have 1 copy in memory and hundreds of programs can share the same copy\n**CGroups**\n\nControl groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups whose usage of various types of resources can then be limited and monitored. The kernel's cgroup interface is provided through a pseudo-filesystem called cgroupfs. Grouping is implemented in the core cgroup kernel code, while resource tracking and limits are implemented in a set of per-resource-type subsystems (memory, CPU, and so on).\nFunctions\n-   **Resource limiting:**a group can be configured not to exceed a specified memory limit or use more than the desired amount of processors or be limited to specific peripheral devices.\n-   **Prioritization:**one or more groups may be configured to utilize fewer or more CPUs or disk I/O throughput.\n-   **Accounting:**a group's resource usage is monitored and measured.\n-   **Control:**groups of processes can be frozen or stopped and restarted.\n<https://itnext.io/breaking-down-containers-part-0-system-architecture-37afe0e51770>\n\n## POSIX**\n\nThePortable Operating System Interface(POSIX)is a family of[standards](https://en.wikipedia.org/wiki/Standardization)specified by the [IEEE Computer Society](https://en.wikipedia.org/wiki/IEEE_Computer_Society)for maintaining compatibility between[operating systems](https://en.wikipedia.org/wiki/Operating_system). POSIX defines the[application programming interface](https://en.wikipedia.org/wiki/Application_programming_interface)(API), along with command line[shells](https://en.wikipedia.org/wiki/Unix_shell)and utility interfaces, for software compatibility with variants of[Unix](https://en.wikipedia.org/wiki/Unix)and other operating systems.\n<https://en.wikipedia.org/wiki/POSIX>\n\n## Boot Process**\n\n![The Boot Process Boot BIOS Boot L Kerhel (from initramfs) grub2 POST initramfs boot dev Stagel + Stage2 /sys/* fi rmware /proc/sys/* /boot/grub2/grub.cfg grub2-insta11 sysctl dracut grub2-mkconfig 'etc/ sysctl . conf .71 /etc/dracut .conf /etc/grub . d /etc/default/grub Boot Parameters rd. break systemd.unit=rescue.target systemd. unit-emergency. target letc/fstab initrd.target Mounts actual rootfs to /sysroot RO rd. break Pivot kernel rootfs to /sysroot systemd (from f') default. target rescue. target emergency. target sulogin s sinit.target FS Checks Remounts / RW ](media/Unix---Linux-image6.jpg)\n**Distributions**\n\n<https://dev.to/pluralsight/which-distribution-of-linux-should-i-use-51g7>\nUbuntu\n\nFedora\n\nDebianOS\n\nCentOS\n\nGlassFish server by sunmicrosystems\n\nRasbian OS\n**Debian Family**\n\nThe Debian distribution is the upstream for several other distributions, including Ubuntu, Linux Mint and others. Debian is a pure open source project, and focuses on a key aspect: stability. It also provides the largest and most complete software repository to its users.\nUbuntu aims at providing a good compromise between long term stability and ease of use. Since Ubuntu gets most of its packages from Debian's unstable branch, we decided to use Ubuntu as the reference Debian-based distribution for our lab exercises:\n-   Commonly used on both servers and desktops\n-   DPKG-based, uses apt-get and front-ends for installing and updating\n-   Upstream for Ubuntu, Linux Mint and others\n-   Current material based upon the latest release of Ubuntu and should work well with later versions\n-   x86 and x86-64 - Long Term Release (LTS)\n**Red Hat / Fedora Family**\n\nFedora is the community distribution that forms the basis of Red Hat Enterprise Linux, CentOS, Scientific Linux and Oracle Linux. Fedora contains significantly more software than Red Hat's enterprise version. One reason for this is that a diverse community is involved in building Fedora; it is not just one company\nThe Fedora community produces new versions every six months or so. For this reason, we decided to standardize the Red Hat / Fedora part of the course material on the lastest version of CentOS 7, which provides much longer release cycles. Once installed, CentOS is also virtually identical to Red Hat Enterprise Linux (RHEL), which is the most popular Linux distribution in enterprise enviroments:\n-   Current material is based upon the latest release of Red Hat Enterprise Linux (RHEL) - 7.x at the time of publication, and should work well with later versions\n-   Supports x86, x86-64, Itanium, PowerPC and IBM System Z\n-   RPM-based, uses yum (or dnf) to install and update\n-   Long release cycle; targets enterprise server environments\n-   Upstream for CentOS, Scientific Linux and Oracle Linux\n**OpenSUSE Family**\n\nThe relationship between openSUSE and SUSE Linux Enterprise Server is similar to the one we just described between Fedora and Red Hat Enterprise Linux. In this case, however, we decided to use openSUSE as the reference distribution for the openSUSE family, due to the difficulty of obtaining a free version of SUSE Linux Enterprise Server. The two products are extremely similar and material that covers openSUSE can typically be applied to SUSE Linux Enterprise Server with no problem:\n-   Current material is based upon the latest release of openSUSE, and should work well with later versions\n-   RPM-based, uses zypper to install and update\n-   YaST available for administration purposes\n-   x86 and x86-64\n-   Upstream for SUSE Linux Enterprise Server (SLES)\n**New Distribution Similarities**\n\nCurrent treds and changes to the distributions have reduced some of the differences between the distributions\n-   **systemd** (system startup and service management)\n\nsystemd is used by the most common distributions, replacing the SysVinit and Upstart packages. Replaces service and chkconfig commands\n-   **journald** (manages system logs)\n\njournald is a systemd service that collects and stores logging data. It creates and maintains structured, indexed journals based on loggin information that is received from a variety of sources. Depeding on the distribution, text-based system logs may be replaced\n-   **firewalld** (firewall management daemon)\n\nfirewalld provides a dynamically managed firewall with support for network/firewall zones to define the trust level of network connections or interfaces. It has support for IPv4, IPv6 firewall settings and Ethernet bridges. This replaces the iptables configurations\n-   **ip** (network display and configuration tool)\n\nThe ip program is part of the net-tools package, and is designed to be a replacement for the ifconfig command. The ip command will show or manipulate routing, network devices, routing information and tunnels\n<https://www.freecodecamp.org/news/securing-linux-servers-with-se-linux>\n"},{"fields":{"slug":"/Computer-Science/Operating-System/Write-Ahead-Log,-WAL/","title":"Write Ahead Log, WAL"},"frontmatter":{"draft":false},"rawBody":"# Write Ahead Log, WAL\n\nCreated: 2019-06-27 16:51:23 +0500\n\nModified: 2022-05-27 14:29:57 +0500\n\n---\n\nIn[computer science](https://en.wikipedia.org/wiki/Computer_science),write-ahead logging(WAL) is a family of techniques for providing[atomicity](https://en.wikipedia.org/wiki/Atomic_(computer_science))and[durability](https://en.wikipedia.org/wiki/Durability_(database_systems))(two of the[ACID](https://en.wikipedia.org/wiki/ACID)properties) in[database systems](https://en.wikipedia.org/wiki/Database_system). The changes are first recorded in the log, which must be written to stable storage, before the changes are written to the database.\nIn a system using WAL, all modifications are written to a[log](https://en.wikipedia.org/wiki/Database_log)before they are applied. Usually both redo and undo information is stored in the log.\nThe purpose of this can be illustrated by an example. Imagine a program that is in the middle of performing some operation when the machine it is running on loses power. Upon restart, that program might need to know whether the operation it was performing succeeded, succeeded partially, or failed. If a write-ahead log is used, the program can check this log and compare what it was supposed to be doing when it unexpectedly lost power to what was actually done. On the basis of this comparison, the program could decide to undo what it had started, complete what it had started, or keep things as they are.\nWAL allows updates of a database to be done[in-place](https://en.wikipedia.org/wiki/In-place_algorithm). Another way to implement atomic updates is with[shadow paging](https://en.wikipedia.org/wiki/Shadow_paging), which is not in-place. The main advantage of doing updates in-place is that it reduces the need to modify indexes and block lists.\n[ARIES](https://en.wikipedia.org/wiki/Algorithms_for_Recovery_and_Isolation_Exploiting_Semantics)is a popular algorithm in the WAL family.\nModern[file systems](https://en.wikipedia.org/wiki/File_system)typically use a variant of WAL for at least file system[metadata](https://en.wikipedia.org/wiki/Metadata)called[journaling](https://en.wikipedia.org/wiki/Journaling_file_system).\n<https://en.wikipedia.org/wiki/Write-ahead_logging>\n\n## InfluxDB WAL**\n\nThe temporary cache for recently written points. To reduce the frequency with which the permanent storage files are accessed, InfluxDB caches new points in the WAL until their total size or age triggers a flush to more permanent storage. This allows for efficient batching of the writes into the TSM.\nPoints in the WAL can be queried, and they persist through a system reboot. On process start, all points in the WAL must be flushed before the system accepts new writes.\nSome data structures are inherently sequential. For example, a Write Ahead Log used by the databases and filesystems. It is used in order to facilitate durability: changes to the data files are first appended to the log sequentially.\nWhen main storage catches up and records are committed to the data files, commit log segment holding recovery data for it is discarded. If the process dies before the main storage has a chance to catch up, Write Ahead Log is replayed to restore the state database had before restart. If we follow this procedure, data files don't have to be flushed on disk on every operation: operations can be batched together, while still guaranteeing durability. Using Write-Ahead Log significantly reduces amount of writes for both mutable and immutable storage types.\nIt's often advised to use a separate physical device for Write Ahead Log to make sure both memory table flushes and WAL writes are sequential. There are many other reasons to do so, too: to avoid IO saturation, for better failover, more predictable latencies.\n**Batching Writes**\n\nLSM-Trees are using Memory tables, where data is stored before it gets to the main storage, for serving reads and batching writes together. After reaching a size threshold, memory table is written on disk.\nHere, memory table serves as a buffer: read, write and update operations are performed against memory tables, allowing batching a few items together. When data is written on disk, it's done sequentially, in one pass. This amortises a cost of small random writes and converts them into larger sequential allocations on disk, transforming updates of logically unrelated data into physically sequential I/O.\nUnlike Write-Ahead Log (which writes items in the incoming order) Memory Tables pre-sort the data before it reaches disk in order to facilitate sequential read access. Records that are more likely to be read together, are written together.\n<https://medium.com/databasss/on-disk-io-access-patterns-in-lsm-trees-2ba8dffc05f9>\n<https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-1-storage-mechanics>\n\n[Redo, Undo and WAL logs | The Backend Engineering Show](https://youtu.be/uHvR7nOu5m4)\n"},{"fields":{"slug":"/Computer-Science/Others/BioInformatics-BioTechnology/","title":"BioInformatics/BioTechnology"},"frontmatter":{"draft":false},"rawBody":"# BioInformatics/BioTechnology\n\nCreated: 2018-08-09 00:57:24 +0500\n\nModified: 2021-12-13 15:08:42 +0500\n\n---\n\n**PDB - Protein Data Bank**\n\nThe**Protein Data Bank**(**PDB**) is a[crystallographic database](https://en.wikipedia.org/wiki/Crystallographic_database)for the three-dimensional structural data of large biological molecules, such as[proteins](https://en.wikipedia.org/wiki/Protein)and[nucleic acids](https://en.wikipedia.org/wiki/Nucleic_acid). The data, typically obtained by[X-ray crystallography](https://en.wikipedia.org/wiki/X-ray_crystallography),[NMR spectroscopy](https://en.wikipedia.org/wiki/Nuclear_magnetic_resonance_spectroscopy_of_proteins), or, increasingly,[cryo-electron microscopy](https://en.wikipedia.org/wiki/Cryo-electron_microscopy),\n**RCSB**\n\nLed by Helen M. Berman, the Research Collaboratory for Structural Bioinformatics (RCSB) became responsible for the management of thePDBin 1998.\n**PyMOL**\n\nMolecular Visualization System\n**mmCIF**\n\n**Crystallographic Information File**(CIF) is a standard text file format for representing[crystallographic](https://en.wikipedia.org/wiki/Crystallographic)information, promulgated by the[International Union of Crystallography](https://en.wikipedia.org/wiki/International_Union_of_Crystallography)(IUCr)\n\n**mmCIF**, macromolecular CIF, which is intended as an alternative to the[Protein Data Bank (PDB) format](https://en.wikipedia.org/wiki/Protein_Data_Bank_(file_format)). Also closely related is**Crystallographic Information Framework**, a broader system of exchange protocols based on data dictionaries and relational rules expressible in different machine-readable manifestations, including, but not restricted to, Crystallographic Information File and[XML](https://en.wikipedia.org/wiki/XML).\n**References**\n\n<https://en.wikipedia.org/wiki/Protein_Data_Bank>\n\n<https://www.youtube.com/watch?v=jBlTQjcKuaY>\n\n<https://www.freecodecamp.org/news/python-for-bioinformatics-use-machine-learning-and-data-analysis-for-drug-discovery>\n"},{"fields":{"slug":"/Computer-Science/Others/Computer-Graphics/","title":"Computer Graphics"},"frontmatter":{"draft":false},"rawBody":"# Computer Graphics\n\nCreated: 2018-08-21 14:32:26 +0500\n\nModified: 2022-04-03 14:22:24 +0500\n\n---\n\n1.  Rasterization\n\n2.  Ray Tracing\n\n3.  Shader program\n\n4.  Subsurface scattering\n\n5.  BVH Tree Traversals\n\n6.  Ray triangle intersection\n\n7.  Binning\n\n8.  Bounding Voluming Hierarchy\n[TU Wien Rendering / Ray Tracing Course](https://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi) (2 Minute Papers)\nPipeline\n\n1.  Deferred shading\n\n2.  Direct shadows\n\n3.  Lighting\n\n4.  Reflections\n\n5.  Global Illuminations\n\n6.  Ambient Occlusion\n\n7.  Transparency and Transluency\n\n8.  Post processing\n**OpenGL**\n\n**Open Graphics Library**(**OpenGL**)^[[3]](https://en.wikipedia.org/wiki/OpenGL#cite_note-3)[[4]](https://en.wikipedia.org/wiki/OpenGL#cite_note-4)^is a[cross-language](https://en.wikipedia.org/wiki/Language-independent_specification),[cross-platform](https://en.wikipedia.org/wiki/Cross-platform)[application programming interface](https://en.wikipedia.org/wiki/Application_programming_interface)(API) for rendering[2D](https://en.wikipedia.org/wiki/2D_computer_graphics)and[3D](https://en.wikipedia.org/wiki/3D_computer_graphics)[vector graphics](https://en.wikipedia.org/wiki/Vector_graphics). The API is typically used to interact with a[graphics processing unit](https://en.wikipedia.org/wiki/Graphics_processing_unit)(GPU), to achieve[hardware-accelerated](https://en.wikipedia.org/wiki/Hardware_acceleration)[rendering](https://en.wikipedia.org/wiki/Rendering_(computer_graphics)).\n<https://en.wikipedia.org/wiki/OpenGL>\n\n<https://www.opengl.org>\n\n<https://learnopengl.com/Getting-started/OpenGL>\n\n<https://www.freecodecamp.org/news/create-complex-graphics-with-opengl>\n\n<https://www.freecodecamp.org/news/advanced-opengl-animation-technique-skeletal-animations>\n\n## Game Development**\n\n<https://www.techspot.com/article/1916-how-to-3d-rendering-texturing>\n\n<https://www.freecodecamp.org/news/how-to-create-3d-and-2d-graphics-with-opengl-and-cpp>\n"},{"fields":{"slug":"/Computer-Science/Others/Digital-Circuits/","title":"Digital Circuits"},"frontmatter":{"draft":false},"rawBody":"# Digital Circuits\n\nCreated: 2019-09-03 08:29:30 +0500\n\nModified: 2019-09-03 08:31:42 +0500\n\n---\n\n**Karnaugh Maps**\n\nTheKarnaugh map(KMorK-map) is a method of simplifying[Boolean algebra](https://en.wikipedia.org/wiki/Boolean_algebra)expressions.[Maurice Karnaugh](https://en.wikipedia.org/wiki/Maurice_Karnaugh)introduced it in 1953as a refinement of[Edward Veitch](https://en.wikipedia.org/wiki/Edward_Veitch)'s 1952Veitch chart,which actually was a rediscovery of[Allan Marquand](https://en.wikipedia.org/wiki/Allan_Marquand)'s 1881logical diagramakaMarquand diagrambut with a focus now set on its utility for switching circuits. Veitch charts are therefore also known asMarquand--Veitch diagrams,and Karnaugh maps asKarnaugh--Veitch maps(KV maps).\nThe Karnaugh map reduces the need for extensive calculations by taking advantage of humans' pattern-recognition capability.It also permits the rapid identification and elimination of potential[race conditions](https://en.wikipedia.org/wiki/Race_condition).\nThe required Boolean results are transferred from a[truth table](https://en.wikipedia.org/wiki/Truth_table)onto a two-dimensional grid where, in Karnaugh maps, the cells are ordered in[Gray code](https://en.wikipedia.org/wiki/Gray_code),[[6]](https://en.wikipedia.org/wiki/Karnaugh_map#cite_note-Wakerly_1994-6)[[4]](https://en.wikipedia.org/wiki/Karnaugh_map#cite_note-Brown_2012-4)and each cell position represents one combination of input conditions, while each cell value represents the corresponding output value. Optimal groups of 1s or 0s are identified, which represent the terms of a[canonical form](https://en.wikipedia.org/wiki/Canonical_form_(Boolean_algebra))of the logic in the original truth table.These terms can be used to write a minimal Boolean expression representing the required logic.\nKarnaugh maps are used to simplify real-world logic requirements so that they can be implemented using a minimum number of physical logic gates. A[sum-of-products expression](https://en.wikipedia.org/wiki/Sum-of-products_expression)can always be implemented using[AND gates](https://en.wikipedia.org/wiki/AND_gate)feeding into an[OR gate](https://en.wikipedia.org/wiki/OR_gate), and a[product-of-sums expression](https://en.wikipedia.org/wiki/Product-of-sums_expression)leads to OR gates feeding an AND gate.Karnaugh maps can also be used to simplify logic expressions in software design. Boolean conditions, as used for example in[conditional statements](https://en.wikipedia.org/wiki/Conditional_(programming)), can get very complicated, which makes the code difficult to read and to maintain. Once minimised, canonical sum-of-products and product-of-sums expressions can be implemented directly using AND and OR logic operators.Diagrammatic and mechanical methods for minimizing simple logic expressions have existed since at least the medieval times. More systematic methods for minimizing complex expressions began to be developed in the early 1950s, but until the mid to late 1980s the Karnaugh map was the most common used in practice.\nIn many digital circuits and practical problems we need to find expression with minimum variables. We can minimize Boolean expressions of 3, 4 variables very easily using K-map without using any Boolean algebra theorems. K-map can take two forms Sum of Product (SOP) and Product of Sum (POS) according to the need of problem. K-map is table like representation but it gives more information than TRUTH TABLE. We fill grid of K-map with 0's and 1's then solve it by making groups.\n**Steps to solve expression using K-map**\n\n1.  Select K-map according to the number of variables.\n\n2.  Identify minterms or maxterms as given in problem.\n\n3.  For SOP put 1's in blocks of K-map respective to the minterms (0's elsewhere).\n\n4.  For POS put 0's in blocks of K-map respective to the maxterms(1's elsewhere).\n\n5.  Make rectangular groups containing total terms in power of two like 2,4,8 ..(except 1) and try to cover as many elements as you can in one group.\n\n6.  From the groups made in step 5 find the product terms and sum them up for SOP form.\n<https://www.geeksforgeeks.org/k-mapkarnaugh-map>\n\n<https://en.wikipedia.org/wiki/Karnaugh_map>\n\n<https://www.tutorialspoint.com/digital_circuits/digital_circuits_k_map_method.htm>\n"},{"fields":{"slug":"/Computer-Science/Others/Game-Development/","title":"Game Development"},"frontmatter":{"draft":false},"rawBody":"# Game Development\n\nCreated: 2021-03-31 20:04:40 +0500\n\nModified: 2022-11-23 22:57:58 +0500\n\n---\n\n<https://docs.microsoft.com/en-us/archive/msdn-magazine/2015/march/game-development-a-web-game-in-an-hour>\n\n<https://www.freecodecamp.org/news/code-an-endless-runner-game-using-unreal-engine-and-c>\n<https://developer.mozilla.org/en-US/docs/Games>\n<https://www.freecodecamp.org/news/create-a-arcade-style-shooting>\n\n## Tools**\n-   Blender\n\n<https://www.blender.org>-   Agones\n\nHost, Run and Scale dedicated game servers on Kubernetes\n\n<https://agones.dev/site>\n\n<https://github.com/googleforgames/agones>\n<https://www.youtube.com/watch?v=lCUs89nAZZA>\n\n<https://games.withgoogle.com>\n\n<https://www.freecodecamp.org/news/code-a-2d-game-engine-using-java>\n\n## Finite State Machine**\n\nA finite-state machine (FSM) is a model of computation. In an FSM, just one of a finite number of hypothetical states can be active at any given time.\nIn the competitive world of gaming, developers strive to offer an entertaining user experience for those who interact with the non-player characters (NPCs) that we create. Developers can deliver this interactivity by using finite-state machines (FSMs) to create[AI](https://www.toptal.com/artificial-intelligence)solutions that simulate intelligence in our NPCs.\nAI trends have shifted to behavioral trees, but FSMs remain relevant. They're incorporated---in one capacity or another---into virtually every electronic game.\n<https://www.toptal.com/unity-unity3d/unity-ai-development-finite-state-machine-tutorial>\n"},{"fields":{"slug":"/Computer-Science/Others/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2018-05-03 22:40:45 +0500\n\nModified: 2022-04-09 13:11:28 +0500\n\n---\n\n**Gamification**\n\nThe use of game elements and game design techniques in non-game contexts\n\nCommon game elements\n\n| **Game element** | **Definition**                                                                            |\n|---------------|---------------------------------------------------------|\n| Feedback         | Immediate notification that keep users constantly aware of progress or failures           |\n| Goals            | Activity goals that are adapted as challenges for the user                                |\n| Badges           | Optional rewards and goals outside the scope of a service's core activities               |\n| Point system     | Reward for completing actions (that is, a numeric value that's added to the total points) |\n| Leaderboard      | Tracking and displaying desired actions to drive desired behavior through competition     |\n| User levels      | Indication of the user's proficiency in the overall gaming experience over time           |\nTwo types of users -\n\n1.  **Achievers -** are eager to complete the many challenges with which they're confronted.\n\n2.  **Explorers -** trive to find hid- den items they can likely collect by exploring a software system's differ- ent areas\n**Performance Modelling**\n\n![performance modeling model as real-world system theoretical model analyze results translate back * makes assumptions about the system: request arrival rate, service order, times. cannot apply the results if your system does not satisfy them! ](media/Others-image1.png)\n![a single server open, closed queueing systems utilization law, the P-K formula, Little's law CoDeI, adaptive LIFO a cluster of many servers the USL scaling bottlenecks stepping back the role of performance modeling ](media/Others-image2.png)\n![model the web server as a queueing system. web server queueing delay + service time = response time assumptions 1. requests are independent and random, arrive at some \"arrival rate\". 2. requests are processed one at a time, in FIFO order; requests queue if server is busy (\"queueing delay\"). ](media/Others-image3.png)\n![\"What's the maximum throughput of this server?\" i.e. given a response time target arrival rate increases Utilization law server utilization increases linearly P-K formula P(request has to queue) increases, so mean queue length increases, so mean queueing delay increases. ](media/Others-image4.png)\n![Pollaczek-l<hinchine (P-K) formula U * (mean service time) * (service time variability)2 mean queueing delay = assuming constant service time and so, request sizes: mean queueing delay (queueing delay + service time) since response time queueing delay Q) utilization (U) Q) utilization (U) ](media/Others-image5.png)\n**Software defined radio**\n\n**WHAT IS SOFTWARE-DEFINED RADIO USED FOR?**\n\nSoftware-defined radio can be used for all traditional radio applications, but much more flexibly. This allows for the prototyping and development of next-generation radio applications. In this article, though, we use an RTL-SDR for simply receiving narrow-band FM signals.\n**HOW DOES SOFTWARE-DEFINED RADIO WORK?**\n\nA software-defined radio works by implementing traditionally hardware components in software on a personal computer or embedded system.\n**WHAT IS A WAVEFORM IN SOFTWARE-DEFINED RADIO?**\n\nA waveform is a representation of a wave over a time period. In software-defined radio, it is the same as a waveform in regular radio. It can be used to represent the sound being modulated or demodulated, the signal being transmitted, radiated, or received, or any other waves in the process.\n**WHAT IS SDR IN HAM RADIO?**\n\nSDR can be used in ham radio like any other radio application. Amateur radio is a hobby about exploration, and so hams can use the SDR as a receiver, or as a transmitter. They can even use it to create completely new radio applications.\n**WHAT IS RTL IN RTL-SDR?**\n\nRTL is short for RTL2832U. The Realtek RTL2832U chipset was a popular choice for digital video broadcast (DVB-T) receivers, whose original purpose was to receive video. It was discovered that these could be hacked and turned into wideband SDR receivers. These devices have come to be known as the RTL-SDR.\n**CAN YOU TRANSMIT WITH SDR?**\n\nIt depends on the SDR. If you have the right hardware, yes you can, but not with the RTL-SDR. Some transmission capable SDRs include the HackRF, PlutoSDR, LimeSDR, LimeSDR Mini, and more.\n<https://www.toptal.com/software/software-defined-radio-tutorial-rtl-sdr>\n\n## Virtual Reality (VR / WebVR / AR - Augmented realityÆ’)**\n\n[https://www.toptal.com/virtual-reality/webvr-and-edge-computing](https://www.toptal.com/virtual-reality/webvr-and-edge-computing?)\n\n<https://www.gather.town>\n\n## GIS (Geographic Information System)**\n\nAGeographic Information System(GIS) is a system designed to capture, store, manipulate, analyze, manage, and present spatial or[geographic data](https://en.wikipedia.org/wiki/Geographic_data_and_information). GIS applications are tools that allow users to create interactive queries (user-created searches), analyze spatial information, edit data in maps, and present the results of all these operations.GIS (more commonly GIScience) sometimes refers to[geographic information science (GIScience)](https://en.wikipedia.org/wiki/Geographic_information_science), the science underlying geographic concepts, applications, and systems.Since the mid-1980s, geographic information systems have become valuable tool used to support a variety of city and regional planning functions.\nGIS can refer to a number of different technologies, processes, techniques and methods. It is attached to many operations and has many applications related to engineering, planning, management, transport/logistics, insurance, telecommunications, and business.For that reason, GIS and[location intelligence](https://en.wikipedia.org/wiki/Location_intelligence)applications can be the foundation for many location-enabled services that rely on analysis and visualization.\nGIS can relate unrelated information by using location as the key index variable. Locations or extents in the Earth[space--time](https://en.wikipedia.org/wiki/Space%E2%80%93time)may be recorded as dates/times of occurrence, and x, y, and z[coordinates](https://en.wikipedia.org/wiki/Coordinate)representing,[longitude](https://en.wikipedia.org/wiki/Longitude),[latitude](https://en.wikipedia.org/wiki/Latitude), and[elevation](https://en.wikipedia.org/wiki/Elevation_(geography)), respectively. All Earth-based spatial--temporal location and extent references should be relatable to one another and ultimately to a \"real\" physical location or extent. This key characteristic of GIS has begun to open new avenues of scientific inquiry.\n<https://en.wikipedia.org/wiki/Geographic_information_system>\n\n## Supercomputer Architecture**\n\nApproaches to**supercomputer architecture**have taken dramatic turns since the earliest systems were introduced in the 1960s. Early[supercomputer](https://en.wikipedia.org/wiki/Supercomputer)architectures pioneered by[Seymour Cray](https://en.wikipedia.org/wiki/Seymour_Cray)relied on compact innovative designs and local[parallelism](https://en.wikipedia.org/wiki/Parallel_computing)to achieve superior computational peak performance.[^[1]^](https://en.wikipedia.org/wiki/Supercomputer_architecture#cite_note-chen-1)However, in time the demand for increased computational power ushered in the age of[massively parallel](https://en.wikipedia.org/wiki/Massively_parallel)systems.\n<https://en.wikipedia.org/wiki/Supercomputer_architecture>\n\n"},{"fields":{"slug":"/Computer-Science/Others/Tech-Books/","title":"Tech Books"},"frontmatter":{"draft":false},"rawBody":"# Tech Books\n\nCreated: 2019-08-11 18:50:30 +0500\n\nModified: 2022-03-06 18:41:25 +0500\n\n---\n\nNetworks, Crowds, and Markets: Reasoning about a Highly Connected World\n-   David Easley\n-   Jon Kleinberg\n**Tech Books**\n-   Algorithms\n-   Beej's Guide to Network Programming [https://beej.us/guide/bgnet/html//index.html](https://beej.us/guide/bgnet/html/index.html)\n-   Building secure and reliable systems\n-   Clean Code\n-   **Database Internals - <https://www.databass.dev>**\n-   Design and Build Great Web APIs - Mike Amundsen\n-   **Designing Data-Intensive Applications by Martin Kleppmann**\n-   **Designing Distributed systems - Brendan Burns**\n-   **Distributed Programming - <http://www.distributedprogramming.net/index.shtml>**\n-   **Distributed systems for fun and profit**\n-   **Code Complete by Steve McConnell**\n-   **Coders at work**\n-   **Communicating Sequential Processes - C.A.R. Hoare**\n-   **Cracking the coding interview**\n-   **Closure -** <https://www.braveclojure.com>\n-   **Elasticsearch the definitive guide**\n-   **Google SRE - <https://landing.google.com/sre/sre-book/toc/index.html>**\n-   **Grokking Algorithms - <https://www.manning.com/books/grokking-algorithms>**\n-   **Hacker's Delight by Henry S. Waren**\n-   **High performance mysql -** <https://www.highperfmysql.com>\n-   <https://www.immutablearchitecture.com>\n-   **Inside the machines by Jon Stokes**\n-   **Introduction to Reliable and Secure Distributed Programming by Christian Cachin**\n-   **Kubernetes in action**\n-   **Kubernetes up and running**\n-   **Practical Grpc**\n-   **Pragmatic Programmer**\n-   **SciPy Lectures**\n-   **Site Reliability Engineering**\n-   **Site Reliability Workbook**\n-   **The Clean Coder**\n-   **The mythical man-month**\n-   **The self taught programmer**\n-   **Wireless Networking <http://wndw.net/book.html>**\n-   **Writing an Interpreter/Compiler in Go**\n-   **You don't know js <https://github.com/getify/You-Dont-Know-JS>**\n-   \n-   A book of Abstract Algebra\n-   Introduction to Statistical Learning With Application in R, by Gareth James\n-   Deep Learning with Python by Francois Chollet\n-   SciPy Lectures\n-   Master Machine Learning Algorithms\n-   Statistics in Plain English\n-   <https://leanpub.com/insidethepythonvirtualmachine/read>\n-   <https://leanpub.com/clean-architectures-in-python>\n-   Don't Make Me Think, Revisited: A Common Sense Approach to Web Usability (Voices That Matter)\n-   Lean UX: Applying Lean Principles to Improve User Experience by Jeff Gothelf & Josh Seiden\n-   Building Microservices: Defining Fine-Grained Systems\n-   Head first Design pattern\n-   The C++ Programming Language 4th edition\n-   Accelerated C++\n-   Scaling-python <https://scaling-python.com>\n-   The Programmer's Guide To Theory: Great ideas explained by Dr. Mike James\n-   Zookeeper - Distributed Process Coordination by Benjamin Reed\n[32 Book Recommendations for the Holidays â€¢ Various Speakers â€¢ GOTO 2021](https://youtu.be/Pg698WXPtYw)\nğŸ“• Statistics Think Stats --\n\nProbability and Statistics [**https://lnkd.in/gjAs_s9**](https://lnkd.in/gjAs_s9)\n\nStatistical Inference for Data Science [**https://lnkd.in/grU8ep7**](https://lnkd.in/grU8ep7)\n\nThink Bayes -- Bayesian Statistics Made Simple [**https://lnkd.in/gW_ebEa**](https://lnkd.in/gW_ebEa)\nğŸ“— Machine Learning\n\nAn Introduction to Statistical Learning [**https://lnkd.in/gqQkbcn**](https://lnkd.in/gqQkbcn) The Elements of Statistical Learning [**https://lnkd.in/g78kwBp**](https://lnkd.in/g78kwBp)\n\nMachine Learning Yearning [**http://www.mlyearning.org/**](http://www.mlyearning.org/)\n\nDeep Learning [**https://lnkd.in/g6HDwN5**](https://lnkd.in/g6HDwN5)\nğŸ“˜ Data Science\n\nData Jujitsu by DJ Patil [**https://lnkd.in/gMS2tyA**](https://lnkd.in/gMS2tyA)\n\nData Science for Business [**https://lnkd.in/g5H7G2b**](https://lnkd.in/g5H7G2b)\n\nR for Data Science [**http://r4ds.had.co.nz/**](http://r4ds.had.co.nz/)\n\nPython Data Science Handbook [**https://lnkd.in/gHWXixJ**](https://lnkd.in/gHWXixJ)\nğŸ“™ Programming\n\nAutomate the Boring Stuff With Python [**https://lnkd.in/gzNdUAb**](https://lnkd.in/gzNdUAb)\n\nR For Beginners [**https://lnkd.in/gzz-niK**](https://lnkd.in/gzz-niK)\nğŸ“’ Other\n\nNatural Language Processing with Python [**https://lnkd.in/gmuQcmt**](https://lnkd.in/gmuQcmt)\n\nThe Data Science Handbook -- Advice & Insights from Data Scientists [**https://lnkd.in/g8t7hk9**](https://lnkd.in/g8t7hk9)\n**Mathematics**\n\n<https://medium.com/however-mathematics/13-classic-mathematics-books-for-lifelong-learners-7ec2759142da>\n<https://www.freecodecamp.org/news/learn-the-history-of-the-internet-in-dr-chucks>\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Cohesion-and-Coupling/","title":"Cohesion and Coupling"},"frontmatter":{"draft":false},"rawBody":"# Cohesion and Coupling\n\nCreated: 2018-08-02 23:52:40 +0500\n\nModified: 2021-10-24 15:35:34 +0500\n\n---\n\n**COUPLING**\n\n**An indication of the strength of interconnections between program units.**\nHighly coupled have program units dependent on each other. Loosely coupled are made up of units that are independent or almost independent.\nModules are independent if they can function completely without the presence of the other. Obviously, can't have modules completely independent of each other. Must interact so that can produce desired outputs. The more connections between modules, the more dependent they are in the sense that more info about one modules is required to understand the other module.\nThree factors: number of interfaces, complexity of interfaces, type of info flow along interfaces.\n\nWant to minimize number of interfaces between modules, minimize the complexity of each interface, and control the type of info flow. An interface of a module is used to pass information to and from other modules.\nIn general, modules tightly coupled if they use shared variables or if they exchange control info.\n\nLoose coupling if info held within a unit and interface with other units via parameter lists. Tight coupling if shared global data.\nIf need only one field of a record, don't pass entire record. Keep interface as simple and small as possible.\nTwo types of info flow: data or control.\n-   Passing or receiving back control info means that the action of the module will depend on this control info, which makes it difficult to understand the module.\n-   Interfaces with only data communication result in lowest degree of coupling, followed by interfaces that only transfer control data. Highest if data is hybrid.\nRanked highest to lowest:\n\n1.  Content coupling: if one directly references the contents of the other.\n    When one module modifies local data values or instructions in another module. (can happen in assembly language)\n    if one refers to local data in another module.\n    if one branches into a local label of another.\n\n2.  Common coupling: access to global data.\n    modules bound together by global data structures.\n\n3.  Control coupling: passing control flags (as parameters or globals) so that one module controls the sequence of processing steps in another module.\n\n4.  Stamp coupling: similar to common coupling except that global variables are shared selectively among routines that require the data. E.g., packages in Ada. More desirable than common coupling because fewer modules will have to be modified if a shared data structure is modified. Pass entire data structure but need only parts of it.\n\n5.  Data coupling: use of parameter lists to pass data items between routines.\n**COHESION**\n\n**Measure of how well module fits together.**\n\nA component should implement a single logical function or single logical entity. All the parts should contribute to the implementation.\n\nCoupling, in the computer science lingo, means to link two parts of a system, so that changes in one of them directly affect the other one, and is usually avoided as much as possible.\n**Many levels of cohesion**\n\n1.  Coincidental cohesion: the parts of a component are not related but simply bundled into a single component.\n    harder to understand and not reusable.\n\n2.  Logical association: similar functions such as input, error handling, etc. put together. Functions fall in same logical class. May pass a flag to determine which ones executed.\n    interface difficult to understand. Code for more than one function may be intertwined, leading to severe maintenance problems. Difficult to reuse\n\n3.  Temporal cohesion: all of statements activated at a single time, such as start up or shut down, are brought together. Initialization, clean up.\n    Functions weakly related to one another, but more strongly related to functions in other modules so may need to change lots of modules when do maintenance.\n\n4.  Procedural cohesion: a single control sequence, e.g., a loop or sequence of decision statements. Often cuts across functional lines. May contain only part of a complete function or parts of several functions.\n    Functions still weakly connected, and again unlikely to be reusable in another product.\n\n5.  Communicational cohesion: operate on same input data or produce same output data. May be performing more than one function. Generally acceptable if alternate structures with higher cohesion cannot be easily identified.\n    still problems with reusability.\n\n6.  Sequential cohesion: output from one part serves as input for another part. May contain several functions or parts of different functions.\n\n7.  Informational cohesion: performs a number of functions, each with its own entry point, with independent code for each function, all performed on same data structure. Different than logical cohesion because functions not intertwined.\n\n8.  Functional cohesion: each part necessary for execution of a single function. e.g., compute square root or sort the array.\n    Usually reusable in other contexts. Maintenance easier.\n\n9.  Type cohesion: modules that support a data abstraction.\n    Not strictly a linear scale. Functional much stronger than rest while first two much weaker than others. Often many levels may be applicable when considering two elements of a module. Cohesion of module considered as highest level of cohesion that is applicable to all elements in the module.\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Dependency-Injection/","title":"Dependency Injection"},"frontmatter":{"draft":false},"rawBody":"# Dependency Injection\n\nCreated: 2018-04-30 16:14:08 +0500\n\nModified: 2021-04-15 11:20:24 +0500\n\n---\n\nIn[software engineering](https://en.m.wikipedia.org/wiki/Software_engineering),dependency injection is a technique whereby one object (or static method) supplies the dependencies of another object. A dependency is an object that can be used (a[service](https://en.m.wikipedia.org/wiki/Service_(systems_architecture))). An injection is the passing of a dependency to a dependent object (a[client](https://en.m.wikipedia.org/wiki/Client_(computing))) that would use it. The service is made part of the client's[state](https://en.m.wikipedia.org/wiki/State_(computer_science)).Passing the service to the client, rather than allowing a client to build or[find the service](https://en.m.wikipedia.org/wiki/Service_locator_pattern), is the fundamental requirement of the pattern.\n**Python Dependency Injection**\n\nAn immediate issue with the code above is thatdraw_squaredepends on a global variable. This has[lots of bad consequences](http://wiki.c2.com/?GlobalVariablesAreBad), and there are two easy ways to fix it. The first would be fordraw_squareto create theTurtleinstance itself (which I'll discuss later). This might not be desirable if we want to use a singleTurtle for all our drawing. So for now, we'll simply use parameterization again to maketurtlea parameter todraw_square:\n\n```\nfrom turtle import Turtle\n\ndef draw_square(turtle, size):\nfor i in range(0, 4):\nturtle.forward(size)\nturtle.left(90)\n\nturtle = Turtle()\ndraw_square(turtle, 100)\n```\n\nThis has a fancy name---dependency injection. It just means that if a function needs some kind of object to do its work, likedraw_squareneeds aTurtle, the caller is responsible for passing that object in as a parameter. No, really, if you were ever curious about Python dependency injection, this is it.\n<https://www.toptal.com/python/python-parameterized-design-patterns>\n\n**Like I'm Five**\n\n**Traditional**: going to a party, bring your own drinks.\n**Inversion of control**: going to a party, there is an ice chest full of drinks, and there's a guy handing out the drinks from the ice chest. You only get to drink what's been handed to you.\n**Dependency injection:** going to a party, and there's an open bar, the bartender makes your drinks, and servers brings you your drinks. You don't know what you're drinking, but it tastes good.\n\nDependency injectionisa fancy kind of Inversion of Control, but also has a bartender & servers.\n\n**Others**\n\n<https://github.com/google/guice>\n\nGuice is a lightweight dependency injection framework for Java 6 and above.\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Inversion-of-Control/","title":"Inversion of Control"},"frontmatter":{"draft":false},"rawBody":"# Inversion of Control\n\nCreated: 2018-04-30 16:00:48 +0500\n\nModified: 2018-12-02 23:29:52 +0500\n\n---\n\nIn[software engineering](https://en.m.wikipedia.org/wiki/Software_engineering),inversion of control(IoC) is a design principle in which custom-written portions of a[computer program](https://en.m.wikipedia.org/wiki/Computer_program)receive the[flow of control](https://en.m.wikipedia.org/wiki/Control_flow)from a generic[framework](https://en.m.wikipedia.org/wiki/Software_framework). A[software architecture](https://en.m.wikipedia.org/wiki/Software_architecture)with this design inverts control as compared to traditional[procedural programming](https://en.m.wikipedia.org/wiki/Procedural_programming): in traditional programming, the custom code that expresses the purpose of the program[calls](https://en.m.wikipedia.org/wiki/Function_call#Main_concepts)into reusable libraries to take care of generic tasks, but with inversion of control, it is the framework that calls into the custom, or task-specific, code.\nThe general concept is also related to[event-driven programming](https://en.m.wikipedia.org/wiki/Event-driven_programming)in that it is often implemented using IoC, so that the custom code is commonly only concerned with the handling of events, whereas the[event loop](https://en.m.wikipedia.org/wiki/Event_loop)and dispatch of events/messages is handled by the framework or the runtime environment.\n**See Also**\n-   Dependency Injection\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Marshalling/","title":"Marshalling"},"frontmatter":{"draft":false},"rawBody":"# Marshalling\n\nCreated: 2018-04-10 00:34:22 +0500\n\nModified: 2018-04-30 22:43:24 +0500\n\n---\n\n**Marshalling**is the process of transforming the memory representation of an[object](https://en.wikipedia.org/wiki/Object_(computer_science))to a data format suitable for storage or transmission, and it is typically used when data must be moved between different parts of a computer program or from one program to another. Marshalling is similar to[serialization](https://en.wikipedia.org/wiki/Serialization)and is used to communicate to remote objects with an object, in this case a serialized object. It simplifies complex communication, using composite objects in order to communicate instead of*primitives*. The inverse, of marshalling is called [*unmarshallin*](https://en.wikipedia.org/wiki/Unmarshalling)g (or *demarshalling*, similar to *deserialization*).\n**Difference between Marshalling and Serialization**\n\nSerialization puts objects in a binary form without regard to data-types. Marshalling will transform the data-type into a predetermined naming convention so that it can be reconstructed with respect to the initial data-type.\nSerialization is almost like a stupid memory-dump of the memory used by the object(s). There could be some transient data in there, and serialization would not be able to pick that up and make some sense out of this without further effort on your part.\nTypically, marshalling touches primitive and known data-types as a freebie, but the most complex data-types (such as your classes) requires that you hint the interpreter on how to marshall and unmarshall.\nI could also add that serialization may have some issues related to big-endian, small-endian if the stream is going from one OS to another if the different OS have different means of representing the same data. On the other hand, marshalling is perfectly fine to migrate between OS because the result is a higher-level representation.\nIn regards to commonality between marshalling and serialization, they both have in common to allow streaming of a representation of an object or a hierarchy of objects to typically be put in a medium (file, memory) for the reverse process to restore the initial object or object hierarchy.\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Metaprogramming/","title":"Metaprogramming"},"frontmatter":{"draft":false},"rawBody":"# Metaprogramming\n\nCreated: 2017-11-06 16:53:55 +0500\n\nModified: 2017-11-06 16:59:59 +0500\n\n---\n\nMetaprogramming is a programming technique in which computer programs have the ability to treat programs as data. It means a program can be designed to read, generate, analyze or transform other programs, or even itself while running. In some cases, it allows programmers to minimize the number of lines of code to express a solution, thus reducing the development time. It also allows programs greater flexibility to efficiently handle new situations without recompilation.\nMetaprogramming can be used to move computations from run-time to compile-time, to generate code using compile time computations, and to enable[self-modifying code](https://en.wikipedia.org/wiki/Self-modifying_code). The language in which the metaprogram is written is called the[**metalanguage**](https://en.wikipedia.org/wiki/Metalanguage). The language of the programs that are manipulated is called the[*object language*](https://en.wikipedia.org/wiki/Object_language). The ability of a programming language to be its own metalanguage is called[*reflection*](https://en.wikipedia.org/wiki/Reflection_(computer_science))or*reflexivity*. Reflection is a valuable language feature to facilitate metaprogramming.\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2018-05-11 23:48:26 +0500\n\nModified: 2022-10-12 11:11:42 +0500\n\n---\n\n**Relationships**\n\ninheritance (is-a relationship)\n\naggregation (has-a relationship)\n**Loitering**\n\nHolding the reference to an object, when it is no longer needed.\n**Reentrency**\n\nIn computing, a computer program or subroutine is called reentrant if it can be interrupted in the middle of its execution and then safely be called again before its previous invocations complete execution. The interruption could be caused by an internal action such as a jump or call, or by an external action such as an interrupt or signal. Once the reentered invocation completes, the previous invocations will resume correct execution.\n**Fuzzing**\n\nFuzzing is a technique for amplifying race conditions.\n\nIn this technique Thread.sleep is added before and after every statement.\n**Conway's Law**\n\nAny organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure.\n\nAny piece of software reflects the organizational structure that produced it.\n\nIf you have four teams working on a compiler you will end up with a four pass compiler\n**Semantic Versioning (SemVer)**\n\nGiven a version number MAJOR.MINOR.PATCH, increment the:\n\na.  MAJOR version when you make incompatible API changes,\n\nb.  MINOR version when you add functionality in a backwards-compatible manner, and\n\nc.  PATCH version when you make backwards-compatible bug fixes.\n\n<https://semver.org>\n\n## Calendar Versioning (CalVer)**\n\nCalVer is a versioning convention based on your project's release calendar, instead of arbitrary numbers.\n\n**YYYY.MINOR.MICRO -** 2020.1.1\n\n<https://calver.org>\n<https://www.wikiwand.com/en/Software_versioning>\n\n## What is Dynamic Binding (late binding)?**\n\nBinding refers to the linking of a procedure call to the code to be executed in response to the call. Dynamic binding means that the code associated with a given procedure call is not known until the time of the call at run-time.\nLate binding,dynamic binding, ordynamic linkageis a computer programming mechanism in which the method being called upon an object or the function being called with arguments is looked up by name at[runtime](https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)).\nWith[early binding](https://en.wikipedia.org/wiki/Early_binding), or[static binding](https://en.wikipedia.org/wiki/Static_binding), in an[object-oriented language](https://en.wikipedia.org/wiki/Object-oriented_programming), the compilation phase fixes all types of variables and expressions. This is usually stored in the compiled program as an offset in a[virtual method table](https://en.wikipedia.org/wiki/Virtual_method_table)(\"v-table\") and is very efficient. With late binding the compiler does not read enough information to verify the method exists or bind its slot on the v-table. Instead the method is looked up by name at runtime.\nThe primary advantage of using late binding in[Component Object Model](https://en.wikipedia.org/wiki/Component_Object_Model)(COM) programming is that it does not require the compiler to reference the libraries that contain the object at[compile time](https://en.wikipedia.org/wiki/Compile_time). This makes the compilation process more resistant to version conflicts, in which the class's v-table may be accidentally modified. (This is not a concern in[JIT](https://en.wikipedia.org/wiki/Just-in-time_compilation)-compiled platforms such as .NET or Java, because the v-table is created at runtime by the[virtual machine](https://en.wikipedia.org/wiki/Virtual_machine)against the libraries as they are being loaded into the running application.)\n<https://en.wikipedia.org/wiki/Late_binding>\n\n## Virtual Method Table**\n\nAvirtual method table(VMT),virtual function table,virtual call table,[dispatch table](https://en.wikipedia.org/wiki/Dispatch_table),vtable, orvftableis a mechanism used in a[programming language](https://en.wikipedia.org/wiki/Programming_language)to support[dynamic dispatch](https://en.wikipedia.org/wiki/Dynamic_dispatch)(or[run-time](https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase))[method](https://en.wikipedia.org/wiki/Method_(computer_programming))[binding](https://en.wikipedia.org/wiki/Name_binding)).\nWhenever a class defines a[virtual function](https://en.wikipedia.org/wiki/Virtual_function)(or method), most compilers add a hidden member variable to the class that points to an array of pointers to (virtual) functions called the virtual method table. These pointers are used at runtime to invoke the appropriate function implementations, because at compile time it may not yet be known if the base function is to be called or a derived one implemented by a class that inherits from the base class.\nThere are many different ways to implement such dynamic dispatch, but use of virtual method tables is especially common among[C++](https://en.wikipedia.org/wiki/C%2B%2B)and related languages (such as[D](https://en.wikipedia.org/wiki/D_(programming_language))and[C#](https://en.wikipedia.org/wiki/C_Sharp_(programming_language))). Languages that separate the programmatic interface of objects from the implementation, like[Visual Basic](https://en.wikipedia.org/wiki/Visual_Basic)and[Delphi](https://en.wikipedia.org/wiki/Object_Pascal), also tend to use this approach, because it allows objects to use a different implementation simply by using a different set of method pointers.\nSuppose a program contains three[classes](https://en.wikipedia.org/wiki/Class_(computer_programming))in an[inheritance](https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming))hierarchy: a[superclass](https://en.wikipedia.org/wiki/Superclass_(computer_science)),Cat, and two[subclasses](https://en.wikipedia.org/wiki/Subclass_(computer_science)),HouseCatandLion. ClassCatdefines a[virtual function](https://en.wikipedia.org/wiki/Virtual_function)namedspeak, so its subclasses may provide an appropriate implementation (e.g. eithermeoworroar). When the program calls thespeakfunction on aCatreference (which can refer to an instance ofCat, or an instance ofHouseCatorLion), the code must be able to determine which implementation of the function the call should bedispatchedto. This depends on the actual class of the object, not the class of the reference to it (Cat). The class cannot generally be determinedstatically(that is, at[compile time](https://en.wikipedia.org/wiki/Compile_time)), so neither can the compiler decide which function to call at that time. The call must be dispatched to the right functiondynamically(that is, at[run time](https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase))) instead.\n<https://en.wikipedia.org/wiki/Virtual_method_table>\n\n## Dynamic Dispatch**\n\nIn[computer science](https://en.wikipedia.org/wiki/Computer_science),dynamic dispatchis the process of selecting which implementation of a[polymorphic](https://en.wikipedia.org/wiki/Polymorphism_(computer_science))operation ([method](https://en.wikipedia.org/wiki/Method_(computer_programming))or function) to call at[run time](https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)). It is commonly employed in, and considered a prime characteristic of,[object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming)(OOP) languages and systems.\nObject-oriented systems model a problem as a set of interacting objects that enact operations referred to by name. Polymorphism is the phenomenon wherein somewhat interchangeable objects each expose an operation of the same name but possibly differing in behavior. As an example, aFileobject and aDatabaseobject both have aStoreRecordmethod that can be used to write a personnel record to storage. Their implementations differ. A program holds a reference to an object which may be either aFileobject or aDatabaseobject. Which it is may have been determined by a run-time setting, and at this stage, the program may not know or care which. When the program callsStoreRecordon the object, something needs to choose which behavior gets enacted. If one thinks of OOP as[sending messages](https://en.wikipedia.org/wiki/Message_passing)to objects, then in this example the program sends aStoreRecordmessage to an object of unknown type, leaving it to the run-time support system to dispatch the message to the right object. The object enacts whichever behavior it implements.\nDynamic dispatch contrasts withstatic dispatch, in which the implementation of a polymorphic operation is selected at[compile time](https://en.wikipedia.org/wiki/Compile_time). The purpose of dynamic dispatch is to defer the selection of an appropriate implementation until the run time type of a parameter (or multiple parameters) is known.\nDynamic dispatch is different from[late binding](https://en.wikipedia.org/wiki/Late_binding)(also known as dynamic binding).[Name binding](https://en.wikipedia.org/wiki/Name_binding)associates a name with an operation. A polymorphic operation has several implementations, all associated with the same name. Bindings can be made at compile time or (with late binding) at run time. With dynamic dispatch, one particular implementation of an operation is chosen at run time. While dynamic dispatch does not imply late binding, late binding does imply dynamic dispatch, since the implementation of a late-bound operation is not known until run time.\n<https://en.wikipedia.org/wiki/Dynamic_dispatch>\n\n## Guard Clauses Technique**\n\n[Nesting \"If Statements\" Is Bad. Do This Instead.](https://www.youtube.com/shorts/Zmx0Ou5TNJs)\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Programming-Styles/","title":"Programming Styles"},"frontmatter":{"draft":false},"rawBody":"# Programming Styles\n\nCreated: 2018-03-26 19:47:24 +0500\n\nModified: 2022-07-18 09:32:08 +0500\n\n---\n\n**duck-typing**\n\nA programming style which does not look at an object's type to determine if it has the right interface; instead, the method or attribute is simply called or used (\"If it looks like a duck and quacks like a duck, it must be a duck.\") By emphasizing interfaces rather than specific types, well-designed code improves its flexibility by allowing polymorphic substitution. Duck-typing avoids tests using[type()](http://library/functions.html)or[isinstance()](http://library/functions.html). (Note, however, that duck-typing can be complemented with[abstract base classes](NULL).) Instead, it typically employs[hasattr()](http://library/functions.html)tests or[EAFP](NULL)programming.\n\"When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck\"\n**EAFP**\n\nEasier to ask for forgiveness than permission. This common Python coding style assumes the existence of valid keys or attributes and catches exceptions if the assumption proves false. This clean and fast style is characterized by the presence of many[try](http://reference/compound_stmts.html)and[except](http://reference/compound_stmts.html)statements. The technique contrasts with the[LBYL](NULL)style common to many other languages such as C.\n**LBYL**\n\nLook before you leap. This coding style explicitly tests for pre-conditions before making calls or lookups. This style contrasts with the[EAFP](NULL)approach and is characterized by the presence of many[if](http://reference/compound_stmts.html)statements.\n\nIn a multi-threaded environment, the LBYL approach can risk introducing a race condition between \"the looking\" and \"the leaping\". For example, the code,ifkeyinmapping:returnmapping[key]can fail if another thread removeskeyfrommappingafter the test, but before the lookup. This issue can be solved with locks or by using the EAFP approach.\n**homoiconicity (noun)**\n\nThe property of a programming language whereby there is no distinction between code and the data on which a program is operating.\n\n<https://www.toptal.com/julia/code-writing-code-modern-metaprogamming>\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Software-Coding---Development-Engineering/","title":"Software/Coding / Development/Engineering"},"frontmatter":{"draft":false},"rawBody":"# Software/Coding / Development/Engineering\n\nCreated: 2018-05-01 20:28:03 +0500\n\nModified: 2022-09-29 00:05:52 +0500\n\n---\n-   **Learning and Discovery -** Iteration, Feedback, Incrementalism, Experimentation and Empiricism\n-   **Managing Complexity -** Modularity, Information Hiring, Seperation of Concerns, Loose-Coupling, Cohesion\n<https://dont.build>\nMaking great software is more about managing complexity and thinking through details than it is about design or pretty pixels. One of the biggest misunderstandings of our era.\n**Research first code later**\n\n**Principles of sofware design**\n\n**YAGNI - You Ain't Gonna Need It. (For new features)**\n\n\"You aren't gonna need it\"(YAGNI)is a principle of[extreme programming](https://en.wikipedia.org/wiki/Extreme_programming)(XP) that states a[programmer](https://en.wikipedia.org/wiki/Programmer)should not add functionality until deemed necessary.\n**KISS - Keep It Simple Stupid**\n\n**DRY - Don't Repeat Yourself**\n\n**DIE - Duplication Is Evil**\n\n**SoC - Separation of Concerns**\n\n**Objectives and Key Results (OKR)**\n\nUsed for setting, communicating and monitoring quarterly goals and results in organizations.\n\n![OKRs cheatsheet DO's Make it a bottom-up exercise Stretch OKRs Talk about goals every week Align individual goals with team.org Make goals accessible to everyone Base goals off of data not \"feelings\" DON'Ts Confuse OKRs with KPls Confuse initiatives with KRs Include recurring tasks ](media/Software-Coding---Development-Engineering-image1.png)\n<https://soapboxhq.com/goal-examples/engineering>\n\n<https://okrexamples.co/technology-engineering-rnd-okr-examples>\n\n## TPM - Total Productive Maintainence**\n**MVP - Minimum Viable Products**\n\n**RAT - Riskiest Assumption Test**\n\n<https://hackernoon.com/the-mvp-is-dead-long-live-the-rat-233d5d16ab02>\n\n## Yak Shaving**\n\nYak shaving is programming lingo for the seemingly endless series of small tasks that have to be completed before the next step in a project can move forward.\n\n**Example**\n-   You start with the desire to wax your car.\n-   To wax your car, you need a water hose. Only, your water hose is busted so you need to go down to the hardware store to get a new hose.\n-   To get to the hardware store, you have to drive across a bridge. The bridge requires a pass or ticket. You can't find your pass, but you know your neighbor has one.\n-   However, your neighbor won't lend you his pass until you return a pillow that you borrowed. The reason you haven't returned it is because the pillow is missing some stuffing.\n-   The pillow was originally stuffed with yak hair. In order to re-stuff the pillow you need to get some new yak hair.\n-   And that's how you end up shaving a yak, when all you really wanted to do was wax your car.\n**SLAP - Single Level of Abstraction Principle**\n![Three stages](media/Software-Coding---Development-Engineering-image2.png)\n**Legacy code**\n\nOne of the definitions - The code without test cases.\n**Greenfield code**\n\nIn many disciplines agreenfield projectis one that lacks constraints imposed by prior work. The analogy is to that of construction on[greenfield land](https://en.m.wikipedia.org/wiki/Greenfield_land)where there is no need to work within the constraints of existing buildings or infrastructure.\nIn[software development](https://en.m.wikipedia.org/wiki/Software_development), a greenfield project could be one of developing a system for a totally new environment, without concern for integrating with other systems, especially not[legacy systems](https://en.m.wikipedia.org/wiki/Legacy_system). Such projects are deemed as higher risk, as they are often for new infrastructure, new customers, and even new owners. For this reason,[agile software development](https://en.m.wikipedia.org/wiki/Agile_software_development)is often deemed the best approach, as it proposes how to handle those risks by developing small slices of complete functionality and getting them in the hands of customers (internal or external) quickly for immediate feedback.\n**Brownfield code**\n\nThose facilities which are modified/upgraded are called[brownfield land](https://en.m.wikipedia.org/wiki/Brownfield_land)projects (often the pre-existing site/facilities are contaminated/polluted.)\nBrownfield development is a term commonly used in the IT industry to describe problem spaces needing the development and deployment of new software systems in the immediate presence of existing (legacy) software applications/systems. This implies that any new software architecture must take into account and coexist with live software already in situ.\n**Self Contained System (SCS)**\n\nThe Self-contained System (SCS) approach is an architecture that focuses on a separation of the functionality into many independent systems, making the complete logical system a collaboration of many smaller software systems. This avoids the problem of large monoliths that grow constantly and eventually become unmaintainable.\n\n<https://scs-architecture.org>\n[**http://engineering-principles.onejl.uk/**](http://engineering-principles.onejl.uk/)\n-   ARCHITECTURE\n    -   [Build Differentiators](http://engineering-principles.onejl.uk/architecture/Build_Differentiators.html)\n    -   [Design for Emergent Reuse](http://engineering-principles.onejl.uk/architecture/Design_for_Emergent_Reuse.html)\n    -   [Evolutionary Systems](http://engineering-principles.onejl.uk/architecture/Evolutionary_Systems.html)\n    -   [Scale Horizontally](http://engineering-principles.onejl.uk/architecture/Scale_Horizontally.html)\n    -   [Small and Simple](http://engineering-principles.onejl.uk/architecture/Small_and_Simple.html)\n    -   [Smarts in the Nodes not the Network](http://engineering-principles.onejl.uk/architecture/Smarts_in_the_Nodes_not_the_Network.html)\n-   OPERATIONAL\n    -   [Cloud Native](http://engineering-principles.onejl.uk/operational/Cloud_Native.html)\n    -   [Data Stewardship](http://engineering-principles.onejl.uk/operational/Data_Stewardship.html)\n    -   [Production Ready](http://engineering-principles.onejl.uk/operational/Production_Ready.html)\n-   ORGANISATION\n    -   [Keep Pace with Technological Change](http://engineering-principles.onejl.uk/organisation/Keep_Pace_with_Technological_Change.html)\n    -   [Model the Business Domain](http://engineering-principles.onejl.uk/organisation/Model_the_Business_Domain.html)\n-   TECHNOLOGY & PRACTICES\n    -   [Secure by Design](http://engineering-principles.onejl.uk/practices/Secure_by_Design.html)\n    -   [Automate by Default](http://engineering-principles.onejl.uk/practices/Automate_by_Default.html)\n    -   [Consistent Environments](http://engineering-principles.onejl.uk/practices/Consistent_Environments.html)\n    -   [Understandability](http://engineering-principles.onejl.uk/practices/Understandability.html)\n    -   [Performance Importance](http://engineering-principles.onejl.uk/practices/Performance_Importance.html)\n    -   [Get Feedback Early and Often](http://engineering-principles.onejl.uk/practices/Get_Feedback_Early_and_Often.html)\n    -   [Design for Testability](http://engineering-principles.onejl.uk/practices/Testability.html)\n**Lean Software Development**\n-   Eliminate Waste\n-   Build Quality In\n-   Create Knowledge\n-   Defer Commitment\n-   Deliver Fast\n-   Respect People\n-   Optimize the Whole\n**OCL (Object Constraint Language)**\n**UML (Unified Modeling Language)**\n\nUnified Modelling Language (UML)is a modeling language in the field of software engineering which aims to set standard ways to visualize the design of a system. UML guides the creation of multiple types of diagrams such as interaction , structure and behaviour diagrams.\n**Types of UML**\n-   Interaction diagram\n\nAn interaction diagram is used to show theinteractive behaviorof a system. Since visualizing the interactions in a system can be a cumbersome task, we use different types of interaction diagrams to capture various features and aspects of interaction in a system.\n-   Sequence diagram\n\nA sequence diagram simply depicts interaction between objects in a sequential order i.e. the order in which these interactions take place. We can also use the terms event diagrams or event scenarios to refer to a sequence diagram. Sequence diagrams describe how and in what order the objects in a system function. These diagrams are widely used by businessmen and software developers to document and understand requirements for new and existing systems.\n\n![1 : Open Application 2 : Access Webcam 3 : Get Photo 4 : Detect face 5 : Retrieve Mood 7 : Display Mood 6 : Mood 8 : Retrieve Music 10 : Playlist 9 : Generated Playlist ](media/Software-Coding---Development-Engineering-image3.png)\n\n<https://www.geeksforgeeks.org/unified-modeling-language-uml-sequence-diagrams>-   Flowchart\n-   Usecase diagram\n-   Class diagram\n-   Activity diagram\n-   Component diagram\n-   State machine diagram\n-   Object diagram\n-   Deployment diagram\n-   Timing diagram\n-   Package diagram\n-   Composite structure diagram\n-   Profile diagram\n-   Communication diagram\n[**https://www.freecodecamp.org/news/uml-diagrams-full-course/**](https://www.freecodecamp.org/news/uml-diagrams-full-course/)\n**Non-UML diagrams**\n-   Wireframe graphical interface\n\n<https://www.freecodecamp.org/news/what-is-a-wireframe-ux-design-tutorial-website>-   Archimate diagram\n-   Specification and Description Language (SDL)\n-   Ditaa diagram\n-   Gantt diagram\n<http://plantuml.com>\n\n<https://mermaidjs.github.io> (Can be rendered using markdown in gitlab)\n**Business Process Model and Notation (BPMN)**\n\nBusiness Process Model and Notation(BPMN) is a[graphical representation](https://en.wikipedia.org/wiki/Information_visualization)for specifying[business processes](https://en.wikipedia.org/wiki/Business_process)in a[business process model](https://en.wikipedia.org/wiki/Business_process_modeling).\nOriginally developed by the[Business Process Management Initiative](https://en.wikipedia.org/wiki/Business_Process_Management_Initiative)(BPMI), BPMN has been maintained by the[Object Management Group](https://en.wikipedia.org/wiki/Object_Management_Group)(OMG) since the two organizations merged in 2005. Version 2.0 of BPMN was released in January 2011,at which point the name was amended toBusiness Process ModelandNotationto reflect the introduction of execution semantics, which were introduced alongside the existing notational and diagramming elements. Though it is an OMG specification, BPMN is also ratified as[ISO](https://en.wikipedia.org/wiki/International_Organization_for_Standardization)19510. The latest version is BPMN 2.0.2, published in January 2014.\n<https://en.wikipedia.org/wiki/Business_Process_Model_and_Notation>\n\n<http://www.bpmn.org>\n\n<https://camunda.com>\n\n## Workflow**\n\nA Workflow is a series of decisions made by different people that determines what happens to a particular request that one of those people made, according to a defined and repeatable process.\n**SwimLane**\n\nAswimlane(orswimlane diagram) is used in[process flow diagrams](https://www.wikiwand.com/en/Flowchart), or flowcharts, that visually distinguishes [job sharing](https://www.wikiwand.com/en/Job_sharing) and responsibilities for sub-processes of a[business process](https://www.wikiwand.com/en/Business_process). Swimlanes may be arranged either horizontally or vertically.\n\n![Swimlane flowchart. Here, the swimlanes are named Customer, Sales, Contracts, Legal, and Fulfillment, and are arranged vertically.](media/Software-Coding---Development-Engineering-image4.png)\n\n<https://www.wikiwand.com/en/Swim_lane>\n\n## Life Critical System**\n\nA life-critical system is a system whose failure or malfunction may result in death or serious injury. It comprises all software and hardware necessary to perform a critical function.\n**Dependability**\n\nDependability is a measure of a system's availability, reliability, and maintainability. In general, it is a measure of the confidence that a system will perform as expected.\n**Safety-critical element**\n\nSafety-critical elements are systems or components that are designed to prevent, control, mitigate, or respond to system malfunctions or accidents that could lead to injury or death.\n**Examples**\n-   Automotive\n-   Aviation\n-   Communications\n<https://www.toptal.com/software/life-critical-systems>\n\n## Non Functional Requirements**\n\n<https://en.wikipedia.org/wiki/Non-functional_requirement>![An Architect's SUCCess Formula Dogma and rules 10% 20 % Experience Pragmatism 20% Flexibility 10% Minimalism 10% Trends and future needs 10% Experiments & PoCs 10% Hands-on participation 10% Vendor advice 0% ](media/Software-Coding---Development-Engineering-image5.png)\n![What architects actually do Do te nical stuff Defend architecture Act as salespeopl 30% 30 % ](media/Software-Coding---Development-Engineering-image6.png)\n![What architects want to do Explore technologies Shape strategy 20 % 30% entor developers 20% Make important decisio 30% ](media/Software-Coding---Development-Engineering-image7.png)\n![Donald Knuth 10-page literal Pascal program, including innovative new data structure Doug Mcllroy tr -cs A-Za-z 'n' I tr A-Z a-z I sort I uniq -c I sort -rn I sed ${l}q ](media/Software-Coding---Development-Engineering-image8.png)\n**Junior developer training guide**\n-   Message format for asking question\n-   technical private messages, all messages with pain points should be public. So if anyone else has the same question, they can refer to previous conversations\n-   There is no such question as a stupid question\n-   Business knowledge\n-   Tooling\n-   Remove or document prolific abstractions\n    -   Dependencies\n    -   Abstractions\n    -   Patterns\n-   Domain level documentation\n-   Make the macro intent of small classes explicit\n-   Screencasts as documentation\n-   Pair programming\n-   Watch other devs work\n-   Pairing is great, but sometimes watching is better. Just watching senior developer work you can learn a lot.\n-   Retention\n-   Promotion\n-   Support time-wasting\n-   Better use of investment time (learning lunches)\n-   Prioritize depth over breadth\n-   Occasionally, allocate full days\n-   Long form technical writing\n-   Solidifies there knowledge\n-   Creates documentation\n-   Build authority and brand\n-   Let devs audit extra meetings\n**Developers Tips**\n-   Draw more pictures (architecture diagram / UML diagram)\n-   Critique your design\n-   Read and write (documentation)\n-   Invent it here (don't use libraries for simple things)\n-   Learn to test\n-   Master the tools (ide, cli)\n-   Focus on fundamentals\n-   Value the individual\n-   Feel the fear\n-   Remember what matters\n\n[What We Left Behind - 10 Valuable Skills From The 1990s â€¢ Garth Gilmour & Eamonn Boyle â€¢ GOTO 2020](https://www.youtube.com/watch?v=DrBPXSiUWbI)\n**Concurrent Engineering**\n\n**Concurrent engineering**(**CE**) is a work methodology emphasizing the parallelisation of tasks (i.e. performing tasks concurrently), which is sometimes called**simultaneous engineering**or**integrated product development**(**IPD**) using an[integrated product team](https://en.wikipedia.org/wiki/Integrated_product_team)approach. It refers to an approach used in[product development](https://en.wikipedia.org/wiki/Product_development)in which functions of design engineering, manufacturing engineering, and other functions are integrated to reduce the time required to bring a new product to market.\n**The Four Keys To Rapid Response Software Development**\n-   Impeccable, reliable, automated build and deployment system\n-   Effective, low-friction collaboration\n-   Constant learning and skills improvement\n-   Design of replaceable, disposable software\n<https://toolshed.com/2019/01/FourKeys.html>\n\n## Lean Software Development**\n-   [Eliminate waste](https://en.wikipedia.org/wiki/Lean_software_development#Eliminate_waste)\n-   [Amplify learning](https://en.wikipedia.org/wiki/Lean_software_development#Amplify_learning)\n-   [Decide as late as possible](https://en.wikipedia.org/wiki/Lean_software_development#Decide_as_late_as_possible)\n-   [Deliver as fast as possible](https://en.wikipedia.org/wiki/Lean_software_development#Deliver_as_fast_as_possible)\n-   [Empower the team](https://en.wikipedia.org/wiki/Lean_software_development#Empower_the_team)\n-   [Build integrity in](https://en.wikipedia.org/wiki/Lean_software_development#Build_integrity_in)\n-   [See the whole](https://en.wikipedia.org/wiki/Lean_software_development#See_the_whole)\n<https://en.wikipedia.org/wiki/Lean_software_development>\n\n## Unified Process**\n\nTheUnified Software Development ProcessorUnified Processis an[iterative and incremental](https://en.wikipedia.org/wiki/Iterative_and_incremental_development)[software development process](https://en.wikipedia.org/wiki/Software_development_process)framework. The best-known and extensively documented refinement of the Unified Process is the[Rational Unified Process](https://en.wikipedia.org/wiki/Rational_Unified_Process)(RUP). Other examples are[OpenUP](https://en.wikipedia.org/wiki/OpenUP)and[Agile Unified Process](https://en.wikipedia.org/wiki/Agile_Unified_Process).\n<https://en.wikipedia.org/wiki/Unified_Process>\n\n## STAR method (Situation-Task-Action-Result)**\n\nHere's the Action-Result method:\n-   Action:Given the task at hand, how did you accomplish it? Once again, be clear and concise.\n-   Result:This is the most important part of the A-R method. Here you'll want to describe using relevant metrics what your impact was from completing the task. This is how you communicate you are a high performer.\nAn example:Launched Facebook Messenger from concept to launch, a way for users to message each other privately within the app, increasing D60 retention by 65% and user stickiness by 325%.\nHere, the applicant clearly describes what they did (launch Facebook Messenger), and the direct result from their action (increasing D60 retention and stickiness). It's important when you articulate your result that you use numbers - they help communicate their story with more impact and believability.\n**Best Practices**\n\n<https://blog.usejournal.com/10-signs-you-will-suck-at-programming-5497a6a52c5c>\n\n[Software Is Details â€¢ Kevlin Henney â€¢ GOTO 2020](https://www.youtube.com/watch?v=kX0prJklhUE)\n-   Any program is a model of a model within a theory of a model of an abstraction of some portion of the world or of some universe of discourse\n-   It is an attitude that can be encapsulated in a simple but demanding rule: always think both big picture and fine detail -- Will Gompertz - Think Like an Artist\n**Cycle time compression**\n\n![What you initially thought the goal was What the initial optimal solution was Optimal solution moved to ](media/Software-Coding---Development-Engineering-image9.png)\n**Time Estimation**\n\nThere is a huge difference between precision and accuracy\n\nprecise != accurate\nHow to estimate\n-   Compare new work with old\n-   Identify work of similar complexity\n-   Use the actual time it took as estimate for new work\n[How To Estimate Software Development Time](https://www.youtube.com/watch?v=v21jg8wb1eU)\n**Antipatterns**\n-   Centralized control and ownership: One size doesn't fit all\n    -   (Dis)economies of scale\n    -   Overzealous guardrails\n    -   Modification is ticket-driven\n-   Fragmented platform implementation\n-   Slow development loops: less time coding, more time toiling\n**Technical Debt**\n\n**Kiss of death**\n\n**Lehman's Laws of Software Evolution**\n-   **Continuing Change**\n\nA system must be continually adapted or it becomes progressively less satisfactory\n-   **Increasing Complexity**\n\nAs a system evolves, it's complexity increases unless work is done to maintan or reduce it-   Are we treating symptoms instead of real issues\n-   Quantifying technical debt\n-   There's always a trade off between improving existing code versus adding new features-   **Symptoms of low code health**\n    -   **Low cohension,** many responsibilities\n    -   **Bumpy road code smell,** lack of encapsulation\n    -   **Deeply nested logic**, if-statements inside if-statements\n    -   **Primitive obsession,** missing a domain language\n    -   **Excess function arguments,** missing abstractions\n<https://alexkreilein.medium.com/death-by-tech-debt-f5146836d2be>\n\n<https://news.ycombinator.com/item?id=13416935>\n[Prioritizing Technical Debt as if Time and Money Matters â€¢ Adam Tornhill â€¢ GOTO 2020](https://www.youtube.com/watch?v=FnmYGqZAAuI)\n\n<https://zerodha.tech/blog/being-future-ready-with-common-sense>\n-   Slow down to speed up\n-   Organisations often overestimate the importance of the features they continuously ship (and underestimate the importance of features they don't ship)\n-   Don't fix what is not broken, but fix what might soon break\n-   Technical debt is a reality of life\n    -   No business goals, vision, strategy, or competitive advantage changes the fact that technical debt is inevitable and that it needs to be handled.\n-   A developer should know why something is being done to the software they write and maintain. That is when they can truly \"own\" it, maintain a current mental model without outright hating the codebase, and assimilate changes meaningfully rather than shoehorning them in.\n**No Silver Bullet-- Essence and Accident in Software Engineering**\n\n\"No Silver Bullet-- Essence and Accident in Software Engineering\" is a widely discussed paper on[software engineering](https://en.wikipedia.org/wiki/Software_engineering)written by[Turing Award](https://en.wikipedia.org/wiki/Turing_Award)winner[Fred Brooks](https://en.wikipedia.org/wiki/Fred_Brooks)in 1987.Brooks argues that \"there is no single development, in either technology or management technique, which by itself promises even one[order of magnitude](https://en.wikipedia.org/wiki/Order_of_magnitude)[tenfold] improvement within a decade in productivity, in reliability, in simplicity.\" He also states that \"we cannot expect ever to see two-fold gains every two years\" in software development, as there is in hardware development ([Moore's law](https://en.wikipedia.org/wiki/Moore%27s_law)).\n**Summary**\n\nBrooks distinguishes between two different types of complexity: **accidental complexity and essential complexity.** This is related to[Aristotle](https://en.wikipedia.org/wiki/Aristotle)'s classification. Accidental complexity relates to problems which engineers create and can fix; for example, the details of writing and optimizing[assembly](https://en.wikipedia.org/wiki/Assembly_language)code or the delays caused by batch processing. Essential complexity is caused by the problem to be solved, and nothing can remove it; if users want a program to do 30 different things, then those 30 things are essential and the program must do those 30 different things.\nBrooks claims that the accidental complexity has decreased substantially, and today's programmers spend most of their time addressing essential complexity. Brooks argues that this means that shrinking all the accidental activities to zero will not give the same order-of-magnitude improvement as attempting to decrease essential complexity. While Brooks insists that there is no one[silver bullet](https://en.wikipedia.org/wiki/Silver_bullet), he believes that a series of innovations attacking essential complexity could lead to significant improvements. One technology that had made significant improvement in the area of accidental complexity was the invention of[high-level programming languages](https://en.wikipedia.org/wiki/High-level_programming_language), such as[Ada](https://en.wikipedia.org/wiki/Ada_(programming_language)).Today's languages, such as[C](https://en.wikipedia.org/wiki/C_(programming_language)),[C++](https://en.wikipedia.org/wiki/C%2B%2B),[C#](https://en.wikipedia.org/wiki/C_Sharp_(programming_language))and[Java](https://en.wikipedia.org/wiki/Java_(programming_language)), are considered to be improvements, but not of the same order of magnitude.\nBrooks advocates \"growing\" software organically through incremental development. He suggests devising and implementing the main and subprograms right at the beginning, filling in the working sub-sections later. He believes that programming this way excites the engineers and provides a working system at every stage of development.\nBrooks goes on to argue that there is a difference between \"good\" designers and \"great\" designers. He postulates that as programming is a creative process, some designers are inherently better than others. He suggests that there is as much as a tenfold difference between an ordinary designer and a great one. He then advocates treating star designers equally well as star managers, providing them not just with equal[remuneration](https://en.wikipedia.org/wiki/Remuneration), but also all the perks of higher status: large office, staff, travel funds, etc.\n<https://en.wikipedia.org/wiki/No_Silver_Bullet>\n\n## Joel Test**\n-   Do you use source control?\n-   Can you make a build in one step?\n-   Do you make daily builds?\n-   Do you have a bug database?\n-   Do you fix bugs before writing new code?\n-   Do you have an up-to-date schedule?\n-   Do you have a spec?\n-   Do programmers have quiet working conditions?\n-   Do you use the best tools money can buy?\n-   Do you have testers?\n-   Do new candidates write code during their interview?\n-   Do you do hallway usability testing?\n<https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code>\n\n<https://medium.com/squad-engineering/squad-takes-the-joel-test-9189709a6235>\n\n## The 7 R's: Common use cases and deployment**\n-   Rehost (also known as \"lift-and-shift\")\n\nQuickly moving applications to the cloud without changing them.\n-   Refactor (also known as rearchitect)\n\nChanging the way applications are architected and developed, usually by employing cloud-native features.\n-   Re-platform (sometimes referred to as \"lift-tinker-and-shift\")\n\nMaking a few optimizations to applications---but without changing their core architecture, like moving from self-managed Kubernetes to Amazon EKS.\n-   Repurchase (casually known as \"drop and shop\")\n\nReplacing your current environment by moving to a newer version of software or purchasing an entirely new solution.\n-   Relocate\n\nUsing VMware Cloud on AWS to quickly relocate up to hundreds of applications virtualized on vSphere to the AWS Cloud in days and without changing them.\n-   Retire\n\nIdentifying assets that are no longer useful and turning them off, strengthening your business case by focusing on more widely used resources.\n-   Retain\n\nLeaving the application on-premises---for now, at least.\n**When building a full-stack feature, start where the data is**\n\n**Exposing some data from the database?**\n\nStart with the backend. Build the API first and then consume it in the front end\n**Is it coming from a user input?**\n\nStart with the front end. Build the form and then build an API to publish it to\nThis helps as you no longer have to make assumptions about\n-   the format of the data,\n-   the data types of each field, or\n-   what data is even available\n**Resources**\n\n[Top 10 Things That Destroy Developer Productivity](https://www.youtube.com/watch?v=O-U11s-Rk_w&ab_channel=CodingTech)\n\n[Top 10 Traits of The Great Technical Leaders](https://youtu.be/3AZi49wyvds)\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Type-Introspection-and-Reflection/","title":"Type Introspection and Reflection"},"frontmatter":{"draft":false},"rawBody":"# Type Introspection and Reflection\n\nCreated: 2017-11-06 16:26:14 +0500\n\nModified: 2018-11-30 12:07:17 +0500\n\n---\n\n**Type Introspection -**\n\nThe ability of a program to examine the type or properties of an object at runtime.\n\nSome languages even allow us to traverse the inheritance hierarchy to see if our object is derived from an inherited base class.\n\nEx - Java\n\nif(obj instanceof Person){\n\nPerson p = (Person)obj;\n\np.walk();\n\n}\n**Reflection -**\n\nThe ability to modify object attributes at runtime.\n\nReflection is the ability of a computer program to examine and modify the structure and behavior (specifically the values, meta-data, properties and functions) of a program at runtime.\n\nIn layman'sterms, what this allows you to dois invoke a method on an object, instantiate a new object, or modify an attribute of an object -- all without knowing the names of the interfaces, fields, methods at compile time.\n\nBecause of the runtime-specificnature of reflection, it's more difficult to implement reflection in a statically-typed language compared to a dynamically-typed language because type checking occurs at compile time in a statically-typed language instead of at runtime.\n\nHowever it is by no means impossible, as Java, C#, and other modern statically-typed languages allow for both type introspection and reflection (but not C++, which allows only type introspection and not reflection).\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Type-Systems/","title":"Type Systems"},"frontmatter":{"draft":false},"rawBody":"# Type Systems\n\nCreated: 2018-11-30 12:01:05 +0500\n\nModified: 2021-04-28 11:47:26 +0500\n\n---\n\nIn[programming languages](https://en.wikipedia.org/wiki/Programming_language), atype systemis a set of rules that assigns a property called[type](https://en.wikipedia.org/wiki/Type_(computer_science))to the various constructs of a[computer program](https://en.wikipedia.org/wiki/Computer_program), such as [variables](https://en.wikipedia.org/wiki/Variable_(computer_science)), [expressions](https://en.wikipedia.org/wiki/Expression_(computer_science)), [functions](https://en.wikipedia.org/wiki/Function_(computer_science)) or [modules](https://en.wikipedia.org/wiki/Modular_programming).These types formalize and enforce the otherwise implicit categories the programmer uses for[algebraic data types](https://en.wikipedia.org/wiki/Algebraic_data_type), data structures, or other components (e.g. \"string\", \"array of float\", \"function returning boolean\").\n**Type System Goals**\n-   Check for bad program behavior\n-   Early detection of program errors\n-   Enable abstractions\n-   Protect integrity of user defined abstractions\n-   Documentation\n    -   Easy to reason code's purpose\n    -   Doesn't drift like code comments\nStatically typed language\n-   Types must be defined for every variable at compile time\n\nDynamically typed language\n-   Types are not required to be defined at compile time\nWeakly typed\n-   Types can be changed at run time.\n\nStrongly typed\n-   Types are fixed and cannot be changed at run time\n\nDependent types (first class types)\n# Statically Typed vs Dynamically Typed language\n\nA language is statically typed if the **type of a variable is known at compile time**. For some languages this means that you as the programmer must specify what type each variable is (e.g.: Java, C, C++); other languages offer some form oftype inference, the capability of the type system to deduce the type of a variable (e.g.: OCaml, Haskell, Scala, Kotlin, Python)\nThe main advantage here is that all kinds of checking can be done by the compiler, and therefore a lot of trivial bugs are caught at a very early stage.\nA language is dynamically typed if the **type is associated with run-time values,** and not named variables/fields/etc. This means that you as a programmer can write a little quicker because you do not have to specify types every time (unless using a statically-typed language withtype inference). Example: Perl, Ruby, Python.\n\n**(In dynamically typed languages, types are associated with the variable's value, not the variable itself)**\nMost scripting languages have this feature as there is no compiler to do static type-checking anyway, but you may find yourself searching for a bug that is due to the interpreter misinterpreting the type of a variable. Luckily, scripts tend to be small so bugs have not so many places to hide.\n**Static Typing Checking**\n-   Early feedback\n-   More reliable\n-   More optimizable\n-   Longer edit-test-compile cycle\n**Dynamic Typing Checking**\n-   Shorter edit-test-compile cycle\n-   More flexible\n-   Good for prototyping\n-   Better at metaprogramming\n-   Get ready for lots and lots of unit testing\n<table>\n<colgroup>\n<col style=\"width: 47%\" />\n<col style=\"width: 52%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>Static</th>\n<th>Dynamic</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><ul class=\"incremental\">\n<li><p>C</p></li>\n<li><p>C++</p></li>\n<li><p>Java</p></li>\n<li><p>C#</p></li>\n<li><p>Scala</p></li>\n<li><p>Haskell</p></li>\n<li><p>Rust</p></li>\n<li><p>Kotlin</p></li>\n<li><p>Go</p></li>\n</ul></td>\n<td><ul class=\"incremental\">\n<li><p>JavaScript</p></li>\n<li><p>Ruby</p></li>\n<li><p>Python</p></li>\n<li><p>Perl</p></li>\n<li><p>PHP</p></li>\n<li><p>Lisp</p></li>\n<li><p>Closure</p></li>\n<li><p>R</p></li>\n<li><p>Bash</p></li>\n</ul></td>\n</tr>\n</tbody>\n</table>\n\n![Erlang Clojure Python Dynamic perl VB Strong Groovy Ruby Magik JavaScript Weak Java c Scala Haskell Static C++ ](media/Type-Systems-image1.png)\n<https://www.sitepoint.com/typing-versus-dynamic-typing>\n\n# Strong Typed vs Weak Typed language\nIn strongly typed languages, variables are necessarily bound to a particular data type. Python is strong typed, and so is Java.\nAs opposed to strong typed languages, weak typed languages are those in which variables are not of a specific data type. It should be noted that this does not imply that variables do not have types; it does mean that variables are not \"bound\" to a specific data type. PHP and C are examples of weak typed languages.\nThus, Python is dynamic typed and strong typed; Java is static typed and strong typed; PHP is dynamic typed and weak typed; C is static typed and weak typed (owing to its casting ability).\n\n**See Also**\n-   Programming Styles > Duck typing\n\n**Others**\n-   Algebraic Data Types (ADT)\n\n**References**\n[Functional Programming: Type Systems](https://www.youtube.com/watch?v=hy1wjkcIBCU)\n![TYPE SYSTEMS ](media/Type-Systems-image2.jpg)"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Architectural---MVVM/","title":"Architectural - MVVM"},"frontmatter":{"draft":false},"rawBody":"# Architectural - MVVM\n\nCreated: 2018-12-08 23:15:26 +0500\n\nModified: 2018-12-08 23:15:58 +0500\n\n---\n\n**Data Flow**\n\n![The data flow of MVVM with Clean Architecture. Data flows from View to ViewModel to Domain to Data Repository, and then to a Data Source (Local or Remote.)](media/Architectural---MVVM-image1.png)\n\n**References**\n\n<https://www.toptal.com/android/android-apps-mvvm-with-clean-architecture>\n\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Behavioral---Chain-of-Responsibility/","title":"Behavioral - Chain of Responsibility"},"frontmatter":{"draft":false},"rawBody":"# Behavioral - Chain of Responsibility\n\nCreated: 2018-05-08 20:17:36 +0500\n\nModified: 2021-05-11 18:39:13 +0500\n\n---\n\n**Chain of Responsibility**is a behavioral design pattern that lets you pass requests along a chain of handlers. Upon receiving a request, each handler decides either to process the request or to pass it to the next handler in thechain.\nIn[object-oriented design](https://en.wikipedia.org/wiki/Object-oriented_design), thechain-of-responsibility patternis a[design pattern](https://en.wikipedia.org/wiki/Design_pattern_(computer_science))consisting of a source of[command objects](https://en.wikipedia.org/wiki/Command_pattern)and a series ofprocessing objects.Each processing object contains logic that defines the types of command objects that it can handle; the rest are passed to the next processing object in the chain. A mechanism also exists for adding new processing objects to the end of this chain. Thus, the chain of responsibility is an object oriented version of theif ... else if ... else if ....... else ... endif idiom, with the benefit that the condition--action blocks can be dynamically rearranged and reconfigured at runtime.\nIn a variation of the standard chain-of-responsibility model, some handlers may act as[dispatchers](https://en.wikipedia.org/wiki/Dynamic_dispatch), capable of sending commands out in a variety of directions, forming atree of responsibility. In some cases, this can occur recursively, with processing objects calling higher-up processing objects with commands that attempt to solve some smaller part of the problem; in this case recursion continues until the command is processed, or the entire tree has been explored. An[XML](https://en.wikipedia.org/wiki/XML)[interpreter](https://en.wikipedia.org/wiki/Interpreter_(computing))might work in this manner.\nThis pattern promotes the idea of[loose coupling](https://en.wikipedia.org/wiki/Loose_coupling).\nThe chain-of-responsibility pattern is structurally nearly identical to the[decorator pattern](https://en.wikipedia.org/wiki/Decorator_pattern), the difference being that for the decorator, all classes handle the request, while for the chain of responsibility, exactly one of the classes in the chain handles the request.\n![Sender hand â€¢Request ha ndk â€¢-mr2 ha ndk ha ndk Sa rnp Samo Smenee I uestn ](media/Behavioral---Chain-of-Responsibility-image1.jpg)\n\n**Advantages of Chain of Responsibility Design Pattern**\n-   To reduce the coupling degree. Decoupling it will request the sender and receiver.\n-   Simplified object. The object does not need to know the chain structure.\n-   Enhance flexibility of object assigned duties. By changing the members within the chain or change their order, allow dynamic adding or deleting responsibility.\n-   Increase the request processing new class of very convenient.\n**Disadvantages of Chain of Responsibility Design Pattern**\n-   The request must be received not guarantee.\n-   The performance of the system will be affected, but also in the code debugging is not easy may cause cycle call.\n-   It may not be easy to observe the characteristics of operation, due to debug.\n<https://en.wikipedia.org/wiki/Chain-of-responsibility_pattern>\n\n<https://refactoring.guru/design-patterns/chain-of-responsibility>\n\n<https://www.geeksforgeeks.org/chain-responsibility-design-pattern>\n\n<https://www.tutorialspoint.com/design_pattern/chain_of_responsibility_pattern.htm>\n\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Behavioral---Iterator/","title":"Behavioral - Iterator"},"frontmatter":{"draft":false},"rawBody":"# Behavioral - Iterator\n\nCreated: 2018-04-05 00:15:55 +0500\n\nModified: 2018-08-20 15:55:00 +0500\n\n---\n\n![Problem How would you iterate over elements from a collection? A first (inept) attempt (imitating C code): >>> my collection 'b', 'c'] >>> for i in range (len(my collection) ) : print my _ collection [i] , But what if my collection does not support indexir >>> my collection = >>> for i in range ( len (my _ collection) ) : print my collection [i], # What will happen here? Should we have to care about this when all we want is it ](media/Behavioral---Iterator-image1.png)\n\n![Description What we want: Standard interface for collections that we can iterat (we call these iterab/es), Iteration state (e.g., the position counter) should bc the container and should be encapsulated. Use an which keeps track of an iteration and knows what t is. What to implement: The iterator has a next( ) method that returns the ne collection. When all items have been returned it raises a ](media/Behavioral---Iterator-image2.png)\n\n![Example class Mylterable(object) : init (self, items) : def ' items List of items. self . items = items iter (self) : def return My Iterator ( self ) class My Iterator( object) : init ( self, my iterable): def self. my iterable = my iterable self . _ position def next (self) : if self ._position >= len(self. my iterable. items) raise Stoplteration( ) = self. my iterable. items [self ._position] value self . _ position += 1 ](media/Behavioral---Iterator-image3.png)\n\n![Example (Il) Lets perform the iteration manually using this interface: iterable = Mylterable( iterator = iter( iterable) # or use iterable. iter ( ) try : while True: i t em = iterator. next( ) print item except Stoplteration: pass print \" Iteration done. \" ..or just use the Python for-loop: for item in iterable: print item print \" Iteration done. \" ](media/Behavioral---Iterator-image4.png)\n\n![Summary Whenever you use a for-loop in Python you use the Iterator Pattern! Implement the iterable interface in your containers. Accept an iterable (or iterator) in your consumers. The iterator has a single responsibility, while the ite have to keep track of the iteration (which isn't its is semantically different for i Note that iter iterators (duck typing fail!). Normally one uses generator functions with yield writing iterator classes. ](media/Behavioral---Iterator-image5.png)"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Behavioral---Mediator/","title":"Behavioral - Mediator"},"frontmatter":{"draft":false},"rawBody":"# Behavioral - Mediator\n\nCreated: 2018-06-29 23:42:21 +0500\n\nModified: 2018-08-20 15:55:03 +0500\n\n---\n\nA mediator is a behavioral design pattern that allows you to reduce the connectivity of multiple classes with each other, by moving these connections to the one mediation class.\nSuppose you have a dialog for creating a user profile. It consists of all possible controls - the text fields, checkboxes, buttons.\n\nThe individual elements of the dialog should interact with each other. For example, the checkbox \"I have a dog\" opens a hidden field for entering the name of a pet, and the clicking on the submit button initiates the checking of all fields values of the form.\n\nBy introducing this very logic directly in the control's code, you'll put an end to their reuse in other places of the application. They'll become too closely connected to the elements of the profile editing dialog that aren't needed in other contexts. Therefore, you can use either all elements at once, or none.\n\nThe Mediator pattern causes objects to communicate not directly with each other, but through a separate mediator object that knows to which one of them a particular request needs to be redirected. Due to this, the system components will depend only on the mediator, and not on dozens of other components. In our example, a dialog could become a mediator. Most likely, the dialog class already knows its elements, so no new connections will have to be added.\n\nThe main changes will occur within the individual elements of the dialog. Whereas previously, upon the receiving of the user's click, the button object checked the values â€‹â€‹of the dialog fields itself, now its only responsibility is to inform the dialog that there was a click. The notified dialog will perform all necessary field verifications. Thus, instead of having several dependencies on other elements, for the button there will be only one - from the dialog itself.\n\nTo make code even more flexible, you can select a common interface for all of the mediators, that is, the program dialogs. Our button will become dependent not on the specific dialog of creating the user, but on the abstract one, thus allowing to use it in other dialogs. This way, the mediator conceals all complex connections and dependencies between the classes of individual program components within itself. And the fewer connections the classes have, the easier it is to modify, extend and reuse them.\n\nLet's look at another example:\n\nThe pilots of landing aircrafts or the ones that are taking off don't communicate directly with other pilots. Instead, they contact the dispatcher, who coordinates the actions of several aircrafts at the same time. Without a dispatcher, pilots would have to be on a high alert all the time and monitor all the surrounding aircraft on their own, which would lead to frequent disasters in the sky. It's important to understand that the dispatcher is not needed during the entire flight. He's involved only in the area of the airport, when it's necessary to coordinate the interactions of a lot of aircrafts.\n**Situations in which the pattern is worth using:**\n\n**1.**When it's difficult for you to change some classes because they have many chaotic connections to other classes.\n\nThe mediator allows you to put all these connections in one class, after which it'll be easier for you to correct them (if necessary), make them more understandable and flexible.\n\n**2.**When you can't reuse a class, because it depends on a lot of other classes.\n\nAfter applying the pattern, the components lose their previous connections to other components, and all their interactions occur strictly through the mediator object.\n\n**3.**When you have to create multiple subclasses of components to use the same components in different contexts.\n\nIf earlier the connection changes in one component could lead to a huge avalanche of changes in all other components, now it's enough for you to create a mediator subclass and change the connections between its components.\n**Pros:**\n\n- it eliminates the dependencies between the components allowing them to be reused;\n\n- simplifies the interaction between the components;\n\n- centralizes management in one place.\n\n**Cons:**\n\n- the mediator class can \"blow itself up\" quite a bit, which can complicate the work with it.\n**References**\n\n<https://py.checkio.org/blog/design-patterns-part-2>\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Behavioral---Memento/","title":"Behavioral - Memento"},"frontmatter":{"draft":false},"rawBody":"# Behavioral - Memento\n\nCreated: 2018-08-20 15:54:27 +0500\n\nModified: 2018-08-20 16:25:32 +0500\n\n---\n\n**Memento**is a behavioral design pattern that allows you to save and restore the past states of the objects without revealing the details of their implementation.\nLet's say that you are writing a text editor program. Apart from the usual addition and removal of the text, your editor allows you to change the text formatting, insert pictures and other. At some point, you've decided to make all of these actions cancelable. To do this, you need to save the current state of the text before performing any actions. If you then decide to cancel your action, you'll get a copy of the state from the history and restore the old state of the text.\nLet's now take a look at the copies of the editor's state. It should have several fields for storing the current text and all of its properties (font, color, etc.), cursor and scroll position of the screen, inserted images and much more. To make a copy of the state, you need to write the values â€‹â€‹of all these fields in a certain \"container\". Most likely, you'll need to store a lot of such containers as a history of operations, so the most convenient thing is to make them the objects of the same class. This class must have many fields, but almost no methods.\nThe Memento pattern perfectly handles all of these requirements. It suggests holding a copy of the state in a special image object with a limited interface, which allows, for example, to find out the date/time of storage or the name of the snapshot. But, on the other hand, the snapshot should be open to its creator, allowing you to read and restore its internal state.\nSuch a scheme allows the creators to take snapshots and give them away for storage to other objects called guardians. The guardians will only have access to a limited image interface, so they won't be able to affect the \"insides\" of the image itself. At the right time, the guardian can ask the creator to restore its state, providing it with the appropriate snapshot.\nIn the example with the editor, you can choose a separate class to be the guardian that will store the list of completed operations. The limited interface of images will show the user a beautiful list with names and dates of the performed operations. And when the user decides to roll back the operation, the history class takes the last snapshot from the stack and sends it to the editor to restore it.\n**Situations in which it's appropriate to use the pattern:**\n\n**1)**When you need to save snapshots of the object's state (or a part of it), so that the object could later be restored in the same state.\n\nThe Memento pattern allows you to create any number of the object's snapshots and store them, regardless of the object from which they've been taken. Snapshots are often used not only to implement a cancellation operation, but also for transactions when the state of the object needs to be \"rolled back\" if the operation failed.\n**2)**When the direct interface for obtaining the state of the object reveals details of the implementation and violates the encapsulation of the object.\n**Pros:**\n\n- Doesn't violate the encapsulation of the input object.\n\n- Simplifies the structure of the input object. It doesn't need to keep a history of its state versions.\n**Cons:**\n\n- Requires a lot of memory if clients create images too often.\n\n- It can entail additional memory costs, if the objects storing the history don't release the resources occupied by the outdated images.**References**\n\n<https://py.checkio.org/blog/design-patterns-part-3>\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Behavioral---Strategy/","title":"Behavioral - Strategy"},"frontmatter":{"draft":false},"rawBody":"# Behavioral - Strategy\n\nCreated: 2018-04-05 00:18:31 +0500\n\nModified: 2018-08-20 15:55:15 +0500\n\n---\n\n![Duck Simulator class Duck (object) : init (self) : def # for simplicity this example class is stateless def quack (self): print \"Quack! \" def display (self) : print \"Boring looking duck. \" def take off (self): print \"I'm running fast, flapping with my wings. \" def fly to(self, destination) : print \"Now flying to \" def land( self) : % destination ](media/Behavioral---Strategy-image1.png)\n\n![Duck Simulator (Il) class RedheadDuck( Duck) : def display (self) : print \"Duck with a read head. \" class RubberDuck(Duck) : def quack (self) : print \" Squeak! \" def display (self) : print \" Small yellow rubber duck. \" Oh, snap! The RubberDuck has same flying behavior duck, must override all the flying related methods. What if we want to introduce a DecoyDuck as well? ( ](media/Behavioral---Strategy-image2.png)\n\n![Solution class FlyingBehavior(object) : def take off (self): print \"I'm running fast, flapping with my wings. \" def fly to(self, destination) : print \"Now flying to % destination def land( self) : print \" Slowing down, extending legs, touch down. \" class Duck (object) : init (self) : def self. flying behavior = FlyingBehavior ( ) def take off (self): self. flying behavior. take off( ) def fly to(self, destination) : ](media/Behavioral---Strategy-image3.png)\n\n![Solution (Il) class NonF1yingBehavior(F1yingBehavior) : def take off (self): print \"It's not working def fly to(self, destination) : raise Exception( \"I 'm not flying anywhere. \" def land( self) : print \"That won't be necessary. \" class RubberDuck(Duck) : init (self) : def self . flying behavior def quack (self) : print \" Squeak! \" def display (self) : = NonF1yingBehavior( ) ](media/Behavioral---Strategy-image4.png)\n\n![Analysis The strategy in this case is the flying behavior. If a poor duck breaks its wing we do: duck. flying behavior = NonF1yingBeh Flexibility to change the behaviour at runtime! Could have avoided code duplication with inheritan NonF1yingDuck). Could make sense, but is less Relying less on inheritance and more on compositio Strategy Pattern means: Encapsulate the different strategies in different clÃ¦ ](media/Behavioral---Strategy-image5.png)\n\n![Strategy Pattern with Functions What if our behavior only needs a single method? Stop writing classes! TM Use a function! Standard examples: Sorting with a customized sort key: >>> sorted( [ \" 12 \" \" 2 \" ] , key---lambda x: '1', '12'] predicate function: Filtering with a >>> predicate = lambda x: int (x) > 2 \" 12 \" ] data --- >>> [x for x in data if predicate (x) ] int(x)) ](media/Behavioral---Strategy-image6.png)\nA strategy is a behavioral design pattern that defines a family of similar algorithms and places each of them in its own class, after which the algorithms can be interchanged right during the execution of the program.\nPros:\n\n- \"hot\" algorithm replacement on the fly;\n\n- the algorithms' code and data isolation from other classes.\n\nCons:\n\n- complicates the program due to the additional classes.**References**\n\n<https://py.checkio.org/blog/design-patterns-part-1>\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Concurrency---Reactor/","title":"Concurrency - Reactor"},"frontmatter":{"draft":false},"rawBody":"# Concurrency - Reactor\n\nCreated: 2018-10-13 21:50:31 +0500\n\nModified: 2018-10-13 21:52:00 +0500\n\n---\n\nThe reactor[design pattern](https://en.wikipedia.org/wiki/Design_pattern_(computer_science))is an[event handling](https://en.wikipedia.org/wiki/Event_handling)pattern for handling service requests delivered[concurrently](https://en.wikipedia.org/wiki/Concurrency_(computer_science))to a service handler by one or more inputs. The service handler then[demultiplexes](https://en.wikipedia.org/wiki/Demultiplex)the incoming requests and dispatches them synchronously to the associated request handlers.\n![Figure 5: the reactor loop](media/Concurrency---Reactor-image1.png)\n\n*fig - the reactor loop*\n**References**\n\n<https://en.wikipedia.org/wiki/Reactor_pattern>\n\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Creational---Abstract-Factory/","title":"Creational - Abstract Factory"},"frontmatter":{"draft":false},"rawBody":"# Creational - Abstract Factory\n\nCreated: 2018-06-23 23:10:44 +0500\n\nModified: 2018-08-20 16:25:08 +0500\n\n---\n\nAn abstract factory is a generative design pattern that allows you to create families of related objects without getting attached to specific classes of created objects. The pattern is being implemented by creating an abstract class (for example - Factory), which is represented as an interface for creating system components. Then the classes that implement this interface are being written.\nPros:\n\n- isolates specific classes;\n\n- simplifies the replacement of the product families;\n\n- guarantees the products' compatibility.\n\nCons:\n\n- it's difficult to add support for the new kinds of products.\n**References**\n\n<https://py.checkio.org/blog/design-patterns-part-1>\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Creational---Object-Pool/","title":"Creational - Object Pool"},"frontmatter":{"draft":false},"rawBody":"# Creational - Object Pool\n\nCreated: 2021-03-20 16:43:36 +0500\n\nModified: 2021-03-20 16:49:23 +0500\n\n---\n\n**Intent**\n\nObject pooling can offer a significant performance boost; it is most effective in situations where the cost of initializing a class instance is high, the rate of instantiation of a class is high, and the number of instantiations in use at any one time is low.\n**Problem**\n\nObject pools (otherwise known as resource pools) are used to manage the object caching. A client with access to a Object pool can avoid creating a new Objects by simply asking the pool for one that has already been instantiated instead. Generally the pool will be a growing pool, i.e. the pool itself will create new objects if the pool is empty, or we can have a pool, which restricts the number of objects created.\nIt is desirable to keep all Reusable objects that are not currently in use in the same object pool so that they can be managed by one coherent policy. To achieve this, the Reusable Pool class is designed to be a singleton class.\n**Discussion**\n\nThe Object Pool lets others \"check out\" objects from its pool, when those objects are no longer needed by their processes, they are returned to the pool in order to be reused.\nHowever, we don't want a process to have to wait for a particular object to be released, so the Object Pool also instantiates new objects as they are required, but must also implement a facility to clean up unused objects periodically.\n<https://sourcemaking.com/design_patterns/object_pool>\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Design-Patterns/","title":"Design Patterns"},"frontmatter":{"draft":false},"rawBody":"# Design Patterns\n\nCreated: 2018-04-04 15:39:14 +0500\n\nModified: 2021-09-10 20:59:55 +0500\n\n---\n\n## Gang of Four (GOF)\n\nTwo main principles are in the bases of the design patterns defined by the GOF:\n-   Program to an interface not an implementation.\n-   Favor object composition over inheritance.\n\n## \n\n## Design Pattern Categorization\n\nDesign patterns can be categorized in multiple ways, but the most popular one is the following:\n-   Creationaldesign patterns\n-   Structuraldesign patterns\n-   Behavioraldesign patterns\n-   Concurrencydesign patterns\n-   Architecturaldesign patterns\n### Creational Design Patterns\n\nThese patterns deal with object creation mechanisms which optimize object creation compared to a basic approach. The basic form of object creation could result in design problems or in added complexity to the design. Creational design patterns solve this problem by somehow controlling object creation. Some of the popular design patterns in this category are:\n-   Abstract factory\n-   Builder\n-   Factory method\n-   Prototype\n-   Singleton (AntiPattern, Overused, replaced with dependency injection)\n-   Object Pool\n### Structural Design Patterns\n\nThese patterns deal with object relationships. They ensure that if one part of a system changes, the entire system doesn't need to change along with it. The most popular patterns in this category are:\n-   Adapter\n-   Bridge\n-   Composite\n\n<https://dev.to/coly010/the-composite-pattern-design-patterns-meet-the-frontend-445e>-   Decorator\n-   Facade\n-   Flyweight\n\nRefers to an[object](https://en.wikipedia.org/wiki/Object_(computer_science))that minimizes[memory](https://en.wikipedia.org/wiki/Computer_memory)usage by sharing some of its data with other similar objects\n\nIn other contexts, the idea of sharing data structures is called[hash consing](https://en.wikipedia.org/wiki/Hash_consing).\n\n<https://en.wikipedia.org/wiki/Flyweight_pattern>\n-   Proxy\n### Behavioral Design Patterns\n\nThese types of patterns recognize, implement, and improve communication between disparate objects in a system. They help ensure that disparate parts of a system have synchronized information. (Behavioural patterns deal with inter-object communication, controlling how various objects interact and perform different tasks.) Popular examples of these patterns are:\n-   Chain of responsibility\n-   Command\n-   Interpreter (Python)\n-   Iterator\n-   Mediator\n-   Memento\n-   Observer\n-   Publisher Subscriber Pattern\n-   State\n-   Strategy\n-   Template (Python) (Composition is generally better)\n-   Visitor\n### Concurrency Design Patterns\n\nThese types of design patterns deal with multi-threaded programming paradigms. Some of the popular ones are:\n-   Active object\n-   Nuclear reaction\n-   Scheduler\n-   Reactor design pattern\n### Architectural Design Patterns\n\nDesign patterns which are used for architectural purposes. Some of the most famous ones are:\n-   MVC (Model-View-Controller)\n-   MVP (Model-View-Presenter) - Android\n-   MVVM (Model-View-ViewModel)\n-   MVVP\n-   Clean Architecture (Uncle Bob Android)\n<https://www.youtube.com/watch?v=BrT3AO8bVQY>\n\n## Others**\n-   **Parameterization - Python**\n\nIn a parameterized function, one or more of the details of what the function does are defined as parameters instead of being defined in the function; they have to be passed in by the calling code.\n<https://www.toptal.com/python/python-parameterized-design-patterns>\n\n## The Scope**\n\n**Class-scope**\n\nwhen pattern is categorized as class-scope, it means that its goals are achieved at the compile time using inheritance\n**Object-scope**\n\nwhen pattern is categorized as object-scope its goals are achieved through composition during the runtime\n**Single Source of Truth (SSOT)**\n\nIn order to centralize the data access, we need something called Single Source of Truth (SSOT).\nIn information systems design and theory, single source of truth (SSOT) is the practice of structuring information models and associated data schema such that every data element is mastered (or edited) in only one place.\nIt is basically a practice to centralize your data access into a single class (based on the usage of the data). For instance, you need to access the data aboutbook, then you have to centralize all the data access which related to book into a single class so that the other class (client) can easily fetch the book data by requesting to \"source of truth\" class.\nAnd because all other locations of the data access just refer back to the primary \"source of truth\" location, updates to the data element in the primary location propagate to the entire system without the possibility of a duplicate value somewhere being forgotten.\nDeployment of a SSOT is becoming increasingly important in enterprise settings where incorrectly linked duplicate or de-normalized data elements (a direct consequence of intentional or unintentional denormalization of any explicit data model) pose a risk for retrieval of outdated, and therefore incorrect, information.\n\nA common example would be the[electronic health record](https://en.wikipedia.org/wiki/Electronic_health_record), where it is imperative to accurately validate patient identity against a single referential repository, which serves as the SSOT.\n**Repository Pattern**\n\n<https://medium.com/swlh/repository-pattern-in-android-c31d0268118c>\n\n## References**\n\n<https://www.toptal.com/python/python-design-patterns>\n\n<https://www.toptal.com/javascript/comprehensive-guide-javascript-design-patterns>\n\n<https://github.com/faif/python-patterns>\n\n<https://stackabuse.com/design-patterns-in-python>\n\n<https://www.freecodecamp.org/news/4-design-patterns-to-use-in-web-development>\n![[hide] Creational Structural Behavioral Functional Concurrency Architectural Cloud Distributed Other Books People Communities Software design patterns Abstract factory â€¢ Builder â€¢ Dependency injection â€¢ Factory method â€¢ Lazy initialization â€¢ Multiton â€¢ Object pool â€¢ Prototype â€¢ RAII â€¢ Singleton Adapter â€¢ Bridge â€¢ Composite â€¢ Decorator â€¢ Delegation â€¢ Facade â€¢ Flyweight â€¢ Front controller â€¢ Marker interface â€¢ Module â€¢ Proxy â€¢ Twin Blackboard â€¢ Chain of responsibility â€¢ Command â€¢ Interpreter â€¢ Iterator â€¢ Mediator â€¢ Memento â€¢ Null object â€¢ Observer â€¢ Servant â€¢ Specification â€¢ State â€¢ Strategy â€¢ Template method â€¢ Visitor Monoid â€¢ Functor â€¢ Applicative â€¢ Monad â€¢ Comonad â€¢ Free monad â€¢ HOF â€¢ Currying â€¢ Function composition â€¢ Closure â€¢ Generator Active object â€¢ Actor â€¢ Balking â€¢ Barrier â€¢ Binding properties â€¢ Coroutine â€¢ Compute kernel â€¢ Double-checked locking â€¢ Event-based asynchronous â€¢ Fiber â€¢ Futex â€¢ Futures and promises â€¢ Guarded suspension â€¢ Immutable object â€¢ Join â€¢ Lock â€¢ Messaging â€¢ Monitor â€¢ Proactor â€¢ Reactor â€¢ Read write lock â€¢ Scheduler â€¢ STM â€¢ Thread pool â€¢ Thread-local storage ADR â€¢ Active record â€¢ Broker â€¢ Client---server â€¢ CBD â€¢ DAO â€¢ DTO â€¢ DDD â€¢ ECB â€¢ ECS â€¢ EDA â€¢ Front controller â€¢ Identity map â€¢ Interceptor â€¢ Implicit invocation â€¢ Inversion of control â€¢ Model 2 â€¢ MOM â€¢ Microservices â€¢ MVA â€¢ MVC â€¢ MVP â€¢ MVVM â€¢ Monolithic â€¢ Multitier â€¢ Naked objects â€¢ ORB â€¢ P2P â€¢ Publish---subscribe â€¢ PAC â€¢ REST â€¢ SOA â€¢ Service locator â€¢ Specification Ambassador â€¢ Anti-Corruption Layer â€¢ Bulkhead â€¢ Cache-Aside â€¢ Circuit Breaker â€¢ CQRS â€¢ Compensating Transaction â€¢ Competing Consumers â€¢ Compute Resource Consolidation â€¢ Event Sourcing â€¢ External Configuration Store â€¢ Federated Identity â€¢ Gatekeeper â€¢ Index Table â€¢ Leader Election â€¢ MapReduce â€¢ Materialized View â€¢ Pipes â€¢ Filters â€¢ Priority Queue â€¢ Publisher-Subscriber â€¢ Queue-Based Load Leveling â€¢ Retry â€¢ Scheduler Agent Supervisor â€¢ Sharding â€¢ Sidecar â€¢ Strangler â€¢ Throttling â€¢ Valet Key Business delegate â€¢ Composite entity â€¢ Intercepting filter â€¢ Lazy loading â€¢ Mangler â€¢ Mock object â€¢ Type tunnel â€¢ Method chaining Design Patterns â€¢ Enterprise Integration Patterns â€¢ Code Complete â€¢ POSA Christopher Alexander â€¢ Erich Gamma â€¢ Ralph Johnson â€¢ John Vlissides â€¢ Grady Booch â€¢ Kent Beck â€¢ Ward Cunningham â€¢ Martin Fowler â€¢ Robert Martin â€¢ Jim Coplien â€¢ Douglas Schmidt â€¢ Linda Rising The Hillside Group â€¢ The Portland Pattern Repository ](media/Design-Patterns-image1.jpg)\n\n<https://en.wikipedia.org/wiki/Software_design_pattern>\n\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Functional-Programming/","title":"Functional Programming"},"frontmatter":{"draft":false},"rawBody":"# Functional Programming\n\nCreated: 2017-11-15 11:58:23 +0500\n\nModified: 2021-11-12 21:46:46 +0500\n\n---\n\n**Everything is immutable**\n[Why Functional Programming Matters](https://www.youtube.com/watch?v=oB8jN68KGcU)\n\n[How Functional Programming Can Make You A Better Developer](https://www.youtube.com/watch?v=EqO4TcNLjl0&t=1s&ab_channel=CodingTech)\n**Basic Concepts**\n-   Functions are first class citizen/constructs\n-   Immutable values over mutable variables\n-   Functions as first-class values\n-   Currying, partial-application of functions\n-   Expressions-not-statements\n-   Laziness/deferred execution\n**Some optional functional concepts**\n\nStrongly-typed, type-inferenced\n\nRecursion\n\nTuples, lists\n\nPattern-matching\n**Immutable values**\n\nOnce bound, a binding remains constant throughout its lifetime, thus offers no opportunity for confusion/reassignment/etc\n**Values over variables**\n\n\"values\" are not \"variables\"\n\nName/value binding is fixed once bound\n**Functions as first-class values**\n-   Pass code as function parameters\n-   Ex\n\n![H tkr-orde ](media/Functional-Programming-image1.png)\nHere map can use computers parallel programming capabilities and split the list in multiple lists and then perform parallel operation as opposed to for loop.\n\nHence Map-Reduce, without Reduce.\n**Partial application**\n-   Providing some of the parameters (but not all) to a function and capturing that as a function to be called\n**Currying**\n-   It turns out that all functions can be reduced to functions taking one parameter and either yielding a result or another function\n-   This permits easy \"pipelining\" and composition of functions in a highly reusable manner\n**Tail recursion optimizations**\n# Monads\n\n**What is the meaning/definition of a monad?**\n\nA monad is an abstract interface that defines \"pure\" and \"flatMap\" functions. Pure lets you convert an ordinary value to monadic value. FlatMap allows functions with ordinary arguments to be applied to monadic arguments.\n**What are monads for?**\n\nMonads are used extensively in functional programming. Their key goal is isolating any unreliable and dangerous side effects in one place, so you can enjoy safe programming in all other parts of the app.\n**What is a monad category?**\n\nTechnically, a monad is an endofunctor, meaning that it's something that maps a category to itself. But you can think of an image of that functor as a new category with monadic side effect(s).\n**What is monad programming?**\n\nMonad programming is a technique of composing different monadic values into one big monad. After that it's easy to process all side effects, because they are concentrated just in one monad, rather than many monads.\n<https://www.toptal.com/javascript/option-maybe-either-future-monads-js>\n[Functional Programming & Haskell - Computerphile](https://www.youtube.com/watch?v=LnX3B9oaKzw)\n\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/OOPS---SOLID/","title":"OOPS / SOLID"},"frontmatter":{"draft":false},"rawBody":"# OOPS / SOLID\n\nCreated: 2018-04-04 15:36:10 +0500\n\nModified: 2022-03-10 12:30:13 +0500\n\n---\n\n**AEIP - Abstraction, Encapsulation, Inheritance and Polymorphism**\n**Modularity**\n\nModularity lets you define a **public interface** to hide **private implementation details**\n**Objects**\n\nCombine state (data) and behavior (algorithms/business logic)\n\nMeans a real world entity (like a car, bus, pen, etc..)\n**Classes**\n\nCollection of objectsis called class. It is a logical entity.\nA class defines the behavior of an object and the kind of information an object can store. The information in a class is stored in attributes, and functions that belong to a class are called methods. A child class inherits the attributes and methods from its parent class.\nClasses are the foundation of object-oriented programming. Classes represent real-world things you want to model in your programs: for example dogs, cars, and robots. You can use a class to make objects, which are specific instances of dogs, cars and robots. A class defines the general behavior that a whole category of objects can have, and the information that can be associated with those objects.\n\nClasses can inherit from each other - you can write a class that extends the functionality of an existing class. This allows you to code efficiently for a wide variety of situations.\nClasses provide a means of bundling data and functionality together.\n\nCreating a new class creates a new type of object, allowing new instances of that type to be made.\n**Abstraction**\n\nHiding internal details and showing functionalityis known as abstraction. For example: phone call, we don't know the internal processing.\nIn java, we use abstract class and interface to achieve abstraction.\n**Leaky Abstraction**\n\n<https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions>\n\nAbstractions fail. Sometimes a little, sometimes a lot. There's leakage. Things go wrong. It happens all over the place when you have abstractions.\nAll non-trivial abstractions, to some degree, are leaky.\nTCP attempts to provide a complete abstraction of an underlying unreliable network, but sometimes, the network leaks through the abstraction and you feel the things that the abstraction can't quite protect you from. This is but one example of what I've dubbed the Law of Leaky Abstractions:\n**Encapsulation**\n\nEncapsulation is defined as the wrapping up of data under a single unit. It is the mechanism that binds together code and the data it manipulates.Other way to think about encapsulation is, it is a protective shield that prevents the data from being accessed by the code outside this shield.\nEncapsulation is a mechanism of wrapping the data (variables) and code acting on the data (methods) together as a single unit. In encapsulation, the variables of a class will be hidden from other classes, and can be accessed only through the methods of their current class. Therefore, it is also known asdata hiding.\n**Inheritance**\n\nWhen one object acquires all the properties and behaviours of parent objecti.e. known as inheritance. It provides code reusability. It is used to achieve runtime polymorphism.\n-   Single Inheritance\n-   Multilevel Inheritance\n-   Hierarchical Inheritance\n-   Hybrid Inheritance\nInterface Inheritance (aka subtyping)\n\nImplementation Inheritance (aka inheritance)\n**Polymorphism**\n\nWhen**one task is performed by different ways**i.e. known as polymorphism. For example: to convince the customer differently, to draw something e.g. shape or rectangle etc.\n\nIn java, we use method overloading and method overriding to achieve polymorphism.\n\nAnother example can be to speak something e.g. cat speaks meaw, dog barks woof etc.\nTypes of polymorphism\n\n1.  Parametric polymorphism\n\nThis is a pretty common technique in many languages, albeit better known as \"Generics\". The core idea is to allow programmers to use a wildcard type when defining data structures that can later be filled with any type.\n\n![class List< > { class Node< T > { T data Node< T > next public Node< T > head public void pushFront(T data The T is the type variable because you can later \"assign\" any type you want: List< String myNumberList List< String >(); new myNumberList . pushFront ( \" foo \" myNumberList . pushFront(8) // Error: 8 is not a string ](media/OOPS---SOLID-image1.png)\n2.  Subtype polymorphism (most used type, using shape like triangle, rectangle and calling area like traingle.area(), rectangle.area())\n\nSubtyping is better known as object oriented inheritance. The classic example is a vehicle type, here in Java:\n\nabstract class Vehicle {\nabstract double getWeight();\n}\n\nclass Car extends Vehicle {\ndouble getWeight() { return 10.0; }\n}\n\nclass Truck extends Vehicle {\ndouble getWeight() { return 100.0; }\n}\n\nclass Toyota extends Car { /* ... */ }\n\nstatic void printWeight(Vehicle v) {\n// Allowed because all vehicles have to have this method\nSystem.out.println(v.getWeight());\n}\n3.  Ad-Hoc polymorphism (AKA Type Classes)\n\nThis is more commonly known as function or operator overloading. In languages that allow this, you can define a function multiple times to deal with different input types. For example in Java:\n\nclass Printer {\npublic String prettyPrint(int x) { /* ... */ }\npublic String prettyPrint(char c) { /* ... */ }\n}\n4.  Row Polymorphism\n\n5.  Kind Polymorphism\n\n6.  Higher-rank Polymorphism\n\n7.  Linearity Polymorphism\n\n8.  Levity Polymorphism\n<https://dev.to/jvanbruegge/what-the-heck-is-polymorphism-nmh>\n\n## Inheritance, Encapsulation and Polymorphism are 3 pillars of OOPs**\n**Problems**\n\n**Inheritance**\n-   Banana Monkey Jungle Problem\n\nThe problem with object-oriented languages is they've got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle.\nSolution\n-   Contain\n-   Delegate-   Diamond Problem\n\n![PoweredDevice Scanner ](media/OOPS---SOLID-image2.png)\n\nBoth Scanner and Printer has the same function, so Copier will inherit which function? (Most OO language doesn't let you do that)-   Fragile Base Class Problem\n\nIf a parent class is changed, than child class can stop working-   The Hierarchy Problem (Categorical Hierarchies)\n\nEvery time I start at a new company, I struggle with the problem when I'm creating a place to put my Company Documents, e.g. the Employee Handbook. Do I create a folder called Documents and then create a folder called Company in that. Or do I create a folder called Company and then create a folder called Documents in that?\n\nUsing tags we can solve this problem. A company document can be labelled as document and also company. So it's both. **Tags have no order or hierarchy.**\nDon't use categorical hierarchies (Use containment hierarchies insted)\n**Encapsulation**\n-   The Reference Problem\n\nObject's reference is passed to functions and changing something to object can change the original object.\nSolution - Deep Copy (But system objects cannot be copied)\n**Others**\n-   **Mixin**\n\nIn[object-oriented programming languages](https://en.wikipedia.org/wiki/Object-oriented_programming_language), a**Mixin**is a[class](https://en.wikipedia.org/wiki/Class_(computer_science))that contains methods for use by other classes without having to be the parent class of those other classes. How those other classes gain access to the mixin's methods depends on the language. Mixins are sometimes described as being \"included\" rather than \"inherited\".\nMixins encourage[code reuse](https://en.wikipedia.org/wiki/Code_reuse)and can be used to avoid the inheritance ambiguity that multiple inheritance can cause(the \"[diamond problem](https://en.wikipedia.org/wiki/Multiple_inheritance#The_diamond_problem)\"), or to work around lack of support for multiple inheritance in a language. A mixin can also be viewed as an[interface](https://en.wikipedia.org/wiki/Interface_(object-oriented_programming))with implemented[methods](https://en.wikipedia.org/wiki/Method_(computer_science)). This pattern is an example of enforcing the[dependency inversion principle](https://en.wikipedia.org/wiki/Dependency_inversion_principle).\nMixins are a language concept that allows a programmer to inject some code into a[class](https://en.wikipedia.org/wiki/Class_(computer_programming)). Mixin programming is a style of[software development](https://en.wikipedia.org/wiki/Software_development), in which units of functionality are created in a class and then mixed in with other classes.\nA mixin class acts as the parent class, containing the desired functionality. A[subclass](https://en.wikipedia.org/wiki/Subclass_(computer_science))can then inherit or simply reuse this functionality, but not as a means of specialization. Typically, the mixin will export the desired functionality to a[child class](https://en.wikipedia.org/wiki/Subclass_(computer_science)), without creating a rigid, single \"is a\" relationship. Here lies the important difference between the concepts of mixins and[inheritance](https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)), in that the child class can still inherit all the features of the parent class, but, the semantics about the child \"being a kind of\" the parent need not be necessarily applied.\n**Advantages**\n\n1.  It provides a mechanism for[multiple inheritance](https://en.wikipedia.org/wiki/Multiple_inheritance)by allowing multiple classes to use the common functionality, but without the complex semantics of multiple inheritance.2.  [Code reusability](https://en.wikipedia.org/wiki/Code_reuse): Mixins are useful when a programmer wants to share functionality between different classes. Instead of repeating the same code over and over again, the common functionality can simply be grouped into a mixin and then included into each class that requires it.3.  Mixins allow inheritance and use of only the desired features from the parent class, not necessarily all of the features from the parent class.-   **Delegation**\n\nDelegation means that an object shall perform only what it knows best, and leave the rest to other objects.\nDelegation can be implemented with two different mechanisms: composition and inheritance. Sadly, very often only inheritance is listed among the pillars of OOP techniques, forgetting that it is an implementation of the more generic and fundamental mechanism of delegation; perhaps a better nomenclature for the two techniques could beexplicit delegation(composition) andimplicit delegation (inheritance).-   **Inheritance**\n    -   Python does not implicitly call the parent implementation when you override a method.\n    -   overriding is a way to block implicit delegation.-   **Composition**\n    -   Composition means that an object knows another object, and explicitly delegates some tasks to it.\n    -   Composition provides a superior way to manage delegation since it can selectively delegate the access, even mask some attributes or methods, while inheritance cannot.\n    -   In Python you also avoid the memory problems that might arise when you put many objects inside another; Python handles everything through its reference, i.e. through a pointer to the memory position of the thing, so the size of an attribute is constant and very limited.-   **Composition over Inheritance**\n\nComposition over inheritance(orcomposite reuse principle) in[object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming)(OOP) is the principle that classes should achieve [polymorphic](https://en.wikipedia.org/wiki/Polymorphism_(computer_science))behavior and[code reuse](https://en.wikipedia.org/wiki/Code_reuse)by their[composition](https://en.wikipedia.org/wiki/Object_composition)(by containing instances of other classes that implement the desired functionality) rather than[inheritance](https://en.wikipedia.org/wiki/Inheritance_(computer_science))from a base or parent class.This is an often-stated principle of OOP, such as in the influential book[Design Patterns](https://en.wikipedia.org/wiki/Design_Patterns).\n<https://en.wikipedia.org/wiki/Composition_over_inheritance>-   **Monkey Patching**\n    -   A MonkeyPatch is a piece of Python code which extends or modifies other code at runtime (typically at startup).\n    -   For instance, consider a class that has a methodget_data. This method does an external lookup (on a database or web API, for example), and various other methods in the class call it. However, in a unit test, you don't want to depend on the external data source - so you dynamically replace theget_datamethod with a stub that returns some fixed data.\n    -   Because Python classes are mutable, and methods are just attributes of the class, you can do this as much as you like - and, in fact, you can even replace classes and functions in a module in exactly the same way.\n**Example**\n\nfrom SomeOtherProduct.SomeModule import SomeClass\ndef speak(self):\nreturn \"ook ook eee eee eee!\"\nSomeClass.speak = speak\n<https://thecodebits.com/monkey-patching-in-python-explained-with-examples>\n\n## OOAD (Object Oriented Analysis and Design)**\n\nObject-oriented analysis and design(OOAD) is a popular technical approach for analyzing and designing an application, system, or business by applying[object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming), as well as using visual modeling throughout the[development life cycles](https://en.wikipedia.org/wiki/Software_development_process)to foster better stakeholder communication and product quality.\nAccording to the popular guide[Unified Process](https://en.wikipedia.org/wiki/Unified_Process), OOAD in modern software engineering is best conducted in an iterative and incremental way. Iteration by iteration, the outputs of OOAD activities, analysis models for OOA and design models for OOD respectively, will be refined and evolve continuously driven by key factors like risks and business value.\n<https://en.wikipedia.org/wiki/Object-oriented_analysis_and_design>\n\n## Important Points**\n-   class, object (and the difference between the two)\n-   method (as opposed to, say, a C function)\n-   virtual method, pure virtual method\n-   class/static method\n-   static/class initializer\n-   constructor\n-   destructor/finalizer\n-   superclass or base class\n-   subclass or derived class\n<https://medium.com/@cscalfani/goodbye-object-oriented-programming-a59cda4c0e53>\n\n<https://en.wikipedia.org/wiki/Mixin>\n\n<http://blog.thedigitalcatonline.com/blog/2014/08/20/python-3-oop-part-3-delegation-composition-and-inheritance>\n\n## SOLID principles**\n\n1.  **Single Responsibility Principle**\n\nA class should have one and only one reason to change, meaning that a class should have only one job\n2.  **Open Closed Principle**\n\nObjects or entities should be open for extension, but closed for modification\n3.  **Liskov Substitution Principle**\n\nObjects in a program should be replaceable with instances of their subtypes without altering the correctness of that program\nLetq(x)be a property provable about objects ofxof typeT. Thenq(y)should be provable for objectsyof typeSwhereSis a subtype ofT.\nAll this is stating is that every subclass/derived class should be substitutable for their base/parent class.\n4.  **Interface Segregation Principle**\n\nMany client-specific interfaces are better than one general-purpose interface\nA client should never be forced to implement an interface that it doesn't use or clients shouldn't be forced to depend on methods they do not use.\n5.  **Dependency Inversion principle**\n\nEntities must depend on abstractions not on concretions\nIt states that the high level module must not depend on the low level module, but they should depend on abstractions.\nThe dependency inversion principle tells us that we should always try to have dependencies on interfaces (or Abstract Classes), not classes.\n<https://javapapers.com/oops/association-aggregation-composition-abstraction-generalization-realization-dependency>\n\n<https://scotch.io/bar-talk/s-o-l-i-d-the-first-five-principles-of-object-oriented-design>\n\n## STUPID**\n\nSTUPIDis an acronym that describes bad practices in Oriented Object Programming:\n-   [Singleton](https://williamdurand.fr/2013/07/30/from-stupid-to-solid-code/#singleton)\n-   [Tight Coupling](https://williamdurand.fr/2013/07/30/from-stupid-to-solid-code/#tight-coupling)\n-   [Untestability](https://williamdurand.fr/2013/07/30/from-stupid-to-solid-code/#untestability)\n-   [Premature Optimization](https://williamdurand.fr/2013/07/30/from-stupid-to-solid-code/#premature-optimization)\n-   [Indescriptive Naming](https://williamdurand.fr/2013/07/30/from-stupid-to-solid-code/#indescriptive-naming)\n-   [Duplication](https://williamdurand.fr/2013/07/30/from-stupid-to-solid-code/#duplication)"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2020-02-16 18:00:14 +0500\n\nModified: 2020-07-23 16:19:31 +0500\n\n---\n\n**Behavior-driven development**\n\nIn[software engineering](https://en.wikipedia.org/wiki/Software_engineering),behavior-driven development(BDD) is an[Agile software development](https://en.wikipedia.org/wiki/Agile_software_development)process that encourages collaboration among developers, QA and non-technical or business participants in a software project.It encourages teams to use conversation and concrete examples to formalize a shared understanding of how the application should behave.It emerged from[test-driven development](https://en.wikipedia.org/wiki/Test-driven_development)(TDD). Behavior-driven development combines the general techniques and principles of TDD with ideas from[domain-driven design](https://en.wikipedia.org/wiki/Domain-driven_design)and[object-oriented analysis and design](https://en.wikipedia.org/wiki/Object-oriented_analysis_and_design)to provide software development and management teams with shared tools and a shared process to collaborate on software development.\nAlthough BDD is principally an idea about how software development should be managed by both business interests and technical insight, the practice of BDD does assume the use of specialized software tools to support the development process.Although these tools are often developed specifically for use in BDD projects, they can be seen as specialized forms of the tooling that supports test-driven development. The tools serve to add automation to the[ubiquitous language](https://en.wikipedia.org/wiki/Domain-driven_design#Building_blocks)that is a central theme of BDD.\nBDD is largely facilitated through the use of a simple[domain-specific language](https://en.wikipedia.org/wiki/Domain-specific_language)(DSL) using natural-language constructs (e.g., English-like sentences) that can express the behaviour and the expected outcomes. Test scripts have long been a popular application of DSLs with varying degrees of sophistication. BDD is considered an effective technical practice especially when the \"problem space\" of the business problem to solve is complex.\n<https://en.wikipedia.org/wiki/Behavior-driven_development>\n\n## Frameworks**\n-   Radish\n\n<http://radish-bdd.io>\n\n## REPL Driven Development**\n\nThe[REPL](https://clojure.org/guides/repl/introduction)puts developers in charge of development. Instead of \"compile time\", \"test time\", and \"production time\", you have dynamic, interactive environments to use as you please. You can move seamlessly back and forth between exploration, development, testing, and release, gaining instant feedback at every step and maintaining rich context and history as you go.\nSome of the implications of this are obvious, while others are less so. This talk will move quickly from the basics of REPL development into more interesting territory, including:\n-   the importance of small steps\n-   using[Clojure spec](https://clojure.org/guides/spec)to explore a domain\n-   turning REPL work into example tests\n-   on-the-fly data visualization\n-   exploring Java libraries\n-   saving your work\n**Model Driven Development (MDD)**\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/References/","title":"References"},"frontmatter":{"draft":false},"rawBody":"# References\n\nCreated: 2018-05-09 21:16:02 +0500\n\nModified: 2018-05-09 21:17:13 +0500\n\n---\n\n\"Observer vs Pub-Sub pattern\" @me_shaon <https://hackernoon.com/observer-vs-pub-sub-pattern-50d3b27f838c>\r\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Structural----Facade/","title":"Structural -  Facade"},"frontmatter":{"draft":false},"rawBody":"# Structural -  Facade\n\nCreated: 2018-05-09 20:45:01 +0500\n\nModified: 2020-10-25 23:16:37 +0500\n\n---\n\nAccording to[Gang of Four](https://en.wikipedia.org/wiki/Design_Patterns)definition,facade provides a simplified interface for a set of interfaces of a subsystem. It defines a convenient higher-level interface to interact with.\nThefacade decouples clients from subinterfaces of our modules. The client uses just one object - the facade, to delegate work to many separated components.The client doesn't have to know the details of our architecture - it is all hidden behind a clean interface of the facade.\nWithout Facade\n\n![Facade Pattern](media/Structural----Facade-image1.jpg)\n\nThe client knows the elements of module and orchestrates the work.\nWith Facade\n\n![Facade Pattern](media/Structural----Facade-image2.jpg)\n\nThe client doesn't know anything about subsystem, it communicates with the facade only.\nWhen to use?\n-   when you want to provide an easy interface to interact with a more complicated system\n-   to decouple the clients from implementation details\n-   to use as an entry point to some submodule\nElements\n-   Facade- knows subsystem, orchestrates work, delegates requests to module components\n-   Submodule Class- elements of our module\n-   Client- uses facade to interact with the system\n**Example implementation**\n\nLet's pretend that we're working on an e-commerce application. We want to implement product ordering. We already got a few services in our app, but they are not integrated.\n\npublic interface InventoryService {\n\nboolean isAvailable(int productId);\n\nint order(int productId);\n\n}\npublic interface ShipmentService {\n\nvoid shipTo(int userId, int productId);\n\n}\npublic interface NotificationService {\n\nvoid notifyAboutOrder(int userId, int orderId);\n\nvoid notifyAboutIssue(int userId);\n\n}\n\n(For simplicity, we use ints to represent users, orders etc.)\nWith those building blocks, we want to provide an API to order a product.\npublic interface OrderFacade {\n\nvoid order(int productId, int userId);\n\n}\n\nWe don't want the client code to interact with all services just to make an order, so we provide nice and clean API with Facade.\n\nHere's its implementation:\n\npublic class DefaultOrderFacade implements OrderFacade {\n\n// Dependencies could be injected using Dependency Injection tool\n\npublic DefaultOrderFacade(final InventoryService inventory,\n\nfinal NotificationService notifications,\n\nfinal ShipmentService shipments) {\n\nthis.inventory = inventory;\n\nthis.notifications = notifications;\n\nthis.shipments = shipments;\n\n}\nprivate final InventoryService inventory;\n\nprivate final NotificationService notifications;\n\nprivate final ShipmentService shipments;\n@Override\n\npublic void order(int productId, int userId) {\n\nif (inventory.isAvailable(productId)) {\n\nfinal int orderId = inventory.order(productId);\n\nshipments.shipTo(userId, productId);\n\nnotifications.notifyAboutOrder(userId, orderId);\n\n} else {\n\nnotifications.notifyAboutIssue(userId);\n\n}\n\n}\n\n}\n\nIn this way, instead of using three different services to make an order, we are using just one - the Facade. All services methods are hidden behind the facade - making an order is much simpler now.\npublic class SomeClientCode {\n\n// This could be injected by some DI framework\n\nprivate OrderFacade orderfacde;\npublic static void main(String[] args) {\n\nfinal int someProductId;\n\nfinal int forSomeUserId;\n// ordering product is much easier now with Facade\n\norderFacade.order(someProductId, forSomeUserId);\n\n}\n\n}\n\nIn real apps the services would have even more methods - we could hide them all and provide higher-level APIs by adding more methods to Facades."},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Structural---Adapter/","title":"Structural - Adapter"},"frontmatter":{"draft":false},"rawBody":"# Structural - Adapter\n\nCreated: 2020-02-10 22:35:02 +0500\n\nModified: 2020-02-10 22:38:31 +0500\n\n---\n\nIf there are 3 languages that need to be typed and you know a 4th language than you will not write the same thing 3 times everytime you have to write something, what we do it to create an adapter for the three languages that sit infront of 3 classes that will translate from 4th language to their specific language. All the translations will be handled by the adapter.\nBut in this design pattern also we have to make 3 calls to the 3 adapters everytime. So to solve this we use facade design pattern. When using interfaces we follow facade design pattern. We will make one call to function and this function will call all other functions in the adapter.\n![Adapter pattern example](media/Structural---Adapter-image1.jpg)\n\nSo, as you can see in the picture, the purpose of the adapter is totake one interface and make it compatible with another. In programming, its purpose is the same - you'll want to use this pattern when you have some type/class and you want to make it compatible with another.\n**How to do the trick?**\n\nIn Java, this is done bycreating an Adapter class, whichextends the type we target (our desired interface)andstores a reference to Adaptee (object that needs to be adapted).\n**Elements of the pattern**\n-   Target - our desired type\n-   Adaptee - type that needs to be adapted\n-   Adapter - class that makes Adaptee compatible with Target\n**Relations between components**\n\nThe client of our code (i.e. any place where our classes are used) use Adapters methods, which are compatible with Targets interface, butthe work underneath is delegated to Adaptee object.\n**Code example**\n\nIf you have any difficulties understanding the purpose of this pattern, looking at some code should definitely help.\nFirst, let's declare the interface that we target (the one we want to adapt to)\n\ninterface Target {\n\nString getFirstName();\n\nString getLastName();\n\n}\nNow, we add a class that will be adapted (it is now incompatible with Target interface)\n\nclass Adaptee {\n\nString getFullName() {\n\nreturn \"Patryk Jeziorowski\";\n\n}\n\n}\n\nAs you can see, interfaces of Target and Adaptee differs.Target has two methods - the first method returns a first name and the second returns a last name. In Adaptee, there's only one method that returns first name + last name at once.We can adjust Adaptee to fit Target interface with an Adapter, though!\npublic class Adapter implements Target {\nprivate final Adaptee adaptee = new Adaptee();\n\n@Override\n\npublic String getFirstName() {\n\nreturn adaptee.getFullName().split(\" \")[0]; // just a dumb example impl\n\n}\n@Override\n\npublic String getLastName() {\n\nreturn adaptee.getFullName().split(\" \")[1]; // just a dumb example impl\n\n}\n}\nThe Adapter is compatible with Target by simply implementing it, and under the hood, it delegates the work to the Adaptee object.This allows us to reuse Adaptee implementation anywhere we need object of Target type.\nAt the end, let's take a look at our code in action:\npublic class ClientCode {\n\npublic static void main(String[] args) {\n\nfinal Target target = new Adapter(); // We need Target type, we can use Adapter!\n\nSystem.out.println(target.getFirstName()); // Work delegated to Adaptee under the hood\n\nSystem.out.println(target.getLastName()); // Work delegated to Adaptee under the hood\n\n// output:\n\n// Patryk\n\n// Jeziorowski\n\n}\n\n}\nAnywhere the Target type is needed, we can use Adapter instead, which delegates the hard work to Adaptee.\nfinal Target target = new Adapter();\n// Adapter impl\n\n@Override\n\npublic String getFirstName() {\n\n// use adaptee\n\nreturn adaptee.getFullName().split(\" \")[0];\n\n}\n<https://talkoverflow.com/adapter>\n\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Structural---Bridge/","title":"Structural - Bridge"},"frontmatter":{"draft":false},"rawBody":"# Structural - Bridge\n\nCreated: 2018-08-20 16:24:26 +0500\n\nModified: 2018-08-20 16:37:33 +0500\n\n---\n\n**Bridge**is a structural design pattern that divides one or more classes into two separate hierarchies - abstraction and implementation, allowing them to be changed independently of each other.\nLet's take a simple example. You have a class called Figure that has the subclasses - Circle and Square. You want to expand the shapes hierarchy in color, that is, to have the Red and Blue figures. But to combine all of this, you'll have to create 4 combinations of subclasses, like BlueCircle and RedSquare.\n\nWhen adding new types of shapes and colors, the number of combinations will grow exponentially. For example, to introduce triangles into the program, you'll have to create two new subclasses of triangles for each color. After this, a new color will require the creation of three classes for all kinds of figures. It only gets worse. Visually, it can be featured as:\n\n![design patterns](media/Structural---Bridge-image1.png)\n\nThe root of the problem lies in the fact that we're trying to expand the classes of shapes in two independent planes - in form and color, which leads to the class tree growth.\n\nThe Bridge pattern suggests replacing inheritance with delegation. In order to do this, one of these \"planes\" needs to be established as a separate hierarchy and you'll have to refer to the object of this hierarchy, instead of storing its state and behavior within one class.\n\nThus, we can make Color a separate class with the Red and Blue subclasses. The Figure class gets a link to the Color object and can delegate the work to it, if necessary. This connection will become the bridge between Figure and Color. When adding new color classes, you won't need to touch the shape classes, and vice versa. Schematically it will look like this:\n\n![design patterns](media/Structural---Bridge-image2.png)\n\n**Situations in which the Bridge pattern can help out:**\n\n**1)**When you want to divide a monolithic class that contains several different implementations of some kind of functionality (for example, if the class can work with the different database systems).\n\nThe larger the class, the harder it is to understand its code, and the more it drags out the development. In addition, changes made to one of the implementations lead to editing the entire class, which can cause random errors in the code. The bridge allows you to divide a monolithic class into several separate hierarchies. After that you can change their code independently from each other. This simplifies the work on the code and reduces the likelihood of making errors.\n\n**2)**When a class needs to be expanded in two independent planes.\n\nThe bridge proposes to allocate one of these planes to a separate hierarchy of classes, storing a link to one of its objects in the original class.\n\n**3)**When you want to be able to change the implementation during the execution of the program.\n\nThe bridge allows you to replace the implementation even during the program execution, since a particular implementation is not included in the abstraction class.\n\nBy the way, because of this point Bridge is often confused with Strategy. Note that this function is the lowest on the scale on importance for Bridge, since its main task is structural.\n\n**Pros:**\n\n- Allows to build platform-independent programs.\n\n- Hides unnecessary or dangerous implementation details from the client code.\n\n**Cons:**\n\n- Complicates the program code due to the introduction of additional classes.\n**References**\n\n<https://py.checkio.org/blog/design-patterns-part-3>"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Structural---Decorator/","title":"Structural - Decorator"},"frontmatter":{"draft":false},"rawBody":"# Structural - Decorator\n\nCreated: 2018-04-05 00:17:55 +0500\n\nModified: 2021-08-29 13:15:15 +0500\n\n---\n\nIn[object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming), the**decorator pattern**is a[design pattern](https://en.wikipedia.org/wiki/Design_pattern_(computer_science))that allows behavior to be added to an individual[object](https://en.wikipedia.org/wiki/Object_(computer_science)), either statically or dynamically, without affecting the behavior of other objects from the same[class](https://en.wikipedia.org/wiki/Class_(computer_science)).The decorator pattern is often useful for adhering to the[Single Responsibility Principle](https://en.wikipedia.org/wiki/Single_responsibility_principle), as it allows functionality to be divided between classes with unique areas of concern.The decorator pattern is structurally nearly identical to the[chain of responsibility pattern](https://en.wikipedia.org/wiki/Chain_of_responsibility_pattern), the difference being that in a chain of responsibility, exactly one of the classes handles the request, while for the decorator, all classes handle the request.\n**Starbuzz Coffee**\n\n![class Beverage (object) : # imagine some attributes like temperature, def self) : return \" beverage \" def get _ cost (self) : return O .00 class Coffee( Beverage) : def self) : \" coffee \" return def : return 3.00 class Tee (Beverage) : def self) : return \" tee \" def get _ cost (self) : return 2.50 amount left, . ](media/Structural---Decorator-image1.jpg)\n**Adding Ingredients: First Try**\n\n![class Beverage (object) : init (self, with milk, with sugar): def self. with milk = with milk self .with_sugar = with _ sugar def self) : description = str( self . _ get _ default description( ) ) if self. with milk: description += if self. with _ sugar: description return description with milk\" with _ sugar \" def _get : return \"beverage \" # same for get _ cost... class Coffee( Beverage) : def _get_default description( self) : return \"normal coffee \" # and so on. ](media/Structural---Decorator-image2.jpg)-   But what if we want more ingredients? Open-closed principle?\n**Adding Ingredients: Second Try**\n\n![class CoffeeWithMiIk(Coffee) : def : return ( super ( CoffeeWithMi1k, with milk\" ) def get _ cost (self): self ) . ) return super (CoffeeWithMiIk, self) . get _ cost ( ) + 0.30 class CoffeeWithMi1kAndSugar(CoffeeWithMi1k) : # And so on, what a mess! ](media/Structural---Decorator-image3.jpg)\nWhat we want:\n-   Adding a new ingredient like soy milk should not modify the original beverage classes\n-   Adding new ingredients should be simple and work automatically across all beverages\n**Solution: Decorator Pattern**\n\n![class BeverageDecorator(Beverage) : def init (self, beverage) : init self). super ( BeverageDecorator , _o self. beverage = beverage class Milk (BeverageDecorator) : def self) : return self . beverage . ) + def get _ cost (self) : return self . beverage + 0.30 coffee_with_milk = Milk(Coffee()) with milk \" ](media/Structural---Decorator-image4.jpg)\n-   Composition solves the problem\n-   Note: Do not confuse this with Python function decorators\n\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Structural---Proxy/","title":"Structural - Proxy"},"frontmatter":{"draft":false},"rawBody":"# Structural - Proxy\n\nCreated: 2018-05-09 19:36:25 +0500\n\nModified: 2019-04-19 11:01:32 +0500\n\n---\n\nExample -\n\nIf a site is image heavy website then sending all the images can slow down the website loading time and thereby degrading the user experience. So proxy pattern suggests to have placeholder in the place of images and load images when user scrolls to that section of the page.\n**Problem**\n-   High resolution image on website\n-   Long loading time\n-   Style images\n**Solution**\n-   Replace with placeholders (proxies)\n-   Style placeholders instead\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Types-of-programming-paradigm/","title":"Types of programming paradigm"},"frontmatter":{"draft":false},"rawBody":"# Types of programming paradigm\n\nCreated: 2018-04-01 22:55:19 +0500\n\nModified: 2018-06-23 23:17:21 +0500\n\n---\n\n**Imperative (allows side effect)**\n\nUses statements that change a program's state\n-   Object Oriented Programming Language\n-   Procedural\n\n[Procedural programming](https://en.wikipedia.org/wiki/Procedural_programming)is a type of imperative programming in which the program is built from one or more procedures (also termed[subroutines](https://en.wikipedia.org/wiki/Subroutine)or functions).\n**Declarative (does not state the order in which operations execute)**\n\nFocuses on*what*the program should accomplish without specifying*how*the program should achieve the result.\n-   Functional\n-   Logic\nExample - SQL is a declarative language, we tell to give data and program automatically finds a way to give that data.\n**Symbolic**\n**Others**\n-   Contract-oriented programming language (for writing smart contracts)\n    -   Solidity (used in Ethereum)### Imperative vs Declarative Programming\n\nWhen you need to do something, there is always thewhatand thehowaspects of it. What exactly needs to be done and how do we do it.\n**Imperative programming is about thehow. Declarative programming is about thewhat.**\nAn imperative approach represents a list of steps. Do this first, then do that, and after that do something else. For example:Go over a list of numbers one by one and for every one add its value to a running sum.\nA declarative approach represents what we have and what we need. For example:We have a list of numbers and we need the sum of those numbers.The imperative language is closer to the computers of today because they only know how to execute instructions. The declarative language is closer to how we think and command. Get it done, please. Somehow!\nThe good news is computer languages have evolved. Computer languages offer declarative ways to do the needed imperative computer instructions. Just as cars have evolved from manual stick shift into automatic and self-driving ones!\nImperative programming is like driving a stick shift car. You need to do manual steps (press the clutch, depress it slowly, change gears incrementally, etc). Declarative programming is like driving an automatic carâ€Š---â€Šyou just specify the \"what\": Park or Drive.\nYou cannot program declaratively unless you have the tools that enable you to do so. While you can imperatively drive an automatic car (by switching to manual mode) you cannot declaratively drive a stick shift car. If all you have is a stick shift car, imperative programming is your only obvious choice. This is unless you take the time to install an automatic gear shifter, which might be worth it in the long term. If you can afford a new car, you will probably go with an automatic one unless you are that true nerd who still likes to program with Assembly!\n[Assembly](https://en.wikipedia.org/wiki/Assembly_language)is the original true imperative low-level computer language with pure instructions that directly translate into machine code.\nNote that imperative programming might produce faster programs. Additionally, declarative programming requires less effort from you. In general, it will also require less effort to be maintained. Coding does not have to be one way or the other. Any non-trivial computer program will most likely have a little bit of both approaches. Also, knowing how to code declaratively is great, but it does not mean that you do not need to learn the imperative ways as well. You should simply be confident using both.\nTools that enable you to program declaratively evolve into better and faster ways to get you where you are heading. The ultimate declarative experience with modern cars is the self-driving ones. The \"what\" becomes the destination and the car will do the rest. This is somehow, probably, the future of programming as well. We will have programs that understand all goals and they can just work their magic to generate logic to get us to those goals.\n**References**\n\n<https://medium.freecodecamp.org/hard-coding-concepts-explained-with-simple-real-life-analogies-280635e98e37>\n"},{"fields":{"slug":"/Computer-Science/Security/Attacks/","title":"Attacks"},"frontmatter":{"draft":false},"rawBody":"# Attacks\n\nCreated: 2018-04-26 23:52:19 +0500\n\nModified: 2022-04-23 12:01:26 +0500\n\n---\n\n**Types of Security Attacks**\n\n1.  Passive Attack\n\nThey are having the nature of eavesdropping or monitoring of transmitting channel or packet sniffing\n-   Release of Message Content\n-   Traffic analysis used by intruder to gain the information\n\n2.  Active Attack\n\nInvolves some modification of data stream or creation of false stream\n-   Masquerade\n-   Replay\n-   Modification\n-   Repudiation\n-   Denial of Service\n**Return to Libc Attack (Buffer/Heap overflow attack)**\n\nIt is a[computer security](https://en.wikipedia.org/wiki/Computer_security)attack usually starting with a[buffer overflow](https://en.wikipedia.org/wiki/Buffer_overflow)in which a subroutine[return address](https://en.wikipedia.org/wiki/Return_statement)on a[call stack](https://en.wikipedia.org/wiki/Call_stack)is replaced by an address of a subroutine that is already present in the process' executable memory, bypassing the[NX bit](https://en.wikipedia.org/wiki/NX_bit)feature (if present) and ridding the attacker of the need to inject their own code.-   Modify the return address to point to some function already residing in memory. (This works because single stack is shared by all functions of a process (library or user program)).\nSmashing the Stack for Fun and Profit by Aleph One\n**MITM Attack (Meet in the Middle Attack)**\n\nMITM attack attempts to find a value using both of the range (ciphertext) and domain (plaintext) of the composition of several functions (or block cipher) such that forward mapping through the first functions is the same as backward mapping (inverse image) through the last functions, quite literally meeting in the middle of the composed function.\nThe Multidimensional MITM (MD-MITM) uses a combination of several simultaneous MITM-attacks as described above, where the meeting happens in the multiple positions in the composed function.\n[Man in the Middle & Needham--Schroeder Protocol - Computerphile](https://youtu.be/EtpdLBeIaus)\n**Return Oriented Programming**\n\nA technique by which an attacker can induce arbitrary behavior in a program whose control flow he has diverted - without injecting any code. A return-oriented programs chains together short instruction sequences already present in a program's address space, each of which ends in a \"return\" instruction.\n**DNS Hijacking**\n\nAdvertise malicious ip address for a DNS name\n\nWhere a different ip address is broadcasted for the DNS name, that is pointing to malicious attacker.\n**SQL Injection**\n\n**Linwood_Cole' OR 1=1;#**\n\n**'admin;--**\n**Types of SQL Injection**\n\n![In-Band (Classic) SQL Injection Inferential (Blind) Boolean Out-of-Band Error Union Time ](media/Attacks-image1.jpeg)\n1.  **In-Band SQL Injection**\n    -   **In-band SQLi occurs when the attacker uses the same communication channel to both launch the attack and gather the result of the attack**\n        -   **Retrieved data is presented directly in the application web page**\n    -   **Easier to exploit than other categories of SQLi**\n    -   **Two common types of in-band SQLi**\n        -   **Error-based SQLi**\n        -   **Union-based SQLi**\n\n2.  **Error-Based SQLi**\n    -   **Error-based SQLi is an in-band SQLi technique that forces the database to generate an error, giving the attacker information upon which to refine their injection**\n\n![](media/Attacks-image2.jpg)\n\n3.  **Union-Based SQLi**\n    -   **Union-based SQLi is an in-band SQLi technique that leverages the UNION SQL operator to combine the results of two queries into a single result set**\n\n![](media/Attacks-image3.jpg)\n\n4.  **Inferential (Blind) SQL Injection**\n    -   **SQLi vulnerability where there is no actual transfer of data via the web application**\n    -   **Just as dangerous as in-band SQL injection**\n        -   **Attacker able to recontrust the information by sending particular requests and observing the resulting behavior of the DB server**\n    -   **Takes longer to exploit than in-band SQL injection**\n    -   **Two common types of blind SQLi**\n        -   **Boolean-based SQLi**\n        -   **Time-based SQLi**\n\n5.  **Boolean-based Blind SQLi**\n    -   **Boolean-based SQLi is a blind SQLi technique that uses Boolean conditions to return a different result depending on whether the query returns a TRUE or FALSE result**\n\n![](media/Attacks-image4.jpeg)\n\n![](media/Attacks-image5.jpeg)\n6.  **Time-based Blind SQLi**\n    -   **Time-based SQLi is a blind SQLi technique that relies on the database pausing for a specified amount of time, then returning the results, indicating a successful SQL query execution**\n    -   **Example Query**\n\nIf the first character of the administrator's hashed password is an 'a', wait for 10 seconds\n-   response takes 10 seconds -> first letter is 'a'\n-   response doesn't take 10 seconds -> first letter is not 'a'\n\n7.  **Out-of-band (OAST) SQLi**\n    -   **Vulnerability that consists of triggering an out-of-band network connection to a system that you control**\n        -   **Not common**\n        -   **A variety of protocols can be used (ex. DNS, HTTP)**\n    -   **Example Payload:**\n\n![, exec master. .xp_dirtree ' //Ã¸efdymgw105w9inae8mg4dfrgim9ay. burpcollaborator .net/aâ€¢ ](media/Attacks-image6.jpg)\n**Tools**\n\n[https://github.com/sqlmapproje ct/sqlmap](https://github.com/sqlmapproje%20ct/sqlmap)\n[**https://www.netsparker.com/blog/web-security/sql-injection-cheat-sheet/**](https://www.netsparker.com/blog/web-security/sql-injection-cheat-sheet/)\n\n<https://dotweak.com/2019/08/16/sql-injection-tutorial-for-beginners-Zm5NSWw3MjJCUVMrT2hmWUdNeTZiQT09>\n\n<https://www.freecodecamp.org/news/what-is-sql-injection-how-to-prevent-it>\n\n## Row Hammer Attack**\n\nRow hammer(also written asrowhammer) is a security exploit that takes advantage of an unintended and undesirable side effect in[dynamic random-access memory](https://en.wikipedia.org/wiki/Dynamic_random-access_memory)(DRAM) in which[memory cells](https://en.wikipedia.org/wiki/Memory_cell_(computing))leak their charges by interactions between themselves, possibly leaking or changing the contents of nearby[memory rows](https://en.wikipedia.org/wiki/Memory_row)that were not[addressed](https://en.wikipedia.org/wiki/Memory_address)in the original memory access. This bypass of the isolation between DRAM memory cells results from the high cell density in modern DRAM, and can be triggered by specially crafted[memory access patterns](https://en.wikipedia.org/wiki/Memory_access_pattern)that rapidly activate the same memory rows numerous times. While cell charge leakage is normal and mitigated by refreshes, additional leakage occurs during a rowhammer attack which causes cells to leak enough charge to change its content within a refresh interval.\nThe row hammer effect has been used in some[privilege escalation](https://en.wikipedia.org/wiki/Privilege_escalation)computer security[exploits](https://en.wikipedia.org/wiki/Exploit_(computer_security)),and network-based attacks are also theoretically possible in a fast network connection between the attacker and victim.\nDifferent hardware-based techniques exist to prevent the row hammer effect from occurring, including required support in some[processors](https://en.wikipedia.org/wiki/Central_processing_unit)and types of DRAM[memory modules](https://en.wikipedia.org/wiki/Memory_module).[[9]](https://en.wikipedia.org/wiki/Row_hammer#cite_note-intel-d2s2e4-9)[[10]](https://en.wikipedia.org/wiki/Row_hammer#cite_note-memcon-net105-10)Row hammer rarely or never affects[DDR](https://en.wikipedia.org/wiki/DDR_SDRAM)and[DDR2](https://en.wikipedia.org/wiki/DDR2_SDRAM)SDRAM modules[[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)]. It affects many[DDR3](https://en.wikipedia.org/wiki/DDR3_SDRAM)and[DDR4](https://en.wikipedia.org/wiki/DDR4_SDRAM)SDRAM modules.\n<https://en.wikipedia.org/wiki/Row_hammer>\n\n## HTTP Desync**\n\nApplication Load Balancer (ALB) and Classic Load Balancer (CLB) now support HTTP Desync Mitigation Mode, a new feature that protects your application from issues due to HTTP Desync. Modern day web applications are typically built with a chain of proxies that ensure fast and reliable communication between clients and servers. While these proxies follow a standard mechanism to parse RFC 7230 compliant HTTP/1.1 requests, they may have differences in interpretation while parsing non-compliant requests. These differences in interpretation can cause Desync where different proxies in the chain may disagree on request boundaries and therefore may not process the same request. This could leave behind arbitrary messages that may be prepended to the next request in the queue and smuggled to the backend. Ultimately, request smuggling can make applications vulnerable to request queue or cache poisoning, which could lead to credential hijacking or execution of unauthorized commands.\n<https://portswigger.net/research/http-desync-attacks-request-smuggling-reborn>\n\n## Man-in-the-Middle attack (MITM)**\n\nA[man-in-the-middle attack](http://searchsecurity.techtarget.com/definition/man-in-the-middle-attack)is one in which the attacker secretly relays and possibly alters the communication between two parties who believe they are directly communicating with each other. One example is active eavesdropping, in which the attacker makes independent connections with the victims and relays messages between them to make them believe they are talking directly to each other over a private connection, when in fact the entire conversation is controlled by the attacker, who even has the ability to modify the content of each message. Often abbreviated to**MITM,MitM, orMITMA,** and sometimes referred to as asession hijacking attack, it has a strong chance of success if the attacker can impersonate each party to the satisfaction of the other. MITM attacks pose a serious threat to online security because they give the attacker the ability to capture and manipulate sensitive information in real-time while posing as a trusted party during transactions, conversations, and the transfer of data. This is straightforward in many circumstances; for example, an attacker within reception range of an unencrypted WiFi access point, can insert himself as a man-in-the-middle.\n"},{"fields":{"slug":"/Computer-Science/Security/Authentication/","title":"Authentication"},"frontmatter":{"draft":false},"rawBody":"# Authentication\n\nCreated: 2018-12-04 12:24:07 +0500\n\nModified: 2022-10-01 10:16:58 +0500\n\n---\n\n**Methods of Authentication**\n-   **Single Factor authentication**\n\nThis is often used as the authentication process for lower risk systems. You only need a single factor to authenticate, with the most common being a password, so it's more vulnerable to phishing attacks and key loggers.\nIn addition to this, a recent[article](https://dataprot.net/statistics/password-statistics/)by DataProt showed that 78% of Gen-Z people utilize the same password for multiple services. This means that if an attacker gained access to one user account, they have a high probability of gaining access to others by simply using the same password.-   **2-Factor Authentication**\n\nThis method is more secure, as it comprises two factors of authentication -- typically something you know, for example username and password , plus something you have / own, for example a phone SMS or a security token.\nFor 2-factor authentication, you would enter a one-time SMS password sent to your device, or perhaps a linked authenticator app code and provide an ever-changing access code.\nAs you can imagine, this is a lot more secure than simply entering a password, or a single authentication credential. You would need to know the login credentials, as well as have access to the physical device for the second part.\n2-factor authentication has become very common amongst online services in recent years, and with many large companies it is the default authentication method. Many require that you setup 2-factor auth in order to even utilize the service.-   **Multi-Factor Authentication**\n\nGoing one step further to make your authentication process even more secure is having 3 or more factors. This form of authentication usually works on the premise of:\n-   something you know (username + password or a username + security question and answer)\n-   something you have (mobile phone sms, authenticator app, USB key)\n-   something you are (like a fingerprint / face recognition)\nFor these reasons, multi-factor authentication offers the most protection, as you would need to compromise multiple factors, and these factors are a lot more difficult to \"hack\" or replicate.\nThe downside to this method of authentication, and the reason it's not utilized in many average systems, is it can be cumbersome to setup and maintain. So the data / system you're protecting really has to justify the need for such security.\n**Types of Authentication**\n-   Cookie-Based authentication\n-   Token-Based authentication\n-   Third party access(OAuth, API-token)\n-   OpenId\n-   SAML (Security Assertion Markup Language)\n**Basic Access Authentication (BA)**\n\nIn the context of an[HTTP](https://www.wikiwand.com/en/HTTP)transaction,basic access authenticationis a method for an[HTTP user agent](https://www.wikiwand.com/en/User_agent)(e.g. a[web browser](https://www.wikiwand.com/en/Web_browser)) to provide a[user name](https://www.wikiwand.com/en/User_name)and[password](https://www.wikiwand.com/en/Password)when making a request. In basic HTTP authentication, a request contains a header field in the form ofAuthorization: Basic < credentials >, where credentials is the[Base64](https://www.wikiwand.com/en/Base64)encoding of ID and password joined by a single colon\nHTTP Basic authentication (BA) implementation is the simplest technique for enforcing[access controls](https://www.wikiwand.com/en/Access_controls)to web resources because it does not require[cookies](https://www.wikiwand.com/en/HTTP_cookie), session identifiers, or login pages; rather, HTTP Basic authentication uses standard fields in the[HTTP header](https://www.wikiwand.com/en/HTTP_header).\nThe BA mechanism does not provide[confidentiality](https://www.wikiwand.com/en/Information_security#Confidentiality)protection for the transmitted credentials. They are merely encoded with[Base64](https://www.wikiwand.com/en/Base64)in transit and not[encrypted](https://www.wikiwand.com/en/Encryption)or[hashed](https://www.wikiwand.com/en/Cryptographic_hash)in any way. Therefore, basic authentication is typically used in conjunction with[HTTPS](https://www.wikiwand.com/en/HTTPS)to provide confidentiality.\n\nBecause the BA field has to be sent in the header of each HTTP request, the web browser needs to[cache](https://www.wikiwand.com/en/Cache_(computing))credentials for a reasonable period of time to avoid constantly prompting the user for their username and password. Caching policy differs between browsers.\nThis is the simplest to implement and for some implementations can work well, however it requires transport level encryption as the user name and password are presented with ever request.\n<https://www.wikiwand.com/en/Basic_access_authentication>\n\n## Digest Access Authentication**\n\nDigest access authenticationis one of the agreed-upon methods a[web server](https://www.wikiwand.com/en/Web_server)can use to negotiate credentials, such as username or password, with a user's[web browser](https://www.wikiwand.com/en/Web_browser). This can be used to confirm the identity of a user before sending sensitive information, such as online banking transaction history. It applies a[hash function](https://www.wikiwand.com/en/Hash_function)to the username and[password](https://www.wikiwand.com/en/Password)before sending them over the network. In contrast,[basic access authentication](https://www.wikiwand.com/en/Basic_access_authentication)uses the easily reversible[Base64](https://www.wikiwand.com/en/Base64)encoding instead of hashing, making it non-secure unless used in conjunction with[TLS](https://www.wikiwand.com/en/Transport_Layer_Security).\nTechnically, digest authentication is an application of[MD5](https://www.wikiwand.com/en/MD5)[cryptographic hashing](https://www.wikiwand.com/en/Cryptographic_hash)with usage of[nonce](https://www.wikiwand.com/en/Cryptographic_nonce)values to prevent[replay attacks](https://www.wikiwand.com/en/Replay_attack). It uses the[HTTP](https://www.wikiwand.com/en/Hypertext_Transfer_Protocol)protocol.\n<https://www.wikiwand.com/en/Digest_access_authentication>\n\n## Cookie-Based Authentication (Stateful)**\n\nThe client posts the login credential to the server, server verifies the credential and creates session id which is stored in server(state-full) and returned to client via set-cookie. On subsequent request the session id from the cookie is verified in the server and the request get processed. Upon logout session id will be cleared from both client cookie and server.\n\n![](media/Authentication-image1.png)\n\n**Flow**\n-   Enter login credentials\n-   Server verifies given credentials, creates a session and stores in database.\n-   Cookie + Session ID will be kept in client side(User browser)\n-   For consequent requests, session ID will be verified against database.\n-   Session will be destroyed from client and server side once the use logs out\nWhen a server receives an HTTP request in the response, it can send aSet-Cookie header. The browser puts it into a cookie jar, and the cookie will be sent along with every request made to the same origin in theCookieHTTP header.\n**Tips**\n-   HttpOnly Cookies - To mitigate the possibility of XSS attacks always use theHttpOnlyflag when setting cookies. This way they won't show up indocument.cookies.\n-   Signed Cookies - With signed cookies, a server can tell if a cookie was modified by the client.\n**Cons**\n-   The main disadvantage of using this authentication method is, server has to store all the session data for each and every user and increases the overhead in the server.\n-   Need to make extra effort to mitigate[CSRF attacks](https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF))\n-   Incompatibility with REST - as it introduces a state into a stateless protocol\n**Token-Based Authentication (Stateless)**\n\nToken based authenticationis gaining in popularity because of the rise in single page applications (SPA) and statelessness (RESTful API's) of the application. There are different ways to implement token based authentication, we will focussing on most commonly used JSON Web Token(JWT). On receiving the credentials from client the server validates the credentials and generates a signed JWT which contains the user information. Note, the token will never get stored in server(stateless). On subsequent request the token will be passed to server and gets verified(decoded) in the server. The token can be maintained at client side in local storage, session storage or even in cookies.\n**Flow**\n-   User provides credentials\n-   Server verifies credentials and returns a signed token.\n-   Token is stored in client side\n-   Subsequent requests to the server will be sent with the token as authentication header (HTTP header).\n-   Server verifies the token (JSON web token) and return required data.\n-   Token is destroyed in client, once the user logs out.\n**Ex - JWT**\n**Third-party Access**\n\nIf we have a need to expose our API's outside of our system like third party app or even to access it from mobile apps we end up in two common ways to share the user information.Via*API-token*which is same as *JWT token*, where the token will be send via Authorization header which will get handled at API gateway to authenticate the user. And the other option is via*Open Authentication(OAuth)*, OAuthis a protocol that allows an application to authenticate against server as a user. The recommendation is to implement OAuth 1.0a or OAuth 2.0. OAuth 2.0 relies on HTTPS for security and it currently implemented by Google, Facebook, Twitter etc., OAuth 2 provides secured delegate access to a resource based on user. OAuth 2 does this by allowing a token to be issued by Identity provider to these third party applications, with the approval of user. The client then uses the token to access the resource on behalf of that user.\n**SAML (Security Assertion Markup Language)**\n\nMakes use of the same Identity provider which we saw in OpenId, but it is XML based and more flexible. The recommended version for SAML is 2.0. SAML also provides a way to achieve Single SignOn(SSO), user can make use of the Identity provider URL to login into the system which redirects with XML data back to your application page which can then be decoded to get the user information. We have SAML providers like G Suite, Office 365, OneLogin, Okta etc.,\nSAML is an XML-based open-standard for transferring identity data between two parties: an identity provider (IdP) and a service provider (SP).\n\nIdentity Provider--- Performs authentication and passes the user's identity and authorization level to the service provider.\n\nService Provider--- Trusts the identity provider and authorizes the given user to access the requested resource.\n**Benefits**\n-   Improved User Experience:Users only need to sign in one time to access multiple service providers.\n-   Increased Security:SAML provides a single point of authentication, which happens at a secure identity provider, SAML provider can apply context-based policies to access applications.\n-   Loose Coupling of Directories:SAML doesn't require user information to be maintained and synchronized between directories.\n-   Reduced Costs for Service Providers:With SAML, you don't have to maintain account information across multiple services. The identity provider bears this burden.\n<https://duo.com/blog/the-beer-drinkers-guide-to-saml>\n\n## Tricks**\n-   For Single SignOn OpenId has taken most of the consumer market, SAML is often the choice for many enterprise application.\n-   If you have to support only web application go for Cookie or Token based authentication.\n-   If you have to support both web as well mobile client go with API-token with that of Cookie based authentication.\n-   On top of above authentication methods if needed we can also implement One Time Password(OTP), Two Factor Authentication(2FA), Email verification etc.,\n**SSO**\n\n[Single Sign On Authentication](https://auth0.com/blog/what-is-and-how-does-single-sign-on-work/)provides your users with a seamless authentication experience when they navigate either through the applications you have built and/or third party apps. That is once you log into one of these applications, you won't have to enter your credentials again when entering another one, as you will be automatically logged in all of them, regardless of the platform, technology, or domain. Don't make your internal employees nor your external users go through the hassle of maintaining and remembering yet another credential.\nSingle Sign On works by having acentral server, which all the applications trust. When you login for the first time a cookie gets created on this central server. Then, whenever you try to access a second application, you get redirected to the central server, if you already have a cookie there, you will get redirected directly to the app with a token, without login prompts, which means you're already logged in.\nFor example, Google implements Single Sign On in its services. Google's central server is[https://accounts.google.com](https://accounts.google.com/). Once you are logged in this server, you will be able to accessGmail,Youtube, andGoogle Docswithout entering your credentials again.\n**WHAT IS MULTIFACTOR AUTHENTICATION?**\n\nMultifactor authentication (MFA) is a method of identifying users by presenting several separate authentication stages. Some of those stages could be Time-based One-Time Password (TOTP), Mobile verification, a hardware token, among others. 2-Factor Authentication (2FA) is the most used type of MFA.\n<https://auth0.com/learn/multifactor-authentication>\nIdP - Identity Provider\n\n**IAM - Identity and Access Management**\n-   [ORY Hydra](https://www.ory.sh/)\n\n<https://www.ory.sh>\n\n<https://github.com/ory/hydra>-   [Keycloak](https://www.keycloak.org/)\n-   Okta\n-   Auth0\n-   Supertokens\n\n<https://supertokens.io>\n-   **jumpcloud (Oyster)**\n\n<https://jumpcloud.com/daas-glossary/identity-management>\n-   <https://magic.link>\n<https://withblue.ink/2020/04/08/stop-writing-your-own-user-authentication-code.html>\n\n## Signed Request**\n\nRequest signing is the process to add authentication information to Credit Saison India requests sent by HTTP.\nFor security, all requests to Credit Saison India must be signed with your credentials, which consists of an Client ID and Client Secret. These two keys are commonly referred to as your security credentials.\nWhen an Credit Saison India service receives the request, it performs the same steps that you did to calculate the signature you sent in your request. Credit Saison India then compares its calculated signature to the one you sent with the request. If the signatures match, the request is processed. If the signatures don't match, the request is denied. To get started with the signing process, see[Signing Credit Saison India Requests](http://ksf-documentation.s3-website.ap-south-1.amazonaws.com/#authentication-signing-credit-saison-india-requests)\n1.  Generate a Hash of your Request Body. Encode the body of your request using UTF-8 and generate a hash for it using the SHA256 algorithm and then converting into a Hex Digest. The Output would then be standardised into Uppercase. This would be calledH1\n\nNote: If your request contains no body. Your signature should be a hash of the Litral `null`\n2.  Adding a TimeStamp to the the request. Using this Credit Saison India's Servers would validate that the requet that has been sent by you has been recieved by us with 10 minutes of your application having sent it.\n    The TimeStamp needs to be in the[UNIX Time](http://unixtimestamp.50x.eu/about.php)milliseconds (13-digit) format. Denoted assignedDateand added to the reqest as a query parameter.\n3.  Create a Canonical Request to generate the Signature. Arrange the contents of your request (host, action, headers, etc.) into a standard (canonical) format. A Canonical request is just the representation of the all parts of a request in a single string seperated using annew line character. We will call this canonical request asC1.\n\nNote: We would not be using the header as part of the canonical request\n\nNote: All query parameters need to be URL Encoded\nQuery parameters needs to be in a sorted in an acending order while generating the canonical request\n4.  Generate a hash of the canonical request. Similar to Step 1 here we create a SHA256 Hash of C1 by first converting it to UTF-8 encoded string and then hashing it. We will call this hash asH2.\n5.  Generate a keyed Hash for the whole Request. The output of Step 4 (H2) is the final string that needs to be hashed using the client secret already shared with you.[HMAC](https://en.wikipedia.org/wiki/HMAC)requires a key and a hashing algorithm to create a signature. We are using the SHA256 algorithm and the client secret to create a signature here. We will call this signature asP1. P1 in turn needs to be BASE64 Encoded.\n6.  Add the Signature to the HTTP Request After you calculate the signature, add it to an HTTP header of the request to Credit Saison India' Servers using the headersignature\n7.  Add Authentication Headers As part of the of your request to the Credit Saison India Servers. You would also have to pass theAPIKeyandUsernameasx-api-keyandusernameHeaders respectively.\n**References**\n\n<https://medium.com/@vivekmadurai/different-ways-to-authenticate-a-web-application-e8f3875c254a>\n\n<https://blog.risingstack.com/web-authentication-methods-explained>\n[Identity and Access Management: Technical Overview](https://www.youtube.com/watch?v=Tcvsefz5DmA)\n**OAuth2**\n\nOAuth2 is a specification that defines several ways to handle authentication and authorization.\nIt is quite an extensive specification and covers several complex use cases.\nIt includes ways to authenticate using a \"third party\".\nThat's what all the systems with \"login with Facebook, Google, Twitter, GitHub\" use underneath.\n**OAuth 1**\n\nThere was an OAuth 1, which is very different from OAuth2, and more complex, as it included directly specifications on how to encrypt the communication.\nIt is not very popular or used nowadays.\nOAuth2 doesn't specify how to encrypt the communication, it expects you to have your application served with HTTPS.\n**OpenID Connect**\n\nOpenID Connect is another specification, based onOAuth2.\nIt just extends OAuth2 specifying some things that are relatively ambiguous in OAuth2, to try to make it more interoperable.\nFor example, Google login uses OpenID Connect (which underneath uses OAuth2).\nBut Facebook login doesn't support OpenID Connect. It has its own flavor of OAuth2.\n**OpenID (not \"OpenID Connect\")**\n\nThere was also an \"OpenID\" specification. That tried to solve the same thing asOpenID Connect, but was not based on OAuth2.\nSo, it was a complete additional system.\nIt is not very popular or used nowadays.\n<https://fastapi.tiangolo.com/tutorial/security>\n\n"},{"fields":{"slug":"/Computer-Science/Security/Concepts/","title":"Concepts"},"frontmatter":{"draft":false},"rawBody":"# Concepts\n\nCreated: 2018-04-27 01:08:04 +0500\n\nModified: 2022-04-03 19:33:20 +0500\n\n---\n\n**Security Services**\n\n1.  Data confidentiality\n\n2.  Data integrity\n\n3.  Authentication\n\n4.  Non-repudiation\n\n5.  Access Control\nAlice, Bob and Trudy (Adversary)\n**Managing Passwords and Application Secrets: Common Anti-Patterns**\n\n1.  The Shared Password\n\n2.  The FILE\n\n3.  Sharing Over Email\n\n4.  Sharing Over Slack, Skype, SMS, iMessage, Whatsapp, etc.\n\n5.  Web-based Pastebin, Exploding Message, and Encrypted Chat Services\n\n6.  The Git Repo, Unencrypted\n\n7.  Gitignored Files and Environment Variables (The 12-Factor App Methodology)\n\n8.  The Git Repo, Encrypted\n\n9.  Not Protecting Development-Level Secrets\n\n10. Custom Secrets Management\n\n11. **Faith-based security (FBS)**, a cousin of Security Through Obscurity (STO)\n<https://blog.envkey.com/managing-passwords-and-secrets-common-anti-patterns-2d5d2ab8e8ca>\n\n## Access control models**\n\n1.  **[Attribute-based Access Control](https://en.wikipedia.org/wiki/Attribute-based_access_control)(ABAC)**\n\nAn access control paradigm whereby access rights are granted to users through the use of policies which evaluate attributes (user attributes, resource attributes and environment conditions)\n\n2.  **[Discretionary Access Control](https://en.wikipedia.org/wiki/Discretionary_Access_Control)(DAC)**\n\nIn DAC, the data owner determines who can access specific resources. For example, a system administrator may create a hierarchy of files to be accessed based on certain permissions.\n\n3.  [History-Based Access Control](https://en.wikipedia.org/w/index.php?title=History-based_Access_Control&action=edit&redlink=1)(HBAC)\n\nAccess is granted or declined based on the real-time evaluation of a history of activities of the inquiring party, e.g. behavior, time between requests, content of requests.For example, the access to a certain service or data source can be granted or declined on the personal behavior, e.g. the request interval exceeds one query per second.\n\n4.  [History-of-Presence Based Access Control](https://en.wikipedia.org/w/index.php?title=History-of-Presence_Based_Access_Control&action=edit&redlink=1)(HPBAC)\n\nAccess control to resources is defined in terms of presence policies that need to be satisfied by presence records stored by the requestor. Policies are usually written in terms of frequency, spread and regularity. An example policy would be \"The requestor has made k separate visitations, all within last week, and no two consecutive visitations are apart by more than T hours.\"\n\n5.  [Identity-Based Access Control](https://en.wikipedia.org/wiki/Identity-based_security)(IBAC)\n\nUsing this network administrators can more effectively manage activity and access based on individual needs.\n\n6.  **[Mandatory Access Control](https://en.wikipedia.org/wiki/Mandatory_Access_Control)(MAC)**\n\nIn MAC, users do not have much freedom to determine who has access to their files. For example, security clearance of users and classification of data (as confidential, secret or top secret) are used as security labels to define the level of trust.\n\n7.  [Organization-Based Access control](https://en.wikipedia.org/wiki/Organisation-based_access_control)(OrBAC)\n\nOrBAC model allows the policy designer to define a security policy independently of the implementation\n\n8.  **[Role-Based Access Control](https://en.wikipedia.org/wiki/Role-based_access_control)(RBAC)**\n\nRBAC allows access based on the job title. RBAC largely eliminates discretion when providing access to objects. For example, a human resources specialist should not have permissions to create network accounts; this should be a role reserved for network administrators.\n<https://en.wikipedia.org/wiki/Role-based_access_control>\n9.  [Rule-Based Access Control](https://en.wikipedia.org/w/index.php?title=Rule-based_access_control&action=edit&redlink=1)(RAC)\n\nRAC method is largely context based. Example of this would be only allowing students to use the labs during a certain time of day.\n\n10. Responsibility Based Access control\n\nInformation is accessed based on the responsibilities assigned to an actor or a business role\n<https://en.wikipedia.org/wiki/Access_control>\n![A CRYPTO NERD'S HIS LAPTOPS ENCRYPTED. LETS NIL.D A MIUOJ-DOUAR CWSTER To CRACK IT. NO GOODI tT's -BIT RSA! OCR EVIL WHAT ACTVALLY HAPPEN: Hts LAPrOPS ENCRYPrzD. Hin AND HIT Hin WITH $5 WRENCH HE TELLS VS THZ PA$..ORD. ](media/Concepts-image1.png)\n**Entity vs Identity**\n\nEntity is a**thing**that exists as an**individual unit**while identity is a**set of attributes that can be used to distinguish the entity within a context**.\nLet's assume we know Mike. He is a young man who lives in Poland. He is working as a software engineer for one of the local IT startups.**Mike is the entity.He has many identities**i.e. he can be defined as a young polish man in one context and as a promising software engineer in another. Other people may perceive Mike (entity) using different subsets of his attributes (identities).\nIn the software world, your backend could be described as an entity. It's thething. Your UI application perceives it as a URL and a certificate (one identity). Your database, on the other hand, sees it as a different identity -- a set of credentials that grants access to the database.\n**Authentication vs Authorization (AuthN vs AuthZ)**\n\n**Authentication**is the process of ascertaining that somebody really is who he claims to be.In practical terms, it's the process of verifying username and password (login).\nAuthentication is a form of confirming the identity of the entity.\n**Authorization**refers to rules that determine who is allowed to do what. E.g. Adam may be authorized to create and delete databases, while Usama is only authorized to read.\nAuthorization is a process of verifying if a given entity can access or perform actions on a given resource\n**AAA (Authentication, Authorization and Audit)**\n\n**AAA**refers to**[Authentication](https://en.wikipedia.org/wiki/Authentication),[Authorization](https://en.wikipedia.org/wiki/Authorization)and[Accounting](https://en.wikipedia.org/wiki/Accounting)**. It is used to refer to a family of protocols that mediate network access.\n**Auditing**\n-   What happened?\n-   When it happened?\n-   Who initiated it?\n-   On what did it happen?\n-   Where was it observed?\n-   From where was it initiated?\n-   To where was it going?\n<https://en.wikipedia.org/wiki/AAA_(computer_security)>\ny\n**Certificate Authority**\n\nCA - Trusted entity that if the public key is from the correct target.\n\nA certificate authority (CA) is a third-party organization with 3 main objectives:\n\n1.  Issuing certificates\n\n2.  Confirming the identity of the certificate owner\n\n3.  Providing proof that the certificate is valid\nA root store is basically a database of trusted CAs.\n\nApple, Windows, and Mozilla run their own root stores that they pre-install in your computer or device.\nWhich certificate should you buy? You have basically 3 flavors.\n\n1.  Domain validated. The certificate just verifies the domain name, and nothing else. You probably need this one.\n\n2.  Organization validated. The certificate requires the validation and manual verification of the organization behind the certificate.\n\n3.  Extended validation. The certificate requires an exhaustive verification of the business.\nHow do certificates get validated?\n-   When a CA issues a certificate, they sign the certificate with their root certificate pre-installed in the root store.\n-   Most of the time it's an intermediate certificate signed with a root certificate.\n-   If a cat-astrophy would occur and the root certificate is compromised, it's easier to revoke the intermediate certificates, since the root certificates are installed on each device.\n-   Let's walk through how a certificate is validated. The process is based on a 'chain of trust'.\n-   Your browser connects to a site via HTTPS and downloads the certificate. (The certificate is not a root certificate.)\n-   Your browser downloads the certificate that was used to sign the certificate on the site. (But this certificate is still not the root certificate.)\n-   Your browser once more looks up the certificate that signed the intermediate certificate. (It's the root certificate!)\n-   The entire certificate chain is trusted, and thus the site certificate is trusted as well.\n-   In the event that the last certificate is not a root certificate, and there are no more certificates to download, the chain is untrusted.\n<https://howhttps.works/certificate-authorities>\n\n## CSR (Certificate Signing Request)**\n\nA CSR or Certificate Signing request is a block of encoded text that is given to a Certificate Authority when applying for an SSL Certificate. It is usually generated on the server where the certificate will be installed and contains information that will be included in the certificate such as the organization name, common name (domain name), locality, and country. It also contains the public key that will be included in the certificate. A private key is usually created at the same time that you create the CSR, making a key pair. A CSR is generally encoded using ASN.1 according to the PKCS #10 specification.\nA[certificate authority](https://www.sslshopper.com/certificate-authority-reviews.html)will use a CSR to create your SSL certificate, but it does not need your private key. You need to keep your private key secret. The certificate created with a particular CSR will only work with the private key that was generated with it. So if you lose the private key, the certificate will no longer work.\n<https://www.sslshopper.com/what-is-a-csr-certificate-signing-request.html>\n\n## Server Name Indication(SNI)**\n\nServer Name Indication(SNI) is an extension to the[Transport Layer Security](https://en.wikipedia.org/wiki/Transport_Layer_Security)(TLS) computer[networking protocol](https://en.wikipedia.org/wiki/Networking_protocol)by which a[client](https://en.wikipedia.org/wiki/Client_(computing))indicates which[hostname](https://en.wikipedia.org/wiki/Hostname)it is attempting to connect to at the start of the handshaking process.This allows a server to present multiple[certificates](https://en.wikipedia.org/wiki/Public_key_certificate)on the same[IP address](https://en.wikipedia.org/wiki/IP_address)and[TCP port](https://en.wikipedia.org/wiki/TCP_port)number and hence allows multiple secure ([HTTPS](https://en.wikipedia.org/wiki/HTTP_Secure)) websites (or any other[service](https://en.wikipedia.org/wiki/Server_(computing)#Types_of_servers)over TLS) to be served by the same IP address without requiring all those sites to use the same certificate. It is the conceptual equivalent to HTTP/1.1 name-based[virtual hosting](https://en.wikipedia.org/wiki/Virtual_hosting), but for HTTPS. The desired hostname is not encrypted in the original SNI extension, so an eavesdropper can see which site is being requested.\nServer Name Indication (SNI) is an extension to the TLS protocol by which a client indicates the hostname to connect to at the start of the TLS handshake. The load balancer can present multiple certificates through the same secure listener, which enables it to support multiple secure websites using a single secure listener. Application Load Balancers also support a smart certificate selection algorithm with SNI. If the hostname indicated by a client matches multiple certificates, the load balancer determines the best certificate to use based on multiple factors including the capabilities of the client.\n<https://en.wikipedia.org/wiki/Server_Name_Indication>\n\n## What is ESNI (Encrypted Server Name Indication)?**\n\nEncrypted Server Name Indication (ESNI) is an extension to TLS 1.3 which prevents eavesdroppers from knowing the domain name of the website network users are connecting to. When combined with encrypted DNS, it is not possible to know which websites a user is visiting.\n**Security Certificates**\n\n1.  .csr (Certificate Signing Request)\n\n2.  .pem (Privacy Enhanced Mail)\n\n3.  .key\n\n4.  .pkcs12 .pfx .p12 (contain both private and public certificate pair)\n\n5.  .cert (recognized by windows explorer)\n<https://serverfault.com/questions/9708/what-is-a-pem-file-and-how-does-it-differ-from-other-openssl-generated-key-file/9717#9717>\n\n<https://cryptopals.com/sets/1>\n\n## PEM file**\n\nPrivacy-Enhanced Mail(PEM) is a[de facto](https://en.wikipedia.org/wiki/De_facto)file format for storing and sending cryptographic[keys](https://en.wikipedia.org/wiki/Key_(cryptography)),[certificates](https://en.wikipedia.org/wiki/Public_key_certificate), and other data, based on a set of 1993[IETF](https://en.wikipedia.org/wiki/Internet_Engineering_Task_Force)standards defining \"privacy-enhanced mail.\" While the original standards were never broadly adopted, and were supplanted by[PGP](https://en.wikipedia.org/wiki/Pretty_Good_Privacy)and[S/MIME](https://en.wikipedia.org/wiki/S/MIME), the textual encoding they defined became very popular.\nPEM data is commonly stored in files with a \".pem\" suffix, a \".cer\" or \".crt\" suffix (for certificates), or a \".key\" suffix (for public or private keys).The label inside a PEM file represents the type of the data more accurately than the file suffix, since many different types of data can be saved in a \".pem\" file.\nA PEM file may contain multiple instances. For instance, an operating system might provide a file containing a list of trusted[CA certificates](https://en.wikipedia.org/wiki/CA_certificate), or a web server might be configured with a \"chain\" file containing an end-entity certificate plus a list of intermediate certificates.\n<https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail>\n\n## IAM**\n\nIdentity and Access Management (IAM) is the security discipline that enables the right individuals to access the right resources at the right times for the right reasons. IAM addresses the mission-critical need to ensure appropriate access to resources across increasingly heterogeneous technology environments.\nEnterprises traditionally used on-premises IAM software to manage identity and access policies, but nowadays, as companies add more cloud services to their environments, the process of managing identities is getting more complex. Therefore, adopting cloud-based Identity-as-a-Service (IDaaS) and cloud IAM solutions becomes a logical step.\nCloud IAM typically includes the following features:\n-   Single Access Control Interface. Cloud IAM solutions provide a clean and consistent access control interface for all cloud platform services. The same interface can be used for all cloud services.\n-   Enhanced Security. You can define increased security for critical applications.\n-   Resource-level Access Control. You can define roles and grant permissions to users to access resources at different granularity levels.\n<https://auth0.com/learn/cloud-identity-access-management>\n\n## Privacy**\n\n**Privacy is not for the passive. -- Jeffrey Rosen**\nPrivacy-enhancing technologies (PETs)\nThe problem of matching records using an identifier while preserving privacy has been well studied as a class of algorithms called private set intersection.\n<https://engineering.fb.com/open-source/private-matching>\nSecret Introduction Problem\n**Differential Privacy**\n\nDifferential privacy aims to maximize the accuracy of queries from statistical databases while minimizing the chances of identifying its records - it adds noise and provides guarantees against a \"privacy budget\"\n![OPINIONS ON INIERNET PRIVACY PHILOSOPHER: *RIVACâ‚¬â€¢ IYPRACTGL ABOUT IN IN UHCH WR SOCI- THE NIHILIST: sacs ON Ti-Ãœ1, GAT-ERING ALL IHS ON ME AS IF TAE CRYPTO NOT: MY IS BERND I-AYERS OFSfrtâ€¢ETRC R-ÃŸuc-â€¢Ã†Y PE?LE 0 D(HlÃŸlTiONffâ€¢. 15.)Ã¦ POPE TAE NSA ME BITE INTO TEE STABERRJES!! r DRPPED sa-E SHRT! BETTER TAKE GOOGLE, T{IS IDIiON FEELS somo cu. 01SPR9CtSTâ€¢. TEE LEAKS JUST IHETP OF ICECRG. TERES A WAREHWSE IN UTAH THE HE T-IE EM77RE ICEÃ†RG I mgr KNOW THERE. SPGE: 1 KNOU RCARE UI-BT DATA IS IS REAL. ](media/Concepts-image2.jpeg)\n**Clients**\n-   Confidential Client\n    -   Applications running on the server\n-   Public Client\n    -   SPA / JS apps running in browsers / Mobile apps / Embedded devices\n<https://www.youtube.com/watch?v=5cQNwifDq1U>"},{"fields":{"slug":"/Computer-Science/Security/Cryptography-Intro/","title":"Cryptography Intro"},"frontmatter":{"draft":false},"rawBody":"# Cryptography Intro\n\nCreated: 2021-06-06 16:37:06 +0500\n\nModified: 2022-04-16 22:40:36 +0500\n\n---\n\nDance like no one is watching; encrypt like everyone is. Encrypt everything.\"\nIf you have a secret and wish to keep it a secret until your death, how do you guarantee its cryptographic integrity until that time, especially with computing advances that could make today's state-of-the-art encryption crackable? And how do you enable the secret to be released, but only after death?\n**Cryptography**\n\nIs the art or science of Secret writing. It is concerned with the development of algorithms\n-   To conceal the content of messages from all except sender and recipient\n-   To verify the correctness of message or its sender and recipient\n**Terminologies**\n-   Encryption / Enciphering, E - Process of encoding the message so that meaning is not obvious or not in understandable form\n-   Decryption / Deciphering, D - Reverse process of encryption\n-   Plain text, P - The original form of the message\n-   Cipher text, C - Disguised (encrypted) message\nC = E(P), P = D(C)\n\nP = D (E (P))-   Key - Critical (secret) information used in cipher and known only to sender and receiver\n    -   Symmetric - Shared Key\n    -   Asymmetric - Public Key\n-   Code - Algorithm used for transforming the intelligible (plain text) to unintelligible (cipher text)\n-   Cipher - Algorithm / Code used for transforming plain text to cipher text\n-   Cryptanalysis (Code Breaking) - Study of method for transforming cipher text to plaintext without having knowledge of any key\n-   Cryptology - Area of cryptography and cryptanalysis together is called as crytology\n-   Protected health information (PHI) and Personally identifiable information (PII)-   **Responsible disclosure**\n**What Principles are Important When You're Developing a Cipher?**\n\n**Kerckhoff's principle** states that a cryptographic system should be secure, even if all the details (other than the key) are known publicly. Claude Shannon later rewrote this message as '**The enemy knows the system.**'\nEssentially, a very well designed system should be able to send secret messages even if an attacker can encrypt and decrypt their own messages using the same algorithm (with a different key). The security of the encrypted message should depend entirely on the key.\nAdditionally, in order to hinder statistical analysis (attempts to break an encryption algorithm), a good cryptographic system should employ the principles of **confusion and diffusion.**\n**Confusion** requires that the key does not relate to the ciphertext in a simple manner. Each character of the ciphertext should depend on multiple parts of the key. The goal is to make it very difficult for an attacker to determine the key from the ciphertext.\n**Diffusion** means that if a single character of the plaintext is changed, then several characters of the ciphertext should change. And if a single character of the ciphertext is changed, then several characters of the plaintext should change.\nIdeally, the relationship between the ciphertext and the plaintext is hidden. No diffusion is perfect (all will have some patterns), but the best diffusion scatters patterns widely, even scrambling several patterns together.\nDiffusion makes patterns hard for an attacker to spot, and requires the attacker to have more data in order to mount a successful attack.\n**Types of Ciphers**\n\nBoth block and stream ciphers are symmetric key ciphers (like DES, RCx, Blowfish, and Rijndael AES).\nMost modern symmetric algorithms are block ciphers, though the block sizes vary (such as DES (64 bits), AES (128, 192, and 256 bits), and so on).\n1.  Stream Cipher - Converts plaintext to ciphertext one bit at a time.\n\n2.  Block Cipher - It takes a given length of data as input and produces different length of encrypted data. Block ciphers convert plaintext to ciphertext block by block\n**What are the common modes of Block Ciphers?**\n\nIn order to encrypt data which is longer than a single block, there are several 'modes' which have been developed. These describe how to apply the single block principles to longer messages.\nThere are 5 confidentiality modes for block ciphers. Some of these modes require an initialization vector (IV) in order to function.\n1.  **Electronic Code Book Mode (ECB)**\n\nThere is a fixed mapping between input blocks of plaintext and output blocks of ciphertext (essentially like an actual code book where ciphertext words directly relate to plaintext words).\n2.  **Cipher Block Chaining Mode (CBC)**\n\nThis mode 'chains' or combines new plaintext blocks with the previous ciphertext block when encrypting them which requires an IV for the first block. The IV doesn't need to be secret, but it needs to be unpredictable.\n3.  **Cipher Feedback Mode (CFB)**\n\nCFB is similar to CBC, but instead of using the entire previous ciphertext block to compute the next block, CFB uses a fraction of the previous block.\n4.  **Output Feedback (OFB)**\n\nOFB is similar to CFB, but instead of processing s < b bits into a b-bits to b-bits transformation, it processes s bits directly. Similarly to CFB, OFB can be functionally used as a stream cipher.\n5.  **Counter (CTR)**\n\nCTR applies the encryption algorithm to a set of unique input blocks (counters) in order to produce outputs which are XORed with the plaintext to produce ciphertext.\n**How do Attackers Attempt to Break Ciphers?**\n\nThere are a number of techniques attackers use, but they broadly fall into the following categories of attack, based on information required to carry it out.\nThis isn't an exhaustive list (there are other attacks such side channel attacks), but many of the most common fall into one of these categories.\n**Known Ciphertext Attack**\n\nAn attacker has some ciphertext, but does not know what plaintext was used to generate this ciphertext. The attacker does not get to choose which ciphertext they have and they cannot obtain/produce more.\n**Known Plaintext Attack**\n\nAn attacker has some plaintext and ciphertext pairs which they didn't choose (so the attacker didn't choose the message that was encrypted, but was able to successfully steal a plaintext message and its associated ciphertext). The attacker cannot obtain/produce more pairs.\n**Chosen Plaintext Attack**\n\nAn attacker can choose any plaintext and obtain the ciphertext in return (but they can't see the key itself).\n**Chosen Ciphertext Attack**\n\nThis is the opposite of the last attack, where the attacker can choose any ciphertext and obtain the plaintext in return (but they can't see the key itself).\n<https://www.freecodecamp.org/news/what-is-a-block-cipher>\n\n## Padding**\n\nPadding standards are mechanisms for appending some predefined values to messages. They are used with algorithms which deal with blocks of data. Typical examples of such operations are[block symmetric ciphers](http://www.crypto-it.net/eng/symmetric/index.html)and[MAC algorithms](http://www.crypto-it.net/eng/theory/mac.html). These algorithms work on the whole data blocks. Therefore, if amessage length is not amultiple of the block size, astardard for adding some number of bytes to the end of the message is required.-   Bit Padding\n-   TBC (Trailing Bit Complement) Padding\n-   PKCS#5 and PKCS#7 Padding\n-   ISO 7816-4 Padding\n-   ISO 10126-2 Padding\n-   ANSI X9.23 Padding\n-   Zero Byte Padding\n<http://www.crypto-it.net/eng/theory/padding.html>\n\n## Courses**\n\n<https://www.youtube.com/playlist?list=PLAwxTw4SYaPnCeih6BPvJ5GdqqThGcWlX>\n\n<https://www.youtube.com/playlist?list=PLE4V3KXzxPRQYUil17HB6XcIu-JMebD7n>\n"},{"fields":{"slug":"/Computer-Science/Security/Ethical-Hacking/","title":"Ethical Hacking"},"frontmatter":{"draft":false},"rawBody":"# Ethical Hacking\n\nCreated: 2020-05-25 23:33:40 +0500\n\nModified: 2022-01-07 22:23:17 +0500\n\n---\n\n<https://www.shodan.io/host/182.71.91.174>\n\n<https://github.com/arthaud/git-dumper/blob/master/git-dumper.py>\n\n<https://securitytrails.com/domain/stashfin.com/history/a>\n\n<https://www.youtube.com/watch?v=3Kq1MIfTWCE>\n\n<https://github.com/Hack-with-Github/Awesome-Hacking>\n\n## CISSP** is a popular cyber security certification. Risk Management, Security Architecture, Network Security, Identity & Access Management, SecOps, and more.\n\n<https://www.freecodecamp.org/news/get-ready-to-pass-cissp-exam>\n\n## Use Cases and Abuse Cases**\n**White Hat hackers**\n\nA white hat hacker is an individual who uses hacking skills to identify security vulnerabilities in hardware, software or networks. However, unlike black hat hackers, white hat hackers respect the rule of law as it applies to hacking.\n**Gray Hat Hackers**\n\nGray hat hackers fall between white and black hats on the moral spectrum. Gray hats generally consider themselves good guys who are more flexible about the rules under which they operate. For example, a gray hat hacker may be more likely than a white hat hacker to access systems without getting permission or authorization from the owners, but would be less likely than a black hat hacker to cause damage to those systems. While not typically motivated by financial gain, gray hat hackers may try to get the owners of a system they've hacked to pay them to patch or fix those systems.\n**Black Hat Hackers**\n\nA black hat hacker has no qualms about selling vulnerabilities and exploits to the highest bidder, such as a criminal organization, usually for the purpose of exploiting them. Black hat hackers are willing to break the law to discover, create and use security flaws maliciously for personal gain or to make a political statement.\n**Websites**\n\n<https://www.quora.com/Are-there-any-website-that-I-can-hack-legally-for-practice>\n\n[**https://hack.me/**](https://hack.me/)\n\n<https://hackthissite.org>\n\n<https://w3challs.com>\n\n<https://dst.com.ng/15-vulnerable-sites-legally-practice-hacking-skills>\n\n[**https://tryhackme.com/**](https://tryhackme.com/)\n\n<https://haveibeenpwned.com>\n\n## Tools**\n\n**Festin**\n\nFestInis a tool for discovering open S3 Buckets starting from a domains.\n<https://github.com/cr0hn/festin>\n\n## Practice / Learning**\n\n**Damn Vulnerable Web Application (DVWA)**\n\n<http://www.dvwa.co.uk>\n\n## Digital Forensics**\n\n![](media/Ethical-Hacking-image1.png)\n![](media/Ethical-Hacking-image2.png)\n![Modus Operandi â€¢ What do attackers do: â€¢ Credential Harvesting â€¢ Credential Stuffing â€¢ Credential Testing â€¢ Account takeover â€¢ Credit card testing â€¢ Penetration testing attacks LexisNexisâ€¢ RISK SOLUTIONS Fraudsters Antimalware DMZ ontentFl r IDPS VOIP Protectio Message Security Application Firewall ICAM Firewall e nse Perimeter Security DLP Network Security emo cess Messa nd oin ecuri a a Monitorin Application assi Ica 10 Sanitization Data Security DLP Data Classification ERM PKI ONFID TIAL Data Integrity Monitoring ](media/Ethical-Hacking-image3.png)\n| **SOLUTIONS**                                           | **PARTNERS/OEM's**                        |\n|-------------------------------------|-----------------------------------|\n| **Cloud Security(WAF, NGFW), Email Security, Firewall** | Cisco/ PaloAlto/Fortinet/Sophos/Barracuda |\n| **DLP**                                                 | Forcepoint, Cososys, Netskope             |\n| **End Point**                                           | Sophos, TrendMicro, Crowdstrike, FireEye  |\n| **Collaboration**                                       | Cisco/ Polycom                            |\n| **Enterprise Networking**                               | Cisco/Hpe                                 |\n| **Network Access Control**                              | Cisco ISE, FortiNac                       |\n| **MultiFactor Authentication**                          | Cisco Duo, RSA                            |\n| **Back up Solutions**                                   | Veeam                                     |\n| **Web Security**                                        | NetSkope, Forcepoint                      |\n| **Micro Segmentation**                                  | Cisco Teteration, Color Token             |\n| **Storage Solutions**                                   | Dell, Netapp                              |\n| **Inventory& Ticketing Tool**                           | Motadata, Sapphire IMS                    |\n| **Vulnerability Assessment**                            | Tenable, Holmsecurity                     |\n| **Network Monitoring Tool**                             | Motadata, Whatsup Gold, Solarwinds        |\n| **Performance Monitoring & Analytics**                  | Acreedian                                |\n| **Load Balancing & WAF**                                | F5, Array Networks                        |\n| **SD-WAN**                                              | Cisco Viptela, CATO Networks              |\n| **CASB**                                                | Netskope, Forcepoint, Paloalto            |\n| **MDM**                                                 | Vmware, SOTI, Citrix, Meraki              |\n| **Structured Cabling**                                  | Belden, Siemon                            |-   Saffron Networks\n"},{"fields":{"slug":"/Computer-Science/Security/Firewall-WAF/","title":"Firewall WAF"},"frontmatter":{"draft":false},"rawBody":"# Firewall WAF\n\nCreated: 2020-01-25 13:05:43 +0500\n\nModified: 2021-08-15 23:00:46 +0500\n\n---\n\n**Web Application Firewall (WAF)**\n\nA WAF creates a shield between a web app and the Internet; this shield can help mitigate many common attacks.\nA WAF or Web Application[Firewall](https://www.cloudflare.com/learning/security/what-is-a-firewall/)helps protect web applications by filtering and monitoring[HTTP](https://www.cloudflare.com/learning/ddos/glossary/hypertext-transfer-protocol-http/)traffic between a web application and the Internet.It typically protects web applications from attacks such as[cross-site forgery](https://www.cloudflare.com/learning/security/threats/cross-site-request-forgery/),[cross-site-scripting (XSS)](https://www.cloudflare.com/learning/security/threats/cross-site-scripting/), file inclusion, and[SQL injection](https://www.cloudflare.com/learning/security/threats/sql-injection/), among others. A WAF is a protocol[layer 7](https://www.cloudflare.com/learning/ddos/what-is-layer-7/)defense (in the[OSI model](https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/)).\nBy deploying a WAF in front of a web application, a shield is placed between the web application and the Internet. While a proxy server protects a client machine's identity by using an intermediary, a WAF is a type of[reverse-proxy](https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/), protecting the server from exposure by having clients pass through the WAF before reaching the server.\nA WAF operates through a set of rules often called policies. These policies aim to protect against vulnerabilities in the application by filtering out malicious traffic.The value of a WAF comes in part from the speed and ease with which policy modification can be implemented, allowing for faster response to varying attack vectors; during a[DDoS attack](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack), rate limiting can be quickly implemented by modifying WAF policies.\n**Blacklist and Whitelist WAFs**\n\nA WAF that operates based on a blacklist (negative security model) protects against known attacks. Think of a blacklist WAF as a club bouncer instructed to deny admittance to guests who don't meet the dress code. Conversely, a WAF based on a whitelist (positive security model) only admits traffic that has been pre-approved. This is like the bouncer at an exclusive party, he or she only admits people who are on the list. Both blacklists and whitelists have their advantages and drawbacks, which is why many WAFs offer a hybrid security model, which implements both.\n**Implementation Techniques**\n-   **Network-based WAF**\n\nA network-based WAF is generally hardware-based. Since they are installed locally they minimize latency, but network-based WAFs are the most expensive option and also require the storage and maintenance of physical equipment.-   **Host-based WAF**\n\nA host-based WAF may be fully integrated into an application's software. This solution is less expensive than a network-based WAF and offers more customizability. The downside of a host-based WAF is the consumption of local server resources, implementation complexity, and maintenance costs. These components typically require engineering time, and may be costly.-   **Cloud-based WAF**\n\n[Cloud](https://www.cloudflare.com/learning/cloud/what-is-the-cloud/)-based WAFs offer an affordable option that is very easy to implement; they usually offer a turnkey installation that is as simple as a change in[DNS](https://www.cloudflare.com/learning/ddos/glossary/domain-name-system-dns/)to redirect traffic. Cloud-based WAFs also have a minimal upfront cost, as users pay monthly or annually for security as a service. Cloud-based WAFs can also offer a solution that is consistently updated to protect against the newest threats without any additional work or cost on the user's end. The drawback of a cloud-based WAF is that users hand over the responsibility to a third-party, therefore some features of the WAF may be a black box to them.\n<https://www.cloudflare.com/learning/ddos/glossary/web-application-firewall-waf>\n\n<https://en.wikipedia.org/wiki/Web_application_firewall>\n\n<https://developers.cloudflare.com/firewall/cf-firewall-rules/actions>\n\n## DDOS Blackhole routing**\n\nDDoS blackhole routing/filtering (sometimes called blackholing), is a countermeasure to mitigate a[DDoS attack](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)in which network traffic is routed into a \"black hole,\" and is lost. When blackhole filtering is implemented without specific restriction criteria, both legitimate and malicious network traffic is routed to a null route or black hole and dropped from the network. When using protocols that are connectionless such as[UDP](https://www.cloudflare.com/learning/ddos/glossary/user-datagram-protocol-udp/), no notification of the dropped data will be returned to the source. With connection oriented protocols like[TCP](https://www.cloudflare.com/learning/ddos/glossary/tcp-ip/), which require a handshake to connect with the target system, a notification will be returned if the data is dropped.\n<https://www.freecodecamp.org/news/protect-against-ddos-attacks>\n"},{"fields":{"slug":"/Computer-Science/Security/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-01-19 21:05:50 +0500\n\nModified: 2022-02-08 21:37:47 +0500\n\n---\n\n**SE Radio - 321: End to End Encryption - Kim Carter with Peter Budai**\n\nIM, Voice over IP, Email scenarios, as well as interservice communication scenarios such as securing data in use with full memory encryption, CPU-based key storage, enclaves, cryptographic protocols (Secure multi-party computation and Homomorphic encryption).\n**Tor (anonymity network)**\n\nToris[free and open-source software](https://en.wikipedia.org/wiki/Free_and_open-source_software)for enabling[anonymous communication](https://en.wikipedia.org/wiki/Internet_anonymity). The name is derived from an acronym for the original software project name \"The Onion Router\".Tor directs Internet traffic through a free, worldwide, volunteer[overlay network](https://en.wikipedia.org/wiki/Overlay_network)consisting of more than seven thousand relaysto conceal a user's location and usage from anyone conducting[network surveillance](https://en.wikipedia.org/wiki/Computer_surveillance#Network_surveillance)or[traffic analysis](https://en.wikipedia.org/wiki/Traffic_analysis#In_computer_security). Using Tor makes it more difficult to trace Internet activity to the user: this includes \"visits to Web sites, online posts, instant messages, and other communication forms\".Tor's intended use is to protect the personal privacy of its users, as well as their freedom and ability to conduct confidential communication by keeping their Internet activities from being monitored.\n[Onion routing](https://en.wikipedia.org/wiki/Onion_routing)is implemented by[encryption](https://en.wikipedia.org/wiki/Encryption)in the[application layer](https://en.wikipedia.org/wiki/Application_layer)of a[communication protocol](https://en.wikipedia.org/wiki/Communication_protocol)stack, nested like the layers of an[onion](https://en.wikipedia.org/wiki/Onion). Tor encrypts the data, including the next node destination[IP address](https://en.wikipedia.org/wiki/IP_address), multiple times and sends it through a[virtual circuit](https://en.wikipedia.org/wiki/Virtual_circuit)comprising successive, random-selection Tor relays. Each relay decrypts a layer of[encryption](https://en.wikipedia.org/wiki/Encryption)to reveal the next relay in the circuit to pass the remaining encrypted data on to it. The final relay decrypts the innermost layer of encryption and sends the original data to its destination without revealing or knowing the source IP address. Because the routing of the communication was partly concealed at every hop in the Tor circuit, this method eliminates any single point at which the communicating peers can be determined through[network surveillance](https://en.wikipedia.org/wiki/Computer_and_network_surveillance)that relies upon knowing its source and destination.\nduckduckgo - <http://3g2upl4pq6kufc4m.onion>\n\nFacebook - <https://www.facebookcorewwwi.onion>\n\nHidden Wiki - <http://zqktlwi4fecvo6ri.onion/wiki/index.php/Main_Page>\n<https://en.wikipedia.org/wiki/Tor_(anonymity_network)>\n\n<https://skerritt.blog/designing-effective-peer-to-peer-networks>\n\n## steganography**\n\nSteganography is the practice of concealing a file, message, image, or video within another file, message, image, or video. The wordsteganographycombines the[Greek](https://en.wikipedia.org/wiki/Greek_language)wordssteganos([ÏƒÏ„ÎµÎ³Î±Î½ÏŒÏ‚](https://en.wiktionary.org/wiki/%CF%83%CF%84%CE%B5%CE%B3%CE%B1%CE%BD%CF%8C%CF%82#Greek)), meaning \"covered, concealed, or protected\", andgraphein([Î³ÏÎ¬Ï†ÎµÎ¹Î½](https://en.wiktionary.org/wiki/%CE%B3%CF%81%CE%AC%CF%86%CE%B5%CE%B9%CE%BD#Greek)) meaning \"writing\".\n<https://en.wikipedia.org/wiki/Steganography>\n\n<https://www.boxentriq.com/code-breaking>\n\n<https://29a.ch/photo-forensics>\n\n## stegosploit**\n\nStegosploit creates a new way to encode \"drive-by\" browser exploits and deliver them through image files. These payloads are undetectable using current means. This paper discusses two broad underlying techniques used for image based exploit delivery - Steganography and Polyglots. Drive-by browser exploits are steganographically encoded into JPG and PNG images. The resultant image file is fused with HTML and Javascript decoder code, turning it into an HTML+Image polyglot. The polyglot looks and feels like an image, but is decoded and triggered in a victim's browser when loaded.\n[https://stegosploit.info](https://stegosploit.info/)\n**DMARC (Domain-based Message Authentication, Reporting and Conformance)**\n**Capture the Flag (CTF)**\n\n<https://0x00sec.org/c/ctf>\n\n<https://picoctf.org>\n\n**CMDCTRL -** <https://cmdnctrl.net>\n\n## NT (New Technology) LAN Manager(NTLM)**\n\nIn a[Windows](https://www.wikiwand.com/en/Microsoft_Windows)network,**NT (New Technology) LAN Manager(NTLM)** is a suite of[Microsoft](https://www.wikiwand.com/en/Microsoft)security protocols intended to provide authentication, integrity, and confidentiality to users.NTLM is the successor to the authentication protocol in Microsoft[LAN Manager](https://www.wikiwand.com/en/LAN_Manager)(LANMAN), an older Microsoft product. The NTLM protocol suite is implemented in a[Security Support Provider](https://www.wikiwand.com/en/SSPI#Windows_SSPs), which combines the[LAN Manager](https://www.wikiwand.com/en/LAN_Manager) authentication protocol, NTLMv1, NTLMv2 and NTLM2 Session protocols in a single package. Whether these protocols are used or can be used on a system is governed by[Group Policy](https://www.wikiwand.com/en/Group_Policy)settings, for which different versions of Windows have different default settings. NTLM passwords are considered weak because they can be brute-forced very easily with modern hardware.\n**Zero Knowledge Proofs**\n\nWhat if the average developer could benefit from proofs of computational integrity (CI) that would normally require an in-depth knowledge of cryptography to implement?\n**CI proofs, of which zero-knowledge proofs (ZKPs)** are a subset, are a cryptographic technology that let you do seemingly impossible things. For example, you can run a computation and get some result. You can then use a CI proof to convince anyone that you did the computation correctly without their having to rerun the computation themselves.And they can verify this correctness in just a few milliseconds, regardless of how complex or long-running the original computation was.\n<https://engineering.fb.com/2021/08/04/open-source/winterfell>\n\n## Others**\n\n<https://www.freecodecamp.org/news/what-is-devsecops>\n"},{"fields":{"slug":"/Computer-Science/Security/Systems-Protection/","title":"Systems Protection"},"frontmatter":{"draft":false},"rawBody":"# Systems Protection\n\nCreated: 2018-04-27 00:13:49 +0500\n\nModified: 2021-04-13 14:28:08 +0500\n\n---\n\nProtection against Buffer Overflow attacks / Stash smashing attacks\n\n1.  NX bit protection\n\n2.  Canaries\n\n3.  SSP (Stack Smashing Protector)4.  **ASLR (Address Space Layout Randomization)**5.  Non-Executable Stacks\n\n6.  OS hardening\n\n7.  Object Code Checking (Provided by GCC)\n<https://www.freecodecamp.org/news/buffer-overflow-attacks>\nAccess Controls -\n\n1.  DAC (Discretionary Access Control) - Owners of objects can modify their permissions\n\n2.  MAC (Mandatory Access Control) - System enforced rules based on access control hierarchy\n\n3.  RBAC (Role Baed Access Control) - Relies on a hierarchy of roles\n\n4.  ACLs (Access Control Lists) in mordern OSes\n\n5.  Access control matrix - describes who has access to which resource.\n\n6.  RBAC (Role Based Access Control)\n\n7.  ABAC (Attribute Based Access Control)\nSecurity Models\n\n1.  Bell-LaPadula Model\n\n2.  Biba Model (Reversed Bell-LaPadula Model)\n\n3.  Saltzer and Shroeder model\nPublic/private cryptography\n\nIntegrity protection\n-   Cryptographic libraries\n    -   OpenSSL\n    -   GPG\n\nGNU Privacy Guard(GnuPGorGPG) is a[free-software](https://en.wikipedia.org/wiki/Free-software)replacement for [Symantec](https://en.wikipedia.org/wiki/NortonLifeLock)'s [PGP](https://en.wikipedia.org/wiki/Pretty_Good_Privacy) [cryptographic](https://en.wikipedia.org/wiki/Cryptography) software suite, and is compliant with[RFC 4880](https://tools.ietf.org/html/rfc4880), the[IETF](https://en.wikipedia.org/wiki/Internet_Engineering_Task_Force)standards-track specification of[OpenPGP](https://en.wikipedia.org/wiki/Pretty_Good_Privacy#OpenPGP). Modern versions of[PGP](https://en.wikipedia.org/wiki/Pretty_Good_Privacy)are[interoperable](https://en.wikipedia.org/wiki/Interoperability)with GnuPG and other OpenPGP-compliant systems.\nGPG is the Gnu Privacy Guard and it is an implementation of OpenPGP (Open Pretty Good Privacy). It is an encryption technique that was originally developed for use in e-mail exchanges that is now used in a number of different applications such as code signing for Linux code repositories and source code repositories like github. OpenPGP is a hybrid of the two-key cryptography approach where the message to be exchanged (called plaintext) is first compressed and then a session key is created as a one-time use secret key. The compressed plaintext is then encrypted with the session key. The session key is then encrypted with the destination's public key and bundled with the encrypted message (called ciphertext). The destination can decrypt the session key with their private key then then decompress it to recover the original plaintext.\nThere are several different ways to create a GPG key. The easiest is with the \"gpg\" or \"gpg2\" commands available on many major operating systems. Many commercial encryption programs also include a way to generate a GPG key. You can then store the public version on a public key server so folks can start sending you encrypted files/message traffic. Only you will be able to decrypt it because only you have the associated private key.\n<https://gnupg.org>\n\n<https://www.digitalocean.com/community/tutorials/how-to-use-gpg-to-encrypt-and-sign-messages>-   LibSSL - SSL/TLS\n-   Libcrypto - Encryption and Decryption\n-   EVP - Envelope Functions\n    -   Can be used for both symmetric and asymmetric cryptography\n    -   Various Encryption / Decryption algorithms\n    -   Various message digest (hash) algorithms\n    -   HMAC - Signing\n**PGP**\n\nPretty Good Privacy(PGP) is an[encryption program](https://en.wikipedia.org/wiki/Encryption_software)that provides [cryptographic](https://en.wikipedia.org/wiki/Cryptographic) [privacy](https://en.wikipedia.org/wiki/Privacy) and [authentication](https://en.wikipedia.org/wiki/Authentication) for [data communication](https://en.wikipedia.org/wiki/Data_communication). PGP is used for[signing](https://en.wikipedia.org/wiki/Digital_signature), encrypting, and decrypting texts,[e-mails](https://en.wikipedia.org/wiki/Email), files, directories, and whole disk partitions and to increase the security of e-mail communications.[Phil Zimmermann](https://en.wikipedia.org/wiki/Phil_Zimmermann)developed PGP in 1991.\nPGP and similar software follow the[OpenPGP](https://en.wikipedia.org/wiki/Pretty_Good_Privacy#OpenPGP), an open standard of PGP encryption software, standard ([RFC 4880](https://en.wikipedia.org/wiki/Request_for_Comments)) for encrypting and decrypting data.\n<https://en.wikipedia.org/wiki/Pretty_Good_Privacy>\n\n## Air Gap Networking**\n\nAn**air gap**,**air wall**or**air gapping**is a[network security](https://en.wikipedia.org/wiki/Network_security)measure employed on one or more computers to ensure that a secure[computer network](https://en.wikipedia.org/wiki/Computer_network)is physically isolated from unsecured networks, such as the public[Internet](https://en.wikipedia.org/wiki/Internet)or an unsecured[local area network](https://en.wikipedia.org/wiki/Local_area_network).[^[2]^](https://en.wikipedia.org/wiki/Air_gap_(networking)#cite_note-2)It means a computer or network has no[network interfaces](https://en.wikipedia.org/wiki/Network_interface)connected to other networks,with a physical or conceptual air gap, analogous to the[air gap](https://en.wikipedia.org/wiki/Air_gap_(plumbing))used in plumbing to maintain water quality.\n<https://en.wikipedia.org/wiki/Air_gap_(networking)>\n\n## Bastion Host**\n\nA**bastion host**is a special purpose computer on a network specifically designed and configured to withstand attacks. The computer generally hosts a single application, for example a[proxy server](https://en.wikipedia.org/wiki/Proxy_server), and all other services are removed or limited to reduce the threat to the computer. It is hardened in this manner primarily due to its location and purpose, which is either on the outside of a[firewall](https://en.wikipedia.org/wiki/Firewall_(computing))or in a demilitarized zone ([DMZ](https://en.wikipedia.org/wiki/Demilitarized_zone_(computing))) and usually involves access from untrusted networks or computers.\n<https://en.wikipedia.org/wiki/Bastion_host>"},{"fields":{"slug":"/Computer-Science/Security/Tools/","title":"Tools"},"frontmatter":{"draft":false},"rawBody":"# Tools\n\nCreated: 2019-12-28 22:47:16 +0500\n\nModified: 2022-08-11 15:37:07 +0500\n\n---\n\n**dex**\n\nDex is an identity service that uses[OpenID Connect](https://openid.net/connect/)to drive authentication for other apps.\nDex acts as a portal to other identity providers through[\"connectors.\"](https://github.com/dexidp/dex#connectors)This lets dex defer authentication to LDAP servers, SAML providers, or established identity providers like GitHub, Google, and Active Directory. Clients write their authentication logic once to talk to dex, then dex handles the protocols for a given backend.\n<https://github.com/dexidp/dex>\n\n## SPIFFE (Secure Production Identity Framework for Everyone)**\n\nInspired by the production infrastructure of Google and others, SPIFFE is a set of open-source standards for securely identifying software systems in dynamic and heterogeneous environments.\nSPIFFE is a set of open-source specifications for a framework capable of bootstrapping and issuing identity to services across heterogeneous environments and organizational boundaries. The heart of these specifications is the one that defines short lived cryptographic identity documents -- called[SVIDs](http://localhost:1313/spiffe/concepts/#spiffe-verifiable-identity-document-svid)via a[simple API](https://spiffe.io/spiffe/concepts/#spiffe-workload-api). Workloads can then use these identity documents when authenticating to other workloads, for example by establishing a TLS connection or by signing and verifying a JWT token.\nSPIFFE - set of specifications\n\nImplementations\n-   The SPIRE project\n-   Istio Citadel\n-   HashiCorp Consul\n**SPIRE**\n\nSPIRE (the[SPIFFE](https://github.com/spiffe/spiffe)Runtime Environment) is a tool-chain for establishing trust between software systems across a wide variety of hosting platforms. Concretely, SPIRE exposes the[SPIFFE Workload API](https://github.com/spiffe/go-spiffe/blob/master/proto/spiffe/workload/workload.proto), which can attest running software systems and issue[SPIFFE IDs](https://github.com/spiffe/spiffe/blob/master/standards/SPIFFE-ID.md)and[SVID](https://github.com/spiffe/spiffe/blob/master/standards/SPIFFE-ID.md)s to them. This in turn allows two workloads to establish trust between each other, for example by establishing an mTLS connection or by signing and verifying a JWT token. Or for a workload to securely authenticate to a secret store, a database, or a cloud provider service.\nspire-server\n-   Identity Mapping\n-   Node Attestation\n\n![Node Attestation spire-agent Node Attestor @evan2645 AWS spire-server Node Attestor S SCYTALE ](media/Tools-image1.png)\n-   SVID Issuance\nspire-agent\n-   Workload Attestation\n\n![kubelet API Socket spire-agent Linux Kernel Server Workload ](media/Tools-image2.png)\n-   Workload API\n**SPIRE Overview**\n\n![spire-agent WL WL spire-server spire-agent WL WL spire-agent WL WL ](media/Tools-image3.png)\n**Workload Identity**\n\n![Parent ID: Selector: Selector: Selector: SPIFFE ID: spiffe://exampLe.org/k8s/cluster/foo k8s:ns:operations k8s:sa:mediawiki docker:image-id:746b819f315e spiffe://example.org/ops/wiki ](media/Tools-image4.png)\n<https://spiffe.io/spiffe>\n\n[Zero Trust Service Mesh with Calico, SPIRE, and Envoy - Shaun Crampton & Evan Gilman](https://www.youtube.com/watch?v=rKOEYoINdOE)\n\n[Securing Multi-Cloud Cross-Cluster Communication with SPIFFE and SPIRE - Evan Gilman, Scytale, Inc.](https://www.youtube.com/watch?v=sLN11qAFAC4)\n**Casbin**\n\nAn authorization library that supports access control models like ACL, RBAC, ABAC in Golang\n<https://casbin.org>\n\n<https://github.com/casbin/casbin>\n\n## Lavabit**\n\nSecure emailfor the world\n<https://lavabit.com>\n\n## Android**\n\n<https://github.com/google/nogotofail>\n\n## Cryptographic tools**\n\n<https://www.devglan.com/online-tools/aes-encryption-decryption>\n\n## NaCl**\n\n<http://nacl.cr.yp.to>\n\n## libsodium**\n\nSodium is a new, easy-to-use software library for encryption, decryption, signatures, password hashing and more.\n\n<https://github.com/jedisct1/libsodium>\n\n## cert-manager**\n\ncert-manager builds on top of Kubernetes, introducing certificate authorities and certificates as first-class resource types in the Kubernetes API. This makes it possible to provide 'certificates as a service' to developers working within your Kubernetes cluster.\n**Highlights**\n-   Provide easy to use tools to manage certificates.\n-   A standardised API for interacting with multiple certificate authorities (CAs).\n-   Gives security teams the confidence to allow developers to self-server certificates.\n-   Support for ACME (LetsEncrypt), HashiCorp Vault, Venafi, self signed and internal certificate authorities.\n-   Extensible to support custom, internal or otherwise unsupported CAs.\n**Concepts**\n-   Issuer\n-   Certificate\n-   CertificateRequest\n-   ACME Orders and Challenges\n-   Webhook\n-   CA Injector\n\n<https://cert-manager.io>\n\n<https://cert-manager.io/docs/concepts>\n\n<https://cert-manager.io/docs/installation/kubernetes>\n<https://www.youtube.com/watch?v=JJTJfl-V_UM>\n\n[Use cert-manager with Let's EncryptÂ® Certificates Tutorial: Automatic Browser-Trusted HTTPS](https://www.youtube.com/watch?v=etC5d0vpLZE)\n**Letsencrypt**\n\n<https://letsencrypt.org/getting-started>\n\n## certbot**\n\n<https://certbot.eff.org>\n\n## SASS**\n\n<https://www.crowdstrike.com>\n\n<https://snyk.io>\n\n"},{"fields":{"slug":"/Computer-Science/Security/Vault/","title":"Vault"},"frontmatter":{"draft":false},"rawBody":"# Vault\n\nCreated: 2020-07-16 13:12:59 +0500\n\nModified: 2021-05-11 18:21:16 +0500\n\n---\n\nVault is a tool for securely accessing*secrets*. A secret is anything that you want to tightly control access to, such as API keys, passwords, or certificates. Vault provides a unified interface to any secret, while providing tight access control and recording a detailed audit log.\nVault was built to solve the secret sprawl problem in a user-friendly and auditable way. Just as immutable infrastructure gives operations certainty in server configurations and eliminates drift, Vault gives security certainty in when, where, and how secrets are being used across a system. It is the central source of security truth for modern architectures.\n**Features**\n-   **Secure Secret Storage**\n\nArbitrary key/value secrets can be stored in Vault. Vault encrypts these secrets prior to writing them to persistent storage, so gaining access to the raw storage isn't enough to access your secrets. Vault can write to disk,[Consul](https://www.consul.io/), and more.-   **Dynamic Secrets**\n\nVault can generate secrets on-demand for some systems, such as AWS or SQL databases. For example, when an application needs to access an S3 bucket, it asks Vault for credentials, and Vault will generate an AWS keypair with valid permissions on demand. After creating these dynamic secrets, Vault will also automatically revoke them after the lease is up. Thus, even if the system is breached,the attacker only has a short window of opportunity before the secret gets re-generated and access is revoked.-   **Data Encryption**\n\nVault can encrypt and decrypt data without storing it. This allows security teams to define encryption parameters and developers to store encrypted data in a location such as SQL without having to design their own encryption methods.-   **Leasing and Renewal**\n\nAll secrets in Vault have aleaseassociated with them. At the end of the lease, Vault will automatically revoke that secret. Clients are able to renew leases via built-in renew APIs.-   **Revocation**\n\nVault has built-in support for secret revocation. Vault can revoke not only single secrets, but a tree of secrets, for example all secrets read by a specific user, or all secrets of a particular type. Revocation assists in key rolling as well as locking down systems in the case of an intrusion.-   **Limit the internal surface area of a breach to a single application instance**\n\nFine-grained ACLs lets Vault grants specific, limited communication permissions for each secret. If a secret is compromised, it only provides access to a single service and not the entire infrastructure. In many existing security managers, obtaining one key will let an attacker have free reign throughout the system.-   **Generate an audit trail of service communication**\n\nEach time a secret is generated it creates an audit log which can be used to determine the specific compromised resource in the datacenter. For example, if an application that does not have access to a database attempts to make a connection, it is clear that there has been a compromise. The audit log can be used by operators and security teams to pinpoint tainted servers and take the proper steps to restore the security of the system as a whole. It is important to note that secrets are generatedafterthe log is stored, meaning that all access actions are guaranteed to be logged. If for some reason the log cannot be written, the secret will not be generated.-   **Simplify security for large operations and infrastructures**\n\nOften the biggest hurdle to proper security is the complexity of the implementing the security solution. With simple installation and setup, Vault lowers the barrier to entry for organizations to use responsible secret management to secure their distributed infrastructure. Secrets can be written from the command line or the[complete HTTP API](http://www.google.com/url?q=http%3A%2F%2Fvaultproject.io%2Fdocs%2Fhttp%2Findex.html&sa=D&sntz=1&usg=AFQjCNFDA0tQawMLYkAwPLjf-UZLCImSPw).-   **In-transit data encryption**\n\nVault can encrypt and decrypt data without storing it. Security teams can define encryption parameters and store encrypted data in a location such as a SQL database without having to design a custom encryption method. You can read more about[how HashiCorp's Atlas uses Vault to encrypt GitHub OAuth tokens](https://www.google.com/url?q=https%3A%2F%2Fhashicorp.com%2Fblog%2Fhow-atlas-uses-vault-for-managing-secrets.html&sa=D&sntz=1&usg=AFQjCNHM-BUqX0jdzPzHtH1Wx88qQw-yJA).-   **High-availability**\n\nHigh-availability is a requirement for large-scale infrastructures to protect against service outages. Vault can run in multi-server mode when using a[backend that supports it](https://www.google.com/url?q=https%3A%2F%2Fvaultproject.io%2Fdocs%2Fconcepts%2Fha.html&sa=D&sntz=1&usg=AFQjCNGaIQAyr3T1AgRc__VI7utYGAx-Jg), such as Consul or etcd.-   **Authentication methods**\n\nVault includes multiple methods for authentication so organizations can select the option that fits best with their setup. Vault[currently supports](https://www.google.com/url?q=https%3A%2F%2Fvaultproject.io%2Fdocs%2Fconcepts%2Fauth.html&sa=D&sntz=1&usg=AFQjCNFkbuRP8tVhRTt8aUYwQETkJZwFVw)tokens, username/password, GitHub, certificates, and more.\n**Best Practices**\n-   Don't let secrets live forever\n-   Distribute secrets securely\n-   Limit exposure if auth secret disclosed\n-   Break-glass procedure if auth secret stolen\n-   Detect unauthoried access to auth secrets\n**Options**\n\n1.  Deploy Vault token alongside app\n\n2.  Deploy approle roleid/secretid alongside app\n\n3.  Deploy TLS client certificates and use cert auth method\n**Option 1: Distributing Tokens**\n\nOne reason you might want to do this instead of using approle is its easy to use envconsul or consul-template\nIf distributing tokens directly:\n-   use a token role, similar to what we do with approle roles\n-   distribute single-use token with a short TTL\n-   use response wrapping to embed another longer-lived token\n**Commands**\n\nbrew install vault\n\nvault -autocomplete-install\nvault server -dev\nexport VAULT_ADDR='http://127.0.0.1:8200'\n\nvault status\n\nvault path-help aws\n\nvault path-help aws/creds/my-non-existent-role\n**#DML**\n\nvault kv put secret/hello foo=world\n\nvault kv put secret/hello foo=world excited=yes\n\nvault kv get secret/hello\n\nvault kv list kv/\n**#DDL**\n\nvault secrets list\n\nvault token create\n**#Enable secret engine**\n\nvault secrets enable -path=kv kv\n\nvault secrets enable kv\n\nvault secrets disable kv/\n\nvault secrets enable -path=aws aws\n**# Gen**\n**# Dynamic Secrets**\n\nvault write aws/config/root \n\naccess_key=AKIAU2R6AAK3FIYUQBXY\n\nsecret_key=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 \n\nregion=ap-south-1\nvault write aws/roles/my-role \n\ncredential_type=iam_user \n\npolicy_document=-<<EOF\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"Stmt1426528957000\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"ec2:*\"\n\n],\n\n\"Resource\": [\n\n\"*\"\n\n]\n\n}\n\n]\n\n}\n\nEOF\nvault read aws/creds/my-role\n\nvault lease revoke aws/creds/my-role/0bce0782-32aa-25ec-f61d-c026ff22106\n**K8s annotations**\n\n**# remove pre-populate-only, to run sidecar that can sync credentials**\n![{{- with secret \"secret/helloworld\" -}} postgresql . Data.data.username }}:{{ . Data.data.password }}@postgres:5432/wizard end vault. hashi corp.com/agent-inj ect-status : \" update\" vault. hashi corp.com/agent-inj ect-secret-payment : \" secret/payment-api \" vault. hashi corp.com/agent-inj ect-template-payment : {{- with secret \"secret/payment-api \" # payment gw api key .Data.data.APIKEY end } } vault. hashi corp.com/agent-inj ect-status : \"update\" vault.hashicorp.com/agent-inject-secret-sendmail: \"secret/sendmail-api \" vault.hashicorp.com/agent-inject-template-sendmail: I {{- with secret \"secret/sendmail-api\" -n # smtp relay api key .Data.data.APIKEY { {- end } } vault.hashicorp.com/role: \"myapp\" ](media/Vault-image1.png)\napiVersion: batch/v1beta1\n\nkind: CronJob\n\nmetadata:\n\nname: reminder-test\n\nnamespace: crons\n\nspec:\n\n# never runs\n\nschedule: \"00 00 31 2 *\"\n\nsuccessfulJobsHistoryLimit: 7\n\nfailedJobsHistoryLimit: 5\n\njobTemplate:\n\nspec:\n\ntemplate:\n\nmetadata:\n\nannotations:\n\n\"vault.hashicorp.com/agent-inject\": \"true\"\n\n\"vault.hashicorp.com/agent-pre-populate-only\": \"true\"\n\n\"vault.hashicorp.com/agent-inject-secret-credentials.json\": \"crons/reminder-sms\"\n\n\"vault.hashicorp.com/role\": \"crons\"\n\n\"vault.hashicorp.com/agent-inject-template-credentials.json\": |\n\n{{ with secret \"crons/reminder-sms\" }}\n\n{\n\n{{ range $k, $v := .Data.data }} \"{{ $k }}\": \"{{ $v }}\",\n\n{{ end }} \"dummy\": \"yes\"\n\n}\n\n{{ end }}\n\nspec:\n\ncontainers:\n\n- name: app\n\nimage: 331916247734.dkr.ecr.ap-south-1.amazonaws.com/reminder-messages:reminder-messages-prod-2020-07-24-22-5\n\nimagePullPolicy: IfNotPresent\n\nenv:\n\n- name: DEBUG\n\nvalue: \"True\"\n\n- name: DEBUG_EMAIL\n\nvalue: \"deepak.sood@stashfin.com\"\n\n- name: DEBUG_PHONE\n\nvalue: \"9425592627\"\n\n- name: DEBUG_LIMIT\n\nvalue: \"1\"\n\ncommand:\n\n- /bin/bash\n\n- -c\n\n- |\n\nsleep infinity\n\n# sh test.sh\n\nrestartPolicy: OnFailure\n**App Roles**\n\n![Setup Administrator Deployer oao vault auth enable approle vault write auth/approle/role/myrole token_policies=\"myappâ€¢ token_ttl=l h 000 vault read -field=role_id auth/approle/role/myrole/role-id > role-id vault write -f auth/approle/role/myrole/secret-id > secret-id ](media/Vault-image2.png)\n![Approle Authentication Application Login $ grep . role-id secret-id role-id:4bdd6e8e-47e5-5d6f-c698-397a373c9c56 secret-id:649e149e-aa11-2cb1-f4ae-b2f9da824a62 $ vault write auth/approle/login role_id=$(cat role-id) secret_id=$(cat secret-id) Key token token duration policies Value s. pstokYLHuv3rBGrb7zHVCF61 lh [ \"default\" \"myapp\"] ](media/Vault-image3.png)\n![](media/Vault-image4.png)\n![Approle workflow example System Engineer retrieves AppRole RoleID, embeds it in smrce code o o Security Engineer configures Orchestrator retrieves SecretlD as it provisions instance trom template containing RolelD Instance enters production state, authenticating to Vault to retrieve ard secrets Instance boots and performs AppRole login event using RoleID and Secret ID. securely introducing authentication token successful validation ](media/Vault-image5.png)\n\n**Review Approle**\n\n![Define an approle role with appropriate privileges, restrictions Bundle Vault Agent and role_id along with your app Deliver single-use secret_id with short TTL to your app/Agent Agent authenticates with role_id, secret_id Agent renders secrets via template, signals your app App reads rendered template, alerts if secrets missing/unuseable ](media/Vault-image6.png)\n**Database Rotation**\n\nThe database secrets engine generates database credentials dynamically based on configured roles. It works with a number of different databases through a plugin interface. There are a number of builtin database types and an exposed framework for running custom database types for extendability. This means that services that need to access a database no longer need to hardcode credentials: they can request them from Vault, and use Vault's leasing mechanism to more easily roll keys. These are referred to as \"dynamic roles\" or \"dynamic secrets\".\n**Static Roles**\n\nThe database secrets engine supports the concept of \"static roles\", which are a 1-to-1 mapping of Vault Roles to usernames in a database. The current password for the database user is stored and automatically rotated by Vault on a configurable period of time. This is in contrast to dynamic secrets, where a unique username and password pair are generated with each credential request. When credentials are requested for the Role, Vault returns the current password for the configured database user, allowing anyone with the proper Vault policies to have access to the user account in the database.-   Database user credentials rotation\n-   Database root credentials rotation\n\nVault's[database secrets engine](https://www.vaultproject.io/docs/secrets/databases/index.html)provides a centralized workflow for managing credentials for various database systems. By leveraging this, every service instance gets a unique set of database credentials instead of sharing one. Having those credentials tied directly to each service instance and live only for the life of the service, any abnormal access pattern can be mapped to a specific service instance and its credential can be revoked immediately.\nThis reduces the manual tasks performed by the database administrator and makes the database access more efficient and secure.\n\n![DB Root Credentials](media/Vault-image7.png)\n<https://learn.hashicorp.com/tutorials/vault/database-root-rotation>\n\n<https://learn.hashicorp.com/tutorials/vault/database-secrets>\n\n<https://www.vaultproject.io/docs/secrets/databases>\n\n## Audit Devices**\n\nAudit devices are the components in Vault that keep a detailed log of all requests and response to Vault. Because every operation with Vault is an API request/response, the audit log containsevery authenticatedinteraction with Vault, including errors.\nMultiple audit devices can be enabled and Vault will send the audit logs to both. This allows you to not only have a redundant copy, but also a second copy in case the first is tampered with.\n**Questions**\n-   How to delete credentials from hashicorp vault and database when pod terminates? (how to tie the credentials lifecyle to a pod)\n-   How applications will refresh the credentials when the credentials expire? - boto3, python, php\n-   How to give specific username (or some prefix for a username) for a database credentials (because we will exclude this user from audit logs)\n-   Give all pod same credentials\n-   Mount vault credentials to a specific location\n-   How to do disaster recovery\n-   Vault rotate master passwords and send emails\n-   Proxy to a master password like api keys\n**Steps**\n\n1.  Enable KV Engine\n\n2.  Add ACL\n\npath \"prod*\" {\n\ncapabilities = [\"read\"]\n\n}\n\n3.  Create role\n<https://www.vaultproject.io/docs/what-is-vault>\n\n<https://github.com/hashicorp/vault>\n\n<https://www.vaultproject.io/docs/internals/architecture>\n\n<https://learn.hashicorp.com/vault>\n\n<https://blog.container-solutions.com/secret-sprawl-and-the-challenges-of-modern-enterprise-security>\n\n<https://www.hashicorp.com/blog/dynamic-database-credentials-with-vault-and-kubernetes>\n[Kubernetes Secret Management guide beginners using Vault](https://www.youtube.com/playlist?list=PLHq1uqvAteVtq-NRX3yd1ziA_wJSBu3Oj)\n![Vault ](media/Vault-image8.jpg)\n**Comparision**\n\n<https://www.cncf.io/announcements/2021/02/23/cncf-provides-insights-into-secrets-management-tools-with-latest-end-user-technology-radar>"},{"fields":{"slug":"/Computer-Science/Security/Vulnerabilities/","title":"Vulnerabilities"},"frontmatter":{"draft":false},"rawBody":"# Vulnerabilities\n\nCreated: 2019-06-09 01:49:38 +0500\n\nModified: 2022-08-27 01:09:01 +0500\n\n---\n\n1.  Spectre\n\n2.  Meltdown\n\n3.  MDS (Microarchitectural Data Sampling)\n\n<https://www.redhat.com/en/blog/understanding-mds-vulnerability-what-it-why-it-works-and-how-mitigate-it>\n4.  HeartBleed (2014)\n\nHeartbleed allows hackers to steal private keys from what should be secure servers. Infected servers were left wide open to let anyone on the Internet read the memory in systems being protected by a vulnerable version of OpenSSL. The breach let threat actors steal data from servers or listen in on conversations or even spoof services and other users.\n<https://access.redhat.com/security/vulnerabilities>\n\n## Open Web Application Security Project (OWASP)**\n\nTheOpen Web Application Security Project(OWASP) is an online community that produces freely-available articles, methodologies, documentation, tools, and technologies in the field of[web application security](https://en.wikipedia.org/wiki/Web_application_security).\n[The History and Future of OWASP](https://youtu.be/FrU2xaOVDgE)\n**OWASP Top Ten**\n\n1.  [Injection](https://owasp.org/www-project-top-ten/OWASP_Top_Ten_2017/Top_10-2017_A1-Injection)\n\nInjection flaws, such as SQL, NoSQL, OS, and LDAP injection, occur when untrusted data is sent to an interpreter as part of a command or query. The attacker's hostile data can trick the interpreter into executing unintended commands or accessing data without proper authorization.\n2.  [Broken Authentication](https://owasp.org/www-project-top-ten/OWASP_Top_Ten_2017/Top_10-2017_A2-Broken_Authentication)\n\nApplication functions related to authentication and session management are often implemented incorrectly, allowing attackers to compromise passwords, keys, or session tokens, or to exploit other implementation flaws to assume other users' identities temporarily or permanently.\n3.  [Sensitive Data Exposure](https://owasp.org/www-project-top-ten/OWASP_Top_Ten_2017/Top_10-2017_A3-Sensitive_Data_Exposure)\n\nMany web applications and APIs do not properly protect sensitive data, such as financial, healthcare, and PII. Attackers may steal or modify such weakly protected data to conduct credit card fraud, identity theft, or other crimes. Sensitive data may be compromised without extra protection, such as encryption at rest or in transit, and requires special precautions when exchanged with the browser.\n4.  [XML External Entities (XXE)](https://owasp.org/www-project-top-ten/OWASP_Top_Ten_2017/Top_10-2017_A4-XML_External_Entities_(XXE))\n\nMany older or poorly configured XML processors evaluate external entity references within XML documents. External entities can be used to disclose internal files using the file URI handler, internal file shares, internal port scanning, remote code execution, and denial of service attacks.\n5.  [Broken Access Control](https://owasp.org/www-project-top-ten/OWASP_Top_Ten_2017/Top_10-2017_A5-Broken_Access_Control)\n\nRestrictions on what authenticated users are allowed to do are often not properly enforced. Attackers can exploit these flaws to access unauthorized functionality and/or data, such as access other users' accounts, view sensitive files, modify other users' data, change access rights, etc.\n6.  [Security Misconfiguration](https://owasp.org/www-project-top-ten/OWASP_Top_Ten_2017/Top_10-2017_A6-Security_Misconfiguration)\n\nSecurity misconfiguration is the most commonly seen issue. This is commonly a result of insecure default configurations, incomplete or ad hoc configurations, open cloud storage, misconfigured HTTP headers, and verbose error messages containing sensitive information. Not only must all operating systems, frameworks, libraries, and applications be securely configured, but they must be patched/upgraded in a timely fashion.\n7.  [Cross-Site Scripting XSS](https://owasp.org/www-project-top-ten/OWASP_Top_Ten_2017/Top_10-2017_A7-Cross-Site_Scripting_(XSS))\n\nXSS flaws occur whenever an application includes untrusted data in a new web page without proper validation or escaping, or updates an existing web page with user-supplied data using a browser API that can create HTML or JavaScript. XSS allows attackers to execute scripts in the victim's browser which can hijack user sessions, deface web sites, or redirect the user to malicious sites.\nA type of computer security vulnerability typically found in web applications. XSS enables attackers to inject client-side scripts into web pages viewed by other users. A cross-site scripting vulnerability may be used by attackers to bypass access controls such as the same-origin policy.-   Reflected XSS\n\nIt depends on site immediately reflecting a user input (the search query) back onto the page.-   Stored XSS\n\nThis happens when the malicious code (usually an injected script, like in our example) isstored on the target site's servers. A classic example is storing user-generated comments without sanitizing them. An attacker could leave a malicious comment that injects a script, andanyone who views that comment would be affected.\nPrevention - Sanitize user inputs\n**XSRF/CSRF - Cross Site Request Forgery**\n\n**Cross-site request forgery**, also known as**one-click attack**or**session riding**and abbreviated as**CSRF orXSRF**, is a type of malicious[exploit](https://en.wikipedia.org/wiki/Exploit_(computer_security))of a[website](https://en.wikipedia.org/wiki/Website)where unauthorized commands are transmitted from a[user](https://en.wikipedia.org/wiki/User_(computing))that the web application trusts.[[2]](https://en.wikipedia.org/wiki/Cross-site_request_forgery#cite_note-Ristic-2)There are many ways in which a malicious website can transmit such commands; specially-crafted image tags, hidden forms, and[JavaScript](https://en.wikipedia.org/wiki/JavaScript)XMLHttpRequests, for example, can all work without the user's interaction or even knowledge. Unlike[cross-site scripting](https://en.wikipedia.org/wiki/Cross-site_scripting)(XSS), which exploits the trust a user has for a particular site, CSRF exploits the trust that a site has in a user's browser.\n<https://en.wikipedia.org/wiki/Cross-site_request_forgery>\n\n<https://victorzhou.com/blog/csrf>\n\n<https://cheatsheetseries.owasp.org/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.html>\n\n<https://www.freecodecamp.org/news/what-is-cross-site-request-forgery>\n<https://victorzhou.com/blog/xss>\n8.  [Insecure Deserialization](https://owasp.org/www-project-top-ten/OWASP_Top_Ten_2017/Top_10-2017_A8-Insecure_Deserialization)\n\nInsecure deserialization often leads to remote code execution. Even if deserialization flaws do not result in remote code execution, they can be used to perform attacks, including replay attacks, injection attacks, and privilege escalation attacks.\n9.  [Using Components with Known Vulnerabilities](https://owasp.org/www-project-top-ten/OWASP_Top_Ten_2017/Top_10-2017_A9-Using_Components_with_Known_Vulnerabilities)\n\nComponents, such as libraries, frameworks, and other software modules, run with the same privileges as the application. If a vulnerable component is exploited, such an attack can facilitate serious data loss or server takeover. Applications and APIs using components with known vulnerabilities may undermine application defenses and enable various attacks and impacts.\n10. [Insufficient Logging & Monitoring](https://owasp.org/www-project-top-ten/OWASP_Top_Ten_2017/Top_10-2017_A10-Insufficient_Logging%252526Monitoring)\n\nInsufficient logging and monitoring, coupled with missing or ineffective integration with incident response, allows attackers to further attack systems, maintain persistence, pivot to more systems, and tamper, extract, or destroy data. Most breach studies show time to detect a breach is over 200 days, typically detected by external parties rather than internal processes or monitoring.\n<https://owasp.org/www-project-top-ten>\n\n<https://www.cloudflare.com/learning/security/threats/owasp-top-10>\n\n<https://www.toptal.com/security/owasp-top-10-changelog-2017-revision>\n\n## OWASP Cheet Sheet**\n-   [AJAX Security](https://cheatsheetseries.owasp.org/cheatsheets/AJAX_Security_Cheat_Sheet.html)\n-   [Abuse Case](https://cheatsheetseries.owasp.org/cheatsheets/Abuse_Case_Cheat_Sheet.html)\n-   [Access Control](https://cheatsheetseries.owasp.org/cheatsheets/Access_Control_Cheat_Sheet.html)\n-   [Attack Surface Analysis](https://cheatsheetseries.owasp.org/cheatsheets/Attack_Surface_Analysis_Cheat_Sheet.html)\n-   [Authentication](https://cheatsheetseries.owasp.org/cheatsheets/Authentication_Cheat_Sheet.html)\n-   [Authorization Testing Automation](https://cheatsheetseries.owasp.org/cheatsheets/Authorization_Testing_Automation_Cheat_Sheet.html)\n-   [Bean Validation](https://cheatsheetseries.owasp.org/cheatsheets/Bean_Validation_Cheat_Sheet.html)\n-   [Choosing and Using Security Questions](https://cheatsheetseries.owasp.org/cheatsheets/Choosing_and_Using_Security_Questions_Cheat_Sheet.html)\n-   [Clickjacking Defense](https://cheatsheetseries.owasp.org/cheatsheets/Clickjacking_Defense_Cheat_Sheet.html)\n-   [Content Security Policy](https://cheatsheetseries.owasp.org/cheatsheets/Content_Security_Policy_Cheat_Sheet.html)\n-   [Credential Stuffing Prevention](https://cheatsheetseries.owasp.org/cheatsheets/Credential_Stuffing_Prevention_Cheat_Sheet.html)\n-   [Cross-Site Request Forgery Prevention](https://cheatsheetseries.owasp.org/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.html)\n-   [Cross Site Scripting Prevention](https://cheatsheetseries.owasp.org/cheatsheets/Cross_Site_Scripting_Prevention_Cheat_Sheet.html)\n-   [Cryptographic Storage](https://cheatsheetseries.owasp.org/cheatsheets/Cryptographic_Storage_Cheat_Sheet.html)\n-   [DOM based XSS Prevention](https://cheatsheetseries.owasp.org/cheatsheets/DOM_based_XSS_Prevention_Cheat_Sheet.html)\n-   [Database Security](https://cheatsheetseries.owasp.org/cheatsheets/Database_Security_Cheat_Sheet.html)\n-   [Denial of Service](https://cheatsheetseries.owasp.org/cheatsheets/Denial_of_Service_Cheat_Sheet.html)\n-   [Deserialization](https://cheatsheetseries.owasp.org/cheatsheets/Deserialization_Cheat_Sheet.html)\n-   [Docker Security](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html)\n-   [DotNet Security](https://cheatsheetseries.owasp.org/cheatsheets/DotNet_Security_Cheat_Sheet.html)\n-   [Error Handling](https://cheatsheetseries.owasp.org/cheatsheets/Error_Handling_Cheat_Sheet.html)\n-   [File Upload](https://cheatsheetseries.owasp.org/cheatsheets/File_Upload_Cheat_Sheet.html)\n-   [Forgot Password](https://cheatsheetseries.owasp.org/cheatsheets/Forgot_Password_Cheat_Sheet.html)\n-   [HTML5 Security](https://cheatsheetseries.owasp.org/cheatsheets/HTML5_Security_Cheat_Sheet.html)\n-   [HTTP Strict Transport Security](https://cheatsheetseries.owasp.org/cheatsheets/HTTP_Strict_Transport_Security_Cheat_Sheet.html)\n-   [Injection Prevention](https://cheatsheetseries.owasp.org/cheatsheets/Injection_Prevention_Cheat_Sheet.html)\n-   [Injection Prevention in Java](https://cheatsheetseries.owasp.org/cheatsheets/Injection_Prevention_in_Java_Cheat_Sheet.html)\n-   [Input Validation](https://cheatsheetseries.owasp.org/cheatsheets/Input_Validation_Cheat_Sheet.html)\n-   [Insecure Direct Object Reference Prevention](https://cheatsheetseries.owasp.org/cheatsheets/Insecure_Direct_Object_Reference_Prevention_Cheat_Sheet.html)\n-   [JAAS](https://cheatsheetseries.owasp.org/cheatsheets/JAAS_Cheat_Sheet.html)\n-   [JSON Web Token for Java](https://cheatsheetseries.owasp.org/cheatsheets/JSON_Web_Token_for_Java_Cheat_Sheet.html)\n-   [Key Management](https://cheatsheetseries.owasp.org/cheatsheets/Key_Management_Cheat_Sheet.html)\n-   [LDAP Injection Prevention](https://cheatsheetseries.owasp.org/cheatsheets/LDAP_Injection_Prevention_Cheat_Sheet.html)\n-   [Logging](https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html)\n-   [Mass Assignment](https://cheatsheetseries.owasp.org/cheatsheets/Mass_Assignment_Cheat_Sheet.html)\n-   [Microservices based Security Arch Doc](https://cheatsheetseries.owasp.org/cheatsheets/Microservices_based_Security_Arch_Doc_Cheat_Sheet.html)\n-   [Multifactor Authentication](https://cheatsheetseries.owasp.org/cheatsheets/Multifactor_Authentication_Cheat_Sheet.html)\n-   [Nodejs Security](https://cheatsheetseries.owasp.org/cheatsheets/Nodejs_Security_Cheat_Sheet.html)\n-   [OS Command Injection Defense](https://cheatsheetseries.owasp.org/cheatsheets/OS_Command_Injection_Defense_Cheat_Sheet.html)\n-   [PHP Configuration](https://cheatsheetseries.owasp.org/cheatsheets/PHP_Configuration_Cheat_Sheet.html)\n-   [Password Storage](https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html)\n-   [Pinning](https://cheatsheetseries.owasp.org/cheatsheets/Pinning_Cheat_Sheet.html)\n-   [Query Parameterization](https://cheatsheetseries.owasp.org/cheatsheets/Query_Parameterization_Cheat_Sheet.html)\n-   [REST Assessment](https://cheatsheetseries.owasp.org/cheatsheets/REST_Assessment_Cheat_Sheet.html)\n-   [REST Security](https://cheatsheetseries.owasp.org/cheatsheets/REST_Security_Cheat_Sheet.html)\n-   [Ruby on Rails](https://cheatsheetseries.owasp.org/cheatsheets/Ruby_on_Rails_Cheat_Sheet.html)\n-   [SAML Security](https://cheatsheetseries.owasp.org/cheatsheets/SAML_Security_Cheat_Sheet.html)\n-   [SQL Injection Prevention](https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html)\n-   [Securing Cascading Style Sheets](https://cheatsheetseries.owasp.org/cheatsheets/Securing_Cascading_Style_Sheets_Cheat_Sheet.html)\n-   [Server Side Request Forgery Prevention](https://cheatsheetseries.owasp.org/cheatsheets/Server_Side_Request_Forgery_Prevention_Cheat_Sheet.html)\n-   [Session Management](https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html)\n-   [TLS Cipher String](https://cheatsheetseries.owasp.org/cheatsheets/TLS_Cipher_String_Cheat_Sheet.html)\n-   [Third Party Javascript Management](https://cheatsheetseries.owasp.org/cheatsheets/Third_Party_Javascript_Management_Cheat_Sheet.html)-   [**Threat Modeling**](https://cheatsheetseries.owasp.org/cheatsheets/Threat_Modeling_Cheat_Sheet.html)\n\nThreat modelingis a process by which potential threats, such as[structural vulnerabilities](https://www.wikiwand.com/en/Structural_vulnerability_(computing))or the absence of appropriate safeguards, can be identified, enumerated, and mitigations can be prioritized. The purpose of threat modeling is to provide defenders with a systematic analysis of what controls or defenses need to be included, given the nature of the system, the probable attacker's profile, the most likely attack vectors, and the assets most desired by an attacker. Threat modeling answers questions like\"Where am I most vulnerable to attack?\",\"What are the most relevant threats?\", and\"What do I need to do to safeguard against these threats?\".\n**Threat modeling methodologies**\n\n1.  **STRIDE**\n\nSTRIDEis a model of threats developed by Praerit Garg and[Loren Kohnfelder](https://www.wikiwand.com/en/Loren_Kohnfelder) at [Microsoft](https://www.wikiwand.com/en/Microsoft) for identifying[computer security](https://www.wikiwand.com/en/Computer_security)[threats](https://www.wikiwand.com/en/Threat_(computer)).It provides a[mnemonic](https://www.wikiwand.com/en/Mnemonic)for security threats in six categories.\nThe threats are:\n-   [Spoofing](https://www.wikiwand.com/en/Spoofing_attack)\n-   [Tampering](https://www.wikiwand.com/en/Tampering_(crime))\n-   [Repudiation](https://www.wikiwand.com/en/Non-repudiation)\n-   Information disclosure ([privacy breach](https://www.wikiwand.com/en/Data_privacy)or[data leak](https://www.wikiwand.com/en/Data_leak))\n-   [Denial of service](https://www.wikiwand.com/en/Denial-of-service_attack)\n-   [Elevation of privilege](https://www.wikiwand.com/en/Privilege_escalation)\n<https://www.wikiwand.com/en/STRIDE_(security)>\n2.  **PASTA**\n\nProcess for Attack Simulation and Threat Analysis (PASTA) is a seven-step, risk-centric methodology.\n3.  **Trike\n    **\n\n<https://www.wikiwand.com/en/Threat_model>-   [Transaction Authorization](https://cheatsheetseries.owasp.org/cheatsheets/Transaction_Authorization_Cheat_Sheet.html)\n-   [Transport Layer Protection](https://cheatsheetseries.owasp.org/cheatsheets/Transport_Layer_Protection_Cheat_Sheet.html)\n-   [Unvalidated Redirects and Forwards](https://cheatsheetseries.owasp.org/cheatsheets/Unvalidated_Redirects_and_Forwards_Cheat_Sheet.html)\n-   [User Privacy Protection](https://cheatsheetseries.owasp.org/cheatsheets/User_Privacy_Protection_Cheat_Sheet.html)\n-   [Virtual Patching](https://cheatsheetseries.owasp.org/cheatsheets/Virtual_Patching_Cheat_Sheet.html)\n-   [Vulnerability Disclosure](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html)\n-   [Vulnerable Dependency Management](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerable_Dependency_Management_Cheat_Sheet.html)\n-   [Web Service Security](https://cheatsheetseries.owasp.org/cheatsheets/Web_Service_Security_Cheat_Sheet.html)\n-   [XML External Entity Prevention](https://cheatsheetseries.owasp.org/cheatsheets/XML_External_Entity_Prevention_Cheat_Sheet.html)\n-   [XML Security](https://cheatsheetseries.owasp.org/cheatsheets/XML_Security_Cheat_Sheet.html)\n<https://cheatsheetseries.owasp.org>\n\n## Social Engineering**\n\n\"Social engineering\" refers to the use of humans as an attack vector to compromise a system. It involves fooling or otherwise manipulating human personnel into revealing information or performing actions on the attacker's behalf. Social engineering is known to be a very effective attack strategy, since even the strongest security system can be compromised by a single poor decision. In some cases, highly secure systems that cannot be penetrated by computer or cryptographic means, can be compromised by simply calling a member of the target organization on the phone and impersonating a colleague or IT professional.\nCommon social engineering techniques include[phishing](https://en.wikipedia.org/wiki/Phishing),[clickjacking](https://en.wikipedia.org/wiki/Clickjacking), and[baiting](https://en.wikipedia.org/wiki/Social_engineering_%28security%29#Baiting), although several other tricks are at an attacker's disposal.\n**Phishing**\n\n**Spear Phishing**\n\nSpear phishing involves selectively targetting employees,[and developers are especially vulnerable](https://www.teiss.co.uk/threats/developers-vulnerable-phishing-attacks/). Spear phishers will discover information about you, and then selectively use it against you.\n**Impersonating Services**\n\nThis is the most well-known form of phishing. It involves posing as a business, often styling emails to look like what that business would typically send.\n**Smishing**\n\nSmishing (SMS phishing) is similar to standard phishing emails, but over SMS instead. Smishing texts will usually impersonate companies and encourage you to click on a link or give away your personal info.\n**Vishing**\n\nVishing (\"voice\" and \"phishing\") involves phishing through phone calls. Of course, this isn't a big deal to us, because what kind of developer seriously answers the phone nowadays? Just send me a text, FFS.\n<https://dev.to/kathyra_/protect-yourself-from-social-engineering-3ihk>\n\n## Kill Chain**\n\nThe termkill chainwas originally used as a[military](https://www.wikiwand.com/en/Military)concept related to the structure of an[attack](https://www.wikiwand.com/en/Offensive_(military)); consisting of target identification, force dispatch to target, decision and order to attack the target, and finally the destruction of the target.Conversely, the idea of \"breaking\" an opponent's kill chain is a method of[defense](https://www.wikiwand.com/en/Defense_(military))or preemptive action.More recently,[Lockheed Martin](https://www.wikiwand.com/en/Lockheed_Martin)adapted this concept to[information security](https://www.wikiwand.com/en/Information_security), using it as a method for modeling intrusions on a[computer network](https://www.wikiwand.com/en/Computer_network).The cyber kill chain model has seen some adoption in the information security community.However, acceptance is not universal, with critics pointing to what they believe are fundamental flaws in the model.\n<https://www.wikiwand.com/en/Kill_chain>\n\n## Tab Nabbing**\n\nTabnabbing is a computer exploit which persuades users to submit their login details and passwords. The attack takes advantage of user trust and inattention to detail in regard totabs, and the ability of browsers to navigate across a page's origin in inactivetabsa long time after the page is loaded. This attack can be done even if JavaScript is disabled, using the \"meta refresh\" meta element, an HTML attribute used for page redirection that causes a reload of a specified new page after a given time interval. The attack takes advantage of the trust of the victim and the ability of modern web pages to rewritetabsand their contents for a long time after the page has been loaded.\n**Air Gap**\n\nAn air gapped machine is simply one that cannot connect to any outside agents. From the highest level being the internet, to the lowest being an intranet or even bluetooth.\nAir gapped machines are isolated from other computers, and are important for storing sensitive data or carrying out critical tasks that should be immune from outside interference. For example, a nuclear power plant should be operated from computers that are behind a full air gap. For the most part, real world air gapped computers are usually connected to some form of intranet in order to make data transfer and process execution easier. However, every connection increases the risk that outside actors will be able to penetrate the system.\n<https://www.toptal.com/security/interview-questions>\n"},{"fields":{"slug":"/Computer-Science/System-Design/API-Gateway/","title":"API Gateway"},"frontmatter":{"draft":false},"rawBody":"# API Gateway\n\nCreated: 2019-04-14 20:11:23 +0500\n\nModified: 2022-02-05 01:13:53 +0500\n\n---\n\n**API Design**\n\n[Designing APIs: Less Data is More || Damir Svrtan](https://www.youtube.com/watch?v=DC9032_nkyc)\n**Features**\n\n1.  Authentication\n\n2.  Authorization\n\n3.  Security\n\n4.  SSL termination\n\n5.  DDOS protection / Throttling\n\n6.  Adaptor (that will consolidate all information from all services and return as one response)\n\n7.  Serve static content\n\n8.  Cache responses\n\n9.  Request routing\n\n10. Router & Load Balancer\n\n11. A/B Testing\n\n12. Canary Testing\n\n13. Protocol Adapter\n\n14. Monitoring all api's performance\n\n15. Rate Limiting\n\n16. Expose as API service\n**Using an API gateway has the following benefits:**\n-   Insulates the clients from how the application is partitioned into microservices\n-   Insulates the clients from the problem of determining the locations of service instances\n-   Provides the optimal API for each client\n-   Reduces the number of requests/roundtrips. For example, the API gateway enables clients to retrieve data from multiple services with a single round-trip. Fewer requests also means less overhead and improves the user experience. An API gateway is essential for mobile applications.\n-   Simplifies the client by moving logic for calling multiple services from the client to API gateway\n-   Translates from a \"standard\" public web-friendly API protocol to whatever protocols are used internally\n**The API gateway pattern has some drawbacks:**\n-   Increased complexity - the API gateway is yet another moving part that must be developed, deployed and managed\n-   Increased response time due to the additional network hop through the API gateway - however, for most applications the cost of an extra roundtrip is insignificant.\n**Tools**\n\n1.  **Cloud**\n\n    a.  Apigee\n\n    b.  AWS API Gateway\n\n    c.  Azure API Gateway\n\n    d.  Google cloud endpoints\n\n    e.  WSO2 API Manager\n\n2.  **Self Managed**\n\n    a.  Apache\n\n    b.  HAProxy\n\n    c.  Nginx\n\n    d.  Spring cloud gateway\n\n    e.  <https://apisix.apache.org>\n\n## API Gateways**\n\n**Gloo**\n\nThe Hybrid Application Gateway built on top of Envoy\nGloo is a feature-rich, Kubernetes-native ingress controller, and next-generation API gateway. Gloo is exceptional in its function-level routing; its support for legacy apps, microservices and serverless; its discovery capabilities; its numerous features; and its tight integration with leading open-source projects. Gloo is uniquely designed to support hybrid applications, in which multiple technologies, architectures, protocols, and clouds can coexist\n<https://github.com/solo-io/gloo>\n\n## Ambassador**\n\n[Ambassador](https://www.getambassador.io/)is an open source Kubernetes-native API Gateway built on[Envoy](https://www.envoyproxy.io/), designed for microservices. Ambassador essentially serves as an Envoy ingress controller, but with many more features.\nKey features include:\n-   Self-service configuration, via Kubernetes annotations\n-   First class[gRPC and HTTP/2 support](https://www.getambassador.io/user-guide/grpc)\n-   Support for CORS, timeouts, weighted round robin ([canary](https://www.getambassador.io/reference/canary)),[rate limiting](https://www.getambassador.io/reference/services/rate-limit-service)\n-   [Istio integration](https://www.getambassador.io/user-guide/with-istio)\n-   [Authentication](https://www.getambassador.io/reference/services/auth-service)\n-   Robust TLS support, including TLS client-certificate authentication\n<https://github.com/datawire/ambassador>\n![Primary use case Learning curve Cost Configurability Config language Config style for endpoints GUI and Drag & Drop Config Authentication support Govemance model Installation and ops Kubernetes Cloud laaS Private Data Center Scalability Primary scaling approach Additional data store required for horizontal scaling Ambassador Microservices gateway Simple Open source YAML (Kubernetes annotations) Declarative No Yes Decentralised, self-service Easy (Official YAML bootstrap available) Horizontal No, state managed via Kubernetes Traefik Microservices gateway Simple Open source TOML Declarative No Only basic auth out-of-the-box Decentralised, self-service Easy (Official YAML bootstrap available) Easy Easy Horizontal Yes, KN store required e.g. Consul, etcd, ZK, boltdb OpenResty Monolith / microservices gateway Moderate Open source Text files (nginx.conf etc) Declarative No Yes Decentralised Moderate (Unofficial / unmaintained Yaml bootstrap available Moderate Moderate Horizontal Yes, KN store required e.g. redis Cloud Vendor Impl Expose backend services, including microservices and FaaS Moderate Typically YAML or JSON Declarative / imperative Not typically Yes (Although may offer limited customisation) Decentralised (typically) Easy-moderate Easy Horizontal Yes. Data management typically integrated into service CA API Gateway Enterprise API management High Text files Declarative / imperative Yes Yes Centralised High High Vertical (horizontal supported) Yes. MySQL cluster required Kong Enterprise Edition Enterprise API management Moderate Admin REST API, Text files (nginx.conf etc) Imperative Yes Yes Configurable Moderate (Official k8s Yaml and Helm chart available; external data store required) High High Horizontal Yes. PostgreSQL or Cassandra cluster required ](media/API-Gateway-image1.png)\n<https://microservices.io/patterns/apigateway.html>\n\nRate Limiting Service\n\n[What is an API Gateway?](https://www.youtube.com/watch?v=vHQqQBYJtLI)\n\n<https://blog.christianposta.com/microservices/do-i-need-an-api-gateway-if-i-have-a-service-mesh>\n\n<https://developer.ibm.com/apiconnect/2018/11/13/service-mesh-vs-api-management>\n\n<https://blog.christianposta.com/microservices/api-gateways-are-going-through-an-identity-crisis>\n\n<https://konghq.com/blog/the-difference-between-api-gateways-and-service-mesh>\n\n"},{"fields":{"slug":"/Computer-Science/System-Design/Addressing-Failures/","title":"Addressing Failures"},"frontmatter":{"draft":false},"rawBody":"# Addressing Failures\n\nCreated: 2018-05-04 20:32:12 +0500\n\nModified: 2021-08-06 00:50:55 +0500\n\n---\n\n**Cascading Failures**\n\nA cascading failure is a failure that grows over time as a result of positive feedback.[^107^](https://landing.google.com/sre/book/chapters/addressing-cascading-failures.html#id-GbduZFnh9)It can occur when a portion of an overall system fails, increasing the probability that other portions of the system fail. For example, a single replica for a service can fail due to overload, increasing load on remaining replicas and increasing their probability of failing, causing a domino effect that takes down all the replicas for a service.\n**Causes**\n\n1.  Server Overload\n\n2.  Resource Exhaustion\n\n3.  CPU\n\n    a.  Increased number of in-flight requests\n\n    b.  Excessively long queue lengths\n\n    c.  Thread starvation\n\n    d.  CPU or request starvation\n\n    e.  Missed RPC Deadlines\n\n    f.  Reduced CPU caching benefits\n\n4.  Memory\n\n    a.  Dying tasks\n\n    b.  Increases rate of Garbage Collection (GC), resulting in increased CPU Usage\n\n    c.  Reduction in cache hit rates\n\n5.  Threads\n\n6.  File Descriptors\n\n7.  Dependencies among resources\n\n8.  Service Unavailability\n**Prevention**\n\n1.  Load test the server's capacity limits, and test the failure mode for overload\n\n2.  Serve degraded results\n\n3.  Instrument the server to reject requests when overloaded\n\n4.  Instrument higher-level systems to reject requests, rather than overloading servers\n\n5.  Perform capacity planning\n\n6.  Queue Management\n\n7.  Load Shedding and Graceful Degradation\n\n**Load Shedding**\n\nThe idea is to ignore some requests rather than crashing a system and making it fail to serve any request.\n![request Source 429 Too Many Requests API Gateway Sink 4. Partial degradation* \"shed load\" response ](media/Addressing-Failures-image1.png)\n\n8.  Retries\n\n9.  Latency and Deadlines\n\n10. Picking a Deadline\n\n11. Missing Deadlines\n\n12. Deadline Propagation\n\n13. Cancellation Propagation\n\n14. Bimodal Latency\n\n15. Slow Startup and Cold Caching\n\n16. Always go Downward in the Stack\n**Triggering Conditions for Cascading Failures**\n\n1.  Process Death\n\n2.  Process Updates\n\n3.  New Rollouts\n\n4.  Organic Growth\n\n5.  Planned Changes, Drains, or Turndowns\n\n6.  Request Profile Changes\n\n7.  Resource Limits\n**Testing for Cascading Failures**\n\n1.  Test Until Failure and Beyond\n\n2.  Test Popular Clients\n\n3.  Test Noncritical Backends\n**Immediate Steps to Address Cascading Failures**\n\n1.  Increase Resources\n\n2.  Stop Health Check Failures/Deaths\n\n3.  Restart Servers\n\n4.  Drop Traffic\n\n5.  Enter Degraded Modes\n\n6.  Eliminate Batch Load\n\n7.  Eliminate Bad Traffic\n**Reference -**\n\n<http://highscalability.com/blog/2018/4/25/google-addressing-cascading-failures.html>\n\n"},{"fields":{"slug":"/Computer-Science/System-Design/Architecture-Guide/","title":"Architecture Guide"},"frontmatter":{"draft":false},"rawBody":"# Architecture Guide\n\nCreated: 2019-09-18 00:09:55 +0500\n\nModified: 2020-08-20 01:58:00 +0500\n\n---\n\n1.  Application Architecture Guide\n\n    a.  Architecture Styles\n\n        i.  Big compute\n\n        ii. Big data\n\n[Big Data](https://docs.microsoft.com/en-us/azure/architecture/guide/architecture-styles/big-data)and[Big Compute](https://docs.microsoft.com/en-us/azure/architecture/guide/architecture-styles/big-compute)are specialized architecture styles for workloads that fit certain specific profiles. Big data divides a very large dataset into chunks, performing parallel processing across the entire set, for analysis and reporting. Big compute, also called high-performance computing (HPC), makes parallel computations across a large number (thousands) of cores. Domains include simulations, modeling, and 3-D rendering.\n\niii. Event-driven architecture\n\niv. Microservicesv.  **N-tier application**\n\nvi. **Web-queue-worker**\n\nFor a purely PaaS solution, consider a[Web-Queue-Worker](https://docs.microsoft.com/en-us/azure/architecture/guide/architecture-styles/web-queue-worker)architecture. In this style, the application has a web front end that handles HTTP requests and a back-end worker that performs CPU-intensive tasks or long-running operations. The front end communicates to the worker through an asynchronous message queue.\nWeb-queue-worker is suitable for relatively simple domains with some resource-intensive tasks. Like N-tier, the architecture is easy to understand. The use of managed services simplifies deployment and operations. But with complex domains, it can be hard to manage dependencies. The front end and the worker can easily become large, monolithic components that are hard to maintain and update. As with N-tier, this can reduce the frequency of updates and limit innovation.\n| **Architecture style**    | **Dependency management**                                                        | **Domain type**                                                  |\n|----------------|--------------------------------|------------------------|\n| N-tier                    | Horizontal tiers divided by subnet                                               | Traditional business domain. Frequency of updates is low.        |\n| Web-Queue-Worker          | Front and backend jobs, decoupled by async messaging.                            | Relatively simple domain with some resource intensive tasks.     |\n| Microservices             | Vertically (functionally) decomposed services that call each other through APIs. | Complicated domain. Frequent updates.                            |\n| Event-driven architecture | Producer/consumer. Independent view per sub-system.                              | IoT and real-time systems                                        |\n| Big data                  | Divide a huge dataset into small chunks. Parallel processing on local datasets.  | Batch and real-time data analysis. Predictive analysis using ML. |\n| Big compute               | Data allocation to thousands of cores.                                           | Compute intensive domains such as simulation.                    |\nb.  Design Principles\n\n    i.  Design for self-healing\n\n    ii. Make all things redundant\n\n    iii. Minimize coordination\n\n    iv. Design to scale out\n\n    v.  Partition around limits\n\n    vi. Design for operations\n\n    vii. Use managed services\n\n    viii. Use the best data store for the job\n\n    ix. Design for evolution\n\n    x.  Build for the needs of business\n\nc.  Best Practices\n\n    i.  API Design\n\n    ii. API Implementation\n\n    iii. Autoscaling\n\n    iv. Background jobs\n\n    v.  Caching\n\n    vi. Content Delivery Network\n\n    vii. Data Partitioning\n\n    viii. Data Partitioning strategies (by service)\n\n    ix. Monitoring and diagnostics\n\n    x.  Naming Conventions\n\n    xi. Retry Guidance for Specific services\n\n    xii. Transient fault handling\n\nd.  Performance Tuning\n\n    i.  Scenario 1 - Distributed Transactions\n\n    ii. Scenario 2 - Multiple backend services\n\n    iii. Scenario 3 - Event Streaming\n\ne.  Performance Antipatterns\n\n    i.  Busy Database\n\n    ii. Busy Front End\n\n    iii. Chatty I/O\n\n    iv. Extraneous Fetching\n\n    v.  Improper Instantiation\n\n    vi. Monolithic Persistence\n\n    vii. No Caching\n\n    viii. Synchronous I/O2.  Design Patterns\n    -   [Ambassador](https://docs.microsoft.com/en-us/azure/architecture/patterns/ambassador)\n    -   [Anti-corruption Layer](https://docs.microsoft.com/en-us/azure/architecture/patterns/anti-corruption-layer)\n    -   [Availability](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/availability)\n    -   [Backends for Frontends](https://docs.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends)\n    -   [Bulkhead](https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead)\n\nThe Bulkhead pattern is a type of application design that is tolerant of failure. In a bulkhead architecture, elements of an application are isolated into pools so that if one fails, the others will continue to function. It's named after the sectioned partitions (bulkheads) of a ship's hull. If the hull of a ship is compromised, only the damaged section fills with water, which prevents the ship from sinking.\n<https://www.youtube.com/watch?v=R2FT5edyKOg>-   [Cache-Aside](https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside)\n-   [Choreography](https://docs.microsoft.com/en-us/azure/architecture/patterns/choreography)\n-   [Circuit Breaker](https://docs.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker)\n-   [Claim Check](https://docs.microsoft.com/en-us/azure/architecture/patterns/claim-check)\n-   [Command and Query Responsibility Segregation (CQRS)](https://docs.microsoft.com/en-us/azure/architecture/patterns/cqrs)\n-   [Compensating Transaction](https://docs.microsoft.com/en-us/azure/architecture/patterns/compensating-transaction)\n-   [Competing Consumers](https://docs.microsoft.com/en-us/azure/architecture/patterns/competing-consumers)\n-   [Compute Resource Consolidation](https://docs.microsoft.com/en-us/azure/architecture/patterns/compute-resource-consolidation)\n-   [Data management](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/data-management)\n-   [Design and implementation](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation)\n-   [Event Sourcing](https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing)\n-   [External Configuration Store](https://docs.microsoft.com/en-us/azure/architecture/patterns/external-configuration-store)\n-   [Federated Identity](https://docs.microsoft.com/en-us/azure/architecture/patterns/federated-identity)\n-   [Gatekeeper](https://docs.microsoft.com/en-us/azure/architecture/patterns/gatekeeper)\n-   [Gateway Aggregation](https://docs.microsoft.com/en-us/azure/architecture/patterns/gateway-aggregation)\n-   [Gateway Offloading](https://docs.microsoft.com/en-us/azure/architecture/patterns/gateway-offloading)\n-   [Gateway Routing](https://docs.microsoft.com/en-us/azure/architecture/patterns/gateway-routing)\n-   [Health Endpoint Monitoring](https://docs.microsoft.com/en-us/azure/architecture/patterns/health-endpoint-monitoring)\n-   [Index Table](https://docs.microsoft.com/en-us/azure/architecture/patterns/index-table)\n-   [Leader Election](https://docs.microsoft.com/en-us/azure/architecture/patterns/leader-election)\n-   [Management and monitoring](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/management-monitoring)\n-   [Materialized View](https://docs.microsoft.com/en-us/azure/architecture/patterns/materialized-view)\n-   [Messaging](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging)\n-   [Performance and scalability](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability)\n-   [Pipes and Filters](https://docs.microsoft.com/en-us/azure/architecture/patterns/pipes-and-filters)\n-   [Priority Queue](https://docs.microsoft.com/en-us/azure/architecture/patterns/priority-queue)\n-   [Publisher/Subscriber](https://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber)\n-   [Queue-Based Load Leveling](https://docs.microsoft.com/en-us/azure/architecture/patterns/queue-based-load-leveling)\n-   [Resiliency](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency)\n-   [Retry](https://docs.microsoft.com/en-us/azure/architecture/patterns/retry)\n-   [Scheduler Agent Supervisor](https://docs.microsoft.com/en-us/azure/architecture/patterns/scheduler-agent-supervisor)\n-   [Security](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/security)\n-   [Sharding](https://docs.microsoft.com/en-us/azure/architecture/patterns/sharding)\n-   [Sidecar](https://docs.microsoft.com/en-us/azure/architecture/patterns/sidecar)\n-   [Static Content Hosting](https://docs.microsoft.com/en-us/azure/architecture/patterns/static-content-hosting)\n-   [Strangler](https://docs.microsoft.com/en-us/azure/architecture/patterns/strangler)\n-   [Throttling](https://docs.microsoft.com/en-us/azure/architecture/patterns/throttling)\n-   [Valet Key](https://docs.microsoft.com/en-us/azure/architecture/patterns/valet-key)\n3.  Pillars of Software Quality\n\n    a.  Reliability\n\n    b.  Resiliency\n\n    c.  Security\n\n    d.  Scalability\n\n4.  Technologies\n\n    a.  AI and machine learning\n\n    b.  Blockchain\n\n    c.  Data architectures\n\n    d.  DevOps\n\n    e.  Enterprise integration\n\n    f.  High performance computing (HPC)\n\n    g.  Identity\n\n    h.  Internet of Things (IoT)\n\n    i.  Microservices\n\n    j.  Networking\n\n    k.  Serverless applications\n\n    l.  VM workloads\n\n    m.  Web apps\n\n5.  Cloud Adoption Framework\n<https://docs.microsoft.com/en-us/azure/architecture/guide>\n\n<https://github.com/MicrosoftDocs/architecture-center>\n<https://thenewstack.io/primer-understanding-software-and-system-architecture>\n<https://www.freecodecamp.org/news/systems-design-for-interviews>\n"},{"fields":{"slug":"/Computer-Science/System-Design/Cloud-Native/","title":"Cloud Native"},"frontmatter":{"draft":false},"rawBody":"# Cloud Native\n\nCreated: 2019-04-07 10:24:34 +0500\n\nModified: 2020-12-16 00:11:35 +0500\n\n---\n\nCloud native is a term used to describe **container-based environments.** Cloud-native technologies are used to develop applications built with services packaged in containers, deployed as microservices and managed on elastic infrastructure through agile DevOps processes and continuous delivery workflows.\nWhere operations teams would manage the infrastructure resource allocations to traditional applications manually, cloud-native applications are deployed on infrastructure that abstracts the underlying compute, storage and networking primitives. Developers and operators dealing with this new breed of applications don't directly interact with application programming interfaces (APIs) exposed by infrastructure providers. Instead, the orchestrator handles resource allocation automatically, according to policies set out by DevOps teams. The controller and scheduler, which are essential components of the orchestration engine, handle resource allocation and the life cycle of applications.\nCloud-native platforms, like Kubernetes, expose a flat network that is overlaid on existing networking topologies and primitives of cloud providers. Similarly, the native storage layer is often abstracted to expose logical volumes that are integrated with containers. Operators can allocate storage quotas and network policies that are accessed by developers and resource administrators. The infrastructure abstraction not only addresses the need for portability across cloud environments, but also lets developers take advantage of emerging patterns to build and deploy applications. Orchestration managers become the deployment target, irrespective of the underlying infrastructure that may be based on physical servers or virtual machines, private clouds or public clouds.\nKubernetes is an ideal platform for running contemporary workloads designed as cloud-native applications. It's become the de facto operating system for the cloud, in meuch the same way Linux is the operating system for the underlying machines.\n**10 Key Attributes of Cloud-Native Applications**\n\n1.  Packaged as lightweight containers\n\n2.  Developed with best-of-breed languages and frameworks\n\n3.  Designed as loosely coupled microservices\n\n4.  Centered around APIs for interaction and collaboration\n\n5.  Architected with a clean separation of stateless and stateful services\n\n6.  Isolated from server and operating system dependencies\n\n7.  Deployed on self-service, elastic, cloud infrastructure\n\n8.  Managed through agile DevOps processes\n\n9.  Automated capabilities\n\n10. Defined, policy-driven resource allocation\n<https://cloud.google.com/blog/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it>\n\n<https://thenewstack.io/primer-distributed-systems-and-cloud-native-computing>\n\n## Self service Edge Stack**\n\n<table>\n<colgroup>\n<col style=\"width: 42%\" />\n<col style=\"width: 57%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th><strong>Traditional on-premises</strong></th>\n<th><strong>Modern cloud</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><p>Monolithic</p>\n<p>Designed for predictable scalability</p>\n<p>Relational database</p>\n<p>Synchronized processing</p>\n<p>Design to avoid failures (MTBF)</p>\n<p>Occasional large updates</p>\n<p>Manual management</p>\n<p>Snowflake servers</p></td>\n<td><p>Decomposed</p>\n<p>Designed for elastic scale</p>\n<p>Polyglot persistence (mix of storage technologies)</p>\n<p>Asynchronous processing</p>\n<p>Design for failure (MTTR)</p>\n<p>Frequent small updates</p>\n<p>Automated self-management</p>\n<p>Immutable infrastructure</p></td>\n</tr>\n</tbody>\n</table>\n"},{"fields":{"slug":"/Computer-Science/System-Design/Enterprise-Integration-Patterns/","title":"Enterprise Integration Patterns"},"frontmatter":{"draft":false},"rawBody":"# Enterprise Integration Patterns\n\nCreated: 2018-11-22 16:48:01 +0500\n\nModified: 2021-08-06 00:50:42 +0500\n\n---\n\nThe patterns provide technology-independent design guidance for developers and architects to describe and develop robust integration solutions.\n![Message Construction Message Command Message Document Message Event Message Request-Reply Retum Address Correlation Identifier Message Sequence Message Expiration Format Indicator Endpoint Message Message Routing Pipes-and-Filters Message Router Content-Based Router Message Filter Dynamic Filter Recisient List Splitter Channel Aggregator Resequencer Composed Processor Scatter-Gather Routing Slip Process Manager Message Broker Router Translator Application Messaging Endpoints Message Endpoint Competing Consumers Messaging Channels Channel Point-to-Point Channel ](media/Enterprise-Integration-Patterns-image1.png)**References**\n\n<https://www.enterpriseintegrationpatterns.com/patterns/messaging/index.html>\n\n<https://www.enterpriseintegrationpatterns.com/patterns/messaging/toc.html>\n\n"},{"fields":{"slug":"/Computer-Science/System-Design/Event-driven-architecture/","title":"Event driven architecture"},"frontmatter":{"draft":false},"rawBody":"# Event driven architecture\n\nCreated: 2018-06-09 22:08:20 +0500\n\nModified: 2022-07-22 17:23:27 +0500\n\n---\n\n**Event Driven Architecture / Event Sourcing**\n\nAn event records the fact that \"something happened\" in the world.Conceptually, an event has a key, value, and timestamp. A concrete event could be a plain notification without any additional information, but it could also include the full details of what exactly happened to facilitate subsequent processing. For instance:\n-   Event key: \"Alice\"\n-   Event value: \"Has arrived in Rome\"\n-   Event timestamp: \"Dec. 3, 2019 at 9:06 a.m.\"\nOther example events include\n-   A good was sold\n-   A row was updated in a database table\n-   A wind turbine sensor measured 14 revolutions per minute\n-   An action occurred in a video game, such as \"White moved the e2 pawn to e4\"\n-   A payment of $200 was made by Frank to Sally on Nov. 24, 2019, at 5:11 p.m.\n1.  Event Notification\n\n    a.  **Events** (Fire and forget, does not mandate someone to take an action. Someone listening can take an action or do nothing)\n\n    b.  **Commands** (Tells some service to do a particular thing)\n\n    c.  Problem\n\n        i.  No statement of overall behavior (There is no fixed set of tasks that must be performed when some event happens)\n\n2.  Event-carried state transfer\n    -   All the state is transfered with the event like what has old state, what is new state.\n    -   Save all the data that system needs locally. Therefore source system broadcast all its event so the downstream system can save all the data locally whenever there is a change.\n\n3.  Event sourcing\n\nEvent sourcing system have two components -\n-   Application state - current state of the systems,\n-   Log - logs for all the events that has happened.\n\nReplaying all the events from the logs can give any state of the system at any time.\n-   Example -\n\n    1.  Version control - git\n\n    2.  Accounting ledgers\n-   Can be used for\n    -   Audit\n    -   Debugging\n    -   Historic State\n    -   Alternative State\n    -   Memory Image\n\n4.  CQRS - Command and Query Responsibility Segregation\n\nCQRS is a fancy name for an architecture that uses different data models to represent read and write operations.\nAt its heart is the notion that you can use a different model to update information than the model you use to read information.\n\n![query services update presentations trom query model reads trom database query mode/ Query In Command Model es ](media/Event-driven-architecture-image1.png)\nEvent-driven architecture (EDA) means constructing your system as a series of commands and/or events. A user submits an online form to make a purchase: that's a command. The items in stock are reserved: that's an event. A confirmation is sent to the user: that's an event. The concept is very simple. Everything in our system is either a command or an event. Commands lead to events and events may lead to new commands and so on.\n**Event Sourcing** is a style of application design where state changes are logged as a time-ordered sequence of records.\n**Publisher subscriber rule**\n-   Consumers should not know who producers are\n-   Producers should not know who consumers are\n\n![png](media/Event-driven-architecture-image2.jpg)\n\n**Streams and Tables in Kafka**\n-   A**stream**provides immutable data. It supports only inserting (appending) new events, whereas existing events cannot be changed. Streams are persistent, durable, and fault tolerant. Events in a stream can be keyed, and you can have many events for one key, like \"all of Bob's payments.\" If you squint a bit, you could consider a stream to be like a table in a relational database (RDBMS) that has no unique key constraint and that is append only.\n-   A**table**provides mutable data. New events---rows---can be inserted, and existing rows can be updated and deleted. Here, an event's key aka row key identifies which row is being mutated. Like streams, tables are persistent, durable, and fault tolerant. Today, a table behaves much like an RDBMS materialized view because it is being changed automatically as soon as any of its input streams or tables change, rather than letting you directly run insert, update, or delete operations against it.\n|                                          | **Stream** | **Table**   |\n|-------------------------------------------|------------|-------------|\n| First event with key bob arrives          | Insert     | Insert      |\n| Another event with key bob arrives        | Insert     | Update      |\n| Event with key bob and value null arrives | Insert     | Delete      |\n| Event with key null arrives               | Insert     | ignored |\n\n**Stream-table duality**\n\nNot withstanding their differences, we can observe that there is a close relationship between a stream and a table. We call this the[stream-table duality](https://www.confluent.io/blog/streams-tables-two-sides-same-coin/). What this means is:\n-   We can turn a stream into a tableby aggregating the stream with operations such asCOUNT()orSUM(), for example. In our chess analogy, we could reconstruct the board's latest state (table) by replaying all recorded moves (stream).\n-   We can turn a table into a streamby capturing the changes made to the table---inserts, updates, and deletes---into a \"change stream.\" This process is often called[change data capture](https://en.wikipedia.org/wiki/Change_data_capture)or CDC for short. In the chess analogy, we could achieve this by observing the last played move and recording it (into the stream) or, alternatively, by comparing the board's state (table) before and after the last move and then recording the difference of what changed (into the stream), though this is likely slower than the first option.\n\nIn fact, a table is fully defined by its underlying change stream. If you have ever worked with a relational database such as Oracle or MySQL, these change streams exist there, too! Here, however, they are a hidden implementation detail---albeit an absolutely critical one---and have names like[redo log](https://docs.oracle.com/cd/B28359_01/server.111/b28310/onlineredo001.htm#ADMIN11302)or[binary log](https://dev.mysql.com/doc/internals/en/binary-log-overview.html). In event streaming, the redo log is much more than an implementation detail. It's a first-class entity: a stream. We can turn streams into tables and tables into streams, which is one reason why we say that event streaming and Kafka are[turning the database inside out](https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/).\n\n![Figure 2. Because of the stream-table duality, we can easily turn a stream into a table, and vice versa. Even more, we can do this in a continuous, streaming manner so that both the stream and the table are always up to date with the latest events.](media/Event-driven-architecture-image3.png)\n<https://www.confluent.io/blog/kafka-streams-tables-part-1-event-streaming>\n\n![Events ItemAdd Bread Basket Bread Tinned Spaghetti emAdd Baked Beans ItemRemove Baked Beans ItemAdd Tinned Spaghetti ](media/Event-driven-architecture-image4.png)\n\n![Basket Bread Tinned Spaghetti ti ](media/Event-driven-architecture-image5.png)\n**Asynchronous Messaging**\n\nConsider these patterns when implementing asynchronous messaging:\n-   [Competing Consumers Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/competing-consumers)\n\nMultiple consumers may need to compete to read messages from a queue. This pattern explains how to process multiple messages concurrently to optimize throughput, to improve scalability and availability, and to balance the workload.\n-   [Priority Queue Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/priority-queue)\n\nFor cases where the business logic requires that some messages are processed before others, this pattern describes how messages posted by a producer that have a higher priority can be received and processed more quickly by a consumer than messages of a lower priority.\n-   [Queue-based Load Leveling Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/queue-based-load-leveling)\n\nThis pattern uses a message broker to act as a buffer between a producer and a consumer to help to minimize the impact on availability and responsiveness of intermittent heavy loads for both those entities.\n-   [Retry Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/retry)\n\nA producer or consumer might be unable connect to a queue, but the reasons for this failure may be temporary and quickly pass. This pattern describes how to handle this situation to add resiliency to an application.\n-   [Scheduler Agent Supervisor Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/scheduler-agent-supervisor)\n\nMessaging is often used as part of a workflow implementation. This pattern demonstrates how messaging can coordinate a set of actions across a distributed set of services and other remote resources, and enable a system to recover and retry actions that fail.\n-   [Choreography pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/choreography)\n\nThis pattern shows how services can use messaging to control the workflow of a business transaction.\n-   [Claim-Check Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/claim-check)\n\nThis pattern shows how to split a large message into a claim check and a payload.\n<https://docs.microsoft.com/en-us/azure/architecture/guide/technology-choices/messaging>\n[Kafka in the Wild â€¢ Laura Schornack & Maureen Penzenik â€¢ GOTO 2021](https://www.youtube.com/watch?v=iMx8otu3rFg&ab_channel=GOTOConferences)\n**Choreography vs Orchestration**\n\n![](media/Event-driven-architecture-image6.jpeg)\n**Orchestration -** command-driven architecture\n\n**Choreography -** event-driven communication\n![Messaoe EvenÅ‚ Record (ommand I InÅ‚end/ EvenÅ‚ FacÅ‚l happend in Å‚he pasÅ‚/ immuÅ‚ab(e WanÅ‚ S.Å‚h. Å‚o happeni The inÅ‚enÅ‚ion iÅ‚se(Å‚ is a Å‚acÅ‚ ](media/Event-driven-architecture-image7.jpeg)\n![Direction of dependency Retrieve paymen+ Event-driven: Decision to couple is on the receivinq Side payment received Command-driven Decision to couple is on the Sendinq Side Direction of dependency ](media/Event-driven-architecture-image8.jpeg)\n<https://www.youtube.com/watch?v=zt9DFMkjkEA&ab_channel=GOTOConferences>\n1.  Microservices - also known as the microservice architecture - is an architectural style that structures an application as a collection of services that are: 1) Highly maintainable and testable 2) Loosely coupled 3) Independently deployable.\n\n2.  But, in microservices architecture too, there are different ways to implement it. At a high level, there are two approaches to getting microservices to work together toward a common goal: orchestration and choreography.\n\n3.  Orchestration entails actively controlling all elements and interactions like a conductor directs the musicians of an orchestra, while choreography entails establishing a pattern or routine that microservices follow as the music plays, without requiring supervision and instructions.\n\n4.  So, in orchestration there is one coordinator which coordinates different mini-services. The coordinates which service to call first, and then which service to call next on the basis of the output of the first service. These are mostly workflow-based systems.\n\n5.  In Choreography, there is an event broker and no coordinator. Each service that is subscribed to this event broker, picks up the message from the queue and performs execution. After completion of its task, it publishes the message on the channel and another service performs its tasks asynchronously. This system can be thought of as consisting of a queue like Kafka, SQS, and various services/lambdas performing computations.\n\n6.  Both of these methods have their own disadvantages and advantages. In some cases, orchestration is better whereas in others choreography is better.\n\n7.  Major advantages of choreography over orchestration:1) Orchestration suffers from tight coupling, whereas choreography offers loose coupling. 2) Orchestration depends on RESTful APIs and hence requires more maintenance due to failures whereas choreography requires less maintenance and is fault-tolerant because it is mostly stateless and messages can be retried.\n<https://www.youtube.com/watch?v=ePHpAPacOdI>\n\n<https://segment.com/blog/exactly-once-delivery>\n\n[GOTO 2017 â€¢ The Many Meanings of Event-Driven Architecture â€¢ Martin Fowler](https://www.youtube.com/watch?v=STKCRSUsyP0)\nLMAX Architecture\n-   <https://martinfowler.com/articles/lmax.html>\n-   <https://lmax-exchange.github.io/disruptor>\n\n<https://cloudplatform.googleblog.com/2018/04/Cloud-native-architecture-with-serverless-microservices-the-Smart-Parking-story.html>\n\n[Design Patterns: Why Event Sourcing?](https://www.youtube.com/watch?v=rUDN40rdly8)\n**Task Queues**\n\nTask queues manage background work that must be executed outside the usual HTTP request-response cycle.\n**Why are task queues necessary?**\n\nTasks are handled asynchronously either because they are not initiated by an HTTP request or because they are long-running jobs that would dramatically reduce the performance of an HTTP response.\nFor example, a web application could poll the GitHub API every 10 minutes to collect the names of the top 100 starred repositories. A task queue would handle invoking code to call the GitHub API, process the results and store them in a persistent database for later use.\nAnother example is when a database query would take too long during the HTTP request-response cycle. The query could be performed in the background on a fixed interval with the results stored in the database. When an HTTP request comes in that needs those results a query would simply fetch the precalculated result instead of re-executing the longer query. This precalculation scenario is a form of[caching](https://www.fullstackpython.com/caching.html)enabled by task queues.\nOther types of jobs for task queues include\n-   spreading out large numbers of independent database inserts over time instead of inserting everything at once\n-   aggregating collected data values on a fixed interval, such as every 15 minutes\n-   scheduling periodic jobs such as batch processes\n<https://www.fullstackpython.com/task-queues.html>\n\n## Message Oriented Architecture (MOM)**\n\nMessage oriented middleware (MOM) refers to the software infrastructure supporting sending and receiving messages between distributed systems. AMQP and MQTT are the two most relevant protocols in this context. They are extensively used for exchanging messages since they provide an abstraction of the different participating system entities, alleviating their coordination and simplifying the communication programming details.\nThe basic idea of MOM is that communication takes place by adding messages to distributed queues, and by getting messages from those queues. Based on the model of Message Oriented Middleware, many protocols have been developed, e.g. DDS, STOMP, XMPP. The two most widely used proposals are: the Advanced Message Queuing Protocol (AMQP) and the Message Queuing Telemetry Transport (MQTT).\nSee also:\n-   AMQP\n-   MQTT\n-   ZeroMQ: Distributed Messaging\n**RabbitMQ: The Polyglot Broker (Distributed Message Broker)**\n\nAll three protocols are supported by RabbitMQ broker, making it an ideal choice for interoperability between applications.\n<https://www.youtube.com/watch?v=Cie5v59mrTg&ab_channel=HusseinNasser>\n\n<https://www.youtube.com/watch?v=FzqjtU2x6YA&ab_channel=ThatDevOpsGuy>\n\n<https://www.youtube.com/watch?v=nxQrpLfX3rs&ab_channel=GOTOConferences>\n\n<https://www.youtube.com/watch?v=fNbdgWe5Tbs&ab_channel=GOTOConferences>\n\n## pika**\n\nPika is a RabbitMQ (AMQP 0-9-1) client library for Python.\n<https://pypi.org/project/pika>\n\n<https://pika.readthedocs.io/en/stable/intro.html>"},{"fields":{"slug":"/Computer-Science/System-Design/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-04-05 00:59:17 +0500\n\nModified: 2022-12-09 23:46:56 +0500\n\n---\n\n**Systems design**is the process of defining the[architecture](https://en.wikipedia.org/wiki/Systems_architecture), modules, interfaces, and[data](https://en.wikipedia.org/wiki/Data)for a[system](https://en.wikipedia.org/wiki/System)to satisfy specified[requirements](https://en.wikipedia.org/wiki/Requirement). Systems design could be seen as the application of[systems theory](https://en.wikipedia.org/wiki/Systems_theory)to[product development](https://en.wikipedia.org/wiki/Product_development). There is some overlap with the disciplines of[systems analysis](https://en.wikipedia.org/wiki/Systems_analysis),[systems architecture](https://en.wikipedia.org/wiki/Systems_architecture)and[systems engineering](https://en.wikipedia.org/wiki/Systems_engineering).\nInversion Of Control\n\nCallbacks\n**Hollywood principle - Don't call me, we will call you**\n**Android - Used Interface for callback onSuccess of async task**\n**Build vs Buy vs Get for Free**\n**Planet Scale Systems**\n**Answering system design interview questions**\n\n1.  **Get domain knowledge of what are we building**\n\n2.  **Core features**\n    -   **Only design 2-3 core features for a system design question**\n\n3.  **For what scale we are building**\n**Introduction**\n\n1.  Features\n\n2.  Define APIs\n\n3.  Availability (CAP Theorem)\n\n    a.  Consistency\n\n    b.  Availability, and\n\n    c.  Partition Tolerance\n\n4.  Latency Performance (If customer facing application, then latency matters)\n\n5.  Scalability (Add more users and requests)\n\n6.  Durability (Data is not lost or compromised)\n\n7.  Class Diagrams\n\n8.  Security & Privacy\n\n9.  Cost Effective\n**How to approach a system design problem**\n-   Gather requirements (Functional and non-functional requirements)\n    -   Functional\n        -   sendMessage(messageBody)\n        -   receiveMessage()\n    -   Non-functional\n        -   Scalable (handles load increases, more queues and messages)\n        -   Highly available (services hardware/network failures)\n        -   Highly performent (single digit latency for main operations)\n        -   Durable (once submitted, data is not lost)\n    -   What is the expected read-to-write ratio?\n    -   How many **concurrent requests** should we expect?\n    -   What's the average expected response time?\n    -   What's the limit of the data we allow users to provide?\n    -   How many customers\n    -   RPS / Traffic\n    -   Who are the users\n-   Establish the Scope\n    -   Do we want to discuss the end-to-end experience or just the API?\n    -   What clients do we want to support (mobile, web, etc)?\n    -   Do we require authentication? Analytics? Integrating with existing systems?\n-   High level design\n    -   Cloud - CDN / LoadBalancers / Servers / Queues\n    -   Database layer\n    -   Scalability\n-   Low level design\n    -   DS + Algo\n    -   API\n    -   Data model\n    -   Database schema\n    -   Optimizations\n    -   Edge cases\n        -   Viral Videos\n        -   High number of followers\n-   Others\n    -   Security\n    -   Reliability\n    -   Replication\n    -   Tradeoffs\n<https://www.freecodecamp.org/news/systems-design-for-interviews>\n\n<https://hackernoon.com/anatomy-of-a-system-design-interview-4cb57d75a53f>\n\n<https://towardsdatascience.com/the-complete-guide-to-the-system-design-interview-ba118f48bdfc>\n\n<https://www.youtube.com/watch?v=UzLMhqg3_Wc>\n\n1.  Scaling\n\n    a.  Vertical Scaling / Scale up (Add more CPU, RAM, Storage to an existing host)\n\n    b.  Horizontal Scaling / Scale out (Keep one host small and add another host)\nVertical Scaling is expensive and have a limit whereas Horizontal Scaling is cheap. But we have to take care of distributed systems problem in Horizontal Scaling and not in Vertical Scaling.\n2.  Data Centers / Rack / Hosts\n\nConsider latency between cross racks, cross hosts\n3.  CPU/ Memory/ HardDrive / Nework Bandwidth\n\nAll of the above are limited resources. How to improve limitations while scaling\n4.  Random vs Sequential read/writes on disk\n\nAlways sequential read is far faster than random read/write on a disk.\n5.  CDNs and Edge\n\nCDN - Content Delivery Network\n\nEdge - You do processing close to the end user. It has dedicated network from the edge to all the way to the data center so your request could be routed over this dedicated network instead of going over the general internet.\n6.  Virtual machines and containers\n\nVirtual Machines - are a way of giving you an OS on top of a shared resource such that you feel like you are the exclusive owner of that hardware while in reality that hardware is shared between different isolated operating system.\nContainers - is a way of running your applications and its dependencies in an isolated environment.\n7.  Publisher - Subscriber or Queue\n    -   Queuing is a *point-to-point communication model* - a pool of consumers may read from a server and each message is delivered to one of them - it allows you to divide up the processing of data over multiple consumer instances and scale your processing.\n    -   Publish-subscribe is a *broadcast communication model* - a message is broadcast to all the consumers.\n8.  Thin clients, Thick clients, Dumb clients, Smart client\n\nKafka clients tend to be \"thick\" and have a lot of complexity. That is, they do a lot because the broker is designed to be simple. That's my guess as to why there are so few native client libraries up to par with the Java client. NATS Streaming clients, on the other hand, are relatively \"thin\" because the server does more. We end up just pushing the complexity around based on our design decisions, but one can argue that the[smart client and dumb server](https://bravenewgeek.com/smart-endpoints-dumb-pipes/)is a more scalable approach.\n[System Design](https://www.youtube.com/playlist?list=PLkQkbY7JNJuBoTemzQfjym0sqbOHt5fnV)\n**ADR - Architecture Design Record**\n\nArchitects are often writing the ADRs ([Architecture Decision Record](https://github.com/joelparkerhenderson/architecture_decision_record#suggestions-for-writing-good-adrs)), those are documents that are helping anyone in the company to understand why a decision was made describing the context, the options available, the chosen one and finally the consequences generated by this decision.-   An**architecture decision record(ADR)** is a document that captures an important architectural decision made along with its context and consequences.\n-   An**architecture decision(AD)** is a software design choice that addresses a significant requirement.\n-   An**architecture decision log(ADL)** is the collection of all ADRs created and maintained for a particular project (or organization).\n-   An**architecturally-significant requirement(ASR)** is a requirement that has a measurable effect on a software system's architecture.\n-   All these are within the topic of**architecture knowledge management(AKM).**\n<https://github.com/joelparkerhenderson/architecture-decision-record>\n\n<https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions>\n\n## Scale Cube**\n\nThescale cubeis a technology model that indicates three methods (or approaches) by which technology platforms may be scaled to meet increasing levels of demand upon the system in question. The three approaches defined by the model include scaling through replication or cloning (the \"X axis\"), scaling through segmentation along service boundaries or dissimilar components (the \"Y axis\") and segmentation or partitioning along similar components (the \"Z axis\").\n**3 Dimensions to Scaling**\n\n![THE ART Y axis - functional decomposition Scale by splitting different things X axis - horizontal duplication Scale by cloning ](media/Intro-image1.jpeg)\n<https://en.wikipedia.org/wiki/Scale_cube>\n\n<https://microservices.io/articles/scalecube.html>\n![](media/Intro-image2.jpeg)"},{"fields":{"slug":"/Computer-Science/System-Design/Lambda-Architecture/","title":"Lambda Architecture"},"frontmatter":{"draft":false},"rawBody":"# Lambda Architecture\n\nCreated: 2019-04-20 17:44:07 +0500\n\nModified: 2019-11-22 13:26:35 +0500\n\n---\n\nLambda architectureis a[data-processing](https://en.wikipedia.org/wiki/Data_processing)architecture designed to handle massive quantities of data by taking advantage of both[batch](https://en.wikipedia.org/wiki/Batch_processing)and[stream-processing](https://en.wikipedia.org/wiki/Stream_processing)methods. This approach to architecture attempts to balance[latency](https://en.wikipedia.org/wiki/Latency_(engineering)),[throughput](https://en.wikipedia.org/wiki/Throughput), and[fault-tolerance](https://en.wikipedia.org/wiki/Fault-tolerance)by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before presentation. The rise of lambda architecture is correlated with the growth of[big data](https://en.wikipedia.org/wiki/Big_data), real-time analytics, and the drive to mitigate the latencies of[map-reduce](https://en.wikipedia.org/wiki/Map-reduce).\nLambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record.It is intended for ingesting and processing timestamped events that are appended to existing events rather than overwriting them. State is determined from the natural time-based ordering of the data.\n![Lambda Architecture](media/Lambda-Architecture-image1.png)\n\n![lambda](media/Lambda-Architecture-image2.png)\n\n**Overview**\n\nLambda architecture describes a system consisting of three layers: batch processing, speed (or real-time) processing, and a serving layer for responding to queries.The processing layers ingest from an immutable master copy of the entire data set.\n**Batch layer**\n\nThe batch layer precomputes results using a distributed processing system that can handle very large quantities of data. The batch layer aims at perfect accuracy by being able to processallavailable data when generating views. This means it can fix any errors by recomputing based on the complete data set, then updating existing views. Output is typically stored in a read-only database, with updates completely replacing existing precomputed views.\n[Apache Hadoop](https://en.wikipedia.org/wiki/Hadoop)is the de facto standard batch-processing system used in most high-throughput architectures.\n**Speed layer**\n\nThe speed layer processes data streams in real time and without the requirements of fix-ups or completeness. This layer sacrifices throughput as it aims to minimize latency by providing real-time views into the most recent data. Essentially, the speed layer is responsible for filling the \"gap\" caused by the batch layer's lag in providing views based on the most recent data. This layer's views may not be as accurate or complete as the ones eventually produced by the batch layer, but they are available almost immediately after data is received, and can be replaced when the batch layer's views for the same data become available.\nStream-processing technologies typically used in this layer include[Apache Storm](https://en.wikipedia.org/wiki/Storm_(event_processor)),[SQLstream](https://en.wikipedia.org/wiki/Sqlstream)and[Apache Spark](https://en.wikipedia.org/wiki/Apache_Spark). Output is typically stored on fast NoSQL databases.\n**Serving layer**\n\nOutput from the batch and speed layers are stored in the serving layer, which responds to ad-hoc queries by returning precomputed views or building views from the processed data.\nExamples of technologies used in the serving layer include[Druid](https://en.wikipedia.org/wiki/Druid_(open-source_data_store)), which provides a single cluster to handle output from both layers.Dedicated stores used in the serving layer include[Apache Cassandra](https://en.wikipedia.org/wiki/Apache_Cassandra),[Apache HBase](https://en.wikipedia.org/wiki/Apache_HBase),[MongoDB](https://en.wikipedia.org/wiki/MongoDB),[VoltDB](https://en.wikipedia.org/wiki/VoltDB)or[Elasticsearch](https://en.wikipedia.org/wiki/Elasticsearch)for speed-layer output, and[Elephant DB](https://github.com/nathanmarz/elephantdb),[Apache Impala](https://en.wikipedia.org/wiki/Apache_Impala),[SAP HANA](https://en.wikipedia.org/wiki/SAP_HANA)or[Apache Hive](https://en.wikipedia.org/wiki/Apache_Hive)for batch-layer output.\n<https://en.wikipedia.org/wiki/Lambda_architecture>\n\n## Common Lambda Architectures: Kafka, Spark, and MongoDB/Elasticsearch**\n\nIf you are a data practitioner, you would probably have either implemented or used a data processing platform that incorporates the Lambda architecture. A common implementation would have large batch jobs in Hadoop complemented by an update stream stored in Apache Kafka. Apache Spark is often used to read this data stream from Kafka, perform transformations, and then write the result to another Kafka log. In most cases, this would not be a single Spark job but a pipeline of Spark jobs. Each Spark job in the pipeline would read data produced by the previous job, do its own transformations, and feed it to the next job in the pipeline. The final output would be written to a serving system like Apache Cassandra, Elasticsearch or MongoDB.\n**Shortcomings of Lambda Architecture**\n\n1.  Maintaining two different processing paths, one via the batch system and another via the real-time streaming system, is inherently difficult. If you ship new code functionality to the streaming software but fail to make the necessary equivalent change to the batch software, you could get erroneous results.\n\n2.  If you are an application developer or data scientist who wants to make changes to your streaming or batch pipeline, you have to either learn how to operate and modify the pipeline, or you have to wait for someone else to make the changes on your behalf. The former option requires you to pick up data engineering tasks and detracts from your primary role, while the latter forces you into a holding pattern waiting on the pipeline team for resolution.\n\n3.  Most of the data transformation happens as new data enters the system at write time, whereas the serving layer is a simpler key-value lookup that does not handle complex transformations. This complicates the job of the application developer because she/he cannot easily apply new transformations retroactively on pre-existing data.\n<https://rockset.com/blog/aggregator-leaf-tailer-an-architecture-for-live-analytics-on-event-streams>\n\n## Others - ALT (Aggregator Leaf Tailer)**"},{"fields":{"slug":"/Computer-Science/System-Design/Microservice-Architecture/","title":"Microservice Architecture"},"frontmatter":{"draft":false},"rawBody":"# Microservice Architecture\n\nCreated: 2017-11-02 23:06:25 +0500\n\nModified: 2022-03-02 20:22:31 +0500\n\n---\n\n**What is microservices architecture?**\n\nA microservicesâ€‘based application is a distributed system running on multiple machines. Each service in the system communicates by passing messages to the others.\nMicroservices ecosystem is a platform of services each encapsulating a business capability. A business capability represents what a business does in a particular domain to fulfill its objectives and responsibilities. Each microservice expose an API that developers can discover and use in a self-serve manner. Microservices have independent lifecycle. Developers can build, test and release each microservice independently. The microservices ecosystem enforces an organizational structure of autonomous long standing teams, each responsible for one or multiple services.\nIf your application has a more complex domain, consider moving to a [Microservices](https://docs.microsoft.com/en-us/azure/architecture/guide/architecture-styles/microservices) architecture. A microservices application is composed of many small, independent services. Each service implements a single business capability. Services are loosely coupled, communicating through API contracts.\nEach service can be built by a small, focused development team. Individual services can be deployed without a lot of coordination between teams, which encourages frequent updates. A microservice architecture is more complex to build and manage than either N-tier or web-queue-worker. It requires a mature development and DevOps culture. But done right, this style can lead to higher release velocity, faster innovation, and a more resilient architecture.\n**Martin Fowler**\n\n\"In short, the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery.\"\n**Top 3 reasons for using microservices**\n-   Zero-downtime independent deployability\n-   Isolation of data and of processing around that data\n-   Use microservices to reflect the organizational structure\n**You should strive for independent deployment because**\n-   It's easier to limit the impact of each release when using microservices (reducing blast radius)\n-   As the team size increases it gets exponentially harder to coordinate a deployment\n**Why microservices**\n-   Tackles Complexity\n\nIt tackles the problem of complexity. It decomposes what would otherwise be a monstrous monolithic application into a set of services. While the total amount of functionality is unchanged, the application has been broken up into manageable chunks or services. Each service has a wellâ€‘defined boundary in the form of an RPC or messageâ€‘driven API. The Microservices Architecture pattern enforces a level of modularity that in practice is extremely difficult to achieve with a monolithic code base. Consequently, individual services are much faster to develop, and much easier to understand and maintain.\n-   Develop and release independently\n    -   This architecture enables each service to be developed independently by a team that is focused on that service. The developers are free to choose whatever technologies make sense, provided that the service honors the API contract.\n    -   The Microservices Architecture pattern enables each microservice to be deployed independently. Developers never need to coordinate the deployment of changes that are local to their service. These kinds of changes can be deployed as soon as they have been tested. The UI team can, for example, perform A/B testing and rapidly iterate on UI changes. The Microservices Architecture pattern makes continuous deployment possible.\n-   Enables scaling of a specific component of an application rather than scaling the entire application when there is a demand for just a specific service.\n**Characteristics of Microservices**\n-   Componentization via services\n-   Organized around business capabilities\n-   Products not projects\n-   Smart endpoints and dumb pipes\n-   Decentralized Governance\n-   Decentralized Data Management\n-   Infrastructure Automation\n-   Design for failure\n-   Evolutionary Design\n**10 Commandments of Microservices Architecture**\n-   Clean separation of stateless and stateful services\n-   Do not share libraries or SDKs\n-   Avoid host affinity\n-   Focus on services with one task in mind\n-   Use lightweight messaging protocol for communication\n-   Design a well-defined entry point and exit point\n-   Implement a self-registration and discovery mechanism\n-   Explicitly check for rules and constraints\n-   Prefer polyglot over single stack\n-   Maintain independent revisions and build environments\n**Things to keep in mind**\n-   Synchronous communication\n-   Asynchronous communication\n-   Transactionality\n-   High availability/Resiliency\n-   Fault tolerant\n-   Reliability\n-   Active-active (where all services is deployed in two or more regions and both keep in-sync)\n-   Active-passive (where services are on standby in another region, and if one region goes down, other will take over but this can take some time, since database sync can take some time)\n**What is monolithic architecture in software?**\n\nIn a monolithic application, all components reside within the same process and communication is usually based on method or function calls within the same process.\n**Important Points**\n-   In cases where a new service ends up with a call back to the monolith, I suggest to expose a new API from the monolith, and access the API through an[anti-corruption](https://martinfowler.com/articles/refactoring-external-service.html#SeparatingTheYoutubeDataStructureIntoAGateway)layer in the new service to make sure that the monolith concepts do not leak out.\n-   Finding domain boundaries in a monolith is very important.\n**Migrating Steps -**\n-   Warm up with a Simple and Fairly Decoupled Capability\n-   Minimize Dependency Back to the Monolith\n-   Split Sticky Capabilities Early\n-   Decouple Vertically and Release the Data Early\n-   Decouple what is important to the Business and Changes Frequently\n-   Decouple Capability and not Code\n-   Go Marco First, then Micro\n-   Migrate in Atomic Evolutionary Steps\n\nThe atomic unit of monolith decomposition includes:\n-   decouple the new service\n-   Redirect all consumers to new service\n-   Retire the old code path in the monolith.\n\nThe anti-pattern: Decouple the new service, use for new consumers and never retire the old.\nNote- Don't move from monolith architecture to microservices in one step.\n-   First move to a service-oriented architecture without using some fancy bus for message passing, use some simple standard for messaging like HTTP.\n-   Then if you really need to, move to microservices\n\n# \n\n# Migrating to Microservice Databases\n\nFrom Relational Monolith to Distributed Data\n-   By Edson Yanaga\n**Strangler Pattern**\n\nThe monolith-first pattern is called the strangler pattern because it resembles the development of a tree called the strangler fig.\n\nAll the enterprise applications are first made monolith because it's hard to determine boundaries, define a wrong boundary while creating an enterprise application and you are doomed.\n\nBy building up our new service at the boundary of our old service we are able to incrementally cut off our old system, or strangle it.\nBackfill refactoring\n**Microservices Characteristics**\n-   Componentization via services\n-   Organized around business capabilities\n-   Products not projects\n-   Smart endpoints and dumb pipes\n-   Decentralized governance\n-   Decentralized data management\n-   Infrastructure automation\n-   Design for failure\n-   Evolutionary design\n**Integration Strategies**\n-   Shared Tables\n-   Database View\n-   Database Materialized View\n-   Database Trigger\n-   Transactional Code\n-   ETL Tools\n-   Data Virtualization\n-   Event Sourcing\n-   Change Data Capture\nThe biggest issue in changing a monolith to a microservice lies in changing the communication pattern\n\n---Martin Fowler\n**References -**\n\n<https://martinfowler.com/articles/break-monolith-into-microservices.html>\n\n<https://martinfowler.com/articles/data-monolith-to-mesh.html>\nMigrating to Microservices Databases, Chapter 5, Integration Strategies\n\n[https://www.toptal.com/devops/scaling-microservices-applications](https://www.toptal.com/devops/scaling-microservices-applications?)\n**Service Oriented Architecture**\n\nService-oriented architecture(SOA) is a style of[software design](https://en.wikipedia.org/wiki/Software_design)where services are provided to the other components by[application components](https://en.wikipedia.org/wiki/Application_components), through a[communication protocol](https://en.wikipedia.org/wiki/Communications_protocol)over a network. A SOA service is a discrete unit of functionality that can be accessed remotely and acted upon and updated independently, such as retrieving a credit card statement online. SOA is also intended to be independent of vendors, products and technologies.\nA service has four properties according to one of many definitions of SOA:\n-   It logically represents a business activity with a specified outcome.\n-   It is self-contained.\n-   It is a[black box](https://en.wikipedia.org/wiki/Black_box)for its consumers, meaning the consumer does not have to be aware of the service's inner workings.\n-   It may consist of other underlying services.\n**Principles**\n\n[**Standardized service contract**](https://en.wikipedia.org/wiki/Standardized_service_contract)\n\nServices adhere to a standard communications agreement, as defined collectively by one or more service-description documents within a given set of services.\n**Service reference autonomy (an aspect of loose coupling)**\n\nThe relationship between services is minimized to the level that they are only aware of their existence.\n**Service location transparency (an aspect of loose coupling)**\n\nServices can be called from anywhere within the network that it is located no matter where it is present.\n**Service longevity**\n\nServices should be designed to be long lived. Where possible services should avoid forcing consumers to change if they do not require new features, if you call a service today you should be able to call the same service tomorrow.\n[**Service abstraction**](https://en.wikipedia.org/wiki/Service_abstraction)\n\nThe services act as black boxes, that is their inner logic is hidden from the consumers.\n[**Service autonomy**](https://en.wikipedia.org/wiki/Service_autonomy_principle)\n\nServices are independent and control the functionality they encapsulate, from a Design-time and a run-time perspective.\n[**Service statelessness**](https://en.wikipedia.org/wiki/Service_statelessness_principle)\n\nServices are stateless, that is either return the requested value or give an exception hence minimizing resource use.\n[**Service granularity**](https://en.wikipedia.org/wiki/Service_granularity_principle)\n\nA principle to ensure services have an adequate size and scope. The functionality provided by the service to the user must be relevant.\n**Service normalization**\n\nServices are decomposed or consolidated (normalized) to minimize redundancy. In some, this may not be done. These are the cases where performance optimization, access, and aggregation are required.\n[**Service composability**](https://en.wikipedia.org/wiki/Service_composability_principle)\n\nServices can be used to compose other services.\n[**Service discovery**](https://en.wikipedia.org/wiki/Service_discovery)\n\nServices are supplemented with communicative meta data by which they can be effectively discovered and interpreted.\n[**Service reusability**](https://en.wikipedia.org/wiki/Service_reusability_principle)\n\nLogic is divided into various services, to promote reuse of code.\n**Service[encapsulation](https://en.wikipedia.org/wiki/Encapsulation_(computer_science))**\n\nMany services which were not initially planned under SOA, may get encapsulated or become a part of SOA.\n<https://en.wikipedia.org/wiki/Service-oriented_architecture>\n\n<https://www.toptal.com/aws/service-oriented-architecture-aws-lambda>\n<https://www.youtube.com/watch?v=j6ow-UemzBc&ab_channel=InfoQ>\n\nGreat Architecture\n-   Scales development teams\n-   Delivers quality\n-   Enables high performance / low cost\n-   Supports future features naturally\nTradeoffs\n-   Near term velocity\n-   Future paralysis\nMisconceptions\n-   Microservices enable our teams to choose the best programming languages and frameworks for their tasks\n-   Code generation is evil\n-   The event log must be the source of truth\n-   Developers can maintain no more than 3 services each\nCritical decisions\n-   Design schema first for all APIs and Events\n    -   Consume events (not API) by default\n-   Invest in automation\n    -   Deployment, code generation, dependency management\n-   Enable teams to write amazing and simple tests\n    -   Drive quality, streamlines maintenance, enables continuous delivery\n[Scale, Flow and Microservices â€¢ James Lewis â€¢ GOTO 2021](https://www.youtube.com/watch?v=LL4SJsBtYw0&ab_channel=GOTOConferences)\n\n[Microservices Architecture Patterns](https://www.youtube.com/playlist?list=PLTyWtrsGknYd0JgqeARypdRy-SX1ORYhs)\n"},{"fields":{"slug":"/Computer-Science/System-Design/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-01-21 10:02:10 +0500\n\nModified: 2021-08-06 00:51:32 +0500\n\n---\n\n**Grid FTP**\n\nGridFTPis an extension of the[File Transfer Protocol (FTP)](https://en.wikipedia.org/wiki/File_Transfer_Protocol)for[grid computing](https://en.wikipedia.org/wiki/Grid_computing).The protocol was defined within the GridFTP working group of the[Open Grid Forum](https://en.wikipedia.org/wiki/Open_Grid_Forum). There are multiple implementations of the protocol; the most widely used is that provided by the[Globus Toolkit](https://en.wikipedia.org/wiki/Globus_Toolkit).\nThe aim of GridFTP is to provide a more reliable and high performance file transfer, for example to enable the transmission of very large files. GridFTP is used extensively within large science projects such as the[Large Hadron Collider](https://en.wikipedia.org/wiki/LHC)and by many supercomputer centers and other scientific facilities.\nGridFTP also addresses the problem of incompatibility between storage and access systems. Previously, each data provider would make their data available in their own specific way, providing a library of access functions. This made it difficult to obtain data from multiple sources, requiring a different access method for each, and thus dividing the total available data into partitions. GridFTP provides a uniform way of accessing the data, encompassing functions from all the different modes of access, building on and extending the universally accepted FTP standard. FTP was chosen as a basis for it because of its widespread use, and because it has a well defined architecture for extensions to the protocol (which may be dynamically discovered).\nGlobus Toolkit - GSIFTP\n\n<https://en.wikipedia.org/wiki/GridFTP>\n\n## Real-Time Analytics**\n\n1.  Ingestion\n\n2.  Store\n\n3.  Visualize\n\n4.  Act\n\n5.  Predict\n**ALT (Aggregator Leaf Tailer) - Real-Time Analytics Without Pipelines**\n\nAggregator Leaf Tailer (ALT)is the data architecture favored by web-scale companies, like Facebook, LinkedIn, and Google, for its efficiency and scalability.\nThe ALT architecture addresses these shortcomings of Lambda architectures. The key component of ALT is a high-performance serving layer that serves complex queries, and not just key-value lookups. The existence of this serving layer obviates the need for complex data pipelines.\n\n![ALT](media/Others-image1.png)\n\n**The ALT architecture described**\n\n1.  TheTailerpulls new incoming data from a static or streaming source into an indexing engine. Its job is to fetch from all data sources, be it a data lake, like S3, or a dynamic source, like Kafka or Kinesis.\n\n2.  TheLeafis a powerful indexing engine. It indexes all data as and when it arrives via the Tailer. The indexing component builds multiple types of indexes---inverted, columnar, document, geo, and many others---on the fields of a data set. The goal of indexing is to make any query on any data field fast.\n\n3.  The scalableAggregatortier is designed to deliver low-latency aggregations, be it columnar aggregations, joins, relevance sorting, or grouping. The Aggregators leverage indexing so efficiently that complex logic typically executed by pipeline software in other architectures can be executed on the fly as part of the query.\n**Advantages of ALT**\n\nThe ALT architecture enables the app developer or data scientist to run low-latency queries on raw data sets without any prior transformation. A large portion of the data transformation process can occur as part of the query itself. How is this possible in the ALT architecture?\n\ni.  Indexing is critical to making queries fast.The Leaves maintain a variety of indexes concurrently, so that data can be quickly accessed regardless of the type of query---aggregation, key-value, time series, or search. Every document and field is indexed, including both value and type of each field, resulting in fast query performance that allows significantly more complex data processing to be inserted into queries.\n\nii. Queries are distributed across a scalable Aggregator tier.The ability to scale the number of Aggregators, which provide compute and memory resources, allows compute power to be concentrated on any complex processing executed on the fly.\n\niii. The Tailer, Leaf, and Aggregator run as discrete microservices in disaggregated fashion.Each Tailer, Leaf, or Aggregator tier can be independently scaled up and down as needed. The system scales Tailers when there is more data to ingest, scales Leaves when data size grows, and scales Aggregators when the number or complexity of queries increases. This independent scalability allows the system to bring significant resources to bear on complex queries when needed, while making it cost-effective to do so.\nThe most significant difference is that the Lambda architecture performs data transformations up front so that results are pre-materialized, while the ALT architecture allows for query on demand with on-the-fly transformations.\n<https://rockset.com/blog/aggregator-leaf-tailer-an-architecture-for-live-analytics-on-event-streams>\n\n## Dead Letter Queues**\n\nIn[message queueing](https://en.wikipedia.org/wiki/Message_queue)thedead letter queueis a service implementation to store messages that meet one or more of the following criteria:\n\n1.  Message that is sent to a queue that does not exist.\n\n2.  Queue length limit exceeded.\n\n3.  Message length limit exceeded.\n\n4.  Message is rejected by another queue exchange.\n\n5.  Message reaches a threshold read counter number, because it is not consumed. Sometimes this is called a \"back out queue\".\nDead letter queue storing of these messages allows developers to look for common patterns and potential software problems.\nQueueing systems that incorporate dead letter queues include[Amazon Simple Queue Service](https://en.wikipedia.org/wiki/Amazon_Simple_Queue_Service),[Apache ActiveMQ](https://en.wikipedia.org/wiki/Apache_ActiveMQ),[HornetQ](https://en.wikipedia.org/wiki/HornetQ),[Microsoft Message Queuing](https://en.wikipedia.org/wiki/Microsoft_Message_Queuing),[WebSphere MQ](https://en.wikipedia.org/wiki/WebSphere_MQ)[[5]](https://en.wikipedia.org/wiki/Dead_letter_queue#cite_note-5),[Rabbit MQ](https://en.wikipedia.org/wiki/Rabbit_MQ)and[Apache Pulsar](https://en.wikipedia.org/w/index.php?title=Apache_Pulsar&action=edit&redlink=1)\n<https://en.wikipedia.org/wiki/Dead_letter_queue>\nAmazon SQS supports*dead-letter queues*, which other queues (*source queues*) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.\nDo use dead-letter queues to decrease the number of messages and to reduce the possibility of exposing your system to*poison-pill messages*(messages that can be received but can't be processed).\n<https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html>\n\n<https://aws.amazon.com/blogs/compute/designing-durable-serverless-apps-with-dlqs-for-amazon-sns-amazon-sqs-aws-lambda>\n\n## Systemantics**\n\nGeneral Systemantics(retitled toSystemanticsin its second edition andThe Systems Biblein its third) is a[systems engineering](https://en.wikipedia.org/wiki/Systems_engineering)treatise by[John Gall](https://en.wikipedia.org/wiki/John_Gall_(author))in which he offers practical principles of systems design based on experience and anecdotes.\nIt is offered from the perspective of hownotto design systems, based on system engineering failures. The primary precept of treatise is that large[complex systems](https://en.wikipedia.org/wiki/Complex_system)are extremely difficult to design correctly despite best intentions, so care must be taken to design smaller, less-complex systems and to do so with incremental functionality based on close and continual touch with user needs and measures of effectiveness.\n<https://en.wikipedia.org/wiki/Systemantics>\n<https://www.freecodecamp.org/news/have-an-idea-want-to-build-a-product-from-scratch-heres-a-checklist-of-things-you-should-go-through-in-your-backend-software-architecture>\n\n**Notification System**\n-   APNS - Apple Push Notification System\n-   FCM - Firebase Cloud Messaging\n**Updates**\n\n**Hard updates**\n\nrefer to when the user is forced to update the client version to a higher version number than what is installed on their mobile.\n\n**Soft updates**\n\nrefer to when the user is shown a prompt that a new version is available and they can update their app to the new version if they want to.\nHard updates are not encouraged, but there are times when you need to enforce them. Whatever the case you should definitely consider how you are going to implement this for your applications.\nYou can do this by implementing or configuring it in the Play Store or App Store. Another way is to create an API in your backend application that will be hit every time the mobile app is launched. This will send two keys: hard_update -> true/false and soft_update -> true/false, depending upon the user's version and the hard and soft update versions set in your backend system.\nA good place to store these versions is in your cache (Redis/Memcache), which you can change on the fly without needing to deploy your application.\n\n"},{"fields":{"slug":"/Computer-Science/System-Design/Rate-Limiting/","title":"Rate Limiting"},"frontmatter":{"draft":false},"rawBody":"# Rate Limiting\n\nCreated: 2020-01-03 00:31:54 +0500\n\nModified: 2022-04-24 11:42:17 +0500\n\n---\n\n**Endpoint Protection**\nIn[computer networks](https://en.wikipedia.org/wiki/Computer_network),rate limitingis used to control the rate of traffic sent or received by a[network interface controller](https://en.wikipedia.org/wiki/Network_interface_controller)and is used to prevent[DoS attacks](https://en.wikipedia.org/wiki/Denial-of-service_attack).\n**Hardware appliances**\n\nHardware appliances can limit the rate of requests on layer 4 or 5 of the[OSI model](https://en.wikipedia.org/wiki/OSI_model).\nRate limiting can be induced by the network protocol stack of the sender due to a received[ECN](https://en.wikipedia.org/wiki/Explicit_Congestion_Notification)-marked packet and also by the[network scheduler](https://en.wikipedia.org/wiki/Network_scheduler)of any router along the way.\nWhile a hardware appliance can limit the rate for a given range of IP-addresses on layer 4, it risks blocking a networks with many users, which are masked by[NAT](https://en.wikipedia.org/wiki/Network_address_translation)with a single IP-address of an[ISP](https://en.wikipedia.org/wiki/Internet_service_provider).\n[Deep packet inspection](https://en.wikipedia.org/wiki/Deep_packet_inspection)can be used to filter on the session layer, but will effectively disarm encryption protocols like[TLS](https://en.wikipedia.org/wiki/Transport_Layer_Security)and[SSL](https://en.wikipedia.org/wiki/Secure_Sockets_Layer)between the appliance and the web server.\n**Web servers**\n\n[Web servers](https://en.wikipedia.org/wiki/Web_server)typically use a central[in-memory](https://en.wikipedia.org/wiki/In-memory_database)[key-value database](https://en.wikipedia.org/wiki/Key-value_database), like[Redis](https://en.wikipedia.org/wiki/Redis)or[Aerospike](https://en.wikipedia.org/wiki/Aerospike_(database)), for session management. A rate limiting algorithm is used to check if the user session (or IP-address) has to be limited based on the information in the session cache.\nIn case a client made too many requests within a given timeframe,[HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol)-Servers can respond with status code[429: Too Many Requests](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes#429_Too_Many_Requests).\nHowever, the session management and rate limiting algorithm usually must be built into the application running on the web server, rather than the web server itself.\n**Datacenters**\n\nDatacenters widely use rate-limiting to control the share of resources given to different tenants and applications according to their service level agreement.A variety of rate-limiting techniques are applied in datacenters using software and hardware. Virtualized datacenters may also apply rate-limiting at the hypervisor layer. Two important performance metrics of rate-limiters in datacenters are resource footprint (memory and CPU usage) which determines scalability, and precision. There usually exists a trade-off, that is, higher precision can be achieved by dedicating more resources to the rate-limiters. A considerable body of research exists with focus on improving performance of rate-limiting in datacenters.\n<https://en.wikipedia.org/wiki/Rate_limiting>\n\n## Uses**\n-   Limit use of an API in API as a service\n-   Prevent DDOS attack\n**Why**\n-   UX\n-   Security\n-   Operational cost\n**Kinds**\n-   User based rate limiting (per api key)\n-   Concurrency (how many sessions per user)\n-   Location / IP\n-   Server\n**Algorithms for rate limiting**\n-   Token bucket\n-   Leaky bucket\n-   Fixed window counter\n-   Sliding window log\n-   Sliding window counter\n**Token Bucket**\n\nThetoken bucketis an[algorithm](https://en.wikipedia.org/wiki/Algorithm)used in[packet switched](https://en.wikipedia.org/wiki/Packet-switching)[computer networks](https://en.wikipedia.org/wiki/Computer_network)and[telecommunications networks](https://en.wikipedia.org/wiki/Telecommunication). It can be used to check that[data transmissions](https://en.wikipedia.org/wiki/Data_transmission), in the form of[packets](https://en.wikipedia.org/wiki/Network_packet), conform to defined limits on[bandwidth](https://en.wikipedia.org/wiki/Bandwidth_(computing))and[burstiness](https://en.wikipedia.org/wiki/Burst_transmission)(a measure of the unevenness or variations in the[traffic](https://en.wikipedia.org/wiki/Network_traffic_measurement)flow). It can also be used as a[scheduling algorithm](https://en.wikipedia.org/wiki/Scheduling_algorithm)to determine the timing of transmissions that will comply with the limits set for the bandwidth and burstiness.\nThe token bucket algorithm can be conceptually understood as follows\n-   A token is added to the bucket every*1/r* seconds.\n-   The bucket can hold at the most*b*tokens. If a token arrives when the bucket is full, it is discarded.\n-   When a packet (network layer[PDU](https://en.wikipedia.org/wiki/Protocol_data_unit)) of*n*bytes arrives,\n-   If at least*n*tokens are in the bucket,*n*tokens are removed from the bucket, and the packet is sent to the network.\n-   If fewer than*n*tokens are available, no tokens are removed from the bucket, and the packet is considered to benon-conformant.\nThis solution can have problem in a distributed environment where each user can come at a different server. Can cause a race condition too.\n![/ c 1<611 (Buckcf 5 /vmin Il : OJ : OS 5 U2 ](media/Rate-Limiting-image1.png)\n**Hierarchical Token Bucket**\n\nThe hierarchical token bucket (HTB) is a faster replacement for the[class-based queueing](https://en.wikipedia.org/wiki/Class-based_queueing) (CBQ) [queuing discipline](https://en.wikipedia.org/wiki/Queuing_discipline) in [Linux](https://en.wikipedia.org/wiki/Linux). It is useful to limit a client's[download](https://en.wikipedia.org/wiki/Download)/[upload](https://en.wikipedia.org/wiki/Upload)rate so that the limited client cannot saturate the total bandwidth.\nConceptually, HTB is an arbitrary number of token buckets arranged in a hierarchy. The primary egress queuing discipline (qdisc) on any device is known as the root qdisc. The root qdisc will contain one class. This single HTB class will be set with two parameters, a rate and a ceil. These values should be the same for the top-level class, and will represent the total available bandwidth on the link.\nIn HTB, rate means the guaranteed bandwidth available for a given class and ceil is short for ceiling, which indicates the maximum bandwidth that class is allowed to consume. Any bandwidth used between rate and ceil is borrowed from a parent class, hence the suggestion that rate and ceil be the same in the top-level class.\nHierarchical Token Bucket implements a classful queuing mechanism for the linux traffic control system, and provides rate and ceil to allow the user to control the absolute bandwidth to particular classes of traffic as well as indicate the ratio of distribution of bandwidth when extra bandwidth become available(up to ceil).\n<https://en.wikipedia.org/wiki/Token_bucket>\n\n## Leaky Bucket**\n\n[Leaky bucket](https://en.wikipedia.org/wiki/Leaky_bucket)(closely related to[token bucket](https://en.wikipedia.org/wiki/Token_bucket)) is an algorithm that provides a simple, intuitive approach to rate limiting via a queue which you can think of as a bucket holding the requests. When a request is registered, it is appended to the end of the queue. At a regular interval, the first item on the queue is processed. This is also known as a first in first out (FIFO) queue. If the queue is full, then additional requests are discarded (or leaked).\n![Capacity Two requests processed per minute Request 3: Dropped Request 2: Queued Request 1: Queued ](media/Rate-Limiting-image2.png)\nThe advantage of this algorithm is that it smooths out bursts of requests and processes them at an approximately average rate. It's also easy to implement on a single server or load balancer, and is memory efficient for each user given the limited queue size.\nHowever, a burst of traffic can fill up the queue with old requests and starve more recent requests from being processed. It also provides no guarantee that requests get processed in a fixed amount of time. Additionally, if you load balance servers for fault tolerance or increased throughput, you must use a policy to coordinate and enforce the limit between them.\n<https://en.wikipedia.org/wiki/Leaky_bucket>\n\n## Fixed Window**\n\nIn afixed windowalgorithm, a window size of n seconds (typically using human-friendly values, such as 60 or 3600 seconds) is used to track the rate. Each incoming request increments the counter for the window. If the counter exceeds a threshold, the request is discarded. The windows are typically defined by the floor of the current timestamp, so 12:00:03 with a 60 second window length, would be in the 12:00:00 window.\n![12:00 Request 3: Denied Request 2: Ok Request 1: Ok 12:01 ](media/Rate-Limiting-image3.png)\nThe advantage of this algorithm is that it ensures more recent requests gets processed without being starved by old requests. However, a single burst of traffic that occurs near the boundary of a window can result in twice the rate of requests being processed, because it will allow requests for both the current and next windows within a short time. Additionally, if many consumers wait for a reset window, for example at the top of the hour, then they may stampede your API at the same time.\n**Sliding Log**\n\nSliding Lograte limiting involves tracking a time stamped log for each consumer's request. These logs are usually stored in a hash set or table that is sorted by time. Logs with timestamps beyond a threshold are discarded. When a new request comes in, we calculate the sum of logs to determine the request rate. If the request would exceed the threshold rate, then it is held.\n![Request at 12:00:35 Holc Request at 12:00:15 0k Request at 12:00:01 0k ](media/Rate-Limiting-image4.png)\nThe advantage of this algorithm is that it does not suffer from the boundary conditions of fixed windows. The rate limit will be enforced precisely. Also, because the sliding log is tracked for each consumer, you don't have the stampede effect that challenges fixed windows. However, it can be very expensive to store an unlimited number of logs for every request. It's also expensive to compute because each request requires calculating a summation over the consumer's prior requests, potentially across a cluster of servers. As a result, it does not scale well to handle large bursts of traffic or denial of service attacks.\n**Sliding Window Counter**\n\nThis is a hybrid approach that combines the low processing cost of the fixed window algorithm, and the improved boundary conditions of the sliding log. Like the fixed window algorithm, we track a counter for each fixed window. Next, we account for a weighted value of the previous window's request rate based on the current timestamp to smooth out bursts of traffic. For example, if the current window is 25% through, then we weight the previous window's count by 75%. The relatively small number of data points needed to track per key allows us to scale and distribute across large clusters.\n![Request 3: Ok Request 2: Denied Request 1: Ok ](media/Rate-Limiting-image5.png)\nWe recommend thesliding windowapproach because it gives the flexibility to scale rate limiting with good performance. The rate windows are an intuitive way she to present rate limit data to API consumers. It also avoids the starvation problem of leaky bucket, and the bursting problems of fixed window implementations.\nğ—Ÿğ—¢ğ—¦ğ—¦ğ—¬ ğ—–ğ—¢ğ—¨ğ—¡ğ—§\n\nIt is used to identify elements in a data stream whose frequency count exceeds a user-given threshold.\n\nğ™ğ™¨ğ™š ğ™˜ğ™–ğ™¨ğ™š: Frequency count over the data streams.\n<https://konghq.com/blog/how-to-design-a-scalable-rate-limiting-algorithm>\n\n<https://blog.cloudflare.com/counting-things-a-lot-of-different-things>\n\n<https://www.figma.com/blog/an-alternative-approach-to-rate-limiting>\n\n**Youtube - [Rate Limiting system design | TOKEN BUCKET, Leaky Bucket, Sliding Logs](https://www.youtube.com/watch?v=mhUQe4BKZXs)**"},{"fields":{"slug":"/Computer-Science/System-Design/Reactive-Microservices---Manifesto/","title":"Reactive Microservices / Manifesto"},"frontmatter":{"draft":false},"rawBody":"# Reactive Microservices / Manifesto\n\nCreated: 2018-11-29 00:55:23 +0500\n\nModified: 2021-11-12 21:57:21 +0500\n\n---\n\nA reactive microservices architecture is an architectural style that strives to provide the highest levels of responsiveness, resiliency, and elasticity, and accomplish this by adopting strong decoupling, isolation, non-blocking, event-driven architecture, and asynchronous messaging, among other techniques.\nReactive persistence uses Command Query Responsibility Segregation (CQRS) and event sourcing to accomplish this asynchronous and decouple interaction.\n**Reactive Manifesto**\n\nReactive Systems are Responsive, Resilient, Elastic and Message Driven.\nSystems built as Reactive Systems are more flexible, loosely-coupled and[scalable](https://www.reactivemanifesto.org/glossary#Scalability). This makes them easier to develop and amenable to change. They are significantly more tolerant of failure and when[failure](https://www.reactivemanifesto.org/glossary#Failure)does occur they meet it with elegance rather than disaster. Reactive Systems are highly responsive, giving[users](https://www.reactivemanifesto.org/glossary#User)effective interactive feedback.\n**Reactive Systems are:**\n\n**Responsive:**The[system](https://www.reactivemanifesto.org/glossary#System)responds in a timely manner if at all possible. Responsiveness is the cornerstone of usability and utility, but more than that, responsiveness means that problems may be detected quickly and dealt with effectively. Responsive systems focus on providing rapid and consistent response times, establishing reliable upper bounds so they deliver a consistent quality of service. This consistent behaviour in turn simplifies error handling, builds end user confidence, and encourages further interaction.\n**Resilient:**The system stays responsive in the face of[failure](https://www.reactivemanifesto.org/glossary#Failure). This applies not only to highly-available, mission-critical systems --- any system that is not resilient will be unresponsive after a failure. Resilience is achieved by[replication](https://www.reactivemanifesto.org/glossary#Replication), containment,[isolation](https://www.reactivemanifesto.org/glossary#Isolation)and[delegation](https://www.reactivemanifesto.org/glossary#Delegation). Failures are contained within each[component](https://www.reactivemanifesto.org/glossary#Component), isolating components from each other and thereby ensuring that parts of the system can fail and recover without compromising the system as a whole. Recovery of each component is delegated to another (external) component and high-availability is ensured by replication where necessary. The client of a component is not burdened with handling its failures.\n**Elastic:**The system stays responsive under varying workload. Reactive Systems can react to changes in the input rate by increasing or decreasing the[resources](https://www.reactivemanifesto.org/glossary#Resource)allocated to service these inputs. This implies designs that have no contention points or central bottlenecks, resulting in the ability to shard or replicate components and distribute inputs among them. Reactive Systems support predictive, as well as Reactive, scaling algorithms by providing relevant live performance measures. They achieve[elasticity](https://www.reactivemanifesto.org/glossary#Elasticity)in a cost-effective way on commodity hardware and software platforms.\n**Message Driven:**Reactive Systems rely on[asynchronous](https://www.reactivemanifesto.org/glossary#Asynchronous)[message-passing](https://www.reactivemanifesto.org/glossary#Message-Driven)to establish a boundary between components that ensures loose coupling, isolation and[location transparency](https://www.reactivemanifesto.org/glossary#Location-Transparency). This boundary also provides the means to delegate[failures](https://www.reactivemanifesto.org/glossary#Failure)as messages. Employing explicit message-passing enables load management, elasticity, and flow control by shaping and monitoring the message queues in the system and applying[back-pressure](https://www.reactivemanifesto.org/glossary#Back-Pressure)when necessary. Location transparent messaging as a means of communication makes it possible for the management of failure to work with the same constructs and semantics across a cluster or within a single host.[Non-blocking](https://www.reactivemanifesto.org/glossary#Non-Blocking)communication allows recipients to only consume[resources](https://www.reactivemanifesto.org/glossary#Resource)while active, leading to less system overhead.\n\n![VALUE FORM MEANS Maintainable Elastic Responsive Message Driven Extensible Resilient ](media/Reactive-Microservices---Manifesto-image1.png)\n\nLarge systems are composed of smaller ones and therefore depend on the Reactive properties of their constituents.\n**Table of Contents**\n\n1.  [Asynchronous](https://www.reactivemanifesto.org/glossary#Asynchronous)\n\n2.  [Back-Pressure](https://www.reactivemanifesto.org/glossary#Back-Pressure)\n\n3.  [Batching](https://www.reactivemanifesto.org/glossary#Batching)\n\n4.  [Component](https://www.reactivemanifesto.org/glossary#Component)\n\n5.  [Delegation](https://www.reactivemanifesto.org/glossary#Delegation)\n\n6.  [Elasticity (in contrast to Scalability)](https://www.reactivemanifesto.org/glossary#Elasticity)\n\n7.  [Failure (in contrast to Error)](https://www.reactivemanifesto.org/glossary#Failure)\n\n8.  [Isolation (and Containment)](https://www.reactivemanifesto.org/glossary#Isolation)\n\n9.  [Location-Transparency](https://www.reactivemanifesto.org/glossary#Location-Transparency)\n\n10. [Message-Driven (in contrast to Event-Driven)](https://www.reactivemanifesto.org/glossary#Message-Driven)\n\n11. [Non-Blocking](https://www.reactivemanifesto.org/glossary#Non-Blocking)\n\n12. [Protocol](https://www.reactivemanifesto.org/glossary#Protocol)\n\n13. [Replication](https://www.reactivemanifesto.org/glossary#Replication)\n\n14. [Resource](https://www.reactivemanifesto.org/glossary#Resource)\n\n15. [Scalability](https://www.reactivemanifesto.org/glossary#Scalability)\n\n16. [System](https://www.reactivemanifesto.org/glossary#System)\n\n17. [User](https://www.reactivemanifesto.org/glossary#User)\n**Asynchronous**\n\nThe Oxford Dictionary defines asynchronous as \"not existing or occurring at the same time\". In the context of this manifesto we mean that the processing of a request occurs at an arbitrary point in time, sometime after it has been transmitted from client to service. The client cannot directly observe, or synchronize with, the execution that occurs within the service. This is the antonym of synchronous processing which implies that the client only resumes its own execution once the service has processed the request.\n**Back-Pressure**\n\nWhen one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. Since it can't cope and it can't fail it should communicate the fact that it is under stress to upstream components and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity.\n**Batching**\n\nCurrent computers are optimized for the repeated execution of the same task: instruction caches and branch prediction increase the number of instructions that can be processed per second while keeping the clock frequency unchanged. This means that giving different tasks to the same CPU core in rapid succession will not benefit from the full performance that could otherwise be achieved: if possible we should structure the program such that its execution alternates less frequently between different tasks. This can mean processing a set of data elements in batches, or it can mean performing different processing steps on dedicated hardware threads.\nThe same reasoning applies to the use of external resources that need synchronization and coordination. The I/O bandwidth offered by persistent storage devices can improve dramatically when issuing commands from a single thread (and thereby CPU core) instead of contending for bandwidth from all cores. Using a single entry point has the added advantage that operations can be reordered to better suit the optimal access patterns of the device (current storage devices perform better for linear than random access).\nAdditionally, batching provides the opportunity to share out the cost of expensive operations such as I/O or expensive computations. For example, packing multiple data items into the same network packet or disk block to increase efficiency and reduce utilisation.\n**Component**\n\nWhat we are describing is a modular software architecture, which is a very old idea, see for example[Parnas (1972)](http://repository.cmu.edu/cgi/viewcontent.cgi?article=2979&context=compsci). We are using the term \"component\" due to its proximity with compartment, which implies that each component is self-contained, encapsulated and isolated from other components. This notion applies foremost to the runtime characteristics of the system, but it will typically also be reflected in the source code's module structure as well. While different components might make use of the same software modules to perform common tasks, the program code that defines the top-level behavior of each component is then a module of its own. Component boundaries are often closely aligned with[Bounded Contexts](http://martinfowler.com/bliki/BoundedContext.html)in the problem domain. This means that the system design tends to reflect the problem domain and so is easy to evolve, while retaining isolation. Message protocols provide a natural mapping and communications layer between Bounded Contexts (components).\n**Delegation**\n\nDelegating a task asynchronously to another component means that the execution of the task will take place in the context of that other component. This delegated context could entail running in a different error handling context, on a different thread, in a different process, or on a different network node, to name a few possibilities. The purpose of delegation is to hand over the processing responsibility of a task to another component so that the delegating component can perform other processing or optionally observe the progress of the delegated task in case additional action is required such as handling failure or reporting progress.\n**Elasticity (in contrast to Scalability)**\n\nElasticity means that the throughput of a system scales up or down automatically to meet varying demand as resource is proportionally added or removed. The system needs to be scalable (see[Scalability](https://www.reactivemanifesto.org/glossary#Scalability)) to allow it to benefit from the dynamic addition, or removal, of resources at runtime. Elasticity therefore builds upon scalability and expands on it by adding the notion of automatic[resource](https://www.reactivemanifesto.org/glossary#Resource)management.\n**Failure (in contrast to Error)**\n\nA failure is an unexpected event within a service that prevents it from continuing to function normally. A failure will generally prevent responses to the current, and possibly all following, client requests. This is in contrast with an error, which is an expected and coded-for condition---for example an error discovered during input validation, that will be communicated to the client as part of the normal processing of the message. Failures are unexpected and will require intervention before the system can resume at the same level of operation. This does not mean that failures are always fatal, rather that some capacity of the system will be reduced following a failure. Errors are an expected part of normal operations, are dealt with immediately and the system will continue to operate at the same capacity following an error.\nExamples of failures are hardware malfunction, processes terminating due to fatal resource exhaustion, program defects that result in corrupted internal state.\n**Isolation (and Containment)**\n\nIsolation can be defined in terms of decoupling, both in time and space. Decoupling in time means that the sender and receiver can have independent life-cycles---they do not need to be present at the same time for communication to be possible. It is enabled by adding asynchronous boundaries between the components, communicating through message-passing. Decoupling in space (defined as Location Transparency) means that the sender and receiver do not have to run in the same process, but wherever the operations division or the runtime itself decides is most efficient---which might change during an application's lifetime.\nTrue isolation goes beyond the notion of encapsulation found in most object-oriented languages and gives us compartmentalization and containment of:\n-   State and behavior: it enables share-nothing designs and minimizes contention and coherence cost (as defined in the[Universal Scalability Law](http://www.perfdynamics.com/Manifesto/USLscalability.html));\n-   Failures: it allows[failures](https://www.reactivemanifesto.org/glossary#Failure)to be captured, signalled and managed at a fine-grained level instead of letting them cascade to other components.\nStrong isolation between components is built on communication over well-defined[protocols](https://www.reactivemanifesto.org/glossary#Protocol)and enables loose coupling, leading to systems that are easier to understand, extend, test and evolve.\n**Location Transparency**\n\nElastic systems need to be adaptive and continuously react to changes in demand, they need to gracefully and efficiently increase and decrease scale. One key insight that simplifies this problem immensely is to realize that we are all doing distributed computing. This is true whether we are running our systems on a single node (with multiple independent CPUs communicating over the QPI link) or on a cluster of nodes (with independent machines communicating over the network). Embracing this fact means that there is no conceptual difference between scaling vertically on multicore or horizontally on the cluster.\nIf all of our[components](https://www.reactivemanifesto.org/glossary#Component)support mobility, and local communication is just an optimization, then we do not have to define a static system topology and deployment model upfront. We can leave this decision to the operations personnel and the runtime, which can adapt and optimize the system depending on how it is used.\nThis decoupling in space (see the the definition for Isolation), enabled through asynchronous message-passing, and decoupling of the runtime instances from their references is what we call Location Transparency. Location Transparency is often mistaken for 'transparent distributed computing', while it is actually the opposite: we embrace the network and all its constraints---like partial failure, network splits, dropped messages, and its asynchronous and message-based nature---by making them first class in the programming model, instead of trying to emulate in-process method dispatch on the network (ala RPC, XA etc.). Our view of Location Transparency is in perfect agreement with[A Note On Distributed Computing](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628)by Waldo et al.\n**Message-Driven (in contrast to Event-Driven)**\n\nA message is an item of data that is sent to a specific destination. An event is a signal emitted by a component upon reaching a given state. In a message-driven system addressable recipients await the arrival of messages and react to them, otherwise lying dormant. In an event-driven system notification listeners are attached to the sources of events such that they are invoked when the event is emitted. This means that an event-driven system focuses on addressable event sources while a message-driven system concentrates on addressable recipients. A message can contain an encoded event as its payload.\nResilience is more difficult to achieve in an event-driven system due to the short-lived nature of event consumption chains: when processing is set in motion and listeners are attached in order to react to and transform the result, these listeners typically handle success or failure directly and in the sense of reporting back to the original client. Responding to the failure of a component in order to restore its proper function, on the other hand, requires a treatment of these failures that is not tied to ephemeral client requests, but that responds to the overall component health state.\n**Non-Blocking**\n\nIn concurrent programming an algorithm is considered non-blocking if threads competing for a resource do not have their execution indefinitely postponed by mutual exclusion protecting that resource. In practice this usually manifests as an API that allows access to the resource if it is available otherwise it immediately returns informing the caller that the resource is not currently available or that the operation has been initiated and not yet completed. A non-blocking API to a resource allows the caller the option to do other work rather than be blocked waiting on the resource to become available. This may be complemented by allowing the client of the resource to register for getting notified when the resource is available or the operation has completed.\n**Protocol**\n\nA protocol defines the treatment and etiquette for the exchange or transmission of messages between components. Protocols are formulated as relations between the participants to the exchange, the accumulated state of the protocol and the allowed set of messages to be sent. This means that a protocol describes which messages a participant may send to another participant at any given point in time. Protocols can be classified by the shape of the exchange, some common classes are request--reply, repeated request--reply (as in HTTP), publish--subscribe, and stream (both push and pull).\nIn comparison to local programming interfaces a protocol is more generic since it can include more than two participants and it foresees a progression of the state of the message exchange; an interface only specifies one interaction at a time between the caller and the receiver.\nIt should be noted that a protocol as defined here just specifies which messages may be sent, but not how they are sent: encoding, decoding (i.e. codecs), and transport mechanisms are implementation details that are transparent to the components' use of the protocol.\n**Replication**\n\nExecuting a component simultaneously in different places is referred to as replication. This can mean executing on different threads or thread pools, processes, network nodes, or computing centers. Replication offers scalability, where the incoming workload is distributed across multiple instances of a component, or resilience, where the incoming workload is replicated to multiple instances which process the same requests in parallel. These approaches can be mixed, for example by ensuring that all transactions pertaining to a certain user of the component will be executed by two instances while the total number of instances varies with the incoming load, (see Elasticity).\n**Resource**\n\nEverything that a component relies upon to perform its function is a resource that must be provisioned according to the component's needs. This includes CPU allocation, main memory and persistent storage as well as network bandwidth, main memory bandwidth, CPU caches, inter-socket CPU links, reliable timer and task scheduling services, other input and output devices, external services like databases or network file systems etc. The elasticity and resilience of all these resources must be considered, since the lack of a required resource will prevent the component from functioning when required.\n**Scalability**\n\nThe ability of a system to make use of more computing resources in order to increase its performance is measured by the ratio of throughput gain to resource increase. A perfectly scalable system is characterized by both numbers being proportional: a twofold allocation of resources will double the throughput. Scalability is typically limited by the introduction of bottlenecks or synchronization points within the system, leading to constrained scalability, see[Amdahl's Law and Gunther's Universal Scalability Model](http://blogs.msdn.com/b/ddperf/archive/2009/04/29/parallel-scalability-isn-t-child-s-play-part-2-amdahl-s-law-vs-gunther-s-law.aspx).\n**System**\n\nA system provides services to its users or clients. Systems can be large or small, in which case they comprise many or just a few components. All components of a system collaborate to provide these services. In many cases the components are in a client--server relationship within the same system (consider for example front-end components relying upon back-end components). A system shares a common resilience model, by which we mean that failure of a component is handled within the system, delegated from one component to the other. It is useful to view groups of components within a system as subsystems if they are isolated from the rest of the system in their function, resources or failure modes.\n**User**\n\nWe use this term informally to refer to any consumer of a service, be that a human or another service.\n**References**\n\n<https://www.reactivemanifesto.org>\n\n<https://www.reactivemanifesto.org/glossary>\n\n"},{"fields":{"slug":"/Computer-Science/System-Design/Serverless-Architecture/","title":"Serverless Architecture"},"frontmatter":{"draft":false},"rawBody":"# Serverless Architecture\n\nCreated: 2018-07-04 00:27:03 +0500\n\nModified: 2022-03-02 20:11:52 +0500\n\n---\n\nServerless computing simply means using existing, auto-scaling cloud services to achieve system behaviours. In other words, I don't manage any servers or docker containers. I don't set up networks or manage operation (ops). I merely provide the serverless solution my recipe and it handles creation of any needed assets and performs the required computational process.\n1.  Containerisation - Gives a uniform development environment that is independent of the systems the code is running on.\n\n2.  Microservices architecture - Developers are able to quickly understand a certain module without going through the whole code base.\n    -   Also changing something on one part of the code doesn't effect the other parts of the product.\n    -   Apis are used to communicate between different modules\n    -   Messaging Queue is used for inter-modules communication (AMQP, MQTT, etc.)\n\n3.  Event Based Architecture\n    -   Events are triggered when some event happens\n    -   Example - When a photo is uploaded automatically trigger the thumbnail creation function.\n    -   Functions subscribe to events, and based on that event react accordingly.\n\n4.  AWS Lambda\n\n5.  AWS CloudFunctions\n\n6.  AWS CloudFormation (for provisioning)\n**Stateless Services**\n-   Not a cache or a database\n-   Frequently accessed metadata\n-   No instance affinity\n-   Loss of a node is a non-event\n**Durable Functions**\n\nDurable Functionsis an extension of[Azure Functions](https://docs.microsoft.com/en-us/azure/azure-functions/functions-overview)that lets you write stateful functions in a serverless compute environment. The extension lets you define stateful workflows by writing[orchestrator functions](https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-orchestrations)and stateful entities by writing[entity functions](https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-entities)using the Azure Functions programming model. Behind the scenes, the extension manages state, checkpoints, and restarts for you, allowing you to focus on your business logic.\n<https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview>\n\n## AWS SAM (Serverless Application Model)**\n\nThe AWS Serverless Application Model (SAM) is an open-source AWS framework that allows developers to more efficiently build serverless applications. It includes SAM CLI options for local testing and integrates with various AWS serverless tools.\n\n<https://www.toptal.com/aws/typescript-jest-aws-sam-tutorial>\n\n## References**\n\n<https://cloudplatform.googleblog.com/2018/04/Cloud-native-architecture-with-serverless-microservices-the-Smart-Parking-story.html>\n\nSE Radio - 320: Serverless Applications - Kishore Bhatia with Nate Taggart\n\n<https://www.freecodecamp.org/news/complete-back-end-system-with-serverless>\n"},{"fields":{"slug":"/Computer-Science/System-Design/Trade-offs/","title":"Trade offs"},"frontmatter":{"draft":false},"rawBody":"# Trade offs\n\nCreated: 2019-12-30 13:29:20 +0500\n\nModified: 2019-12-30 13:47:22 +0500\n\n---\n\n**Tradeoffs**\n-   Speed vs. memory\n-   battery life vs. accuracy\n-   fairness vs. accuracy\n-   precision vs. recall\n-   ease of implementation vs. maintainability\n-   explore vs exploit\n**Questions**\n\nCost\n-   Operational complexity\n-   How easy is it to scale if the TSDB storage is low?\n-   How easy is it to scale the performance with increased data?\n-   Are there any operations tasks that regularly need to be carried out?\nCapabilities\n-   Does the TSDB support metrics and events?\n-   Can metadata be associated with an event?\n-   What is the precision i.e. the smallest increment of time between events?\n-   What is the consistency model?\n-   Can the data have a Time To Live?\nPerformance\n-   How many events per second can be written in, for a given scale of the system?\n-   How many events per second can be read out, for a given scale of the system?\n-   Bytes per point after any compression that occurs? (What volume of space is required?)\nQuery capabilities\n-   How is the data queried? DSL? API?\n-   How easy is the query mechanism to use?\n-   Can the query mechanism aggregate data? Can it do it by date?\n-   Can we de-dupe data? e.g., if we ingested the same event twice? A uniqueness constraint.\nIngestion\n-   How is the data sent to the TSDB? API? Log scraping? Multi language clients?\nExport\n-   How could the entire TSDB be exported for use elsewhere or in another TSDB?\nMaturity\n-   How mature is the TSDB and its ecosystem? Is it likely to die soon? Does it have good support?\nCommunity\n-   How large and active is the community using the TSDB? If I have a problem will I be able to find someone to help me solve it?\nSupport\n-   Is there any paid support for the TSDB and if so how much?\nSecurity\n-   What are the mechanisms for authentication and authorisation?\n-   What other security implications are there?\nIntegration\n-   How easy is it to integrate with other services? Is there anything specific that helps?\nVisualisation\n-   How can the data from the TSDB be visualised?\n-   Are there any dashboards / high level visualisations?\n-   Are the dashboards internal to the TSDB or can they be shared on a website?\n<https://medium.com/kudos-engineering/choosing-the-elastic-stack-as-a-time-series-database-9fac202c53ba>\n\n## TSDB**\n\n<https://outlyer.com/blog/top10-open-source-time-series-databases>\n"},{"fields":{"slug":"/Computer-Science/System-Design/Twelve-Factor-App/","title":"Twelve-Factor App"},"frontmatter":{"draft":false},"rawBody":"# Twelve-Factor App\n\nCreated: 2018-06-24 19:25:58 +0500\n\nModified: 2020-08-18 02:20:28 +0500\n\n---\n\nIn the modern era, software is commonly delivered as a service: calledweb apps, orsoftware-as-a-service. The twelve-factor app is a methodology for building software-as-a-service apps that:\n-   Use**declarativeformats** for setup automation, to minimize time and cost for new developers joining the project\n-   Have a**clean contract**with the underlying operating system, offering**maximum portability**between execution environments\n-   Are suitable for**deployment**on modern**cloud platforms**, obviating the need for servers and systems administration\n-   **Minimize divergence**between development and production, enabling**continuous deployment**for maximum agility\n-   And can**scale up**without significant changes to tooling, architecture, or development practices\nThe twelve-factor methodology can be applied to apps written in any programming language, and which use any combination of backing services (database, queue, memory cache, etc).\nThese best practices are designed to enable applications to be built with portability and resilience when deployed to[the web](https://en.wikipedia.org/wiki/The_web).\n| **#** | **Factor**              | **Description**                                                                                                     |\n|---------|-------------|--------------------------------------------------|\n| I      | **Codebase**            | There should be exactly one codebase for a deployed service with the codebase being used for many deployments.      |\n| II     | **Dependencies**        | All dependencies should be declared, with no implicit reliance on system tools or libraries.                        |\n| III    | **Config**              | Configuration that varies between deployments should be stored in the environment.                                  |\n| IV     | **Backing services**    | All backing services are treated as attached resources and attached and detached by the execution environment.      |\n| V      | **Build, release, run** | The delivery pipeline should strictly consist of build, release, run.                                               |\n| VI     | **Processes**           | Applications should be deployed as one or more stateless processes with persisted data stored on a backing service. |\n| VII    | **Port binding**        | Self-contained services should make themselves available to other services by specified ports.                      |\n| VIII   | **Concurrency**         | Concurrency is advocated by scaling individual processes.                                                           |\n| IX     | **Disposability**       | Fast startup and shutdown are advocated for a more robust and resilient system.                                     |\n| X      | **Dev/Prod parity**     | All environments should be as similar as possible.                                                                  |\n| XI     | **Logs**                | Applications should produce logs as event streams and leave the execution environment to aggregate.                 |\n| XII    | **Admin Processes**     | Any needed admin tasks should be kept in source control and packaged with the application.                          |\nReferences\n\n<https://12factor.net>\n"},{"fields":{"slug":"/Computer-Science/Testing/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-03-17 23:27:01 +0500\n\nModified: 2021-11-25 13:31:50 +0500\n\n---\n\n**Testing Vocabulary**\n-   The**units**we are testing in the section above are functions ---fibandnext_collatz_element.\n-   We have 5**unit tests**; all of them intest_math_functions.py: Thetest_* functions.\n-   Thepytestcommand-line executable is called a**test runner**. It executes (runs) the tests.\n-   A**test suite**is an arbitrary collection of tests. Usually, you mean all tests.\n**Why do we test at all?**\n-   **Trust:** You checked at least some cases if they work. So others can have more trust in the quality of your work and you can also put more trust in it.\n-   **Breaking Changes:** For a bigger project, it is sometimes hard to have every part in mind. By writing tests, you make it easier to change something and see if / where things break. This does not only help you but also team members. Including once that are not there yet.\n-   **Code Style:** When you know that you have to write tests, you write some things slightly differently. Those slight differences usually improve the coding style. Sometimes, they are crucial. For example, if you have to thoroughly test your code you will make smaller chunks.\n-   **Documentation:** Some test cases show a little bit of how the code is intended to be used.\n**Good Tests**\n\nIt's pretty hard to write good tests and when you measure your test coverage it is tempting to quickly write a couple of bad tests.\nWorst is no testing at all.\nA little bit better is a test that just executes a function but does not check if the return value/the side effects are what you expect. So you simply run it to check if the code crashes.\n**Happy-Tests** where you check the output of the tested function and a typical input is even better. I call themhappybecause they test what you expect to get.\nIn contrast, an**unhappyexecution path** is dealing with unwanted inputs. This is also called[**negative testing**](https://en.wikipedia.org/wiki/Negative_testing). You check if you actually throw an error. Not throwing an error and silently failing is bad as it hides bugs.\n**Property testing**is pretty cool. There you don't test for single values, but you check if a property is still held. For example, the output of a factorization function can be multiplied and should equal the input.\n<https://medium.com/swlh/unit-testing-in-python-basics-21a9a57418a0>\n\n## Property Based Testing**\n\n<https://levelup.gitconnected.com/unit-testing-in-python-property-based-testing-892a741fc119>\n\n[Code Checking Automation - Computerphile](https://www.youtube.com/watch?v=AfaNEebCDos)\n**Blameless Root Cause Analysis (RCA) / Post Incident Analysis / Post Mortem / Incident Management**\n\nRoot cause analysis (RCA) is a problem-solving method which is used to pinpoint the exact cause of a problem or event.\nThe root cause is the actual cause of a specific problem or set of problems, and when that cause is removed, it prevents the final undesirable effect from occurring.\nRCA is a reactive method, as opposed to preventive, since it will be employed only after a problem has occurred in order to find its cause and prevent it from happening again.\n<https://www.techopedia.com/definition/30361/root-cause-analysis-rca>\n\n<https://www.freecodecamp.org/news/what-is-a-software-post-mortem>\n\n<https://about.gitlab.com/handbook/customer-success/professional-services-engineering/workflows/internal/root-cause-analysis.html>\n\n## Pre Production Testing**\n-   Shadowing\n-   Mutation tests\n-   Contract tests\n-   Unit tests\n-   Functional tests\n-   Component tests\n-   Integration tests\n-   Fuzz tests\n-   Load tests\n-   Smoke tests\n-   Coverage tests\n-   Regression tests\n\n<https://www.toptal.com/web-qa/ui-visual-regression-testing>\n-   Acceptance tests\n-   Property based tests\n-   Usability tests\n-   Benchmarking\n-   Stress test\n-   Config tests\n**Testing in Production**\n-   Canarying\n-   Monitoring\n-   Exploration\n-   Profiling\n-   Distributed tracing\n-   Dynamic instrumentation\n-   Chaos engineering\n-   Feature flagging\n-   Real user monitoring\n-   User engagement tests\n-   **A/B testing**\n    -   <https://www.optimizely.com>\n\n## Multivariate Testing**\n\nMultivariate testing(or A/B testing)is when you make product changes that are only seen by some of your users. This gives you some people that see the A version of your product and other people that see the B version of your product. Then you can see if version A or B gives you the results you want. A debate within multivariate testing is whether or not multi-armed bandit testing is the best kind of A/B test. Bandit testing is a continuous form of A/B testing that always send people toward the best performing options. In essence, the experiment never ends. I'm not going to get into this debate here, but I wanted you to know that there is a debate.\nWhen running A/B tests it is important to remember that sample size matters. If option A and B are only shown to a few hundred visitors then it doesn't really matter what the results say. You don't have enough participants in the test to make the statistics meaningful. Here is an anecdote to illuminate this point. If you run an A/B test for 1 day, and on that day you have 2,000 visitors, and option A gave you the results you wanted 70% of the time, then you'd think this experiment is a success. Later you find out that this was the same day that a new blog linked to your product and the traffic from this blog accounted for 90% of your traffic that day. At best, your A/B test has uncovered something about the audience of this blog, not about the users of your product as a whole.-   Traffic shifting\n<https://copyconstruct.medium.com/testing-in-production-the-safe-way-18ca102d0ef1>\n\n[Why Good Developers Write Bad Tests](https://www.youtube.com/watch?v=oO-FMAdjY68)\n\n<https://www.toptal.com/test-automation-engineer/automated-testing-best-practices>\n"},{"fields":{"slug":"/Computer-Science/Testing/Load---Performance-Testing-QA-Tools/","title":"Load / Performance Testing/QA Tools"},"frontmatter":{"draft":false},"rawBody":"# Load / Performance Testing/QA Tools\n\nCreated: 2019-01-15 16:37:11 +0500\n\nModified: 2022-01-05 23:09:26 +0500\n\n---\n\n**Website Performance Testing Tools**\n-   <https://gtmetrix.com>\n-   LightHouse\n-   <https://www.browserstack.com>\n-   <https://www.sitespeed.io>\n-   <https://estimator.dev>\n\n## Bash - add artificial load to the CPU**\n\nwhile true; do i=0; done\ni = 0\n\nx = 2\n\nwhile True:\n\nx = x*x\n\ni += 1\n\nif i == 32:\n\nbreak\nwhile true:\n\nfor i in range(1,1000000):\n\npass\n**dd -- convert and copy a file (Check read and write throughput)**\n\nThe dd utility copies the standard input to the standard output. Input data is read and written in 512-byte blocks. If input reads are short, input from multiple reads are aggregated to form the output block. When finished, dd displays the number of complete and partial input and output blocks and truncated input records to the standard error output.\n**server latency**\n\ndd if=/dev/zero of=/tmp/test2.img bs=512 count=1000 oflag=dsync\n**server throughput**\n\nif=/dev/zero of=/tmp/test1.img bs=1G count=1 oflag=dsync\n**Grinder**\n**JMeter**\n\nTheApache JMeter application is open source software. It is a pure Java application designed to load test an application and measure its performance.\n\n<https://towardsdatascience.com/load-testing-of-a-real-time-pipeline-d32475163285>\n\n## Gatling**\n\nGatling is a highly capable load testing tool. It is designed for ease of use, maintainability and high performance.\n**Gatling**is an open-source load and performance testing framework based on[Scala](https://en.wikipedia.org/wiki/Scala_(programming_language)),[Akka](https://en.wikipedia.org/wiki/Akka_(toolkit))and[Netty](https://en.wikipedia.org/wiki/Netty_(software)). The first stable release was published on January 13, 2012. In 2015, Gatling's founder, StÃ©phane Landelle, created a company (named \"Gatling Corp\"), dedicated to the development of the open-source project. According to Gatling Corp's official blog, Gatling was downloaded more than 800,000 times (August 2017).[^[1]^](https://en.wikipedia.org/wiki/Gatling_(software)#cite_note-1)In June 2016, Gatling officially presented Gatling FrontLine, Gatling's Enterprise Version with additional features.\n<https://en.wikipedia.org/wiki/Gatling_(software)>\n\n[https://gatling.io](https://gatling.io/)\n**Tsung**\n\nTsung is an open-source multi-protocol distributed load testing tool. It can be used to stress HTTP, WebDAV, SOAP, PostgreSQL, MySQL, LDAP, MQTT, and Jabber/XMPP servers.\n**Siege**\n\nSiege is an HTTP load testing and benchmarking utility. Siege supports basic authentication, cookies, HTTP, HTTPS and FTP protocols. It lets its user hit a server with a configurable number of simulated clients.\n**Httperf**\n\nHttperf is a tool for measuring web server performance. It provides a flexible facility for generating various HTTP workloads and for measuring server performance.\n**Taurus**\n\nAlthough not specifically related to Perf testing, Taurus provides an automation-friendly framework for continuous testing, including functional and performance.\n**Artillery**\n\nArtillery is a modern, powerful & easy-to-use load testing and functional testing toolkit. Use it to shipscalable applications that stayperformant &resilientunder high load.\n**Goad**\n\nGoad takes full advantage of the power of Amazon Lambdas for distributed load testing. You can use goad to launch HTTP loads from up to four AWS regions at once. Each lambda can handle hundreds of concurrent connections, able to achieve peak loads of up to100,000 concurrent requests.\n**Apache Bench**\n\nabis a tool for benchmarking your Apache Hypertext Transfer Protocol (HTTP) server. It is designed to give you an impression of how your current Apache installation performs.\n**See Also**\n\nLocust\n**HTTP Load Testing Tools**\n-   [wrk](https://github.com/wg/wrk)\n```\nwrk --duration 20s --threads 10 --connections 200 [URL]\n\nwrk -c 5 -t 5 -d 99999 -H \"Connection: Close\" <http://application-cpu>\n\nwrk -c 5 -t 5 -d 99999 -H \"Connection: Close\" <https://facebook.com>\n```\n\n-   **Apache Bench - [Apache HTTP Server Benchmarking Tool](https://httpd.apache.org/docs/2.4/programs/ab.html) (for percentiles)**\n\n```\napt install apache2\nbrew install apache2\nab -c 50 -n 500 -s 90 <http://www.stashfin.com>\n\nab -c 50 -n 500 -s 90 <http://stashfin-website.staging>\n\nab -c 100 -n 500 -s 90 <http://stashfin-website.staging> (Success)\n\nab -c 500 -n 5000 -s 90 <http://stashfin-website.staging> (Fail)\n\nab -c 500 -n 5000 -s 90 <http://bigbet-nlb-7ac1185001d91c31.elb.us-west-2.amazonaws.com>\n\n**ab -c 500 -n 5000 -s 90 [URL]**\n\n**ab -c 500 -n 500 -s 90**\n\nab -c 50 -n 5000 -s 90 -p data.json -T application/json -rk <https://staff.lendenclub.com/core/lender_app/prospect/verify>\n\n-r Don't exit on socket receive errors.\n\n-k Enable the HTTP KeepAlive feature, i.e., perform multiple requests within one HTTP session. Default is no KeepAlive.\n\n```\n\n- [Siege](https://github.com/JoeDog/siege) (for constant load)\n```\napt-get install -y siege\n\nsiege -c2 -t2m [URL]\n```\n\n-   hey / boom\n```\nhey <https://dev.stashfin.com>\n<https://github.com/rakyll/hey>\n```\n\n-   <https://k6.io>\nOpen source load testing tool and SaaS for engineering teams\n\n-   [**https://fortio.org/**](https://fortio.org/)\n\nFortio load testing library, command line tool, advanced echo server and web UI in go (golang). Allows to specify a set query-per-second load and record latency histograms and other useful stats.\nFortio runs at a specified query per second (qps) and records an histogram of execution time and calculates percentiles (e.g. p99 ie the response time such as 99% of the requests take less than that number (in seconds, SI unit)). It can run for a set duration, for a fixed number of calls, or until interrupted (at a constant target QPS, or max speed/load per connection/thread).\n\n**Datasets for load testing databases**\n-   TPCC benchmark\n-   Yahoo! Cloud Serving Benchmark (YCSB)\n    -   Key value pair workload\n    -   20 million tuples\n    -   Each tuple is 1KB (total database is ~20GB)\n    -   Each transactions reads/modifies 16 tuples\n\n**Test application**\n<https://github.com/blueperf>\n\n**References**\n\n<https://www.testingexcellence.com/top-10-open-source-performance-testing-tools>\n\n**Locust**\nLocust is an easy-to-use, distributed, user load testing tool. It is intended for load-testing websites (or other systems) and figuring out how many concurrent users a system can handle.\nLocust is a scalable load testing framework written in Python\nLocust is completely event-based, and therefore it's possible to support thousands of concurrent users on a single machine. In contrast to many other event-based apps it doesn't use callbacks. Instead it uses light-weight processes, through[gevent](http://www.gevent.org/). Each locust swarming your site is actually running inside its own process (or greenlet, to be correct). This allows you to write very expressive scenarios in Python without complicating your code with callbacks.\n\n**Running Locust Distributed**\nYou start one instance of Locust in master mode using the--masterflag. This is the instance that will be running Locust's web interface where you start the test and see live statistics. The master node doesn't simulate any users itself. Instead you have to start one or ---most likely---multiple slave Locust nodes using the--slaveflag, together with the--master-host(to specify the IP/hostname of the master node).\n\n**Commands**\n```\nlocust -f tasks.py --host localhost:5000\nlocust --no-reset-stats -f mqttClient.py\n\nlocust -f mqttClient.py\n\nlocust -f mqttClient.py --master\nlocust -f mqttClient.py --slave --master-host=[master-ip]\n\nlocust -f mqttClient.py --master --expect-slaves=5 #when running locust without webui in distributed mode\n\nlocust -f mqttClient.py --no-web -c 1000 -r 100 #running locust without webui\n\nlocust -f mqttClient.py --no-web -c 1000 -r 100 --run-time 1h30m #setting time limit for each test\n\nlocust -f mqttClient.py --csv=example --no-web -t10m #retrieve test statistics in csv format\nlocust -f mqttClient.py --no-web -c 2 -r 1\n**Dashboard**\n\nlocalhost:8089\n<https://locust.io>\n\n<https://docs.locust.io/en/stable/quickstart.html>\n\n<https://docs.locust.io/en/stable/running-locust-distributed.html>\n\n## MQTT Stresser**\n\ndocker run inovex/mqtt-stresser -broker tcp://[104.211.220.105:1883](http://104.211.220.105:1883/) -num-clients 100 -num-messages 10 -rampup-delay 1s -rampup-size 10 -global-timeout 180s -timeout 20s -username *zenatix_mqtt_client -password xitanez123*\n**EMQTT Benchmark**\n\ndocker run -d --name bench --network zenatix-docker rkosyk/emqtt-benchmark\n\ndocker exec -it bench /bin/bash\n**Subscribe**\n\ncreate 50K concurrent clients at the arrival rate of 100/sec:\n\n./emqtt_bench_sub -h mqtt.zenatix.com -c 50000 -i 10 -t bench/%i -q 2\n\n./emqtt_bench_sub -h mqtt.zenatix.com -c 2000 -i 10 -t bench/%i -q 2\n\n./emqtt_bench_sub -h mqtt.zenatix.com -u zenatix_mqtt_client -p xitanez123 -c 5 -i 5 -t bench/%i -q 1\n\n./emqtt_bench_sub -h mqtt.zenatix.com -c 2200 -i 10 -t bench/%i -q 2\n\n./emqtt_bench_sub -h emqx -c 10 -i 10 -t bench/%i -q 2\nAround 1985 connections are no longer accepted (because of local)\n./emqtt_bench_sub -h mqtt.zenatix.com -c 10 -i 10 -t bench/%i -q 2 -C false\n**Publish**\n\ncreate 100 clients and each client publish messages at the rate of 100 msg/sec with 256 Byte data size\n\n./emqtt_bench_pub -h mqtt.zenatix.com -c 100 -I 10 -t bench/%i -s 256\n\n./emqtt_bench_pub -h mqtt.zenatix.com -c 10 -I 1 -t bench/%i -s 256 -q 1 -C false\n**Final**\n\nSubscribe\n\n./emqtt_bench_sub -h mqtt.zenatix.com -u zenatix_mqtt_client -P xitanez123 -c 1000 -i 10 -t bench/%i -q 1 -C false\n./emqtt_bench_sub -h mqtt.zenatix.com -u zenatix_mqtt_client -P xitanez123 -c 1 -i 10 -t bench/+ -q 1 -C false\n**Publish**\n\n**Publish doesn't need persistent connection (clean session = true)**\n\n# payload size 10KB\n\n./emqtt_bench_pub -h mqtt.zenatix.com -u zenatix_mqtt_client -P xitanez123 -c 500 -I 10000 -t bench/%i -s 10000 -q 1\n# payload size 1KB\n\n./emqtt_bench_pub -h mqtt.zenatix.com -u zenatix_mqtt_client -P xitanez123 -c 100 -I 1000 -t bench/%i -s 1000 -q 1\n10000 Clients with 10KB Payload at 30 sec at QoS 1\n\n./emqtt_bench_pub -h mqtt.zenatix.com -u zenatix_mqtt_client -P xitanez123 -c 10000 -I 30000 -t bench/%i -s 10000 -q 1\n<https://github.com/emqtt/emqtt_benchmark>\n```\n\n## QA Companies\n-   Browserstack\n-   Saucelabs\n-   Lambdatest\n\n## Others\n<https://aws.amazon.com/about-aws/whats-new/2021/05/introducing-distributed-load-testing-v1-3>\n"},{"fields":{"slug":"/Computer-Science/Testing/Mocking/","title":"Mocking"},"frontmatter":{"draft":false},"rawBody":"# Mocking\n\nCreated: 2020-08-23 02:32:39 +0500\n\nModified: 2021-05-29 21:57:41 +0500\n\n---\n\n**The Abstract Pattern of the Problem**\n\nA dependency of the function we want to test can have an effect in three different ways: By side-effects, return values or exceptions.\n**Problem 1: A dependencies side-effect**\n\ndef a_function():\n\n... # Application code to be tested\n\na_dependency()\n\n... # Application code to be tested\n**Problem 2: A dependencies return value**\n\ndef a_function():\n\n... # Application code to be tested\n\nfoo = a_dependency()\n\n... # Application code to be tested; it might use foo\n**Problem 3: A dependency throws an Exception**\n\ndef a_function():\n\n... # Application code to be tested\n\ntry:\n\nfoo = a_dependency()\n\nexcept:\n\n... # Application code to be tested\n\n... # this might depend on the type of Exception\n\n... # Application code to be tested\n**The Problem --- Simple Examples**\n\nExample 1: We want to add a user to a database. You can see thatdbdoes not return anything, but we change the state of our system. And we want to be sure that we don't actually change our production system when the unit tests are running!\nExample 2: Generate a file name based on the current date. You can see that the dependencydatetimereturns a value:\n\n```\nimport datetime\ndef generate_filename():\n\nreturn f\"{datetime.datetime.now():%Y-%m-%d}.png\"\n```\n\nSimilarly, you could imagine a function which returns the weather in an English sentence and uses an API to get the actual weather ([example](https://gist.github.com/MartinThoma/5c7224ceae47e74645e0145d26dc03ec)).\nExample 3: In my project[edapy](https://github.com/MartinThoma/edapy)I looked at metadata from PDF files. I use the dependency PdfFileReader and have the file itself as an dependency. As the PDF file could be broken, PyPDF2 might throw an exception. So you can imagine code like this:\n```\nimport PyPDF2.utils\n\nfrom PyPDF2 import PdfFileReader\ndef get_pdf_info(pdf_path):\n\ninfo = {}\n\ntry:\n\npdf_toread = PdfFileReader(fp, strict=False)\n\nexcept PyPDF2.utils.PdfReadError:\n\ninfo[\"is_errornous\"] = True\n\nreturn info\n\n# a lot more\n\nreturn info\n```\n\nWhen you want to test such functions, you have the problem that the expected output is not only dependent on the function itself, but also on something external. In the cases above, the system time, an external service, and the file system.\n\n## Examples for External Dependencies\n\nThere are lots of external dependencies your tests might have:\n-   Date or time\n-   Internet: A web service you need to use\n-   File System: A file you need to create / read / edit / delete\n-   Database: Data you select / insert / update/ delete\n-   Randomness: Your code might make use ofrandomornp.random\n\nJust like the example above, they make isolated unit testing hard or even impossible.\n\n## The solution: Patching!\n\nThe overall strategy to test this is always the same: Replace the external dependency that is causing headaches by something in your control. The act of replacing the dependency is called**patching**, the replacement is called a**mock**. Depending on what exactly the mock does, you might also hear this being called a **Test Double, Test Stub, Test Spy or a Fake Object**. In practice in Python, the distinction does not matter.\nLet's make a tiny example how to use patch!\n```\nfrom external_dependency import dark_magic\ndef is_credit_card_fraud(transaction):\n\nfraud_probability = dark_magic(transaction)\n\nif fraud_probability > 0.99:\n\nreturn True\n\nelse:\n\nreturn False\ndef dark_magic(transaction):\n\nraise ValueError()\n```\nNo matter which transaction you would use, the function is_credit_card_fraud would throw a ValueError.\nThis is how you patch that dependency away with a decorator@patch:\n```\nfrom unittest.mock import patch, MagicMock\ndef the_mock(input):\n\nreturn 0.999\n@patch(\"fraud_example.dark_magic\", the_mock)\n\ndef test_is_credit_card_fraud():\n\nimport fraud_example\ntransaction = {\"amount_usd\": \"9999.99\", \"overnight_shipping\": True}\n\nis_fraud = fraud_example.is_credit_card_fraud(transaction)\n\nassert is_fraud == True\nAnd this is how you patch the dependencyfraud_example.dark_magicaway with a context handler (with ...):\n\ndef test_is_credit_card_fraud_context_handler():\n\nimport fraud_example\ntransaction = {\"amount_usd\": \"9999.99\", \"overnight_shipping\": True}\n\nwith patch(\"fraud_example.dark_magic\", the_mock):\n\nis_fraud = fraud_example.is_credit_card_fraud(transaction)\n\nassert is_fraud == True\n```\nWhen you now execute pytest, the test will succeed. You will always get 0.999 as a return value ofdark_magicğŸ‰\nA part that might be surprising in this example is the first parameter of thepatchdecorator: It's\"fraud_example.dark_magic\"and NOT\"external_dependency.dark_magic\"! The target of your replacement is always what was loaded within the file you want to test, not where it was loaded from.\n\n**Direct replacement: Don't do this!**\n\nThe following is an example which does not usepatchand seems to work, but it has a big flaw. If you directly replacedatetime.datetimeinstead of patching it, it will be overwritten in all other contexts after that as well! âš ï¸\n\n# Core Library modules\n```\nimport datetime\n\nfrom unittest import mock\n# First party modules\n\nfrom mock_example import generate_filename\nclass NewDate(datetime.datetime):\n\n@classmethod\n\ndef now(cls):\n\nreturn cls(1990, 4, 28)\ndef test_generate_filename():\n\ndatetime.datetime = NewDate\n\nassert generate_filename() == \"1990-04-28.png\"\n```\n\n## Mock and MagicMock\n\nYou now know how to replace a dependency, hence it is time to talk about what to replace it with. This is whereunittest.mock.Mockandunittest.mock.MagicMockcome into play.\nEverything you do with Mock will return a Mock. Call a function? Get a Mock as a return value. Access an attribute? Get a Mock as a value.\nPython has so called \"magic\" methods. I like the term \"dunder\" methods better --- it just means all methods which start and end with adoubleunderscore. Examples are__iter__or__contains__. MagicMock has those defined, Mock doesn't. I would use MagicMock everywhere, except if the mocked object doesn't define any of the magic functions.\nA core feature of mock classes is that they allow you to not only remove a dependency which is hard to test, but also to assert on the way the mock was interacted with. Typical methods are[assert_called](https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.assert_called)(),[assert_called_with](https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.assert_called_with)(),[assert_not_called](https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.assert_not_called)().\n\n**spec, autospec & spec_set**\n\nA part that is really bad aboutMagicMockis that you can do anything with it --- including accessing non-existing attributes, calling non-existing methods or calling existing methods with the wrong count of parameters. The mock object is missing aspecification. If you don't like that, useautospec=Truewhen patching the object:\n\npatch.object(Foo, 'foo', autospec=True)\nOr you can create a Mock like this:\n\n```\n>>> import datetime\n\n>>> from unittest.mock import Mock\n\n>>> a = Mock(spec=datetime)# Not Ok!\n\n>>> a.foo\n\nTraceback (most recent call last):\n\nFile \"/home/moose/.pyenv/versions/3.8.1/lib/python3.8/unittest/mock.py\", line 635, in __getattr__\n\nraise AttributeError(\"Mock object has no attribute %r\" % name)\n\nAttributeError: Mock object has no attribute 'foo'# That is ok:\n\n>>> a.datetime\n\n< Mock name='mock.datetime' id='139883597784544' >\nThe next parameter of**patch** is **autospec**. Where spec looks at the mocked object,autospecalso looks at the attributes of that object (and their attributes and those attributes, ...).\nFinally, there is**spec_set**. That one prevents you from setting attributes that don't exist.\nUsually, I would useautospec=Trueandspec_set=Trueeverywhere. Code which uses introspection might be an example where you don't want that.\n```\n\n## pytests monkeypatch\n\nmonkeypatchis a fixture from pytest. I will explain what a fixture is in the next article. For now, just accept it as a parameter you can give to your tests without specifying it and pytest will take care of it. You don't even need to import anything.\nFor the credit card fraud example, it looks like this:\n```\ndef test_is_credit_card_fraud_monkeypatch(monkeypatch):\n\nmonkeypatch.setattr(\"fraud_example.dark_magic\", the_mock)\n\nimport fraud_example\ntransaction = {\"amount_usd\": \"9999.99\", \"overnight_shipping\": True}\n\nis_fraud = fraud_example.is_credit_card_fraud(transaction)\n\nassert is_fraud == True\n```\n\nThe question when you should useunittest.mock.patchand --- if necessary ---unittest.mock.Mockor pytestsmonkeypatchboils pretty much down to personal taste nowadays. The core Pythons patch / Mock only exist since Python 3.3 which, I guess, is a big part of the reason whymonkeypatchexists in the first place.\n\n## External Packages\n\nThere are a couple of packages designed for simplifying the patching and giving better mocks for well-known dependencies.\nFor example, you can use[freezegun](https://pypi.org/project/freezegun/)for mocking the system time:\n```\nimport freezegun\n\nfrom mock_example import generate_filenamedef test_generate_filename():\n\nwith freeze_time(\"1990-04-28\"):\n\nassert generate_filename() == \"1990-04-28\"\nFor boto3 / botocore (Cloud-stuff), there is[moto](https://pypi.org/project/moto/).\nFor[requests](https://pypi.org/project/requests/), there is[responses](https://pypi.org/project/responses/):\n\nimport requests\ndef get_ip():\n\nresp = requests.get(\"http://ip.jsontest.com/\")\n\nreturn resp.json()\nif __name__ == \"__main__\":\n\nprint(get_ip())from requests_example import get_ip\n\nimport responses\n@responses.activate\n\ndef test_get_ip():\n\nresponses.add(\n\nresponses.GET,\n\n\"<http://ip.jsontest.com>\",\n\njson={\"ip\": \"123.456.789.0\"},\n\nstatus=404,\n\n)\n\nassert get_ip() == {\"ip\": \"123.456.789.0\"}**Dependency Injection**\n\nIf the above sounded complicated, there is a simpler alternative: Dependency Injection. Essentially adding the external state explicitly as a parameter which makes it easy to adjust in tests. For example, the code from above could be:\n\nimport datetime\ndef generate_filename(now=None):\n\nif now is None:\n\nnow = datetime.datetime.now()\n\nreturn f\"{now:%Y-%m-%d}.png\"\nNow testing is trivial:\n\nimport datetime\n\nfrom mock_example import generate_filename\n\ndef test_generate_filename():\n\nnow = datetime.datetime(1990, 4, 28)\n\nassert generate_filename(now) == \"1990-04-28.png\"\nIn some cases it feels very natural to apply such a pattern, in others it doesn't. Do this only when it feels natural. For example, it's very unlikely that I would ever pass a module as a parameter although it's possible. That would just feel very weird.\n```\n\n## Temporary files: Are Mocks a Code Smell?\n\nIt depends very much on the details, but I like to mock as little as possible. Simply for the reason that not mocking means that you test more of your system. Strictly speaking you can't call the test aunit testanymore if you test more than one unit. It would be an integration test then --- but that is also essential, right? You wouldn't be happy with BMW selling you a motor, some seats and a steering wheel and claiming \"all units work\". They need work together. Extensive mocks might prevent you from testing how things work together.\nIn an ideal world, you would have both: Unit tests which are very controlled and in case of failure make it easy to narrow down the source of the error. And integration / end-to-end tests which show that the complete system works.\nThere are also people who think that the need to mock is an indicator for a need to refactor ([discussion](https://github.com/pytest-dev/pytest/issues/4576#issuecomment-449865322)). Harry Percival gave the talk[Stop Using Mocks (for a while)](https://www.youtube.com/watch?v=rk-f3B-eMkI)at PyCon 2020 and pointed out that testing code which is using mocks tends to be brittle as it is tightly coupled to implementation details.\nA good example where I usually don't mock anything are file system interactions. If possible, I write the file just like it would be in the real application. When the test is finished, the test needs to clean up as well. I use the[tempfile](https://docs.python.org/3/library/tempfile.html)module for that.\n\n## Dependency Injection: Randomness\n\nJust like adding a time parameter for functions which use by default the current time might make your code way easier to test, adding arandom_stateparameter or aseedparameter to functions which use randomness helps.\nHere are some ways to seed random number generators:\n```\n>>> import random\n\n>>> random.seed(0)\n\n>>> random.random()\n\n0.8444218515250481>>> import numpy as np\n\n>>> np.random.seed(0)\n\n>>> np.random.random()\n\n0.5488135039273248>>> random_state = np.random.RandomState(seed=0)\n\n>>> random_state.random()\n\n0.5488135039273248\n```\nSetting a random state / seed is also very helpful for debugging. If you haven't heard of the Heisenbug or the Higgs-Bugson, you missed some[programming jargon](https://blog.codinghorror.com/new-programming-jargon/). And if your interested in research, reproducibility matters.\n\n## Terminology\n-   **Patching vs Mocking:** Patching a function is adjusting it's functionality. In the context of unit testing we patch a dependency away; so we replace the dependency. Mocking is imitating. Usually we patch a function to use a mock we control instead of a dependency we don't control.\n-   **Monkey patching vs Mocking:** Within a development context, mocking is pretty clearly about unit testing ([example](https://stackoverflow.com/a/2666006/562769)). However, monkey patching has several applications besides unit testing. For example, you can patch third party code during runtime if there is a small functionality missing or a part of the code is broken. You just extend the code. Monkey patching is used in the PyCharm debugger\n-   **Monkey patching vs pytest.monkeypatch:** The first one is a general concept, the second one is a concrete function within pytest which applies monkey patching for unit tests.\n-   **unittest.mock.patch vs pytest.monkeypatch:** This is personal preference. I prefer to stick with built-ins whenever the third-party option does not have big advantages. In this case, I even think that the core Python unittest.mock.patch is cleaner. For this reason I didn't explain pytest.monkeypatch so far. If you like to see the differences, there is a nice[blog post](https://krzysztofzuraw.com/blog/2016/mocks-monkeypatching-in-python.html)about it.\n**A note about Architecture**\n\nTo keep your code clean, it is often a good idea to wrap third party dependencies. For example, you could have one module with deals with I/O. Or a module which deals with API requests. Then you have a couple of modules which might require a lot of mocking or where unit tests are pointless because the interesting part is the integration with the third party. The rest of your code stays easy to test, keeps the language you defined and cares about the objects you know. This is called the[Adapter pattern](https://en.wikipedia.org/wiki/Adapter_pattern).\n**What else is there?**\n-   Other types of Mocks, such as[PropertyMock](https://docs.python.org/3/library/unittest.mock.html#unittest.mock.PropertyMock)or\n-   [pytest-mock](https://pypi.org/project/pytest-mock/)which provides the mocker fixture; I'm not really sure though if this is mainly a left-over from the time before Python 3.3 or if it actually makes things easier.\n-   The 3rd party package[mock](https://pypi.org/project/mock/), which should not be installed with Python 3.3+ as it was put in the standard library.\n**Tools**\n\n**Mock Server**\n\nFor any system you integrate with via HTTP or HTTPS MockServer can be used as:\n-   a[mock](https://www.mock-server.com/mock_server/getting_started.html)configured to return specific responses for different requests\n-   a[proxy](https://www.mock-server.com/proxy/getting_started.html)recording and optionally modifying requests and responses\n-   both a[proxy](https://www.mock-server.com/proxy/getting_started.html)for some requests and a[mock](https://www.mock-server.com/mock_server/getting_started.html)for other requests at the same time\n<https://www.mock-server.com>\n\n<https://github.com/mock-server/mockserver>\nIf you want to learn more about the default mocks, have a look at the awesome article by Yeray Diaz:[What the mock? --- A cheatsheet for mocking in Python](https://medium.com/@yeraydiazdiaz/what-the-mock-cheatsheet-mocking-in-python-6a71db997832).\n<https://levelup.gitconnected.com/unit-testing-in-python-mocking-patching-and-dependency-injection-301280db2fed>\n\n<https://martinfowler.com/articles/mocksArentStubs.html>\n<https://medium.com/@yeraydiazdiaz/what-the-mock-cheatsheet-mocking-in-python-6a71db997832>\n"},{"fields":{"slug":"/Computer-Science/Testing/Postman/","title":"Postman"},"frontmatter":{"draft":false},"rawBody":"# Postman\n\nCreated: 2020-04-08 23:59:58 +0500\n\nModified: 2022-08-10 10:47:14 +0500\n\n---\n\n**Fake API REST Mocks Server Tests**\n\n<https://reqres.in>\n\n<http://example.com>\n\n[**https://jsonplaceholder.typicode.com/**](https://jsonplaceholder.typicode.com/)\n\n<https://fakerapi.it/api/v1/persons?_quantity=1>\n\n<https://github.com/n0shake/Public-APIs>\n\n[http://slowwly.robertomurray.co.uk/delay/3000/url/http://www.google.co.uk](http://slowwly.robertomurray.co.uk/delay/3000/url/http:/www.google.co.uk)\n\n[**https://fakestoreapi.com/**](https://fakestoreapi.com/)\n\n<https://anapioficeandfire.com>\n\n**Public APIs**\n\n<https://github.com/public-apis/public-apis>\n\n<https://anapioficeandfire.com>\n\n[**https://httpdump.io/**](https://httpdump.io/)\n\n<https://github.com/typicode/json-server>\n\n<https://my-json-server.typicode.com>\n\n## API Marketplace**\n\n<https://www.programmableweb.com>\n\n<https://rapidapi.com>\n\n## Postman VScode**\n-   Thunder Client - <https://www.thunderclient.com>\n\n## Variables**\n\nVariables allow you to store and reuse values in your requests and scripts. By storing a value in a variable, you can reference it throughout your collections, environments, and requests---and if you need to update the value, you only have to change it in one place.\nPostman supports the following variable scopes:\n-   Global\n-   Collection\n-   Environment\n-   Data\n-   Local\n<https://learning.postman.com/docs/postman/variables-and-environments/variables>\n\n## Environments**\n\nEnvironments allow you to run requests and collections against different data sets. For example, you could have an environment for development, one for testing, and another for production. You can use variables to pass data between requests and tests, for example if you are chaining requests using a collection.\n**Dynamic Variables**\n\nPostman provides dynamic variables that you can use in your requests.\n\nExamples of dynamic variables are as follows:\n-   {{$guid}}: Av4 style guid\n-   {{$timestamp}}: The current timestamp (Unix timestamp in seconds)\n-   {{$randomInt}}: A random integer between 0 and 1000\n-   {{$randomPhoneNumber}}\n<https://learning.postman.com/docs/postman/variables-and-environments/variables/#using-dynamic-variables>\n\n## Runner**\n\nThe Collection Runner allows you to run sets of requests in a specified sequence. The Collection Runner will log your request test results, and your scripts can pass data between requests as well as altering the request workflow.\nYou can run collections against specific environments, and can pass data files into a run. Collection runs allow you to automate your API testing, and you can schedule runs using monitors. You can integrate collection runs to your CI/CD pipeline using Postman's CLI Newman.\n<https://learning.postman.com/docs/running-collections/intro-to-collection-runs>\n\n## Mock Server**\n**Monitoring**\n<https://medium.com/aubergine-solutions/api-testing-using-postman-323670c89f6d>\n\n<https://www.guru99.com/api-testing.html>\n\n## Newman**\n\nNewman is a command line Collection Runner for Postman. It allows you to run and test a Postman Collection directly from the command line. It is built with extensibility in mind so that you can easily integrate it with your continuous integration servers and build systems.\n<https://learning.postman.com/docs/running-collections/using-newman-cli/command-line-integration-with-newman>\n\n## Pre-Request Script**\n\nvarkey**=**'6aeb53996e29f845ae6b52dbda150df5fb03b981da8eaac63e3faa327fdce343';\n\nvarfinalstring**=**'';\n\n**for**(vari**=**0;i**<**pm.request.body.urlencoded.toJSON().length;i**++**){\n\n**if**(pm.request.body.urlencoded.toJSON()[i].key**!=**'checksum'){\n\n**if**(i**!=**0){\n\nfinalstring**+=**'&';\n\n}\n\nfinalstring**+=**pm.request.body.urlencoded.toJSON()[i].key**+**'='**+**pm.request.body.urlencoded.toJSON()[i].value;\n\n}\n\n}\n\nfinalstring**=**finalstring;\n\nvarsignature**=**CryptoJS.HmacSHA256(finalstring,key).**toString**();\n\npostman.setEnvironmentVariable(\"checksum\",signature);\n\npm.variables.**get**(\"variable_key\");\n**Shortcuts**\n\nCMD + / - show all shortcuts\n\nCMD + ? - show all shortcuts\n\nCMD +  - show/hide sidebar\n**OAuth 2.0 - Authentication and Authorization with Identity Server (KeyCloak)**\n\n![](media/Postman-image1.png)\n**Alternatives**\n\nHoppscotch - <https://github.com/hoppscotch/hoppscotch>\n\n<https://hypertest.co>\n\n## Others**\n\n<https://www.freecodecamp.org/news/what-is-an-api-and-how-to-test-it>\n\n[Testing your APIs with Postman and Newman](https://www.youtube.com/watch?v=fTtA9qXkNAk)\n\n[Testing Your APIs: Postman, Newman & Beyond, Mike Amundsen | Postman Galaxy 2021](https://www.youtube.com/watch?v=DGxvFSy-i78&ab_channel=Postman)\n\n"},{"fields":{"slug":"/Computer-Science/Testing/Terms/","title":"Terms"},"frontmatter":{"draft":false},"rawBody":"# Terms\n\nCreated: 2018-08-02 23:57:00 +0500\n\nModified: 2021-08-08 09:21:53 +0500\n\n---\n\n# A\n-   [Acceptance Testing](https://www.tutorialspoint.com/software_testing_dictionary/acceptance_testing.htm)\n-   [Accessibility Testing](https://www.tutorialspoint.com/software_testing_dictionary/accessibility_testing.htm)\n-   [Active Testing](https://www.tutorialspoint.com/software_testing_dictionary/active_testing.htm)\n-   [Actual Outcome](https://www.tutorialspoint.com/software_testing_dictionary/actual_outcome.htm)\n-   [Ad Hoc Testing](https://www.tutorialspoint.com/software_testing_dictionary/adhoc_testing.htm)\n-   [Age Testing](https://www.tutorialspoint.com/software_testing_dictionary/age_testing.htm)\n-   [Agile Testing](https://www.tutorialspoint.com/software_testing_dictionary/agile_testing.htm)\n-   [All-pairs Testing](https://www.tutorialspoint.com/software_testing_dictionary/all_pairs_testing.htm)\n-   [Alpha Testing](https://www.tutorialspoint.com/software_testing_dictionary/alpha_testing.htm)\n-   [API Testing](https://www.tutorialspoint.com/software_testing_dictionary/api_testing.htm)\n-   [Arc Testing](https://www.tutorialspoint.com/software_testing_dictionary/arc_testing.htm)\n-   [Anomaly](https://www.tutorialspoint.com/software_testing_dictionary/anomaly.htm)\n-   [Assertion Testing](https://www.tutorialspoint.com/software_testing_dictionary/assertion_testing.htm)\n-   [Audit](https://www.tutorialspoint.com/software_testing_dictionary/audit.htm)\n-   [Automated Software Testing](https://www.tutorialspoint.com/software_testing_dictionary/automated_software_testing.htm)\n# B\n-   [Backward Compatibility Testing](https://www.tutorialspoint.com/software_testing_dictionary/backward_compatibility_testing.htm)\n-   [Baseline Artifacts](https://www.tutorialspoint.com/software_testing_dictionary/baseline_artifacts.htm)\n-   [Basis Path Testing](https://www.tutorialspoint.com/software_testing_dictionary/basis_path_testing.htm)\n-   [Basis Test Set](https://www.tutorialspoint.com/software_testing_dictionary/basis_test_set.htm)\n-   [Bebugging](https://www.tutorialspoint.com/software_testing_dictionary/bebugging.htm)\n-   [Behavior Testing](https://www.tutorialspoint.com/software_testing_dictionary/behaviour_testing.htm)\n-   [Benchmark Testing](https://www.tutorialspoint.com/software_testing_dictionary/benchmark_testing.htm)\n-   [Beta Testing](https://www.tutorialspoint.com/software_testing_dictionary/beta_testing.htm)\n-   [Big-Bang Testing](https://www.tutorialspoint.com/software_testing_dictionary/big_bang_testing.htm)\n-   [Binary Portability Testing](https://www.tutorialspoint.com/software_testing_dictionary/binary_portability_testing.htm)\n-   [Black box Testing](https://www.tutorialspoint.com/software_testing_dictionary/black_box_testing.htm)\n-   [Bottom Up Testing](https://www.tutorialspoint.com/software_testing_dictionary/bottom_up_testing.htm)\n-   [Boundary Testing](https://www.tutorialspoint.com/software_testing_dictionary/boundary_testing.htm)\n-   [Branch Testing](https://www.tutorialspoint.com/software_testing_dictionary/branch_testing.htm)\n-   [Breadth Testing](https://www.tutorialspoint.com/software_testing_dictionary/breadth_testing.htm)\n-   [Bug](https://www.tutorialspoint.com/software_testing_dictionary/bug.htm)\n-   [Build Validation](https://www.tutorialspoint.com/software_testing_dictionary/build_validation.htm)\n-   [Business Process](https://www.tutorialspoint.com/software_testing_dictionary/business_process.htm)\n-   [Business Requirement](https://www.tutorialspoint.com/software_testing_dictionary/business_requirement.htm)\n\n# \n\n# C\n-   [Capability Maturity Model](https://www.tutorialspoint.com/software_testing_dictionary/capability_maturity_model.htm)\n-   [Capture/Replay Tool](https://www.tutorialspoint.com/software_testing_dictionary/capture_replay_tool.htm)\n-   [Cause-Effect Graph](https://www.tutorialspoint.com/software_testing_dictionary/cause_effect_graph.htm)\n-   [Code Coverage](https://www.tutorialspoint.com/software_testing_dictionary/code_coverage.htm)\n-   [Code Freeze](https://www.tutorialspoint.com/software_testing_dictionary/code_freeze.htm)\n-   [Code Inspection](https://www.tutorialspoint.com/software_testing_dictionary/code_inspection.htm)\n-   [Code Review](https://www.tutorialspoint.com/software_testing_dictionary/code_review.htm)\n-   [Code Walkthrough](https://www.tutorialspoint.com/software_testing_dictionary/code_walkthrough.htm)\n-   [Code-Based Testing](https://www.tutorialspoint.com/software_testing_dictionary/code_based_testing.htm)\n-   [Code Driven Testing](https://www.tutorialspoint.com/software_testing_dictionary/code_driven_testing.htm)\n-   [Code Free Testing](https://www.tutorialspoint.com/software_testing_dictionary/code_free_testing.htm)\n-   [Comparison Testing](https://www.tutorialspoint.com/software_testing_dictionary/comparison_testing.htm)\n-   [Compatibility Testing](https://www.tutorialspoint.com/software_testing_dictionary/compatibility_testing.htm)\n-   [Compliance Testing](https://www.tutorialspoint.com/software_testing_dictionary/compliance_testing.htm)\n-   [Concurrency Testing](https://www.tutorialspoint.com/software_testing_dictionary/concurrency_testing.htm)\n-   [Condition Coverage Testing](https://www.tutorialspoint.com/software_testing_dictionary/condition_coverage_testing.htm)\n-   [Configuration Testing](https://www.tutorialspoint.com/software_testing_dictionary/configuration_testing.htm)\n-   [Conformance Testing](https://www.tutorialspoint.com/software_testing_dictionary/conformance_testing.htm)\n-   [Context Driven Testing](https://www.tutorialspoint.com/software_testing_dictionary/context_driven_tesing.htm)\n-   [Control Flow Path](https://www.tutorialspoint.com/software_testing_dictionary/control_flow_path.htm)\n-   [Conversion Testing](https://www.tutorialspoint.com/software_testing_dictionary/conversion_testing.htm)\n-   [Correctness](https://www.tutorialspoint.com/software_testing_dictionary/correctness.htm)\n-   [Coverage Items](https://www.tutorialspoint.com/software_testing_dictionary/coverage_items.htm)\n-   [Cyclomatic Complexity](https://www.tutorialspoint.com/software_testing_dictionary/cyclomatic_complexity.htm)\n# D\n-   [Data Integrity Testing](https://www.tutorialspoint.com/software_testing_dictionary/data_and_database_integrity_testing.htm)\n-   [Data Driven Testing](https://www.tutorialspoint.com/software_testing_dictionary/data_driven_testing.htm)\n-   [Data Flow Testing](https://www.tutorialspoint.com/software_testing_dictionary/data_flow_testing.htm)\n-   [Database Testing](https://www.tutorialspoint.com/software_testing_dictionary/database_testing.htm)\n-   [Debugging](https://www.tutorialspoint.com/software_testing_dictionary/debugging.htm)\n-   [Decision Coverage Testing](https://www.tutorialspoint.com/software_testing_dictionary/decision_coverage_testing.htm)\n-   [Defect](https://www.tutorialspoint.com/software_testing_dictionary/defect.htm)\n-   [Defect Logging and Tracking](https://www.tutorialspoint.com/software_testing_dictionary/defect_logging_and_tracking.htm)\n-   [Defect Life Cycle](https://www.tutorialspoint.com/software_testing_dictionary/defect_life_cycle.htm)\n-   [Delta Release](https://www.tutorialspoint.com/software_testing_dictionary/delta_release.htm)\n-   [Dependency Testing](https://www.tutorialspoint.com/software_testing_dictionary/dependency_testing.htm)\n-   [Depth Testing](https://www.tutorialspoint.com/software_testing_dictionary/depth_testing.htm)\n-   [Destructive Testing](https://www.tutorialspoint.com/software_testing_dictionary/destructive_testing.htm)\n-   [Development Environment](https://www.tutorialspoint.com/software_testing_dictionary/development_environment.htm)\n-   [Documentation Testing](https://www.tutorialspoint.com/software_testing_dictionary/documentation_testing.htm)\n-   [Domain Testing](https://www.tutorialspoint.com/software_testing_dictionary/domain_testing.htm)\n-   [Durability Testing](https://www.tutorialspoint.com/software_testing_dictionary/durability_testing.htm)\n-   [Dynamic Testing](https://www.tutorialspoint.com/software_testing_dictionary/dynamic_testing.htm)\n# E\n-   [Emulator](https://www.tutorialspoint.com/software_testing_dictionary/emulator.htm)\n-   [End-to-End Testing](https://www.tutorialspoint.com/software_testing_dictionary/end_to_end_testing.htm)\n-   [Endurance Testing](https://www.tutorialspoint.com/software_testing_dictionary/endurance_testing.htm)\n-   [Entry Criteria](https://www.tutorialspoint.com/software_testing_dictionary/entry_criteria.htm)\n-   [Equivalence Partitioning Testing](https://www.tutorialspoint.com/software_testing_dictionary/equivalence_partitioning_testing.htm)\n-   [Error](https://www.tutorialspoint.com/software_testing_dictionary/error.htm)\n-   [Error Guessing](https://www.tutorialspoint.com/software_testing_dictionary/error_guessing.htm)\n-   [Error Seeding](https://www.tutorialspoint.com/software_testing_dictionary/error_seeding.htm)\n-   [Exhaustive Testing](https://www.tutorialspoint.com/software_testing_dictionary/exhaustive_testing.htm)\n-   [Exit Criteria](https://www.tutorialspoint.com/software_testing_dictionary/exit_criteria.htm)\n-   [Expected Outcome](https://www.tutorialspoint.com/software_testing_dictionary/expected_outcome.htm)\n-   [**Exploratory Testing**](https://www.tutorialspoint.com/software_testing_dictionary/exploratory_testing.htm)\n\nExploratory testing is a form of testing that is done without a plan. In an exploratory test, you're just exploring the application.\nTo have a complete set of manual tests, all you need to do is make a list of all the features your application has, the different types of input it can accept, and the expected results. Now, every time you make a change to your code, you need to go through every single item on that list and check it.\n# F\n-   [Failover Testing](https://www.tutorialspoint.com/software_testing_dictionary/failover_testing.htm)\n-   [Failure](https://www.tutorialspoint.com/software_testing_dictionary/failure.htm)\n-   [Fault](https://www.tutorialspoint.com/software_testing_dictionary/fault.htm)\n-   [Fault injection Testing](https://www.tutorialspoint.com/software_testing_dictionary/fault_injection_testing.htm)\n-   [Feasible Path](https://www.tutorialspoint.com/software_testing_dictionary/feasible_path.htm)\n-   [Feature Testing](https://www.tutorialspoint.com/software_testing_dictionary/feature_testing.htm)\n-   [Functional Decomposition](https://www.tutorialspoint.com/software_testing_dictionary/functional_decomposition.htm)\n-   [Functional Requirements](https://www.tutorialspoint.com/software_testing_dictionary/functional_requirements.htm)\n-   [Functional Testing](https://www.tutorialspoint.com/software_testing_dictionary/functional_testing.htm)\n-   [Fuzz Testing](https://www.tutorialspoint.com/software_testing_dictionary/fuzz_testing.htm)\n\nFuzz testing is a software testing technique using which a random data is given as the inputs to the system. If the application fails, then those issues/defects are to be addressed by the system. In short, unexpected or random inputs might lead to unexpected results\n**Attack Types**\n-   Number/Character Fuzzing\n-   Application Fuzzing\n-   Protocol Fuzzing\n-   File Format Fuzzing\n<https://github.com/minimaxir/big-list-of-naughty-strings>\n\n<https://www.freecodecamp.org/news/whats-fuzzing-fuzz-testing-explained>\n# G\n-   [Glass Box Testing](https://www.tutorialspoint.com/software_testing_dictionary/glass_box_testing.htm)\n-   [Globalization Testing](https://www.tutorialspoint.com/software_testing_dictionary/globalization_testing.htm)\n-   [Gorilla Testing](https://www.tutorialspoint.com/software_testing_dictionary/gorilla_testing.htm)\n-   [Grey Box Testing](https://www.tutorialspoint.com/software_testing_dictionary/grey_box_testing.htm)\n-   [GUI Software Testing](https://www.tutorialspoint.com/software_testing_dictionary/gui_software_testing.htm)\n# H\n-   [Harness](https://www.tutorialspoint.com/software_testing_dictionary/harness.htm)\n-   [Heuristics](https://www.tutorialspoint.com/software_testing_dictionary/heuristics.htm)\n-   [Hybrid Integration Testing](https://www.tutorialspoint.com/software_testing_dictionary/hybrid_integration_testing.htm)\n# I\n-   [Implementation Testing](https://www.tutorialspoint.com/software_testing_dictionary/implementation_testing.htm)\n-   [Incremental Testing](https://www.tutorialspoint.com/software_testing_dictionary/incremental_testing.htm)\n-   [Independent testing](https://www.tutorialspoint.com/software_testing_dictionary/independent_testing.htm)\n-   [Infeasible Path](https://www.tutorialspoint.com/software_testing_dictionary/infeasible_path.htm)\n-   [Inspection](https://www.tutorialspoint.com/software_testing_dictionary/inspection.htm)\n-   [Install/Uninstall Testing](https://www.tutorialspoint.com/software_testing_dictionary/install_uninstall_testing.htm)\n-   [Integration Testing](https://www.tutorialspoint.com/software_testing_dictionary/integration_testing.htm)\n-   [Interface Testing](https://www.tutorialspoint.com/software_testing_dictionary/interface_testing.htm)\n-   [Internationalization Testing](https://www.tutorialspoint.com/software_testing_dictionary/internationalization_testing.htm)\n-   [Inter Systems Testing](https://www.tutorialspoint.com/software_testing_dictionary/inter_systems_testing.htm)\n-   [Isolation Testing](https://www.tutorialspoint.com/software_testing_dictionary/isolation_testing.htm)\n-   [Issues](https://www.tutorialspoint.com/software_testing_dictionary/issues.htm)\n# K\n-   [Keyword Driven Testing](https://www.tutorialspoint.com/software_testing_dictionary/keyword_driven_testing.htm)\n-   [Key Performance Indicator](https://www.tutorialspoint.com/software_testing_dictionary/key_performance_indicator.htm)\n-   [Known Issues](https://www.tutorialspoint.com/software_testing_dictionary/known_issues.htm)\n# L\n-   [LCSAJ Testing](https://www.tutorialspoint.com/software_testing_dictionary/lcsaj_testing.htm)\n-   [Load Generator](https://www.tutorialspoint.com/software_testing_dictionary/load_generator.htm)\n-   [Load Testing](https://www.tutorialspoint.com/software_testing_dictionary/load_testing.htm)\n-   [Localization Testing](https://www.tutorialspoint.com/software_testing_dictionary/localization_testing.htm)\n-   [Logic Coverage Test](https://www.tutorialspoint.com/software_testing_dictionary/logic_coverage_test.htm)\n-   [Loop Testing](https://www.tutorialspoint.com/software_testing_dictionary/loop_testing.htm)\n# M\n-   [Maintainability](https://www.tutorialspoint.com/software_testing_dictionary/maintainability.htm)\n-   [Manual Testing](https://www.tutorialspoint.com/software_testing_dictionary/manual_testing.htm)\n-   [Model Based Testing](https://www.tutorialspoint.com/software_testing_dictionary/model_based_testing.htm)\n-   [Modified Condition Testing](https://www.tutorialspoint.com/software_testing_dictionary/modified_condition_coverage.htm)\n-   [Modularity Driven Testing](https://www.tutorialspoint.com/software_testing_dictionary/modularity_driven_testing.htm)\n-   [Monkey Testing](https://www.tutorialspoint.com/software_testing_dictionary/monkey_testing.htm)\n-   [**Mutation Testing**](https://www.tutorialspoint.com/software_testing_dictionary/mutation_testing.htm)\n\n<https://medium.com/analytics-vidhya/unit-testing-in-python-mutation-testing-7a70143180d8>\n# N\n-   [Negative Testing](https://www.tutorialspoint.com/software_testing_dictionary/negative_testing.htm)\n-   [Non-functional Testing](https://www.tutorialspoint.com/software_testing_dictionary/non_functional_testing.htm)\n# O\n-   [Operational Testing](https://www.tutorialspoint.com/software_testing_dictionary/operational_testing.htm)\n-   [Orthogonal Array Testing](https://www.tutorialspoint.com/software_testing_dictionary/orthogonal_array_testing.htm)\n# P\n-   [Pair Testing](https://www.tutorialspoint.com/software_testing_dictionary/pair_testing.htm)\n-   [Pairwise Testing](https://www.tutorialspoint.com/software_testing_dictionary/pairwise_testing.htm)\n-   [Parallel Testing](https://www.tutorialspoint.com/software_testing_dictionary/parallel_testing.htm)\n-   [Partial Test Automation](https://www.tutorialspoint.com/software_testing_dictionary/partial_test_automation_testing.htm)\n-   [Passive Testing](https://www.tutorialspoint.com/software_testing_dictionary/passive_testing.htm)\n-   [Path Testing](https://www.tutorialspoint.com/software_testing_dictionary/path_testing.htm)\n-   [Peer Review](https://www.tutorialspoint.com/software_testing_dictionary/peer_review.htm)\n-   [Penetration Testing](https://www.tutorialspoint.com/software_testing_dictionary/penetration_testing.htm) / pentest\n\n\"Pentest\" is short for \"penetration test\", and involves having a trusted security expert attack a system for the purpose of discovering, and repairing, security vulnerabilities before malicious attackers can exploit them. This is a critical procedure for securing a system, as the alternative method for discovering vulnerabilities is to wait for unknown agents to exploit them. By this time it is, of course, too late to do anything about them.\nIn order to keep a system secure, it is advisable to conduct a pentest on a regular basis, especially when new technology is added to the stack, or vulnerabilities are exposed in your current stack.-   [Performance Testing](https://www.tutorialspoint.com/software_testing_dictionary/performance_testing.htm)\n-   [Portability Testing](https://www.tutorialspoint.com/software_testing_dictionary/portability_testing.htm)\n-   [Positive Testing](https://www.tutorialspoint.com/software_testing_dictionary/positive_testing.htm)\n-   [Post Condition](https://www.tutorialspoint.com/software_testing_dictionary/post_condition.htm)\n-   [Precondition](https://www.tutorialspoint.com/software_testing_dictionary/pre_condition.htm)\n-   [Predicted Outcome](https://www.tutorialspoint.com/software_testing_dictionary/predicted_outcome.htm)\n-   [Priority](https://www.tutorialspoint.com/software_testing_dictionary/priority.htm)\n-   [Process Cycle Test](https://www.tutorialspoint.com/software_testing_dictionary/process_cycle_test.htm)\n-   [Progressive Testing](https://www.tutorialspoint.com/software_testing_dictionary/progressive_testing.htm)\n-   [Prototype Testing](https://www.tutorialspoint.com/software_testing_dictionary/prototype_testing.htm)\n# Q\n-   [Quality Assurance](https://www.tutorialspoint.com/software_testing_dictionary/quality_assurance.htm)\n    -   How to hire a Great QA Engineer - <https://www.toptal.com/qa>\n-   [Quality Control](https://www.tutorialspoint.com/software_testing_dictionary/quality_control.htm)\n-   [Quality Management](https://www.tutorialspoint.com/software_testing_dictionary/quality_management.htm)\n# R\n-   [Random Testing](https://www.tutorialspoint.com/software_testing_dictionary/random_testing.htm)\n-   [Recovery Testing](https://www.tutorialspoint.com/software_testing_dictionary/recovery_testing.htm)\n-   [Release Candidate](https://www.tutorialspoint.com/software_testing_dictionary/release_candidate.htm)\n-   [Release Note](https://www.tutorialspoint.com/software_testing_dictionary/release_note.htm)\n-   [Reliability Testing](https://www.tutorialspoint.com/software_testing_dictionary/reliability_testing.htm)\n-   [Requirements](https://www.tutorialspoint.com/software_testing_dictionary/requirements.htm)\n-   [Requirements Based Testing](https://www.tutorialspoint.com/software_testing_dictionary/requirements_based_testing.htm)\n-   [Requirements Traceability Matrix](https://www.tutorialspoint.com/software_testing_dictionary/requirements_traceability_matrix.htm)\n-   [Result](https://www.tutorialspoint.com/software_testing_dictionary/result.htm)\n-   [Retesting](https://www.tutorialspoint.com/software_testing_dictionary/retesting.htm)\n-   [Review](https://www.tutorialspoint.com/software_testing_dictionary/review.htm)\n-   [Risk](https://www.tutorialspoint.com/software_testing_dictionary/risk.htm)\n-   [Risk Management](https://www.tutorialspoint.com/software_testing_dictionary/risk_management.htm)\n-   [Root Cause](https://www.tutorialspoint.com/software_testing_dictionary/root_cause.htm)\n# S\n-   [Safety Testing](https://www.tutorialspoint.com/software_testing_dictionary/safety_testing.htm)\n-   [Sanity Testing](https://www.tutorialspoint.com/software_testing_dictionary/sanity_testing.htm)\n-   [Scalability Testing](https://www.tutorialspoint.com/software_testing_dictionary/scalability_testing.htm)\n-   [Scenario Testing](https://www.tutorialspoint.com/software_testing_dictionary/scenario_testing.htm)\n-   [Schedule](https://www.tutorialspoint.com/software_testing_dictionary/schedule.htm)\n-   [Script](https://www.tutorialspoint.com/software_testing_dictionary/script.htm)\n-   [Security Testing](https://www.tutorialspoint.com/software_testing_dictionary/security_testing.htm)\n-   [Simulation](https://www.tutorialspoint.com/software_testing_dictionary/simulation.htm)\n-   [Smoke Testing](https://www.tutorialspoint.com/software_testing_dictionary/smoke_testing.htm)\n    -   They're used as a sanity check that your site's core functionality isn't wrecked\n-   [Soak Testing](https://www.tutorialspoint.com/software_testing_dictionary/soak_testing.htm)\n-   [Software Requirement Specification](https://www.tutorialspoint.com/software_testing_dictionary/software_requirement_specification.htm)\n-   [Stability Testing](https://www.tutorialspoint.com/software_testing_dictionary/stability_testing.htm)\n-   [State Transition](https://www.tutorialspoint.com/software_testing_dictionary/state_transition.htm)\n-   [Static Testing](https://www.tutorialspoint.com/software_testing_dictionary/static_testing.htm)\n-   [Statistical Testing](https://www.tutorialspoint.com/software_testing_dictionary/statistical_testing.htm)\n-   [Storage Testing](https://www.tutorialspoint.com/software_testing_dictionary/storage_testing.htm)\n-   [Stress Testing](https://www.tutorialspoint.com/software_testing_dictionary/stress_testing.htm)\n-   [Structural Testing](https://www.tutorialspoint.com/software_testing_dictionary/structural_testing.htm)\n-   [Structured Walkthrough](https://www.tutorialspoint.com/software_testing_dictionary/structured_walkthrough.htm)\n-   [Stub](https://www.tutorialspoint.com/software_testing_dictionary/stub.htm)\n-   [Symbolic Execution](https://www.tutorialspoint.com/software_testing_dictionary/symbolic_execution.htm)\n-   [Syntax Testing](https://www.tutorialspoint.com/software_testing_dictionary/syntax_testing.htm)\n-   [System Integration Testing](https://www.tutorialspoint.com/software_testing_dictionary/system_Integration_testing.htm)\n-   [System Testing](https://www.tutorialspoint.com/software_testing_dictionary/system_testing.htm)\n-   [System Under Test](https://www.tutorialspoint.com/software_testing_dictionary/system_under_test.htm)\n# T\n-   [Technical Review](https://www.tutorialspoint.com/software_testing_dictionary/technical_review.htm)\n-   [Test Approach](https://www.tutorialspoint.com/software_testing_dictionary/test_approach.htm)\n-   [Test Automation](https://www.tutorialspoint.com/software_testing_dictionary/test_automation.htm)\n-   [Test Basis](https://www.tutorialspoint.com/software_testing_dictionary/test_basis.htm)\n-   [Test Bed](https://www.tutorialspoint.com/software_testing_dictionary/test_bed.htm)\n-   [Test Case](https://www.tutorialspoint.com/software_testing_dictionary/test_case.htm)\n-   [Test Case Design Technique](https://www.tutorialspoint.com/software_testing_dictionary/test_case_design_technique.htm)\n-   [Test Suite](https://www.tutorialspoint.com/software_testing_dictionary/test_suite.htm)\n-   [Test Completion Criterion](https://www.tutorialspoint.com/software_testing_dictionary/test_completion_criterion.htm)\n-   [Test Completion Report](https://www.tutorialspoint.com/software_testing_dictionary/test_completion_report.htm)\n-   [Test Completion Matrix](https://www.tutorialspoint.com/software_testing_dictionary/test_completion_matrix.htm)\n-   [Test Data](https://www.tutorialspoint.com/software_testing_dictionary/test_data.htm)\n-   [Test Data Management](https://www.tutorialspoint.com/software_testing_dictionary/test_data_management.htm)\n-   [Test Driven Development](https://www.tutorialspoint.com/software_testing_dictionary/test_driven_development.htm)\n-   [Test Driver](https://www.tutorialspoint.com/software_testing_dictionary/test_driver.htm)\n-   [Test Environment](https://www.tutorialspoint.com/software_testing_dictionary/test_environment.htm)\n-   [Test Execution](https://www.tutorialspoint.com/software_testing_dictionary/test_execution.htm)\n-   [Test Management](https://www.tutorialspoint.com/software_testing_dictionary/test_management.htm)\n-   [Test Maturity Model](https://www.tutorialspoint.com/software_testing_dictionary/test_maturity_model.htm)\n-   [Test Plan](https://www.tutorialspoint.com/software_testing_dictionary/test_plan.htm)\n-   [Test Steps](https://www.tutorialspoint.com/software_testing_dictionary/test_steps.htm)\n-   [Test Strategy](https://www.tutorialspoint.com/software_testing_dictionary/test_strategy.htm)\n-   [Test Tools](https://www.tutorialspoint.com/software_testing_dictionary/test_tools.htm)\n-   [Thread Testing](https://www.tutorialspoint.com/software_testing_dictionary/thread_testing.htm)\n-   [Top Down Integ. Testing](https://www.tutorialspoint.com/software_testing_dictionary/top_down_integration_testing.htm)\n-   [Total Quality Management](https://www.tutorialspoint.com/software_testing_dictionary/total_quality_management.htm)\n-   [Traceability](https://www.tutorialspoint.com/software_testing_dictionary/traceability.htm)\n# U\n-   [Unit Testing](https://www.tutorialspoint.com/software_testing_dictionary/unit_testing.htm)\n-   [Unreachable Code](https://www.tutorialspoint.com/software_testing_dictionary/unreachable_code.htm)\n-   [Usability Testing](https://www.tutorialspoint.com/software_testing_dictionary/usability_testing.htm)\n-   [Use Case Testing](https://www.tutorialspoint.com/software_testing_dictionary/use_case_testing.htm)\n-   [User Acceptance Testing](https://www.tutorialspoint.com/software_testing_dictionary/use_acceptance_testing.htm)\n-   [User Interface Testing](https://www.tutorialspoint.com/software_testing_dictionary/use_interface_testing.htm)\n# V\n-   [V Model](https://www.tutorialspoint.com/software_testing_dictionary/v_model.htm)\n-   [Validation Testing](https://www.tutorialspoint.com/software_testing_dictionary/validation_testing.htm)\n-   [Verification Testing](https://www.tutorialspoint.com/software_testing_dictionary/verification_testing.htm)\n-   [Virtual Users](https://www.tutorialspoint.com/software_testing_dictionary/virtual_users.htm)\n-   [Volume Testing](https://www.tutorialspoint.com/software_testing_dictionary/volume_testing.htm)\n-   [Vulnerability Testing](https://www.tutorialspoint.com/software_testing_dictionary/vulnerability_testing.htm)\n# W\n-   [Web Application Testing](https://www.tutorialspoint.com/software_testing_dictionary/web_application_testing.htm)\n-   [White box Testing](https://www.tutorialspoint.com/software_testing_dictionary/white_box_testing.htm)\n-   [Workflow Testing](https://www.tutorialspoint.com/software_testing_dictionary/workflow_testing.htm)\n<https://dev.to/maxwell_dev/the-testing-introduction-i-wish-i-had-2dn>\n\n<https://dev.to/conw_y/towards-zero-bugs-1bop>\n\n## Testing terms**\n-   Code Freeze\n-   Code Inspection\n-   Code Review\n-   Code Walkthrough\n-   Code Based Testing\n-   Code Driven Testing\n-   Code Free Testing\n-   Comparision Testing\n-   Combatibility Testing\n-   Compliance Testing\n-   Concurrency Testing\n-   Configuration Testing\n-   Conformance Testing\n-   Context Driven Testing\n-   Control Flow Path\n-   Conversion Testing\n<https://www.tutorialspoint.com/software_testing_dictionary/index.htm>\n"},{"fields":{"slug":"/Computer-Science/Testing/Test-Pyramid/","title":"Test Pyramid"},"frontmatter":{"draft":false},"rawBody":"# Test Pyramid\n\nCreated: 2019-03-19 16:01:01 +0500\n\nModified: 2021-05-15 23:50:40 +0500\n\n---\n\n**Typical Test Pyramid**\n\n**Unit tests**\n\nTests that cover the smallest piece of testable functionality in your software.\nUnit testing the process of testing an atomic function by passing known values into that function and asserting an expected result is produced by the function.\n**Integration tests**\n\nIntegration tests, in this context, deal with testing integrations and interface defects for components within your service; these are more granular tests.\n**Component tests**\n\nWhen you look at component tests for microservices, a component is a service that exposes certain functionalities. Therefore, component tests for microservice can just be acceptance tests for services and your tests need to validate whether the service provides the functionality that it promises to.\n**Contract tests**\n\nAnother category of tests that's very applicable to microservices are contract tests. They test the contracts of APIs of your services to see if the API is valid or if the microservice honors its API. A cool variation of these contract tests is consumer driven contract tests. These tests are written by consumer services of an API; the consumers codify this contract in a suite of tests that get run on every change to the API. That way, if a change to the API breaks a contract that one of its consumers expect, this breaking change is caught early in the CD pipeline.\n**End-to-end tests**\n\nThe test suites we discussed earlier are applicable to testing individual services. End-to-end tests, however, are more coarse-grained and try to test the functionality of an overall system. Depending on the deployment architecture you're going for, if you are deploying all of your services in a pre-production environment in an aggregate manner, you can run end-to-end tests there. Since end-to-end tests are usually brittle and take a long time to run, you'll usually want to restrict the number of these tests to as few as possible. If you have microservices that are completely independent and don't get deployed to a pre-production test environment, then consider approaches that test in production.\n![E2E CONTRACT TESTS COMPONENT TESTS INTEGRATION TESTS UNIT TESTS ](media/Test-Pyramid-image1.png)\n**Fault Injection**\n\nIntroducing errors in a controlled manner in production to see if your system can hold up to those errors.\n**Multivariate testing**\n\nAnother interesting variation of this kind of testing is multivariate testing, where you're not really testing your new service against defects, instead, you are A/B testing new release features behind A/B testing toggles. The purpose of this type of testing is to see how well these features are received. You can decide roll it out to your entire set of users or make fixes where necessary.\n<https://martinfowler.com/articles/practical-test-pyramid.html>\n\n## Automated vs. Manual Testing**\n\n**Exploratory testing (Manual Testing)** is a form of testing that is done without a plan. In an exploratory test, you're just exploring the application.\nThis is where **automated testing** comes in. Automated testing is the execution of your test plan (the parts of your application you want to test, the order in which you want to test them, and the expected responses) by a script instead of a human. Python already comes with a set of tools and libraries to help you create automated tests for your application. We'll explore those tools and libraries in this tutorial.\n**Unit vs Integration Tests**\n\nThink of how you might test the lights on a car. You would turn on the lights (known as thetest step) and go outside the car or ask a friend to check that the lights are on (known as thetest assertion). Testing multiple components is known asintegration testing.\nA major challenge with integration testing is when an integration test doesn't give the right result. It's very hard to diagnose the issue without being able to isolate which part of the system is failing. If the lights didn't turn on, then maybe the bulbs are broken. Is the battery dead? What about the alternator? Is the car's computer failing?\nIf you have a fancy modern car, it will tell you when your light bulbs have gone. It does this using a form of**unit test.**\nA unit test is a smaller test, one that checks that a single component operates in the right way. A unit test helps you to isolate what is broken in your application and fix it faster.\nYou have just seen two types of tests:\n\n1.  An integration test checks that components in your application operate with each other.\n\n2.  A unit test checks a small component in your application.\n**Application Security Testing**\n\nSecurity testing helps you identify application vulnerabilities that could be exploited by hackers and correct them before you release your product or app.\nThere are a[range of application security tests](https://securityboulevard.com/2020/03/application-security-testing-trends-in-2020/)available to you with different tests that are applicable at different parts of the software development life cycle.\n**Static Application Security Testing (SAST)**\n\nSAST analyzes the code itself rather than the final application, and you can run it without actually executing the code.\n\n![05. Reporting Detailed report on critical vulnerabilities along with remediation guidelines 04. Analysis & Verification Manual Triage of code security flaws to identify exploitable security critical vulnerabilities after eliminating false positives. Static Application Security Testing (SAST) 01. Information Gathering Analyze application tech stack (languages and frameworks), core security critical functionalities and the build process 02. Preparation and compilation Configure application source code and required dependencies for SCA build process. 03. Source Code Vulnerability Scanning Run automated code scan through build integrated process or offline scans on your application code base ](media/Test-Pyramid-image2.jpeg)\n\n**Dynamic Application Security Testing (DAST)**\n\nRuns on fully compiled application. You design and run these tests without any knowledge of the underlying structures or code.\nBecause DAST applies the hacker's perspective, it is known as black box, or outside in, testing.\nDAST operates by attacking the running code and seeking to exploit potential vulnerabilities. DAST may employ such common attack techniques as cross-site scripting and SQL injection.\n**Interactive Application Security Testing**\n\nInteractive application security testing (IAST) is a newer testing methodology that[combines the effectiveness of SAST and DAST](https://developer.ibm.com/recipes/tutorials/what-is-interactive-application-security-testing/)while overcoming the issues associated with these more established tests.\nIAST conducts continuous real-time scanning of an application for errors and vulnerabilities using an inserted monitoring agent. Even though IAST operates in a running application, it is considered an early SDLC test process.\n**Compatibility Testing**\n\nCompatibility testing assesses how your application operates and how secure it is on various devices and environments, including mobile devices and on different operating systems.\nCompatibility testing can also assess whether a current version of software is compatible with other software versions. Version testing can be backward or forward facing.\n\n![Devices Software Browser Compatibility Testing Operating System Network Versions 1 Mobile ](media/Test-Pyramid-image3.jpg)\n\nExamples of compatibility testing include:\n-   browser testing (checking to make sure your website or mobile site is fully compatible with different browsers)\n-   mobile testing (making sure your application is compatible with iOS and Android)\n-   or software testing (if you're going to be creating multiple software applications that need to be interacting with one another, you'll need to conduct compatibility testing to ensure that they actually do so).\n<https://www.freecodecamp.org/news/types-of-software-testing>\n"},{"fields":{"slug":"/Computer-Science/Testing/Tools/","title":"Tools"},"frontmatter":{"draft":false},"rawBody":"# Tools\n\nCreated: 2019-01-15 16:50:10 +0500\n\nModified: 2021-11-26 16:17:19 +0500\n\n---\n\n**Google Test**\n\nGoogle Testing and Mocking Framework\n**Google Test**(also known as gtest for e.g. the[ROS](https://en.wikipedia.org/wiki/Robot_Operating_System)environment) is a[unit testing](https://en.wikipedia.org/wiki/Unit_testing)library for the[C++ programming language](https://en.wikipedia.org/wiki/C%2B%2B), based on the[xUnit](https://en.wikipedia.org/wiki/XUnit) architecture.The library is released under the BSD 3-clause license.It can be compiled for a variety of[POSIX](https://en.wikipedia.org/wiki/POSIX)and[Windows](https://en.wikipedia.org/wiki/Microsoft_Windows)platforms, allowing unit-testing of C sources as well as C++ with minimal source modification. The tests themselves could be run one at a time, or even be called to run all at once. This makes the debugging process very specific and caters to the need of many programmers and coders alike.\n<https://en.wikipedia.org/wiki/Google_Test>\n\n<https://github.com/google/googletest>\n\n## Other Tools**\n-   Selenium\n\n<https://zenq.com/blogs/whats-new-in-selenium-4-and-the-impact-on-previous-versions>\n-   Puppeteer - <https://github.com/puppeteer/puppeteer>\n-   <https://cucumber.io>\n-   NCover\n-   Magellan\n-   JTest\n-   Cobertura\n-   Emma\n-   Bazel\n\nBuild and test software of any size, quickly and reliably\n\n<https://www.bazel.build>\nREST Assured (REST Test Tool)\n\nMockito (Mocking)\n\nJUnit\n**REST Assured**\n\nTesting and validating REST services in Java is harder than in dynamic languages such as Ruby and Groovy. REST Assured brings the simplicity of using these languages into the Java domain.\n<http://rest-assured.io>\n<https://www.toptal.com/java/unit-integration-junit-tests>\n<https://getopentest.org>\n\nOpenTest is a free and open source functional test automation framework for web applications, mobile apps and APIs, built for scalability and extensibility, with a focus on enabling the mainstream test automation practices. OpenTest is a feature-reach tool that requires little to no coding skills and can handle virtually any type of functional test automation project.\n**Katalon**\n\n[Katalon Studio](https://www.katalon.com)is a free and complete automation testing solution for Web, Mobile, and API testing with modern methodologies (Data-Driven Testing, TDD/BDD, Page Object Model, etc.) as well as advanced integration (JIRA, qTest, Slack, CI, Katalon TestOps, etc.). Learn more about[Katalon Studio features](https://www.katalon.com/features/).\n**Katalon TestOps**\n\n[Katalon TestOps](https://analytics.katalon.com)is a web-based application that provides dynamic perspectives and an insightful look at your automation testing data. You can leverage your automation testing data by transforming and visualizing your data; analyzing test results; seamlessly integrating with such tools as Katalon Studio and Jira; maximizing the testing capacity with remote execution.\n**Katalon Runtime Engine (KRE)**\n\nKatalon Runtime Engine (KRE)is the test execution add-on of Katalon Studio. KRE allows you to execute automation tests in CLI mode. It can be used in a variety of scenarios, such as scheduling your tests, integrating with CI/CD system, or bundling your tests to execute in virtual containers like Docker.\n<https://www.katalon.com>\n\n<https://github.com/katalon-studio/katalon-studio>\n\n## TestLink**\n\nTestLink Open Source Test & Requirement Management System\nTestLink is a web based test management and test execution system. It enables quality assurance teams to create and manage their test cases as well as to organize them into test plans. These test plans allow team members to execute test cases and track test results dynamically.\n<https://github.com/TestLinkOpenSourceTRMS/testlink-code>\n\n## UIPath (Robotic Process Automation - RPA)**\n\n<https://www.uipath.com>\nRobotic Process Automation (RPA) is a software program that imitates human actions while interacting with a computer application and accomplishing automation of repetitive, rule-based processes. UiPath is reliable, fast and one of the most popular among other existing automation tools.\n\n"},{"fields":{"slug":"/Computer-Science/Testing/iperf3-Testing/","title":"iperf3 Testing"},"frontmatter":{"draft":false},"rawBody":"# iperf3 Testing\n\nCreated: 2020-07-18 00:11:49 +0500\n\nModified: 2020-08-23 01:11:43 +0500\n\n---\n\n**iperf (Network throughput)**\n\niPerfis a widely used tool for network performance measurement and tuning. It is significant as a cross-platform tool that can produce standardized performance measurements for any network. Iperf has[client](https://en.wikipedia.org/wiki/Client_(computing))and[server](https://en.wikipedia.org/wiki/Server_(computing))functionality, and can create data streams to measure the throughput between the two ends in one or both directions. Typical Iperf output contains a time-stamped report of the amount of data transferred and the throughput measured.\nThe data streams can be either[Transmission Control Protocol](https://en.wikipedia.org/wiki/Transmission_Control_Protocol)(TCP) or[User Datagram Protocol](https://en.wikipedia.org/wiki/User_Datagram_Protocol)(UDP):\n-   UDP: When used for testing UDP capacity, Iperf allows the user to specify the[datagram](https://en.wikipedia.org/wiki/Datagram#Packets_vs._datagrams)size and provides results for the datagram throughput and the[packet](https://en.wikipedia.org/wiki/Packet_(information_technology))loss.\n-   TCP: When used for testing TCP capacity, Iperf measures the throughput of the payload. Iperf uses 1024 Ã— 1024 for[mebibytes](https://en.wikipedia.org/wiki/Mebibyte)and 1000 Ã— 1000 for[megabytes](https://en.wikipedia.org/wiki/Megabyte).\niperf is a tool for active measurements of the maximum achievable bandwidth on IP networks. It supports tuning of various parameters related to timing, protocols, and buffers. For each test it reports the measured throughput / bitrate, loss, and other parameters.\nThis version, sometimes referred to as iperf3, is a redesign of an original version developed at NLANR/DAST. iperf3 is a new implementation from scratch, with the goal of a smaller, simpler code base, and a library version of the functionality that can be used in other programs. iperf3 also has a number of features found in other tools such as nuttcp and netperf, but were missing from the original iperf. These include, for example, a zero-copy mode and optional JSON output. Note that iperf3 isnotbackwards compatible with the original iperf.\n**Commands**\n\nbrew install iperf3\n\nsudo apt-get install iperf3\n**# server**\n\niperf3 -s -p 5002\n**# client**\n\niperf3 -c localhost -p 5002\n**# Verbose, debug**\n\niperf3 -V -d -s -p 5002\n**#The number of simultaneous connections to make to the server. Default is 1.**\n\niperf3 -V -d -s -p 5002 -P 10\n**#Run in reverse mode (server sends, client receives).**\n\niperf3 -V -d -s -p 5002 -P 10 -R\n<https://en.wikipedia.org/wiki/Iperf>\n\n<https://github.com/esnet/iperf>\n\n<https://iperf.fr/iperf-doc.php>\n\n## Retr**\n\nIt's the number of TCP segments retransmitted. This can happen if TCP segments are lost in the network due to congestion or corruption.\n**Localhost in laptop**\n\n[ 7] 0.00-10.00 sec 56.7 GBytes 48.7 Gbits/sec sender\n\n[ 7] 0.00-10.00 sec 56.7 GBytes 48.7 Gbits/sec receiver\n**API-v1 -> API-v1 localhost**\n\n[ 5] 0.00-10.00 sec 38.3 GBytes 32.9 Gbits/sec 0 sender\n\n[ 5] 0.00-10.04 sec 38.3 GBytes 32.8 Gbits/sec receiver\n[ 5] 0.00-10.00 sec 38.0 GBytes 32.7 Gbits/sec 0 sender\n\n[ 5] 0.00-10.04 sec 38.0 GBytes 32.5 Gbits/sec receiver\n**API-v2 -> API-v1 using pod container ip (in same node)**\n\n[ 5] 0.00-10.00 sec 27.7 GBytes 23.8 Gbits/sec 29 sender\n\n[ 5] 0.00-10.04 sec 27.7 GBytes 23.7 Gbits/sec receiver\n[ ID] Interval Transfer Bitrate Retr\n\n[ 5] 0.00-10.00 sec 27.2 GBytes 23.4 Gbits/sec 31 sender\n\n[ 5] 0.00-10.04 sec 27.2 GBytes 23.3 Gbits/sec receiver\n[ ID] Interval Transfer Bitrate Retr\n\n[ 5] 0.00-10.00 sec 27.2 GBytes 23.4 Gbits/sec 525 sender\n\n[ 5] 0.00-10.04 sec 27.2 GBytes 23.3 Gbits/sec receiver\n[ ID] Interval Transfer Bitrate Retr\n\n[ 5] 0.00-10.00 sec 27.1 GBytes 23.3 Gbits/sec 189 sender\n\n[ 5] 0.00-10.05 sec 27.1 GBytes 23.2 Gbits/sec receiver\n**test -> API-v1 using pod container ip (in different nodes)**\n\n[ ID] Interval Transfer Bitrate Retr\n\n[ 5] 0.00-10.00 sec 5.71 GBytes 4.90 Gbits/sec 223 sender\n\n[ 5] 0.00-10.04 sec 5.70 GBytes 4.88 Gbits/sec receiver\n[ ID] Interval Transfer Bitrate Retr\n\n[ 5] 0.00-10.00 sec 5.77 GBytes 4.96 Gbits/sec 233 sender\n\n[ 5] 0.00-10.04 sec 5.77 GBytes 4.94 Gbits/sec receiver\n[ ID] Interval Transfer Bitrate Retr\n\n[ 5] 0.00-10.00 sec 5.77 GBytes 4.95 Gbits/sec 121 sender\n\n[ 5] 0.00-10.04 sec 5.76 GBytes 4.93 Gbits/sec receiver\n**API-v2, API-v1 (localhost) (in same node), test (in different node) -> API-v1 using pod container ip**\n\ncan't be done via iperf3\n**API-v1 -> test (different nodes)**\n\n[ 5] 0.00-10.00 sec 5.77 GBytes 4.95 Gbits/sec 189 sender\n\n[ 5] 0.00-10.04 sec 5.76 GBytes 4.93 Gbits/sec receiver\n**API-v1 -> svc test (ip) -> test pod (different nodes)**\n\n[ 5] 0.00-10.00 sec 5.77 GBytes 4.95 Gbits/sec 108 sender\n\n[ 5] 0.00-10.04 sec 5.76 GBytes 4.93 Gbits/sec receiver\n[ 5] 0.00-10.00 sec 5.72 GBytes 4.92 Gbits/sec 393 sender\n\n[ 5] 0.00-10.04 sec 5.72 GBytes 4.90 Gbits/sec receiver\n**API-v1 -> svc test (dns) -> test pod (different nodes)**\n\n[ 5] 0.00-10.00 sec 5.78 GBytes 4.97 Gbits/sec 83 sender\n\n[ 5] 0.00-10.04 sec 5.78 GBytes 4.95 Gbits/sec receiver\n[ 5] 0.00-10.00 sec 5.74 GBytes 4.93 Gbits/sec 284 sender\n\n[ 5] 0.00-10.04 sec 5.74 GBytes 4.91 Gbits/sec receiver\n# 20 parallel connections\n\n[SUM] 0.00-10.00 sec 11.7 GBytes 10.0 Gbits/sec 1886 sender\n\n[SUM] 0.00-10.01 sec 11.7 GBytes 10.0 Gbits/sec receiver\n# 30 parallel connections\n\n[SUM] 0.00-10.00 sec 11.7 GBytes 10.1 Gbits/sec 10305 sender\n\n[SUM] 0.00-10.01 sec 11.7 GBytes 10.0 Gbits/sec receiver"},{"fields":{"slug":"/Data-Structures/General/DS-Intro/","title":"DS Intro"},"frontmatter":{"draft":false},"rawBody":"# DS Intro\n\nCreated: 2018-01-12 08:56:09 +0500\n\nModified: 2021-06-06 16:40:07 +0500\n\n---\n\n1.  **Implicit Data Structure -**\n\nIn[computer science](https://en.wikipedia.org/wiki/Computer_science), an**implicit[data structure](https://en.wikipedia.org/wiki/Data_structure)**or**space-efficient data structure**is a data structure that stores very little information other than the main or required data: a data structure that requires low[overhead](https://en.wikipedia.org/wiki/Overhead_(computing)). They are called \"implicit\" because the position of the elements carries meaning and relationship between elements.\nFormally, an implicit data structure is one with constant*O*(1)space overhead\n2.  **Explicit data structure -**\n3.  **Succinct data structure -**\n\nA**succinct data structure**is a[data structure](https://en.wikipedia.org/wiki/Data_structure)which uses an amount of space that is \"close\" to the[information-theoretic](https://en.wikipedia.org/wiki/Information-theoretic)lower bound, but (unlike other compressed representations) still allows for efficient query operations.\n<https://en.wikipedia.org/wiki/Succinct_data_structure>\n\n## Linear Data Structure (they all have a logical start and a logical end)**\n\n1.  Array\n\n2.  Linked List\n\n    a.  Doubly Linked List\n\n    b.  Circular Linked List\n\n3.  Stack\n\n4.  Queue\n\n    a.  Queue FIFO\n\n    b.  Dequeue\n\n    c.  Randomized Queue\n\n    d.  Priority Queue (Using array)\n**Hierarchical Data Structure**\n\n1.  Tree(Ex - Family tree, HTML DOM)\n\n    a.  Binary Tree\n\n    b.  Binary Search Tree\n\n    c.  AVL Tree\n\n    d.  2-3 Tree\n\n    e.  Red Black Tree\n\n2.  Trie\n\n3.  Heap\n\n    a.  Binary Heap\n\n    b.  Fibonacci Heap\n\n4.  Hash Map\n\n5.  Graph\n\n    a.  Undirected graph\n\n    b.  Directed graph\n\n6.  Rope (Efficient for String concatenation)\n<https://www.wisdomjobs.com/e-university/data-structures-tutorial-290.html>\n\n<https://medium.freecodecamp.org/all-you-need-to-know-about-tree-data-structures-bceacb85490c>\n\n<https://www.freecodecamp.org/news/learn-all-about-data-structures-used-in-computer-science>\n"},{"fields":{"slug":"/Data-Structures/General/Data/","title":"Data"},"frontmatter":{"draft":false},"rawBody":"# Data\n\nCreated: 2021-04-04 13:54:24 +0500\n\nModified: 2021-04-05 23:39:21 +0500\n\n---\n\n**Big Data**includes huge volume, high velocity, and extensible variety of data. These are 3 types: Structured data, Semi-structured data, and Unstructured data.\n\n1.  **Structured data**\n\nStructured data is data whose elements are addressable for effective analysis. It has been organized into a formatted repository that is typically a database. It concerns all data which can be stored in database SQL in a table with rows and columns. They have relational keys and can easily be mapped into pre-designed fields. Today, those data are most processed in the development and simplest way to manage information.Example:Relational data.**\n**\n\n2.  **Semi-Structured data**\n\nSemi-structured data is information that does not reside in a relational database but that have some organizational properties that make it easier to analyze. With some process, you can store them in the relation database (it could be very hard for some kind of semi-structured data), but Semi-structured exist to ease space.Example: XML data.3.  **Unstructured data**\n\nUnstructured data is a data which is not organized in a predefined manner or does not have a predefined data model, thus it is not a good fit for a mainstream relational database. So for Unstructured data, there are alternative platforms for storing and managing, it is increasingly prevalent in IT systems and is used by organizations in a variety of business intelligence and analytics applications.Example: Word, PDF, Text, Media logs.**Differences between Structured, Semi-structured and Unstructured data**\n\n| **Properties**             | **Structured data**                                    | **Semi-structured data**                                                          | **Unstructured data**                              |\n|-------------|--------------------|------------------------|-----------------|\n| **Technology**             | It is based on Relational database table               | It is based on XML/RDF(Resource Description Framework).                           | It is based on character and binary data           |\n| **Transaction management** | Matured transaction and various concurrency techniques | Transaction is adapted from DBMS not matured                                      | No transaction management and no concurrency       |\n| **Version management**     | Versioning over tuples,row,tables                      | Versioning over tuples or graph is possible                                       | Versioned as a whole                               |\n| **Flexibility**            | It is schema dependent and less flexible               | It is more flexible than structured data but less flexible than unstructured data | It is more flexible and there is absence of schema |\n| **Scalability**            | It is very difficult to scale DB schema                | It's scaling is simpler than structured data                                      | It is more scalable.                               |\n| **Robustness**             | Very robust                                            | New technology, not very spread                                                   | ---                                                |\n| **Query performance**      | Structured query allow complex joining                | Queries over anonymous nodes are possible                                         | Only textual queries are possible                  |\n<https://www.geeksforgeeks.org/difference-between-structured-semi-structured-and-unstructured-data>\n\n## Types of Large Objects (LOBs)**\n\nThere are different kinds of LOBs which can be stored either in the database or in external files.\n**Internal LOBs**\n\nLOBs in the database are stored in a way that optimizes the space and provides efficient access within the database tablespaces.\nInternal LOBs (BLOBs, CLOBs, NCLOBs) also provide transactional support (Commit, Rollback, and so on) of the database server.\n-   **BLOBs (Binary LOBs)**used to store unstructured binary (also called \"raw\") data, such as video clips.\n-   **CLOBs (Character LOBs)**used to store large blocks of character data from the database character set.\n-   **NCLOBs (National Character LOBs)**used to store large blocks of character data from the National Character Set.\n**Persistent and Temporary LOBs**\n\nInternal LOBs can be either persistent or temporary. A persistent LOB is an instance of LOB that exists in a table row in the database. A temporary LOB instance is created when you instantiate a LOB only within the scope of your local application.\nA temporary instance would become a persistent instance when you insert the instance into a table row.\nPersistent LOBs uses copy semantics method and also participate in database transactions. You can also recover persistent LOB in any events of transaction or system failure & could be easily committed or rolled back. In other words, as per ACID property that pertains to use database objects pertain to use persistent LOBs.\n**External LOBs and the BFILE Datatype**\n\nExternal LOBs are data objects stored in operating system files outside the database tablespaces, that have no transactional support from the database server.\nBFILEs are having read-only datatypes. The database allows read-only byte stream access to data stored in BFILEs. You cannot write to a BFILE from within your application.\nThe database uses reference semantics with BFILE columns. Data stored in a table column of type BFILE is physically located in an operating system file, not in the database tablespace.\nBFILEs basically used to hold:\n-   Binary data, which does not change while your application is running, such as graphics.\n-   Data that is loaded into other large object types, such as a CLOB or BLOB where the data can be manipulated.\n-   Data which is appropriate for byte-stream access, such as multimedia.\n-   Read-only data which is relatively large in size, so that it will avoid taking up large amounts database tablespace.\nAny storage device accessed by the operating system can hold BFILE data, including hard disk drives, CD-ROMs, CDs and DVDs. The database can access BFILEs provided the operating system supports stream-mode access to the operating system files.\n**Security for BFILEs**\n\nBasically, a DIRECTORY object is used to access and use BFILEs. The DIRECTORY is an alias name for the actual physical directory in the server file system containing the file. Users are permitted to access the file only if authorized on the DIRECTORY object.\n-   The DDL (Data Definition Language) SQL statements like CREATE, REPLACE, ALTER, and DROP are used with DIRECTORY database objects.\n-   The DML (Data Management Language) SQL statements are used to GRANT and REVOKE object privileges on DIRECTORY objects.\nUp to 10 BFILES can be opened simultaneously in one session.\n| **Large Objects Datatypes**             | **Description**                                                                                                                                                                        |\n|------------------|------------------------------------------------------|\n| Binary Large Object (BLOB)              | Stores any kind of data in binary format such as images, audio, and video.                                                                                                             |\n| Character Large Object (CLOB)           | Stores string data in the database having character set format. Used for large set of characters/strings or documents that use the database character.                                 |\n| National Character Large Object (NCLOB) | Stores string data in National Character Set format. Used for large set of characters/strings or documents in the National Character Set. Supports characters of varying width format. |\n| External Binary File (BFILE)            | BFILEs can be accessed from your application on a read-only basis. Use BFILEs to store static data, such as image data, that does not need to be manipulated in applications.          |\n<https://www.geeksforgeeks.org/types-of-large-objectslobs>\n\n<https://www.geeksforgeeks.org/introducing-lob-locators>\n\n<https://www.geeksforgeeks.org/basic-operations-and-working-of-lob>\n\n<https://www.geeksforgeeks.org/large-objectslobs-for-semi-structured-and-unstructured-data>\n"},{"fields":{"slug":"/Data-Structures/General/Disjoint-Set-Data-Structure/","title":"Disjoint-Set Data Structure"},"frontmatter":{"draft":false},"rawBody":"# Disjoint-Set Data Structure\n\nCreated: 2018-01-30 17:31:56 +0500\n\nModified: 2018-02-08 21:39:47 +0500\n\n---\n\nAlso called as union-find data structure or merge-find set, is a DS that keeps track of set of elements partitioned into a number of disjoint (non-overlapping) subset. It provides near constant time operation (bounded by inverse- Ackermann function) to add new sets, to merge existing sets and to determine whether elements are in the same set.\n**Representation**\n\nA disjoint set forest consists of a number of elements each of which contains an id, a parent pointer and in efficient algorithms, a value called the rank.\nThe parent pointers of elements are arranged to form one or more[trees](https://en.wikipedia.org/wiki/Tree_data_structure), each representing a set. If an element's parent pointer points to no other element, then the element is the root of a tree and is the representative member of its set. A set may consist of only a single element. However, if the element has a parent, the element is part of whatever set is identified by following the chain of parents upwards until a representative element (one without a parent) is reached at the root of the tree.\nForests can be represented compactly in memory as arrays in which parents are indicated by their array index.\n**Operations**\n\n1.  MakeSet\n\nThe*MakeSet*operation makes a new set by creating a new element with a unique id, a rank of 0, and a parent pointer to itself. The parent pointer to itself indicates that the element is the representative member of its own set.\n\nThe*MakeSet*operation hasO(1)time complexity.\n\nPseudocode:\n\n**function** *MakeSet*(*x*)\nif *x* is not already present:\nadd *x* to the disjoint-set tree\nx.parent := x\nx.rank := 0\n2.  Find\n\n*Find(x)*follows the chain of parent pointers from*x*upwards through the tree until an element is reached whose parent is itself. This element is the root of the tree and is the representative member of the set to which*x*belongs, and may be*x*itself.\n\n*Path compression*, is a way of flattening the structure of the tree whenever*Find*is used on it. Since each element visited on the way to a root is part of the same set, all of these visited elements can be reattached directly to the root. The resulting tree is much flatter, speeding up future operations not only on these elements, but also on those referencing them.\n\nPseudocode:\n\n**function** *Find*(x)\nif x.parent != x\nx.parent := *Find*(x.parent)\nreturn x.parent\n3.  Union\n\n*Union(x,y)*uses*Find*to determine the roots of the trees*x*and*y*belong to. If the roots are distinct, the trees are combined by attaching the root of one to the root of the other. If this is done naively, such as by always making*x*a child of*y*, the height of the trees can grow as\n\n. To prevent this*union by rank*is used.\n\n*Union by rank*always attaches the shorter tree to the root of the taller tree. Thus, the resulting tree is no taller than the originals unless they were of equal height, in which case the resulting tree is taller by one node.\n\nTo implement*union by rank*, each element is associated with a rank. Initially a set has one element and a rank of zero. If two sets are unioned and have the same rank, the resulting set's rank is one larger; otherwise, if two sets are unioned and have different ranks, the resulting set's rank is the larger of the two. Ranks are used instead of height or depth because path compression will change the trees' heights over time.\n\nPseudocode:\n\n**function** *Union*(x, y)\nxRoot := *Find*(x)\nyRoot := *Find*(y)\n\n// x and y are already in the same set\nif xRoot == yRoot\nreturn\n\n// x and y are not in same set, so we merge them\nif xRoot.rank < yRoot.rank\nxRoot.parent := yRoot\nelse if xRoot.rank > yRoot.rank\nyRoot.parent := xRoot\nelse\n//Arbitrarily make one root the new parent\nyRoot.parent := xRoot\nxRoot.rank := xRoot.rank + 1**Optimizations**\n\n1.  Weighted Union (Union by rank)\n\n2.  Path Compression during Find\n**Time Complexity**\n\nWithout any optimizations, Union and Find will take O(n)\n\nWith both optimizations i.e. Union by Rank and Path Compression, the time complexity is O(x(n)) where x is inverse Ackermann function. This value is < 5 for any value of n that can be written in this physical universe, so disjoint set operations take place in essentially constant time\n**Applications**\n\n1.  Crucial role in finding Minimum Spanning Tree of a graph in Kruskal's algorithm.\n\n2.  Keep track of connected components in an undirected graph\n**References**\n\n<https://en.wikipedia.org/wiki/Disjoint-set_data_structure>\n"},{"fields":{"slug":"/Data-Structures/General/Elementary-Symbol-Tables/","title":"Elementary Symbol Tables"},"frontmatter":{"draft":false},"rawBody":"# Elementary Symbol Tables\n\nCreated: 2018-05-12 19:55:30 +0500\n\nModified: 2018-12-23 22:42:43 +0500\n\n---\n\n**Symbol Tables (Associative Arrays, Maps and Dictionaries)-**\n\nKey-value pair abstraction -\n-   Insert a value with specified key\n-   Given a key, search for the corresponding value\n**Applications -**\n-   DNS lookup\n    -   Insert domain name with specified IP address\n    -   Give domain name, find corresponding IP address\n-   Reverse DNS lookup\n-   Dictionary\n-   Web search\n-   Compiler\n-   Routing table\n-   Genomics\n-   File system\n-   Symbol tables first used in compilers, for doing fast lookups for symbols\n**Associative array abstraction**\n\nAssociate one value with each key\n**Keys and values**\n\nValue type: Any generic type\n\nKey type:\n-   Assume keys are Comparable, use compareTo()\n-   Assume keys are any generic type, use equals() to test equality; use hashCode() to scramble key.\n-   Use immutable types for symbol table keys\n    -   Immutable in java - Integer, Double, String, java.io.File\n    -   Mutable in java - StringBuilder, java.net.URL, arrays\n**Implementation -**\n\na.  Using Linked List (Unordered)b.  Using Ordered Array\n\n    i.  Using Binary Search\n\nKeep two array, one for keys and other for values (order by keys)\n\nc.  Using Binary Search Tree\n\nA BST is a binary tree in symmetric order\n\nEach node has a key, and every node's key is:\n-   Larger than all keys in its left subtree\n-   Smaller than all keys in its right subtree\n| implementation                     | worst-case cost (after N inserts) |       |       | average case (after N random inserts) |             |             | ordered iteration? | key interface       |\n|-------------|--------|------|-------|--------|------|-------|---------|-----------|\n|                                   | search                            | insert | delete | search hit                            | insert       | delete       |                   |                    |\n| sequential search (unordered list) | N                                 | N      | N      | N/2                                   | N            | N/2          | no                 | equals()            |\n| binary search (ordered array)      | lg N                              | N      | N      | lg N                                  | N/2          | N/2          | yes                | compareTo()         |\n| BST                                | N                                 | N      | N      | 1.39 lg N                             | 1.39 lg N    | ?            | yes                | compareTo()         |\n| 2-3 tree                           | c lg N                            | c lg N | c lg N | c lg N                                | c lg N       | c lg N       | yes                | compareTo()         |\n| red-black BST                      | 2 lg N                            | 2 lg N | 2 lg N | 1.00 lg N *                          | 1.00 lg N * | 1.00 lg N * | yes                | compareTo()         |\n| separate chaining                  | lg N*                            | lg N* | lg N* | 3.5*                                 | 3.5*        | 3.5*        | no                 | equals() hashCode() |\n| linear probing                     | lg N*                            | lg N* | lg N* | 3.5*                                 | 3.5*        | 3.5*        | no                 | equals() hashCode() |\n# Symbol Table Applications -\n\n1.  **Sets**\n\nA collection of distinct keys\n\nApplication - Exception filter\n-   Read in a list of words from one file\n-   Print out all words from standard input that are {in, not in} the list\n    -   Whitelist a set of strings or objects\n    -   Blacklist a set of strings or objects\n\nApplication of Exception filter -\n-   Spell checker\n-   Browser\n-   Parental controls\n-   Chess\n-   Spam filter\n-   Credit cards\n2.  **Dictionary Clients**\n\nApplications -\n-   DNA sequence - CODON\n-   IP - URL\n-   Email- Username\n3.  **Indexing Clients**\n    -   **File Indexing**\n\nIndex a PC (or the web)\n\nGoal: Given a list of files specified, create an index so that you can efficiently find all files containing a given query string\n\nSolution: Key = query string; value = set of files containing that string\n-   Book index\n-   Concordance (with the search result give context of the word, like few texts before the search word and few texts after)\n\nGoal: Preprocess a text corpus to support concordance queries: given a word, find all occurrences with their immediate contexts.\n4.  **Sparse Vectors**\n\nSparse Matrix-vector multiplication\n\nUsing symbol table, we can have space proportional to number of nonzero values. We can use hash table because the order is also not important. Keys will be the position of the nonzero element in each row and value will be the actual value.\n![Summary of the performance of symbol-table implemen Order of growth of the frequency of operations. implementation red-black BST hash table search log N It typical case insert log N delete log N ordered operations yes no t under uniform ](media/Elementary-Symbol-Tables-image1.png)\n\n![String symbol table basic API String symbol table.](media/Elementary-Symbol-Tables-image2.png)\n\n![implementation red-black BST hashing (linear probing) Parameters â€¢ N = number of strings â€¢ L = length of string String symbol table implementations cost summary character accesses (typical case) â€¢ R = radix search hit L + cig2N search miss clg2N insert c lg2N space (references) 4N to 16N file moby.txt actors.txt size 1.2 ME 82 ME ](media/Elementary-Symbol-Tables-image3.png)\n\n# Character Based Operations\n\n![String symbol table API Character-based operations. The string symbol table API sur useful character-based operations. Prefix match. key by sea sells she shells shore the Keys with prefix sh: she, value 4 6 3 7 5 shells, and shore. ](media/Elementary-Symbol-Tables-image4.png)\n\n![String symbol table](media/Elementary-Symbol-Tables-image5.png)\n\n![Warmup: ordered iteration To iterate through all keys in sorted order: â€¢ Do inorder traversal of trie; add keys encountered to a â€¢ Maintain sequence of characters on path from root to no by sea keys ( ) key b by s se sea sel sell sells sh she shell shells by by by by sea sea sea sells sells sells b shells s O h O O s t h e o O O she she ](media/Elementary-Symbol-Tables-image6.png)\n\n![Ordered iteration](media/Elementary-Symbol-Tables-image7.png)\n\n![String symbol tables summary A success story in algorithm design and analysis. Red-black BST. â€¢ Performance guarantee: log N key compares. â€¢ Supports ordered symbol table API. Hash tables. â€¢ Performance guarantee: constant number of probes. â€¢ Requires good hash function for key type. Tries. R-way, TST. â€¢ Performance guarantee: log N characters accessed. ](media/Elementary-Symbol-Tables-image8.png)"},{"fields":{"slug":"/Data-Structures/General/Endianness/","title":"Endianness"},"frontmatter":{"draft":false},"rawBody":"# Endianness\n\nCreated: 2019-07-30 21:24:25 +0500\n\nModified: 2019-12-26 01:24:21 +0500\n\n---\n\n**endian**\n\n*adjective*\n\ndenoting or relating to a system of ordering bytes in a word, or bits in a byte, in which the most significant (or least significant) item is put first.\n\"big-endian vs. little-endian representation of data\"\n**Endianness**\n-   **Little Endian Format - LSB is stored first**\n\nIn little endian machines, last byte of binary representation of the multibyte data-type is stored first.-   **Big Endian Format - MSB is stored first**\n\nIn big endian machines, first byte of binary representation of the multibyte data-type is stored first.-   **Middle Endian Format**\n-   **Bi-endianness**\n\nThe word*bi-endian*, when said of hardware, denotes the capability of the machine to compute or pass data in either endian format.\n**Example**\n\nSuppose integer is stored as 4 bytes (For those who are using DOS based compilers such as C++ 3.0 , integer is 2 bytes) then a variable x with value 0x01234567 will be stored as following.\n\n![EO | , 0 : 0 0 to ã€ ã€ 0 001 ' 0 â‚¬0t ä¸¶ 0 : 0 ã€ 0 ã€ 01 0 ](media/Endianness-image1.gif)\n\n<https://www.geeksforgeeks.org/little-and-big-endian-mystery>\nHistorically, various methods of endianness have been used in computing, including exotic forms such as middle-endianness. Today, however, big-endianness is the dominant ordering in networking protocols ([IP](https://en.wikipedia.org/wiki/Internet_Protocol),[TCP](https://en.wikipedia.org/wiki/Transmission_Control_Protocol),[UDP](https://en.wikipedia.org/wiki/User_Datagram_Protocol)). Conversely, little-endianness is the dominant ordering for processor architectures ([x86](https://en.wikipedia.org/wiki/X86), most[ARM](https://en.wikipedia.org/wiki/ARM_architecture)implementations) and their associated[memory](https://en.wikipedia.org/wiki/Computer_memory).[File formats](https://en.wikipedia.org/wiki/File_format)can use either ordering; some formats use a mixture of both.\nIn[left-to-right scripts](https://en.wikipedia.org/wiki/Writing_system#Directionality), numbers are written with their digits in big-endian order. Similarly, programming languages use big-endian digit ordering for numeric[literals](https://en.wikipedia.org/wiki/Literal_(computer_programming))as well as big-endian language (\"left\" and \"right\") for[bit-shift](https://en.wikipedia.org/wiki/Bitwise_operation#Logical_shift)operations, regardless of the endianness of the target architecture. This can lead to confusion when interacting with little-endian numbers.\n![Big-Endian](media/Endianness-image2.png)\n\n![Little-Endian](media/Endianness-image3.png)<https://en.wikipedia.org/wiki/Endianness>\n"},{"fields":{"slug":"/Data-Structures/General/Mutable-Immutable-Data-Structures/","title":"Mutable/Immutable Data Structures"},"frontmatter":{"draft":false},"rawBody":"# Mutable/Immutable Data Structures\n\nCreated: 2019-06-27 15:46:39 +0500\n\nModified: 2019-06-27 16:06:25 +0500\n\n---\n\n**Immutable Data Structure**\n\nThe obvious advantage of the immutable data structures is that the storage overhead can be minimized: we do not have to reserve any extra space for data that is going to be inserted later or for the cases when the updated records require more space than the originally written ones.\nKeeping the data structure immutable favors the sequential writes: data is written on disk in a single pass, append-only. The mutable data structure will be pre-allocated in a single pass, but the subsequent writes will be random. Some structures require node splitting, that will relocate already written parts. After some time, randomly written file might require defragmentation.\nSome databases, instead of doing in-place updates, just mark the outdated record as \"deleted\" (so that it will eventually be garbage-collected) and append new records to the specially designated update area of the file. While this is a nice compromise, sometimes writes fill up all the designated space and overflow areas have to be created. All of this might slow down both subsequent reads and writes.\nAnother advantage of immutable files is that data can be read from the disk without any segment locking between operations, which significantly simplifies concurrent access. In contrast, mutable data structures employ hierarchical[locks and latches](http://15721.courses.cs.cmu.edu/spring2017/papers/06-latching/a16-graefe.pdf)in order to ensure on disk data structure integrity, allow multiple readers at the same time but give exclusive ownership for parts of tree to writers.\nBoth mutable and immutable data structures require some housekeeping in order to optimize performance but for different reasons. Since amount of allocated files constantly grows, immutable data structures have to merge and rewrite files in order to make sure that the least amount of files is hit during the query, as the requested record might be spread across multiple files. On the other hand, mutable files may have to be rewritten partially or completely to decrease fragmentation, merge overflow areas and reclaim space occupied by updated or deleted records (as their new contents were written elsewhere). Of course, the exact scope of work done by the housekeeping process heavily depends on the concrete implementation.\n**Summary**\n\nUsing immutable data structures can often simplify the work of programmer. When using immutable on-disk structures, you trade the need to occasionally merge your tables for better space management (by avoiding overflow pages and boosting space occupancy to 100%), concurrency (because readers and writers are never compete over the same file, therefore requiring no mutual exclusion) and potentially simpler implementations.\n<https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f>\n\n## Mutable Data Structure**\n\nMutable storage is often implemented using Heap Table File, combined with some sort of index.\nExample\n-   B-Tree\n<https://medium.com/databasss/on-disk-storage-part-4-b-trees-30791060741>\n"},{"fields":{"slug":"/Data-Structures/Graph/Adjacency-List/","title":"Adjacency List"},"frontmatter":{"draft":false},"rawBody":"# Adjacency List\n\nCreated: 2018-02-08 17:28:40 +0500\n\nModified: 2018-02-08 17:29:22 +0500\n\n---\n\n![adjacency-list graph](media/Adjacency-List-image1.png)\n![adjacency-list](media/Adjacency-List-image2.png)\n\n"},{"fields":{"slug":"/Data-Structures/Graph/Adjacency-Matrix/","title":"Adjacency Matrix"},"frontmatter":{"draft":false},"rawBody":"# Adjacency Matrix\n\nCreated: 2018-02-08 17:28:34 +0500\n\nModified: 2018-02-08 17:29:16 +0500\n\n---\n\n![Adiacency-matrix graph representation Maintain a two-dimensional V-by-V boolean array; for each edge in graph: adj [v] [w] two entries for each edge = adj 1 o o o o o o o o o o = true. o o o o o o o o o o o o 1 1 o o o o o o o o 1 1 o o o o o o 1 o o 1 o o o o o 1 o o o 1 o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o 1 1 1 10 o o o o o o o 1 o o o 11 o o o o o o o 1 o o 1 12 o o o 1 o 1 11 10 12 10 11 12 o 1 1 o o 1 1 o o o o o o ](media/Adjacency-Matrix-image1.png)\n"},{"fields":{"slug":"/Data-Structures/Graph/Digraphs-(Directed-Graphs)/","title":"Digraphs (Directed Graphs)"},"frontmatter":{"draft":false},"rawBody":"# Digraphs (Directed Graphs)\n\nCreated: 2018-02-10 19:50:27 +0500\n\nModified: 2018-05-22 19:28:12 +0500\n\n---\n\n![Digraph. directed path Set of vertices connected pairwise by directed edges. from O to 2 3 5 vertex of outdegree 4 and indegree 2 6 2 4 8 9 11 7 10 directed cycle 12 ](media/Digraphs-(Directed-Graphs)-image1.png)\n1.  Path\n\n2.  Shortest path\n\n3.  Topological Sorting (can you draw a digraph so that all edges point upwards)\n\n4.  Strong Connectivity\n\n5.  Transitive closure (For which vertices v and w is there a path from v to w)\n\n6.  PageRank\n![Adjacency-lists digraph representation Maintain vertex-indexed array of lists. O O 11 O 10 12 adj [ ] 10 11 12 11 12 10 12 ](media/Digraphs-(Directed-Graphs)-image2.png)\n![Digraph-processing summary: single-source reachability in a digraph topological sort in a DAG strong components in a digraph algorithms of the day 10 11 11 12 DFS DFS Kosaraju-Sharir DFS (twice) ](media/Digraphs-(Directed-Graphs)-image3.png)\n![Digraph applications digraph transportation web food web Word Net scheduling financial cell phone infectious disease game citation object graph inheritance hierarchy control flow vertex street intersection web page species synset task bank person person board position journal article object class code block directed edge one-way street hyperlink predator-prey relationship hypernym precedence constraint transaction placed call infection legal move citation pointer inherits from jump ](media/Digraphs-(Directed-Graphs)-image4.png)\n**Digraph Search**\n\n1.  Reachability - Find all vertices reachable from s along a directed path. (Use DFS)\n\n![Reachability application: program control-flow analysis Every program is a digraph. â€¢ Vertex = basic block of instructions (straight-line program). â€¢ Edge = jump. Dead-code elimination. Find (and remove) unreachable code. Infinite-loop detection. Determine whether exit is unreachable. n t2t3t4t10 t' t20tsst'0t11 t' t2t3'4t10 t' t2t3'4t10 t3ror' t3taro,l to: t2ert t2t3t4 t2t3t4 t' Qt3tSt10t11 t1t2t3tsnot11 t1t2t3t10t11 t' t2t3t4 t3t10 t3t10 t1t2t3tst10t11 t2t3tSt10t11 ](media/Digraphs-(Directed-Graphs)-image5.png)\n![Reachability application: mark-sweep garbage collector Every data structure is a digraph. â€¢ Vertex = object. Edge = reference. Roots. Objects known to be directly accessible by program (e.g., stack). Reachable objects. Objects indirectly accessible by program (starting at a root and following a chain of pointers). ](media/Digraphs-(Directed-Graphs)-image6.png)\n![Reachability application: mark-sweep garbage collector Mark-sweep algorithm. [McCarthy, 1 960] â€¢ Mark: mark all reachable objects. â€¢ Sweep: if object is unmarked, it is garbage (so add to free list). Memory cost. Uses I extra mark bit per object (plus DFS stack). ](media/Digraphs-(Directed-Graphs)-image7.png)\n2.  ![Multiple-source shortest paths Multiple-source shortest paths. Given a digraph and a set of source vertices, find shortest path from any vertex in the set to each other vertex. Ex. s = {1, 7, 10}. â€¢ Shortest path to 4 is 7-+6---4. â€¢ Shortest path to 5 is â€¢ Shortest path to 12 is 10-+12. 2 3 4 5 9 11 10 12 Q. How to implement multi-source shortest paths algorithm? A. Use BFS, but initialize by enqueuing all source vertices. ](media/Digraphs-(Directed-Graphs)-image8.png)\n![](media/Digraphs-(Directed-Graphs)-image9.png)\n**Edge-Weighted Graph API**\n\nWe use adjacency-list representation for representing a weighted graph, where each edge has a weight associated with it\n**See also**\n-   Topological Sort\n"},{"fields":{"slug":"/Data-Structures/Graph/Implementation/","title":"Implementation"},"frontmatter":{"draft":false},"rawBody":"# Implementation\n\nCreated: 2018-04-04 15:28:54 +0500\n\nModified: 2018-04-04 15:28:58 +0500\n\n---\n\n**Python**\n\nUse dictionary to represent a graph in Python\n\ngraph = { \"a\" : [\"c\"],\n\"b\" : [\"c\", \"e\"],\n\n\"c\" : [\"a\", \"b\", \"d\", \"e\"],\n\"d\" : [\"c\"],\n\"e\" : [\"c\", \"b\"],\n\"f\" : []\n\n}"},{"fields":{"slug":"/Data-Structures/Graph/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-05-21 23:25:39 +0500\n\nModified: 2021-06-06 16:54:17 +0500\n\n---\n\nRepresentation\n\n1.  Adjacency Matrix\n\n2.  Adjacency List\n![Graph applications graph communication circuit mechanical financial transportation internet game social relationship neural network protein network molecule vertex telephone, computer gate, register, processor joint stock, currency street intersection, airport class C network board position person, actor neuron protein atom edge fiber optic cable wire rod, beam, spring transactions highway, airway route con nection legal move friendship, movie cast synapse protein-protein interaction bond ](media/Intro-image1.png)\nGraph types -\n\n1.  Sparse graph\n\n2.  Dense graph\n![In practice. Use adjacency-lists representation. Algorithms based on iterating over vertices adjacent to v. â€¢ Real-world graphs tend to be sparse. huge number of vertices, small average vertex degree representation list of edges adjacency matrix adjacency lists space E+V add edge edge between v and w? degree(v) iterate over vertices adjacent to v? v deg ree(v) * disallows parallel edges ](media/Intro-image2.png)"},{"fields":{"slug":"/Data-Structures/Graph/Questions/","title":"Questions"},"frontmatter":{"draft":false},"rawBody":"# Questions\n\nCreated: 2018-08-04 12:24:36 +0500\n\nModified: 2019-12-02 14:01:58 +0500\n\n---\n-   Implement Breadth and Depth First Search\n-   Check if a graph is a tree or not\n-   Count number of edges in a graph\n-   Find the shortest path between two vertices\r\n"},{"fields":{"slug":"/Data-Structures/Graph/Undirected-Graph/","title":"Undirected Graph"},"frontmatter":{"draft":false},"rawBody":"# Undirected Graph\n\nCreated: 2018-02-08 16:57:53 +0500\n\nModified: 2018-05-21 23:25:33 +0500\n\n---\n\n**Undirected Graphs**\n\nPath: Sequence of vertices connected by edges\n\nCycle: Path whose first and last vertices are the same\nTwo vertices are connected if there is a path between them\n![cycle of length 5 vertex of degree 3 vertex path of length 4 connected components ](media/Undirected-Graph-image1.png)\n**Handshaking Lemma**\n\nIn every finite undirected graph number of vertices with odd degree is always even.The handshaking lemma is a consequence of the degree sum formula (also sometimes called the handshaking lemma)\n\n![](media/Undirected-Graph-image2.png)\n**Properties**\n\n1.  **In a k-ary tree where every node has either 0 or k children, following property is always true.**\n\nL = (k - 1)*I + 1\nWhere L = Number of leaf nodes\nI = Number of internal nodes\n2.  **In Binary tree, number of leaf nodes is always one more than nodes with two children.**\n\nL = T + 1\nWhere L = Number of leaf nodes\nT = Number of internal nodes with two children"},{"fields":{"slug":"/Data-Structures/HashTable/Bloom-Filters/","title":"Bloom Filters"},"frontmatter":{"draft":false},"rawBody":"# Bloom Filters\n\nCreated: 2018-01-28 09:33:59 +0500\n\nModified: 2019-12-30 15:56:07 +0500\n\n---\n\nA**Bloom filter**is a space-efficient[probabilistic](https://en.wikipedia.org/wiki/Probabilistic)[data structure](https://en.wikipedia.org/wiki/Data_structure), conceived by[Burton Howard Bloom](https://en.wikipedia.org/w/index.php?title=Burton_Howard_Bloom&action=edit&redlink=1)in 1970, that is **used to test whether an[element](https://en.wikipedia.org/wiki/Element_(mathematics))is a member of a[set](https://en.wikipedia.org/wiki/Set_(computer_science))**.[False positive](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors)matches are possible, but[false negatives](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors)are not -- in other words, a query returns either \"possibly in set\" or \"definitely not in set\". Elements can be added to the set, but not removed (though this can be addressed with a \"counting\" filter); the more elements that are added to the set, the larger the probability of false positives. So if our design can tolerate false positive then we should consider using bloom filters because it's very space efficient.\n-   Fast, compressed storage-free data structure used to check for set membership\n-   Implemented as a set of hash functions pointing to locations in a bit array\n-   No Retrieval\n-   No Removal\n**Applications -**\n\n1.  Checking availability of username is a set membership problem, where the set is the list of all registered username. The price we pay for efficiency is that it is probabilistic in nature that means, there might be some False Positive results.**False positive means**, it might tell that given username is already taken but actually it's not.\n\n2.  Early spellcheckers\n\n3.  List of forbidden passwords\n\n4.  Network routers\n**Interesting Properties of Bloom Filters**\n-   Unlike a standard hash table, a Bloom filter of a fixed size can represent a set with an arbitrarily large number of elements.\n-   Adding an element never fails. However, the false positive rate increases steadily as elements are added until all bits in the filter are set to 1, at which point all queries yield a positive result.\n-   Bloom filters never generate**false negative**result, i.e., telling you that a username doesn't exist when it actually exists.\n-   Deleting elements from filter is not possible because, if we delete a single element by clearing bits at indices generated by k hash functions, it might cause deletion of few other elements. Example -- if we delete \"geeks\" (in given example below) by clearing bit at 1, 4 and 7, we might end up deleting \"nerd\" also Because bit at index 4 becomes 0 and bloom filter claims that \"nerd\" is not present.\nA Bloom filter is a bit array of m bits initialized to 0. To add an element, feed it to k hash functions to get k array position and set the bits at these positions to 1. To query an element, feed it to k hash functions to obtain k array positions. **If any of the bits at these positions is 0, then the element is definitely not in the set.** If the bits are all 1, then the element might be in the set. A Bloom filter with 1% false positive rate only requires 9.6 bits per element regardless of the size of the elements.\n**Bloom filter requires the following inputs:**\n\nm: size of the bit array\n\nn: estimated insertion\n\np: false positive probability\nThe optimum number of hash functions k can be determined using the formula:\n\n![](media/Bloom-Filters-image1.png)\n\nGiven false positive probabilitypand the estimated number of insertionsn, the length of the bit array can be calculated as:\n\n![](media/Bloom-Filters-image2.png)\n\nThe hash functions used for bloom filter should generally be faster than cryptographic hash algorithms with good distribution and collision resistance. Commonly used hash functions for bloom filter include Murmur hash, fnv series of hashes and Jenkins hashes. Murmur hash is the fastest among them. MurmurHash3 is used by Google Guava library's bloom filter implementation. \nThe solution is we can't support Remove operation in this simple bloom filters. But if we really need to have a Removal functionality we can use a variation of the bloom filter known as\"**Counting bloom filter**\". The idea is simple. Instead of storing a single bit of values, we will store an integer value and our bit vector will then be an integer vector. This will increase the size and costs more space to gives us the Removal functionality. Instead of just marking a bit value to '1' when inserting a value, we will increment the integer value by 1. To check if an element exists, check if the corresponding indexes after hashing the element is greater than 0.\n![Bloom Filter â€¢ Compact way of representing a set of items â€¢ Checking for existence in set is cheap â€¢ Some probability of false positives: an item not in set may check true as being in set â€¢ Never false negatives Key-K Ha Has Ha k Large Bit Map 1 2 3 69 111 127 On insert, set all hashed bits. On check-if-present, return true if all hashed bits set. â€¢ False positives False positive rate low â€¢ m=4 hash functions . 100 items 3200 bits â€¢ FP rate 0.02% Big Data Computing Design of Key-Value Stores ](media/Bloom-Filters-image3.png)\n**References -**\n\n<https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation>\n\n<https://en.wikipedia.org/wiki/Bloom_filter>\n\n<https://blog.medium.com/what-are-bloom-filters-1ec2a50c68ff>\n\n<https://www.quora.com/What-are-the-best-applications-of-Bloom-filters>\n\nPython Implentation - <https://diogodanielsoaresferreira.github.io/bloom-filter>\n"},{"fields":{"slug":"/Data-Structures/HashTable/Chord/","title":"Chord"},"frontmatter":{"draft":false},"rawBody":"# Chord\n\nCreated: 2019-03-17 18:45:49 +0500\n\nModified: 2019-03-17 18:52:31 +0500\n\n---\n\nIn[computing](https://en.wikipedia.org/wiki/Computing),Chordis a protocol and[algorithm](https://en.wikipedia.org/wiki/Algorithm)for a[peer-to-peer](https://en.wikipedia.org/wiki/Peer-to-peer)[distributed hash table](https://en.wikipedia.org/wiki/Distributed_hash_table). A distributed hash table stores[key-value pairs](https://en.wikipedia.org/wiki/Associative_array)by assigning keys to different computers (known as \"nodes\"); a node will store the values for all the keys for which it is responsible. Chord specifies how keys are assigned to nodes, and how a node can discover the value for a given key by first locating the node responsible for that key.\nChord is one of the four original[distributed hash table](https://en.wikipedia.org/wiki/Distributed_hash_table)protocols, along with[CAN](https://en.wikipedia.org/wiki/Content_addressable_network),[Tapestry](https://en.wikipedia.org/wiki/Tapestry_(DHT)), and[Pastry](https://en.wikipedia.org/wiki/Pastry_(DHT)). It was introduced in 2001 by[Ion Stoica](https://en.wikipedia.org/wiki/Ion_Stoica),[Robert Morris](https://en.wikipedia.org/wiki/Robert_Tappan_Morris),[David Karger](https://en.wikipedia.org/wiki/David_Karger),[Frans Kaashoek](https://en.wikipedia.org/wiki/Frans_Kaashoek), and[Hari Balakrishnan](https://en.wikipedia.org/wiki/Hari_Balakrishnan), and was developed at[MIT](https://en.wikipedia.org/wiki/MIT).\n**References**\n\n<https://en.wikipedia.org/wiki/Chord_(peer-to-peer)>\n"},{"fields":{"slug":"/Data-Structures/HashTable/Count-min-Sketch/","title":"Count-min Sketch"},"frontmatter":{"draft":false},"rawBody":"# Count-min Sketch\n\nCreated: 2018-04-10 00:29:00 +0500\n\nModified: 2022-04-24 11:38:42 +0500\n\n---\n\nSpace efficient probabilistic based data structure.\nCount-min sketch is a probabilitstic data strucure that is used to count the frequency of events. Consider we have millions of events and we want to count top k events, then we can use count-min sketch instead of keeping the count of all the events. So for fraction of space it will give answer which will be close enough answer to the actual answer with some error rate.\nCount-Min sketch is a probabilistic sub-linear space streaming algorithm. It is somewhat similar to bloom filter. The main difference is that bloom filter represents a set as a bitmap, while Count-Min sketch represents a multi-set which keeps a frequency distribution summary.\n![](media/Count-min-Sketch-image1.jpeg)\n<https://youtu.be/ibxXO-b14j4>\n\n"},{"fields":{"slug":"/Data-Structures/HashTable/DHT---Distributed-Hash-Tables/","title":"DHT - Distributed Hash Tables"},"frontmatter":{"draft":false},"rawBody":"# DHT - Distributed Hash Tables\n\nCreated: 2018-12-11 12:12:48 +0500\n\nModified: 2019-12-06 22:00:19 +0500\n\n---\n\nAdistributed hash table(DHT) is a class of a decentralized[distributed system](https://en.wikipedia.org/wiki/Distributed_computing)that provides a lookup service similar to a[hash table](https://en.wikipedia.org/wiki/Hash_table): (key,value) pairs are stored in a DHT, and any participating[node](https://en.wikipedia.org/wiki/Node_(networking))can efficiently retrieve the value associated with a given key.Keysare unique identifiers which map to particularvalues, which in turn can be anything from addresses, to[documents](https://en.wikipedia.org/wiki/Electronic_document), to arbitrary [data](https://en.wikipedia.org/wiki/Data_(computing)). Responsibility for maintaining the mapping from keys to values is distributed among the nodes, in such a way that a change in the set of participants causes a minimal amount of disruption. This allows a DHT to[scale](https://en.wikipedia.org/wiki/Scale_(computing))to extremely large numbers of nodes and to handle continual node arrivals, departures, and failures.\nDHTs form an infrastructure that can be used to build more complex services, such as[anycast](https://en.wikipedia.org/wiki/Anycast), cooperative[Web caching](https://en.wikipedia.org/wiki/Web_cache),[distributed file systems](https://en.wikipedia.org/wiki/Distributed_file_system),[domain name services](https://en.wikipedia.org/wiki/Domain_name_system),[instant messaging](https://en.wikipedia.org/wiki/Instant_messaging),[multicast](https://en.wikipedia.org/wiki/Multicast), and also[peer-to-peer](https://en.wikipedia.org/wiki/Peer-to-peer)[file sharing](https://en.wikipedia.org/wiki/File_sharing)and[content distribution](https://en.wikipedia.org/wiki/Content_distribution)systems. Notable distributed networks that use DHTs include[BitTorrent](https://en.wikipedia.org/wiki/BitTorrent_(protocol))'s distributed tracker, the[Coral Content Distribution Network](https://en.wikipedia.org/wiki/Coral_Content_Distribution_Network), the[Kad network](https://en.wikipedia.org/wiki/Kad_network), the[Storm botnet](https://en.wikipedia.org/wiki/Storm_botnet), the[Tox instant messenger](https://en.wikipedia.org/wiki/Tox_(protocol)),[Freenet](https://en.wikipedia.org/wiki/Freenet), the[YaCy](https://en.wikipedia.org/wiki/YaCy)search engine, and the[InterPlanetary File System](https://en.wikipedia.org/wiki/InterPlanetary_File_System).\n![Data across acros the ice function Hash function Hash fun ction Key oe79E 604284 Distributed Network peers ](media/DHT---Distributed-Hash-Tables-image1.png)\n**Properties**\n\nDHTs characteristically emphasize the following properties:\n-   [Autonomy and decentralization](https://en.wikipedia.org/wiki/Decentralized_computing): the nodes collectively form the system without any central coordination.\n-   [Fault tolerance](https://en.wikipedia.org/wiki/Fault_tolerance): the system should be reliable (in some sense) even with nodes continuously joining, leaving, and failing.\n-   [Scalability](https://en.wikipedia.org/wiki/Scale_(computing)): the system should function efficiently even with thousands or millions of nodes.\nA key technique used to achieve these goals is that any one node needs to coordinate with only a few other nodes in the system -- most commonly,[O](https://en.wikipedia.org/wiki/Big_O_notation)(logn) of thenparticipants -- so that only a limited amount of work needs to be done for each change in membership.\n**Structure**\n\nThe structure of a DHT can be decomposed into several main components.The foundation is an abstractkeyspace, such as the set of 160-bit[strings](https://en.wikipedia.org/wiki/String_(computer_science)). Akeyspace partitioningscheme splits ownership of this keyspace among the participating nodes. Anoverlay networkthen connects the nodes, allowing them to find the owner of any given key in the keyspace.\n**Keyspace partitioning**\n\nMost DHTs use some variant of[consistent hashing](https://en.wikipedia.org/wiki/Consistent_hashing)or[rendezvous hashing](https://en.wikipedia.org/wiki/Rendezvous_hashing)to map keys to nodes. The two algorithms appear to have been devised independently and simultaneously to solve the distributed hash table problem.\nBoth consistent hashing and rendezvous hashing have the essential property that removal or addition of one node changes only the set of keys owned by the nodes with adjacent IDs, and leaves all other nodes unaffected. Contrast this with a traditional[hash table](https://en.wikipedia.org/wiki/Hash_table)in which addition or removal of one bucket causes nearly the entire keyspace to be remapped. Since any change in ownership typically corresponds to[bandwidth](https://en.wikipedia.org/wiki/Bandwidth_(computing))-intensive movement of objects stored in the DHT from one node to another, minimizing such reorganization is required to efficiently support high rates of churn (node arrival and failure).\n-   Consistent Hashing\n-   Rendezvous Hashing\n-   Locality-preserving Hashing\n**Overlay network**\n\nEach node maintains a set of[links](https://en.wikipedia.org/wiki/Data_link)to other nodes (itsneighborsor[routing table](https://en.wikipedia.org/wiki/Routing_table)). Together, these links form the[overlay network](https://en.wikipedia.org/wiki/Overlay_network). A node picks its neighbors according to a certain structure, called the[network's topology](https://en.wikipedia.org/wiki/Network_topology).\nAll DHT topologies share some variant of the most essential property: for any keyk, each node either has a node ID that ownskor has a link to a node whose node ID isclosertok, in terms of the keyspace distance defined above. It is then easy to route a message to the owner of any keykusing the following[greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm)(that is not necessarily globally optimal): at each step, forward the message to the neighbor whose ID is closest tok. When there is no such neighbor, then we must have arrived at the closest node, which is the owner ofkas defined above. This style of routing is sometimes called[key-based routing](https://en.wikipedia.org/wiki/Key-based_routing).\nBeyond basic routing correctness, two important constraints on the topology are to guarantee that the maximum number of[hops](https://en.wikipedia.org/wiki/Hop_(networking))in any route (route length) is low, so that requests complete quickly; and that the maximum number of neighbors of any node (maximum node[degree](https://en.wikipedia.org/wiki/Degree_(graph_theory))) is low, so that maintenance overhead is not excessive. Of course, having shorter routes requires higher[maximum degree](https://en.wikipedia.org/wiki/Maximum_degree). Some common choices for maximum degree and route length are as follows, wherenis the number of nodes in the DHT, using[Big O notation](https://en.wikipedia.org/wiki/Big_O_notation):\n\n<table>\n<colgroup>\n<col style=\"width: 13%\" />\n<col style=\"width: 15%\" />\n<col style=\"width: 13%\" />\n<col style=\"width: 57%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>Max. degree</th>\n<th>Max route length</th>\n<th>Used in</th>\n<th>Note</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>O(1)</td>\n<td>O(n)</td>\n<td></td>\n<td>Worst lookup lengths, with likely much slower lookups times</td>\n</tr>\n<tr class=\"even\">\n<td>O(log n)</td>\n<td>O(log n)</td>\n<td><p><a href=\"https://en.wikipedia.org/wiki/Chord_(peer-to-peer)\">Chord</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Kademlia\">Kademlia</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Pastry_(DHT)\">Pastry</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Tapestry_(DHT)\">Tapestry</a></p></td>\n<td>Most common, but not optimal (degree/route length). Chord is the most basic version, with Kademlia seeming the most popular optimized variant (should have improved average lookup)</td>\n</tr>\n<tr class=\"odd\">\n<td>O(log n)</td>\n<td>O(log n / log(log(n))</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Koorde\">Koorde</a></td>\n<td>Likely would be more complex to implement, but lookups might be faster (have a lower worst case bound)</td>\n</tr>\n<tr class=\"even\">\n<td>O(sqrt(x))</td>\n<td>O(1)</td>\n<td></td>\n<td>Worst local storage needs, with lots of communication after any node connects or disconnects</td>\n</tr>\n</tbody>\n</table>\n\n## References**\n\n<https://en.wikipedia.org/wiki/Distributed_hash_table>\n\n"},{"fields":{"slug":"/Data-Structures/HashTable/Dictionaries/","title":"Dictionaries"},"frontmatter":{"draft":false},"rawBody":"# Dictionaries\n\nCreated: 2018-03-24 13:53:14 +0500\n\nModified: 2018-03-24 13:53:18 +0500\n\n---\n\n![Dictionaries Hash Tables Hash tables are a Simple and effective method to implement cfctionarieg Average time to Search for an element is 1) while time is O(n). and both contain d&CuSSiong Theory A hash table ig Simply an array that is addressed via a hash For example. in Figure 3-1, hashTab1e is an array with 8 elements, Each element is a pointer to a limkÃ¦d list of numeric data. The function fcy this example Simply divides the data key by 8. and the ag an index into the table. This yields a number from O 7, Since the range o' indices for hash Table is O to 7. we are guaranteed that the is valid. Figure A Hash Table To insert a new tem in the table, we hash the key to determine mich list the item goes on. and the at the beginning Of list. Fwexample, to 1 1, divide 1 1 by B giving a remainder of 3, Thus. Il goes on the list staring a' hashTable[31, To find a number. we hash number and Chain list to gee if it is the TO a we find the rwrnber and remove the frcm the inked list Entries in the hash table are dynamicany allocated and entered on a Iin,kÃ¦d list associated with each hash entry. This is ag Chainin% An alternative meth:yd, where all entries are stored in the hash table itself. is knt:wn as open addressing and may be found in the references. If the hash function is uniform. or equally distributes the data keys among the hash table Ouiices. hashing effectively the list to be Searched. WcySt-Cage behavicy all keys hash to the same index. Then we simply have a single linked list that must be sequentially Consequently. it is to chcx3Se a hash furwtioru Sewral may be used to hash key values. To illustrate the u2chniques. will assume unsigned char is 8â€¢bits. unsigned Short int is 16-bitg and mtSigned long is 32-bite ](media/Dictionaries-image1.png)\n\n![Division method (tablesize â€¢ prime). This technique was used in the preceeding example. A haghVaIue, tom O to - 1). is Ccmputed by dividing the key value by the size of the hash table and taking the remainder. For example: typedef int key) key HASH TABLE SIZE; Selecting an appropriate is important to the success ot this metht:xi. For example. a divisible by two yield even hash fcy even keys. and odd hash values for odd keys. This is an undesirable property. as all keys would hash to even values if trwy to be everL If HASH _ TABLE_SIZE is a of then the hash function simply selects a subset of the key bits as the table index. To obtain a more random Scattering, Should be a prime close to a power Multiplication method (tablesize â€¢ The multiplication method may be used for a that is a of 2 _ The key is multiplied by a and the necessary bits are extracted to index into the table. Knulh rernmmends using the the gouen ratio, (Sqrt(5) - 11/2. to deten.ne the Aggurne hash table 32 (25) entries and is indexed by an unsigned char (8 bits). First construct a multiplier based on the indu and golden raticx In this example, the is 28 x (Sqrt(5)- IV2. or 158. This Stales the ratio so that the first bit ot the multiplier is \"I â€¢ Multiply the key by 158 and extract the 5 most sâ€¢nificant bits of the least significant word. These bits are by \"bbbbb\" the regresenta the multiplier and key. The following definitions maybe used for the multiplication method: / â€¢ index â€¢ / unsigned char se.eie u ISE: / â€¢ 1 S---bit â€¢ / type def HuhIndexType; se.eie u 40503: / â€¢ 32-bie index â€¢ / type def static = 2654435769; /â€¢ , size of â€¢ ](media/Dictionaries-image2.png)\n\n![static int S = --- (u â€¢ key) s; For example, if is 1024 (210). then a 16-bit index is sufficient and S would be a Of 16- 10 = 6. we have: typedef HuhInduType key) se.eie u 40503; static int S = (u â€¢ key) s; Variable addition method (tablesize 2561 TO hash a each character is added. mcnluto 256, to a A hashVaIue. range Oâ€¢2S5. is computed. char â€¢ser) while ( â€¢ser) h Variable string exclusive-or method (tablesize â€¢ 256). This is similar to the addition but sim.lar mrds anagrame TO Obtain a hash value In the range Oâ€¢2S5. all bytes in the string are exclusiveâ€¢or'd together. in the process of d&ng each exeuSive-Or. a random *ltrcxduced while ( â€¢ser) h = md8[h A Rand8 is a table Of 256 8-bit unique The exact Ordering is not Criteal_ The exclusiveÂ«r metht:u.i has its basis in cryptography. and is quite effective ](media/Dictionaries-image3.png)\n\n![Variable string exclusive-or method (tablesize 65536), If We hash the String we may deriÃ¦ a value for an arbitwy table Size up to 65538 The Second time the String is hashed. one is added to the first character. Then the two 8â€¢bit hash values are concatenated together to form a 16---bit hash int huh Chu hi , h2 ; ( â€¢ser O) O; h2 = + while ('ser) h2 th2 â€¢ h is in O. .6SS3S h = ( (I-migned int) hi 8) | h HASH TABLE srze int) h2 Assuming n data items, the hash table size should be large enough to accommcâ€¢date a number Of entries AS Seen Table 3-1. a Small table Size Substantially the average time to find a key, A hash table may be viewed as a collection of linked lists, As the table the rwrnber Of lists increases. the average rwdeS each list decreases. If the table size is I. then the table is really a single linked list of length n, ASSÃ„ng a perfect hash a table Size Of 2 hag two ligtg of length 7/2 _ If the table Size is ICO then we have 100 lists of length n/IOO This considerably reduces the length of the list be There ig Choice Of Size. 16 32 64 time 869 432 214 106 54 28 15 Size 1024 2048 8192 Table 3-1: vs. Average search Time (us), 4096 entries ](media/Dictionaries-image4.png)\n"},{"fields":{"slug":"/Data-Structures/HashTable/Hash-Functions/","title":"Hash Functions"},"frontmatter":{"draft":false},"rawBody":"# Hash Functions\n\nCreated: 2020-01-07 12:52:07 +0500\n\nModified: 2020-01-20 18:16:29 +0500\n\n---\n\n**Bad Hash Functions**\n-   First 3 digits of a phone number\n-   Memory address of an object (will only be power of 2's so odd locations will never get filled)\n**MurmurHash / MurmurHash3**\n\nMurmurHashis a non-[cryptographic](https://en.wikipedia.org/wiki/Cryptographic_hash_function)[hash function](https://en.wikipedia.org/wiki/Hash_function)suitable for general hash-based lookup.It was created by Austin Appleby in 2008and is currently hosted on GitHub along with its test suite named 'SMHasher'. It also exists in a number of variants,all of which have been released into the public domain. The name comes from two basic operations, multiply (MU) and rotate (R), used in its inner loop.\nUnlike[cryptographic hash functions](https://en.wikipedia.org/wiki/Cryptographic_hash_function), it is not specifically designed to be difficult to reverse by an adversary, making it unsuitable for cryptographic purposes.\n<https://en.wikipedia.org/wiki/MurmurHash>\n\n## SipHash (for strings in python dictionary implementation)**\n\nSipHashis a relatively fast hash function. On a 64-bit machine, SipHash returns a 64-bit hash. The hash is then converted into an index to be used in an array.\nSipHashis an[add--rotate--xor](https://en.wikipedia.org/wiki/Block_cipher#ARX_(add%E2%80%93rotate%E2%80%93xor))(ARX) based family of[pseudorandom functions](https://en.wikipedia.org/wiki/Pseudorandom_function)created by[Jean-Philippe Aumasson](https://en.wikipedia.org/w/index.php?title=Jean-Philippe_Aumasson&action=edit&redlink=1)and[Daniel J. Bernstein](https://en.wikipedia.org/wiki/Daniel_J._Bernstein)in 2012, in response to a spate of \"hash flooding\"[denial-of-service attacks](https://en.wikipedia.org/wiki/Denial-of-service_attack)in late 2011.\nAlthough designed for use as a[hash function](https://en.wikipedia.org/wiki/Hash_function)in the computer science sense, SipHash is fundamentally different from[cryptographic hash functions](https://en.wikipedia.org/wiki/Cryptographic_hash_functions)like[SHA](https://en.wikipedia.org/wiki/Secure_Hash_Algorithm)in that it is only suitable as a[message authentication code](https://en.wikipedia.org/wiki/Message_authentication_code): akeyedhash function like[HMAC](https://en.wikipedia.org/wiki/HMAC). That is, SHA is designed so that it is difficult for an attacker to find two messagesXandYsuch that SHA(X) = SHA(Y), even though anyone may compute SHA(X). SipHash instead guarantees that, having seenXiand SipHash(Xi,k), an attacker who does not know the keykcannot find (any information about)kor SipHash(Y,k) for any messageYâˆ‰ {Xi} which they have not seen before.\n<https://en.wikipedia.org/wiki/SipHash>\n"},{"fields":{"slug":"/Data-Structures/HashTable/Hash-Tables/","title":"Hash Tables"},"frontmatter":{"draft":false},"rawBody":"# Hash Tables\n\nCreated: 2018-02-07 09:20:14 +0500\n\nModified: 2021-04-21 20:28:39 +0500\n\n---\n\n*hash tables, a data structure that achieves constant-time performance for core symbol table operations, provided that search keys are standard data types or simply defined.*\nHash tables utilize hashing to form a data structure.\nHash tables use an associative method to store data by using what is known as a key-value lookup system. All that means is that, in a hash table, keys are mapped to unique values.\nThis system of organizing data results in a very fast way to find data efficiently. This is because since each key is mapped to a unique value -- once we know a key then we can find the associated value instantly.\nHash tables are extremely fast, having a time complexity that is in the order of O(1).\nA function that takes a key and reduces it to an integer (array index) to be mapped to an array.\n\n**Issues -**\n-   Computing the hash function\n-   Equality test\n-   Collision resolution: Algorithm and data structure to handle two keys that hash to the same array index\n\n**Classic space-time tradeoff -**\n-   No space limitation: trivial has function with key as index\n-   No time limitation: trivial collision resolution with sequential search\n**Computing the hash function -**\n\nScramble the keys uniformly to produce a table index\n-   Efficiently computable\n-   Each table index equally likely for each key\n**NOTE: Always try to use all the data for calculating hash (try to involve all the bits for computing the hash function)**\n**Modular Hashing**\n\nHash code - An int between -2^31 and 2^31 - 1\n\nHash function - An int between 0 to M-1 (for use as array index)\n-   First convert from negative to positive and then use modulus to get it to the size.\n\n![private int hash(Key key) { return (key . hashCode() & Ox7fffffff) % M; correct ](media/Hash-Tables-image1.png)\n**Uniform hashing assumption -**\n\nEach key is equally likely to hash to an integer between 0 to M - 1.\n\n1.  Bins and balls Model: Throw balls uniformly at random into M bins\n\n2.  Birthday problem: Expect two balls in the same bin after ~ sqrt(pie M / 2) tosses.\n\n3.  Coupon collector problem: Expect every bin has >= 1 ball after ~ M ln M tosses.\n\n4.  Load balancing: After M tosses, expect most loaded bin has theta(log M / log log M) balls.\n\n**Collisions -** Two distinct keys hashing to same index\n-   Birthday problem: can't avoid collisions unless you have a ridiculous (quadratic) amount of memory.\n-   Coupon collector + load balancing: collisions are evenly distributed\n\n**Collision Resolution methods**\n\n1.  **Separate Chaining symbol table**\n\nUse linked list for storing the values that hash to the same value.\n![Separate chaining](media/Hash-Tables-image2.png)\n\n-   Easier to implement delete\n-   Performance degrades gracefully\n-   Clustering less sensitive to poorly-designed hash function\n2.  **Linear Probing (Open addressing)**\n\nWhen a new key collides, find next empty slot, and put it there\n![Linear probing](media/Hash-Tables-image3.png)\n\n-   Less wasted space\n-   Better cache performance\n\n![Linear probing](media/Hash-Tables-image4.png)\n![Linear probing](media/Hash-Tables-image5.png)\n\n**Problems**\n-   Clustering\n-   Hash functions are often publically available (like java) so if some client has implemented back-end in java, then an adversary can send some data that can make the Hashing perform poorly and can make the system crash\n\n**Optimization**\n\n1.  Create a bigger hash table when older hash table gets full and rehash all old values to new hash table\n\n2.  Can use Consistent Hashing Functions -\n\nConsistent Hashing is a special kind of hashing such that when a hash table is resized, only K/n keys need to be remapped on average, where K is the number of the keys, and n is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and slots are defined by a modular operation.\n\n**Hash Table Context -**\n\nOne-way hash function - \"Hard\" to find a key that will hash to a desired value (or two keys that hash to same value)\n\nEx - MD4, MD5, SHA-0, SHA-1, SHA-2, WHIRLPOOL, RIPEMD-160\n\nApplications - Digital fingerprint, message digest, storing passwords.\nHash tables vs. balanced search trees\n\nHash tables.\n\nãƒ»Simpler to code.\n\nãƒ»No effective alternative for unordered keys.\n\nãƒ»Faster for simple keys (a few arithmetic ops versus log *N* compares).\n\nãƒ»Better system support in Java for strings (e.g., cached hash code).\nBalanced search trees.\n\nãƒ»Stronger performance guarantee.\n\nãƒ»Support for ordered ST operations.\n\nãƒ»Easier to implement compareTo() correctly than equals() and hashCode().\nJava system includes both.\n\nãƒ»Red-black BSTs: java.util.TreeMap, java.util.TreeSet.\n\nãƒ»Hash tables: java.util.HashMap, java.util.IdentityHashMap.\n**Implementation -**\n\nAll java classes inherit a method hashCode(), which returns a 32 bit int\n\n**Integer -** return value\n\n**Double** - convert 64 bit to 32 bit by XOR(^) MSB 32 bot LSB 32\n\n**Boolean -** return fixed numbers\n\n**Strings -** Use Horner's method to hash string and cache the hash value in an instance variable for better performance, since strings are immutable.\n\n**User-defined types -** Include all method variables in the hash code evaluation. Use hashCode implementation for each data type with some small prime number and multiply by 31\n\nãƒ»Combine each significant field using the 31*x* + *y* rule.\n\nãƒ»If field is a primitive type, use wrapper type hashCode().\n\nãƒ»If field is null, return 0.\n\nãƒ»If field is a reference type, use hashCode().\n\nãƒ»If field is an array, apply to each entry. or use Arrays.deepHashCode()\n<http://blog.chapagain.com.np/hash-table-implementation-in-python-data-structures-algorithms>"},{"fields":{"slug":"/Data-Structures/HashTable/Hashing-Techniques/","title":"Hashing Techniques"},"frontmatter":{"draft":false},"rawBody":"# Hashing Techniques\n\nCreated: 2020-01-20 18:15:29 +0500\n\nModified: 2020-12-27 15:51:16 +0500\n\n---\n\n1.  **Separate chaining (open hashing)**\n\n    a.  Chained hash table (Using linked list if collision)\n\nChainingis where each item in the hash table array is a list. When an item is added to the array at an index, it's added to corresponding list.\n![](media/Hashing-Techniques-image1.png)\n**Variants**\n-   Two-probe hashing\n    -   Hash to two positions, insert key in shorter of the two chains\n    -   Reduces expected length of the longest chain to log log N\n2.  **Open Addressing (closed hashing)**\n\nOpen addressinghandles collisions by searching for an empty slot in the array by following a deterministic sequence. This checking is known asprobing, and the sequence is known as a probing sequence. A simple probing sequence is to check each slot one after the other.\n![Collision! ](media/Hashing-Techniques-image2.png)\n\nThe benefits of this approach are:\n-   **Predictable memory usage\n    **No allocation of new nodes when keys are inserted\n-   **Less memory overhead\n    **No next pointers\n-   **Memory locality\n    **A linear memory layout provides better cache characteristics\nProbing techniques / Searching techniques / Search sequences\n\ni.  **Linear Probing -** Find next empty slot and put the key there\n\nii. **Double Hashing -** Use two hash functions, if there is collision on first hash, use second hash function to get the bucket address.\n\niii. **Quadratic probing -** Quadratic probing operates by taking the original hash index and adding successive values of an arbitrary[quadratic polynomial](https://en.wikipedia.org/wiki/Quadratic_polynomial)until an open slot is found.\n     -   Use linear probing, but skip a variable amount, not just 1 each time.\n     -   Effectively eliminates clustering\n     -   Can allow table to become nearly full\n     -   More difficult to implement delete\n![å·¥ å‹ qnoa 61Jlqo ã€ d 0 ä¸€ ä¸€ e ã€ peno 6UlqOJd ã€ 2 â‘  u ã‚³ VVVVV ](media/Hashing-Techniques-image3.png)\n**Removal**\n-   Tombstones (Can create contamination)\n**Variants**\n-   Coalesced Hashing\n-   Cuckoo Hashing\n-   Robin Hood Hashing\n-   Hopscotch Hashing\n<https://programming.guide/hash-tables-open-addressing.html>\n3.  **Coalesced Hashing**\n\nCoalesced hashing is a technique for implementing a hash table. It's an[open addressing](https://programming.guide/hash-tables-open-addressing.html) technique which means that all keys are**stored in the array itself**(e.g. it doesn't use for example linked lists to handle collisions). As opposed to other open addressing techniques however, it also uses**nodes with next-poiners**to form collision chains.\n**Example:**Coalesced hash table holding five keys in two collision chains. (Keys of the same color hash to the same bucket.)\n\n![](media/Hashing-Techniques-image4.png)\n**Removal**\n-   clearing out a slot might break a chain, and cause future lookups to fail. To avoid this problem, one could instead use 'deleted' markings but this is subject to so called **contamination**.-   The approach commonly used in practice is to clear the slot holding the key, and then**reinsert**all following keys in the chain.\n-   This maintains the invariants, avoids contamination and potentially even breaks apart previously coalesced chains.\n**Cellar**\n\nA common optimization (so common in fact, that it is almost to be considered a part of the standard implementation) is to reserve part of the hash table array to be used only for storing colliding keys. This part is called**the cellar**.\n**Example:**A coalescing hash table array with*M*âŸ=âŸ10and*N*âŸ=âŸ3.\n\n![The addressable part: The slots to which keys can hash to. The cellar: Slots used when dealing with collisions. ](media/Hashing-Techniques-image5.png)\n<https://programming.guide/coalesced-hashing.html>\n4.  **Robin hood hashing**\n5.  **Cuckoo Hashing**\n\nCuckoo Hashing is a technique for implementing a hash table. As opposed to most other hash tables, it achieves**constant time worst-case complexity for lookups**.\nCollisions are handled by evicting existing keys and moving them from one array to the other. This resembles the way a cuckoo chick[pushes out an egg from the nest](https://www.youtube.com/watch?v=SO1WccH2_YM)to make room for itself, hence the name Cuckoo Hashing.\n**Representation**\n\nIt is implemented using**two arrays of equal size**and**two hash functions**:\n**Insertion**\n\nA new element is always inserted in the first hash table. Should a collision occur, the existing element is kicked out and inserted in the second hash table. Should that in turn cause a collision, the second existing element will be kicked out and inserted in the first hash table, and so on. This continues until an empty bucket is found.\nIf the number of displacements reaches a certain threshold (for instance due to a cycle among the inserted keys) rehashing takes place.\nRehashing is a linear operation, so**worst-case complexity is*O*(*n*)**. Just as with other hashing techniques however,**the ammortized run time can be shown to be*O*(1)**.\n**Stashing**\n\nThere's a small probability that a cycle is formed among the first few elements inserted. This may trigger a rehash even at a low load factor. To mitigate this, a constant-sized array called the**stash**can be used.\nWhen a key is to be inserted, and a free bucket can't be found, the key is stored in the stash instead. The lookup algorithm is modified to search in the stash in addition to the two arrays. Rehashing is performed when a key can't be inserted and the stash is full.\nEven with a stash of just three or four cells, rehashing can be postponed significantly and allow the hash table to function with higher load factors.\n**D-Cuckoo Hashing**\n\nCuckoo hashing can be generalized to use an arbitrary but fixed number of internal hash tables.\n<https://programming.guide/cuckoo-hashing.html>\n\n## Separate chaining vs. linear probing**\n\n**Separate chaining**\n-   Easier to implement delete.\n-   Performance degrades gracefully.\n-   Clustering less sensitive to poorly-designed hash function.\n**Linear probing**\n-   Less wasted space.\n-   Better cache performance."},{"fields":{"slug":"/Data-Structures/HashTable/Hashing/","title":"Hashing"},"frontmatter":{"draft":false},"rawBody":"# Hashing\n\nCreated: 2017-09-12 08:22:41 +0500\n\nModified: 2021-10-25 22:56:10 +0500\n\n---\n\n<https://www.hackerearth.com/practice/data-structures/hash-tables/basics-of-hash-tables/tutorial>\n\n## Hash Function**\n\nA**hash function**is any[function](https://en.wikipedia.org/wiki/Function_(mathematics))that can be used to map[data](https://en.wikipedia.org/wiki/Data_(computing))of arbitrary size to data of fixed size. The values returned by a hash function are called*hash values*,*hash codes*,*digests*, or simply*hashes*. The values are used to index a fixed-size table called a*hash table*. Use of a hash function to index a hash table is called *hashing* or *scatter storage addressing*.\n**Consistent Hashing**\n\nConsistent Hashing is a special kind of hashing such that when a hash table is resized, only K/n keys need to be remapped on average, where K is the number of the keys, and n is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and slots are defined by a modular operation.\n<https://www.akamai.com/es/es/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf>\n\n<https://www.toptal.com/big-data/consistent-hashing>\n\n## Modular based hashing**\n**Applications**\n-   **Associative arrays:** Hash tables are commonly used to implement many types of in-memory tables. They are used to implement associative arrays (arrays whose indices are arbitrary strings or other complicated objects).\n-   **Database indexing:** Hash tables may also be used as disk-based data structures and database indices (such as in dbm).\n-   **Caches:** Hash tables can be used to implement caches i.e. auxiliary data tables that are used to speed up the access to data, which is primarily stored in slower media.\n-   **Object representation:** Several dynamic languages, such as Perl, Python, JavaScript, and Ruby use hash tables to implement objects.\n-   Hash Functions are used in various algorithms to make their computing faster\n-   De-Duplication\n    -   Remove duplicates\n        -   Report unique visitors to web site\n        -   avoid duplicates in search results\n        -   Do not crawl same page twice\n-   Symbol tables in compilers\n-   Blocking network traffic\n-   Search algorithms (e.g. game tree exploration)\n    -   use hash table to avoid exploring any configuration (e.g. arrangement of chess pieces) more than once\n**How to choose n = # of buckets (When using modulus to hash values to buckets)**\n\n1.  Choose n to be prime (should be few factors, within constant factor of # of objects in table)\n\n2.  not too close to a power of 2\n\n3.  not too close to a power of 10\n**The load of a hash table**\n\n**alpha = # of objects in hash table / # of buckets of hash table**\nQuestion - which hash table implementation strategy is feasible for load factors larger than 1 (Answer - Only chaining, because # of objects are greater than # of buckets, so only using linked list for collision can be used)\nTherefore load factor for a hash table must be << 1, for constant time operations, since if chaining is used, than we have to use exhaustive search for getting values whose key hash to same place\nSo if load factor increases, we increase the number of buckets.\nA super hash function, that can handle all types of data **does not exists** (for every hash function there exists a pathological data set)\n**Solution to pathological data set**\n-   Use a cryptographic hash function (e.g. SHA-2)\n\nThis works because it's infeasible to reverse engineer a pathological data set\n-   Use randomization\n\nDeisgn a family of hash function, and choose randomly. Such that, for all data sets S, \"almost all\" functions spread out \"pretty evenly\"\n**Universal Hashing**\n**Perfect Hash Function**\n\nA hash function that maps each item into a unique slot is referred to as a perfect hash function.\n**Hashing Integer Data Types**\n-   **Identity Hash Function**\n-   **Trivial Hash Function**\n-   **Folding Method**\n    -   The folding method for constructing the hash functions begins by dividing the item into equal-size pieces (the last piece may not be of equal size)\n    -   These pieces are then added together to give the resulting hash value.-   **Mid-square method**\n    -   We first square the item, and then extract some portion of the resulting digits.\n    -   For example, if the item were 44, we would first compute 44^2^ = 1936\n    -   By extracting the middle two digits, 93, and performing the remainder step, we get 93 % 11 = 5-   **Division hashing**\n-   **Algebraic coding**\n-   **Unique permuatation hashing**\n-   **Multiplicative hashing**\n-   **Fibonacci hashing**\n-   **Zobrist hashing**\n**Hashing Variable Length Data**\n-   **Middle and ends**\n-   **Character folding**\n-   **Word length folding**\n-   **Radix conversion hashing**\n-   **Rolling hash**\n**2-choice Hashing**\n\nExplain the 2-choice hashing algorithm for exact-match hash tables.\n\nIn the 2-choice hashing algorithm, we use two independent hash functions to compute two separate indices into the hash table. Then, we look at both indices and find whichever one has lower occupancy.We insert a new element at this index. To lookup an element, we again hash the element using both hash functons to find 2 indices, and compare the element against the elements stored at each of the 2 indices.\nOn what metric is the 2-choice algorithm better than the standard hashing algorithm? Why?\n\nThe 2-choice algorithm is better on the metric of likelihood of overflowing the hash tables given a certain number of inserts into the hash table (equivalently, given a certain occupancy ratio). This is because the 2-choice algorithm explicitly favors the hash table location that has lower occupancy, which means that it is less likely to overflow an already full location in the hash table. The standard hashing algorithm, on the other hand, pays no attention to how occupied a location is.\n<https://en.wikipedia.org/wiki/2-choice_hashing>\n\n## 2-left Hashing**\n\nA[dictionary](https://xlinux.nist.gov/dads/HTML/dictionary.html)implemented with two[hash tables](https://xlinux.nist.gov/dads/HTML/hashtab.html)of equal size, T1and T2, and two different[hash functions](https://xlinux.nist.gov/dads/HTML/hash.html), h1and h2. A new[key](https://xlinux.nist.gov/dads/HTML/key.html)is put in table 2 only if there are fewer (colliding) keys at T2[h2(key)] than at T1[h1(key)], otherwise it is put in table 1. With n keys and two tables of size n/2, the most collisions is 0.69... log2ln n +[O](https://xlinux.nist.gov/dads/HTML/bigOnotation.html)(1) with high probability.\n**Hash Table Implementations**\n-   Using Array\n-   Alternatively, we can implement the hash table with a binary search tree. We can then guarantee an 0(log n) lookup time, since we can keep the tree balanced. Additionally, we may use less space, since a large array no longer needs to be allocated in the very beginning.\n**Locality Sensitive Hashing (LSH)**\n\nIn computer science,locality-sensitive hashing(LSH) is an algorithmic technique that hashes similar input items into the same \"buckets\" with high probability.(The number of buckets are much smaller than the universe of possible input items.)Since similar items end up in the same buckets, this technique can be used for[data clustering](https://en.wikipedia.org/wiki/Cluster_analysis) and [nearest neighbor search](https://en.wikipedia.org/wiki/Nearest_neighbor_search). It differs from[conventional hashing techniques](https://en.wikipedia.org/wiki/Hash_function)in that hash collisions are maximized, not minimized. Alternatively, the technique can be seen as a way to[reduce the dimensionality](https://en.wikipedia.org/wiki/Dimension_reduction)of high-dimensional data; high-dimensional input items can be reduced to low-dimensional versions while preserving relative distances between items.\nHashing-based approximate[nearest neighbor search](https://en.wikipedia.org/wiki/Nearest_neighbor_search)algorithms generally use one of two main categories of hashing methods: either data-independent methods, such as locality-sensitive hashing (LSH); or data-dependent methods, such as[Locality-preserving hashing](https://en.wikipedia.org/wiki/Locality-preserving_hashing)(LPH).\n<https://en.wikipedia.org/wiki/Locality-sensitive_hashing>\n\n<https://towardsdatascience.com/understanding-locality-sensitive-hashing-49f6d1f6134>\n\n## Further Reading**\n-   Birthday Paradox\n-   Pigeonhole Principle\n<https://en.wikipedia.org/wiki/Hash_function>\n"},{"fields":{"slug":"/Data-Structures/HashTable/HyperLogLog/","title":"HyperLogLog"},"frontmatter":{"draft":false},"rawBody":"# HyperLogLog\n\nCreated: 2018-05-17 22:17:36 +0500\n\nModified: 2020-01-08 23:25:26 +0500\n\n---\n\nHyperLogLog is a streaming algorithm used for estimating the number of distinct elements (the cardinality) of very large data sets. HyperLogLog counter can count one billion distinct items with an accuracy of 2% using only 1.5 KB of memory. It is based on the bit pattern observation that for a stream of randomly distributed numbers, if there is a number x with the maximum of leading 0 bits k, the cardinality of the stream is very likely equal to 2^k.\nHyperLogLog, it's a statistical data structure that derives approximations- in O(1) time complexity and O(log(log(n)) space complexity. The catch is that you get about 1.5% accuracy, configurable of course by taking up more space. As an example, 1.3KB can estimate the cardinality of tens of billions of unique values with an accuracy of a few percent.\n**Reference**\n\n<https://dzone.com/articles/introduction-probabilistic-0>\n"},{"fields":{"slug":"/Data-Structures/HashTable/Kademlia/","title":"Kademlia"},"frontmatter":{"draft":false},"rawBody":"# Kademlia\n\nCreated: 2019-03-17 19:01:12 +0500\n\nModified: 2019-03-17 19:07:04 +0500\n\n---\n\nKademliais a[distributed hash table](https://en.wikipedia.org/wiki/Distributed_hash_table)for decentralized[peer-to-peer](https://en.wikipedia.org/wiki/Peer-to-peer)[computer networks](https://en.wikipedia.org/wiki/Computer_network)designed by Petar Maymounkov and David MaziÃ¨res in 2002.[[1]](https://en.wikipedia.org/wiki/Kademlia#cite_note-kademlia-paper-1)[[2]](https://en.wikipedia.org/wiki/Kademlia#cite_note-2)It specifies the structure of the network and the exchange of information through[node](https://en.wikipedia.org/wiki/Node_(networking))lookups. Kademlia nodes communicate among themselves using[UDP](https://en.wikipedia.org/wiki/User_Datagram_Protocol). A virtual or[overlay network](https://en.wikipedia.org/wiki/Overlay_network)is formed by the participant nodes. Each node is identified by a number ornode ID. Thenode IDserves not only as identification, but the Kademlia algorithm uses thenode IDto locate values (usually file[hashes](https://en.wikipedia.org/wiki/Hash_function)or keywords). In fact, thenode IDprovides a direct map to file hashes and that node stores information on where to obtain the file or resource.\nWhen searching for some value, the algorithm needs to know the associated key and explores the network in several steps. Each step will find nodes that are closer to the key until the contacted node returns the value or no more closer nodes are found. This is very efficient: like many other DHTs, Kademlia contacts onlyO(log(n))nodes during the search out of a total ofnnodes in the system.\nFurther advantages are found particularly in the decentralized structure, which increases the resistance against a[denial-of-service attack](https://en.wikipedia.org/wiki/Denial-of-service_attack). Even if a whole set of nodes is flooded, this will have limited effect on network availability, since the network will recover itself by knitting the network around these \"holes\".\n**References**\n\n<https://en.wikipedia.org/wiki/Kademlia>\n\n<https://github.com/bmuller/kademlia>\n"},{"fields":{"slug":"/Data-Structures/HashTable/List-of-Hash-Functions/","title":"List of Hash Functions"},"frontmatter":{"draft":false},"rawBody":"# List of Hash Functions\n\nCreated: 2020-01-07 12:53:09 +0500\n\nModified: 2020-01-07 22:37:19 +0500\n\n---\n\n**Cyclic redundancy checks**\n\nMain article:[Cyclic redundancy check](https://en.wikipedia.org/wiki/Cyclic_redundancy_check)\n\n| Name                                                                                            | Length  | Type                                                                              |\n|------------------------|--------------|-----------------------------------|\n| [cksum (Unix)](https://en.wikipedia.org/wiki/Cksum)                                             | 32 bits | [CRC](https://en.wikipedia.org/wiki/Cyclic_redundancy_check)with length appended |\n| [CRC-16](https://en.wikipedia.org/wiki/CRC-16)                                                  | 16 bits | [CRC](https://en.wikipedia.org/wiki/Cyclic_redundancy_check)                      |\n| [CRC-32](https://en.wikipedia.org/wiki/CRC-32)                                                  | 32 bits | [CRC](https://en.wikipedia.org/wiki/Cyclic_redundancy_check)                      |\n| [CRC-32 MPEG-2](https://en.wikipedia.org/w/index.php?title=CRC-32_MPEG-2&action=edit&redlink=1) | 32 bits | [CRC](https://en.wikipedia.org/wiki/Cyclic_redundancy_check)                      |\n| [CRC-64](https://en.wikipedia.org/wiki/CRC-64)                                                  | 64 bits | [CRC](https://en.wikipedia.org/wiki/Cyclic_redundancy_check)                      |\n\n[Adler-32](https://en.wikipedia.org/wiki/Adler-32)is often mistaken for a CRC, but it is not, it is a[checksum](https://en.wikipedia.org/wiki/List_of_hash_functions#Checksums).\n**Checksums**\n\nMain article:[Checksum](https://en.wikipedia.org/wiki/Checksum)\n\n| Name                                                                            | Length          | Type                                                                                                               |\n|--------------------------|-------------------|----------------------------|\n| [BSD checksum (Unix)](https://en.wikipedia.org/wiki/BSD_checksum)               | 16 bits         | sum with circular rotation                                                                                         |\n| [SYSV checksum (Unix)](https://en.wikipedia.org/wiki/SYSV_checksum)             | 16 bits         | sum with circular rotation                                                                                         |\n| sum8                                                                            | 8 bits          | sum                                                                                                                |\n| [sum16](https://en.wikipedia.org/w/index.php?title=Sum16&action=edit&redlink=1) | 16 bits         | sum                                                                                                                |\n| sum24                                                                           | 24 bits         | sum                                                                                                                |\n| sum32                                                                           | 32 bits         | sum                                                                                                                |\n| [fletcher-4](https://en.wikipedia.org/wiki/Fletcher%27s_checksum)               | 4 bits          | sum                                                                                                                |\n| [fletcher-8](https://en.wikipedia.org/wiki/Fletcher%27s_checksum)               | 8 bits          | sum                                                                                                                |\n| [fletcher-16](https://en.wikipedia.org/wiki/Fletcher%27s_checksum)              | 16 bits         | sum                                                                                                                |\n| [fletcher-32](https://en.wikipedia.org/wiki/Fletcher%27s_checksum)              | 32 bits         | sum                                                                                                                |\n| [Adler-32](https://en.wikipedia.org/wiki/Adler-32)                              | 32 bits         | sum                                                                                                                |\n| [xor8](https://en.wikipedia.org/wiki/Longitudinal_redundancy_check)             | 8 bits          | sum                                                                                                                |\n| [Luhn algorithm](https://en.wikipedia.org/wiki/Luhn_algorithm)                  | 1 decimal digit | sum                                                                                                                |\n| [Verhoeff algorithm](https://en.wikipedia.org/wiki/Verhoeff_algorithm)          | 1 decimal digit | sum                                                                                                                |\n| [Damm algorithm](https://en.wikipedia.org/wiki/Damm_algorithm)                  | 1 decimal digit | [Quasigroup](https://en.wikipedia.org/wiki/Quasigroup)[operation](https://en.wikipedia.org/wiki/Binary_operation) |\n**Universal hash function families**\n\nMain article:[Universal hashing](https://en.wikipedia.org/wiki/Universal_hashing)\n\n| Name                                                                                             | Length   | Type     |\n|--------------------------------------------|---------------|--------------|\n| [Rabin fingerprint](https://en.wikipedia.org/wiki/Rabin_fingerprint)                             | variable | multiply |\n| [tabulation hashing](https://en.wikipedia.org/wiki/Tabulation_hashing)                           | variable | XOR      |\n| [universal one-way hash function](https://en.wikipedia.org/wiki/Universal_one-way_hash_function) |         |         |\n| [Zobrist hashing](https://en.wikipedia.org/wiki/Zobrist_hashing)                                 | variable | XOR      |\n**Non-cryptographic hash functions**\n\n<table>\n<colgroup>\n<col style=\"width: 37%\" />\n<col style=\"width: 29%\" />\n<col style=\"width: 32%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>Name</th>\n<th>Length</th>\n<th>hideType</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><a href=\"https://en.wikipedia.org/wiki/Pearson_hashing\">Pearson hashing</a></td>\n<td>8 bits (or more)</td>\n<td>XOR/table</td>\n</tr>\n<tr class=\"even\">\n<td>Paul Hsieh's SuperFastHash<a href=\"https://en.wikipedia.org/wiki/List_of_hash_functions#cite_note-1\">[1]</a></td>\n<td>32 bits</td>\n<td></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://en.wikipedia.org/wiki/Rolling_hash#Cyclic_polynomial\">Buzhash</a></td>\n<td>variable</td>\n<td>XOR/table</td>\n</tr>\n<tr class=\"even\">\n<td><p><a href=\"https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function\">Fowlerâ€“Nollâ€“Vo hash function</a></p>\n<p>(FNV Hash)</p></td>\n<td><p>32, 64, 128, 256,</p>\n<p>512, or 1024 bits</p></td>\n<td><p>xor/product or</p>\n<p>product/XOR</p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://en.wikipedia.org/wiki/Jenkins_hash_function\">Jenkins hash function</a></td>\n<td>32 or 64 bits</td>\n<td>XOR/addition</td>\n</tr>\n<tr class=\"even\">\n<td>Bernstein hashdjb2</td>\n<td>32 bits</td>\n<td></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://en.wikipedia.org/wiki/PJW_hash_function\">PJW hash / Elf Hash</a></td>\n<td>32 or 64 bits</td>\n<td>add,shift,xor</td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://en.wikipedia.org/wiki/MurmurHash\">MurmurHash</a></td>\n<td>32, 64, or 128 bits</td>\n<td>product/rotation</td>\n</tr>\n<tr class=\"odd\">\n<td>Fast-Hash</td>\n<td>32, 64 bits</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Xorshift\">xorshift</a>operations</td>\n</tr>\n<tr class=\"even\">\n<td>SpookyHash</td>\n<td>32, 64, or 128 bits</td>\n<td>see<a href=\"https://en.wikipedia.org/wiki/Jenkins_hash_function\">Jenkins hash function</a></td>\n</tr>\n<tr class=\"odd\">\n<td>CityHash</td>\n<td>32, 64, 128, or 256 bits</td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>FarmHash</td>\n<td>32, 64 or 128 bits</td>\n<td></td>\n</tr>\n<tr class=\"odd\">\n<td>MetroHash</td>\n<td>64 or 128 bits</td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>numeric hash (nhash)</td>\n<td>variable</td>\n<td>division/modulo</td>\n</tr>\n<tr class=\"odd\">\n<td>xxHash</td>\n<td>32, 64 bits</td>\n<td>product/rotation</td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://en.wikipedia.org/w/index.php?title=T1ha_(Fast_Positive_Hash)&amp;action=edit&amp;redlink=1\">t1ha (Fast Positive Hash)</a><a href=\"https://en.wikipedia.org/wiki/List_of_hash_functions#cite_note-t1ha_github-9\">[9]</a></td>\n<td>64 and 128 bits</td>\n<td>product/rotation/XOR/add</td>\n</tr>\n</tbody>\n</table>\n\n## Keyed cryptographic hash functions**\n\nMain article:[Message authentication code](https://en.wikipedia.org/wiki/Message_authentication_code)\n\n| Name                                                                     | Tag Length | hideType                                                                                              |\n|-----------------------------|-------------|-------------------------------|\n| [BLAKE2](https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE2)     | arbitrary  | keyed hash function (prefix-MAC)                                                                      |\n| [HMAC](https://en.wikipedia.org/wiki/HMAC)                               |           |                                                                                                      |\n| [KMAC](https://en.wikipedia.org/wiki/SHA-3#Additional_instances)         | arbitrary  | based on Keccak                                                                                       |\n| [MD6](https://en.wikipedia.org/wiki/MD6)                                 | 512 bits   | [Merkle tree](https://en.wikipedia.org/wiki/Merkle_tree)[NLFSR](https://en.wikipedia.org/wiki/NLFSR) |\n| [One-key MAC](https://en.wikipedia.org/wiki/One-key_MAC)(OMAC; CMAC)    |           |                                                                                                      |\n| [PMAC (cryptography)](https://en.wikipedia.org/wiki/PMAC_(cryptography)) |           |                                                                                                      |\n| [Poly1305-AES](https://en.wikipedia.org/wiki/Poly1305-AES)               | 128 bits   | nonce-based                                                                                           |\n| [SipHash](https://en.wikipedia.org/wiki/SipHash)                         | 64 bits    | non-collision-resistant PRF                                                                           |\n| [UMAC](https://en.wikipedia.org/wiki/UMAC)                               |           |                                                                                                      |\n| [VMAC](https://en.wikipedia.org/wiki/VMAC)                               |           |                                                                                                      |\n**Unkeyed cryptographic hash functions**\n\nMain article:[Cryptographic hash function](https://en.wikipedia.org/wiki/Cryptographic_hash_function)\n\nSee also:[Comparison of cryptographic hash functions](https://en.wikipedia.org/wiki/Comparison_of_cryptographic_hash_functions)\n\n| Name                                                                               | Length          | hideType                                                                                                                                 |\n|-------------------|-------------|----------------------------------------|\n| [BLAKE-256](https://en.wikipedia.org/wiki/BLAKE_(hash_function))                   | 256 bits        | HAIFA structure                                                                                                                          |\n| [BLAKE-512](https://en.wikipedia.org/wiki/BLAKE_(hash_function))                   | 512 bits        | HAIFA structure                                                                                                                          |\n| [BLAKE2s](https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE2)              | up to 256 bits  | HAIFA structure                                                                                                                          |\n| [BLAKE2b](https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE2)              | up to 512 bits  | HAIFA structure                                                                                                                          |\n| [BLAKE2X](https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE2)              | arbitrary       | HAIFA structure,extensible-output functions (XOFs) design                                                                               |\n| [ECOH](https://en.wikipedia.org/wiki/Elliptic_curve_only_hash)                     | 224 to 512 bits | hash                                                                                                                                     |\n| [FSB](https://en.wikipedia.org/wiki/Fast_Syndrome_Based_Hash)                      | 160 to 512 bits | hash                                                                                                                                     |\n| [GOST](https://en.wikipedia.org/wiki/GOST_(hash_function))                         | 256 bits        | hash                                                                                                                                     |\n| [GrÃ¸stl](https://en.wikipedia.org/wiki/Gr%C3%B8stl)                                | up to 512 bits  | hash                                                                                                                                     |\n| [HAS-160](https://en.wikipedia.org/wiki/HAS-160)                                   | 160 bits        | hash                                                                                                                                     |\n| [HAVAL](https://en.wikipedia.org/wiki/HAVAL)                                       | 128 to 256 bits | hash                                                                                                                                     |\n| [JH](https://en.wikipedia.org/wiki/JH_(hash_function))                             | 224 to 512 bits | hash                                                                                                                                     |\n| LSH[[12]](https://en.wikipedia.org/wiki/List_of_hash_functions#cite_note-lsw-12) | 256 to 512 bits | wide-pipe[Merkle--DamgÃ¥rd construction](https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction)                         |\n| [MD2](https://en.wikipedia.org/wiki/MD2_(cryptography))                            | 128 bits        | hash                                                                                                                                     |\n| [MD4](https://en.wikipedia.org/wiki/MD4)                                           | 128 bits        | hash                                                                                                                                     |\n| [MD5](https://en.wikipedia.org/wiki/MD5)                                           | 128 bits        | [Merkle--DamgÃ¥rd construction](https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction)                                   |\n| [MD6](https://en.wikipedia.org/wiki/MD6)                                           | up to 512 bits  | [Merkle tree](https://en.wikipedia.org/wiki/Merkle_tree)[NLFSR](https://en.wikipedia.org/wiki/NLFSR)(it is also a keyed hash function) |\n| [RadioGatÃºn](https://en.wikipedia.org/wiki/RadioGat%C3%BAn)                        | arbitrary       | ideal mangling function                                                                                                                  |\n| [RIPEMD](https://en.wikipedia.org/wiki/RIPEMD)                                     | 128 bits        | hash                                                                                                                                     |\n| [RIPEMD-128](https://en.wikipedia.org/wiki/RIPEMD)                                 | 128 bits        | hash                                                                                                                                     |\n| [RIPEMD-160](https://en.wikipedia.org/wiki/RIPEMD)                                 | 160 bits        | hash                                                                                                                                     |\n| [RIPEMD-320](https://en.wikipedia.org/wiki/RIPEMD)                                 | 320 bits        | hash                                                                                                                                     |\n| [SHA-1](https://en.wikipedia.org/wiki/SHA-1)                                       | 160 bits        | [Merkle--DamgÃ¥rd construction](https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction)                                   |\n| [SHA-224](https://en.wikipedia.org/wiki/SHA-2)                                     | 224 bits        | [Merkle--DamgÃ¥rd construction](https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction)                                   |\n| [SHA-256](https://en.wikipedia.org/wiki/SHA-2)                                     | 256 bits        | [Merkle--DamgÃ¥rd construction](https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction)                                   |\n| [SHA-384](https://en.wikipedia.org/wiki/SHA-2)                                     | 384 bits        | [Merkle--DamgÃ¥rd construction](https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction)                                   |\n| [SHA-512](https://en.wikipedia.org/wiki/SHA-2)                                     | 512 bits        | [Merkle--DamgÃ¥rd construction](https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction)                                   |\n| [SHA-3](https://en.wikipedia.org/wiki/SHA-3)(subset of Keccak)                    | arbitrary       | [sponge function](https://en.wikipedia.org/wiki/Sponge_function)                                                                         |\n| [Skein](https://en.wikipedia.org/wiki/Skein_(hash_function))                       | arbitrary       | [Unique Block Iteration](https://en.wikipedia.org/w/index.php?title=Unique_Block_Iteration&action=edit&redlink=1)                        |\n| [Snefru](https://en.wikipedia.org/wiki/Snefru)                                     | 128 or 256 bits | hash                                                                                                                                     |\n| [Spectral Hash](https://en.wikipedia.org/wiki/Spectral_Hash)                       | 512 bits        | wide-pipe Merkle--DamgÃ¥rd construction                                                                                                   |\n| [Streebog](https://en.wikipedia.org/wiki/Streebog)                                 | 256 or 512 bits | [Merkle--DamgÃ¥rd construction](https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction)                                   |\n| [SWIFFT](https://en.wikipedia.org/wiki/SWIFFT)                                     | 512 bits        | hash                                                                                                                                     |\n| [Tiger](https://en.wikipedia.org/wiki/Tiger_(cryptography))                        | 192 bits        | [Merkle--DamgÃ¥rd construction](https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction)                                   |\n| [Whirlpool](https://en.wikipedia.org/wiki/Whirlpool_(cryptography))                | 512 bits        | hash                                                                                                                                     |\n"},{"fields":{"slug":"/Data-Structures/HashTable/Merkle-Trees/","title":"Merkle Trees"},"frontmatter":{"draft":false},"rawBody":"# Merkle Trees\n\nCreated: 2018-09-08 17:33:06 +0500\n\nModified: 2021-12-29 00:11:04 +0500\n\n---\n\nA merkle tree, also known as a binary hash tree, is a data structure used for efficiently summarizing and verifying the integrity of large sets of data.\nA Merkle tree is a[hash-based data structure](https://brilliant.org/wiki/hash-based-data-structure/)that is a generalization of the[hash list](https://brilliant.org/wiki/hash-list/). It is a[tree](https://brilliant.org/wiki/trees-basic/)structure in which each leaf node is a hash of a block of data, and each non-leaf node is a hash of its children. Typically, Merkle trees have a branching factor of 2, meaning that each node has up to 2 children.\nMerkle trees are used in distributed systems for efficient data verification. They are efficient because they use hashes instead of full files. Hashes are ways of encoding files that are much smaller than the actual file itself. Currently, their main uses are in[peer-to-peer networks](https://brilliant.org/wiki/peer-to-peer-networks/?wiki_title=peer-to-peer%20networks)such as Tor,[Bitcoin](https://brilliant.org/wiki/bitcoin/), and Git.\n![Ğ( Ğ(Ğ°Ñ’) + H(cd) = Ğ(Ğ°Ñ’) = ĞÑ„) ](media/Merkle-Trees-image1.png)\n\nImage - A basic Merkle Tree. I've abbreviated the middle nodes as H(cd) and H(ab) respectively, but without this shorthand the root hash could also be called H(H(H(a) + H(b)) + H(H(c) +H(d)))\n<https://brilliant.org/wiki/merkle-tree>\n\n<https://hackernoon.com/merkle-tree-introduction-4c44250e2da7>\n\n"},{"fields":{"slug":"/Data-Structures/HashTable/Probabilistic-Data-Structure/","title":"Probabilistic Data Structure"},"frontmatter":{"draft":false},"rawBody":"# Probabilistic Data Structure\n\nCreated: 2018-05-17 22:09:27 +0500\n\nModified: 2022-04-24 11:40:18 +0500\n\n---\n\nProbabilistic data structures are a group of data structures that are extremely useful for big data and streaming applications. Generally speaking, these data structures use hash functions to randomize and compactly represent a set of items. Collisions are ignored but errors can be well-controlled under certain threshold. Comparing with error-free approaches, these algorithms use much less memory and have constant query time. They usually support union and intersection operations and therefore can be easily parallelized.\n1.  **Membership Query - Bloom filter**\n\n2.  **Cardinality - HyperLogLog**\n\n3.  **Frequency - Count-Min Sketch**\n\n4.  **Frequency Estimation: Count-Mean-Min Sketch**\n\n5.  **Locality-sensitive hashing**(**LSH**)\n![106 distinct values domain of 32-bit integers 4 MB 106 pairs 32-bit value, 24-bit counter 107 elements Estimation, Range Query --- Sorted Table or Hash Map 40 MB Raw Data 0.6 MB Membership Query with 4% error --- Bloom Filter Exact Membership Query, Cardinality Estimation --- Sorted IDs or Hash Table 48 KB Frequenes of top-IOO most frequent elements with 4% error ---Count-Min Sketch â€¢ 14 KB Top-IOO most frequent elements with 4% error --- Stream-summary Cardinality Estimation with 4% error --- Loglog Counter 125 KB Cardinality Estimation with 4% error --- Linear Counter Exact Frequency ](media/Probabilistic-Data-Structure-image1.png)\n**Ribbon Filter (practically smaller than Bloom and Xor)**\n\n<https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter>\n\n<https://arxiv.org/abs/2103.02515>\n\n## Others**\n\n**Frugal Streaming**\n\nIt uses only one unit of memory per group to compute a quantile for each group.\n\n**Use case:** Find the nth percentile of the data stream.\n<https://link.springer.com/chapter/10.1007/978-3-642-40273-9_7>\n\n<https://agkn.wordpress.com/2013/09/16/sketch-of-the-day-frugal-streaming>\nğ—šğ—˜ğ—¢ğ—›ğ—”ğ—¦ğ—›/ ğ—¦ğŸ® ğ—šğ—˜ğ—¢ğ— ğ—˜ğ—§ğ—¥ğ—¬\n\nA collection of efficient yet exact mathematical predicates for testing relationships among geometric primitives.\n\nğ™ğ™¨ğ™š ğ™˜ğ™–ğ™¨ğ™š:Location-based search results with DynamoDb and Geohash.**References**\n\n<https://dzone.com/articles/introduction-probabilistic-0>\n\n<https://highlyscalable.wordpress.com/2012/05/01/probabilistic-structures-web-analytics-data-mining>\n\n<https://www.kdnuggets.com/2019/08/count-big-data-probabilistic-data-structures-algorithms.html>\n\n"},{"fields":{"slug":"/Data-Structures/HashTable/Questions/","title":"Questions"},"frontmatter":{"draft":false},"rawBody":"# Questions\n\nCreated: 2018-08-04 12:29:27 +0500\n\nModified: 2019-12-16 12:11:32 +0500\n\n---\n\n**Hash Table**\n-   Find symmetric pairs in an array\n-   Trace complete path of a journey\n-   Find if an array is a subset of another array\n-   Check if given arrays are disjoint\n**Why databases use ordered indexes but programming uses hash tables**\n\nB-Trees are more \"general purpose,\" which results in lower \"total cost\" for very large persistent data. In other words, even though they are slower for single value accesses that make up the majority of the workload, they are better when you consider rare operations and the cost of multiple indexes.\n\nAnd also range queries are better in B-Trees than in Hash Tables\n<https://www.evanjones.ca/ordered-vs-unordered-indexes.html>\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/2-3-Search-Trees/","title":"2-3 Search Trees"},"frontmatter":{"draft":false},"rawBody":"# 2-3 Search Trees\n\nCreated: 2018-02-05 19:33:10 +0500\n\nModified: 2018-05-21 23:01:07 +0500\n\n---\n\n**Properties**\n-   Every non-leaf is a 2-node or a 3-node. A 2-node contains one data item and has two children. A 3-node contains two data items and has 3 children.\n-   All leaves are at the same level (the bottom level)\n-   All data is kept in sorted order\n-   Every non-leaf node will contain 1 or 2 fields.**2-3 Tree**\n\nAllow 1 or 2 keys per node -\n-   2-node: one key, two children\n-   3-node: two keys, three children\n\nSymmetric order: Inorder traversal yields keys in ascending order\n\nPerfect balance: Every path from root to null links has same length\n\nEach transformation maintains symmetric order and perfect balance\n**Search.**\n\nãƒ»Compare search key against keys in node.\n\nãƒ»Find interval containing search key.\n\nãƒ»Follow associated link (recursively).\n**Insertion into a 3-node at bottom.**\n\nãƒ»Add new key to 3-node to create temporary 4-node.\n\nãƒ»Move middle key in 4-node into parent.\n\nãƒ»Repeat up the tree, as necessary.\n\nãƒ»If you reach the root and it's a 4-node, split it into three 2-nodes.\n\n**Height of a 2-3 tree increases only when root node of the tree splits**\n**Tree height.**\n\nãƒ»Worst case: lg *N*. [all 2-nodes]\n\nãƒ»Best case: log3 *N* â‰ˆ .631 lg *N*. [all 3-nodes]\n\nãƒ»Between 12 and 20 for a million nodes.\n\nãƒ»Between 18 and 30 for a billion nodes.\n\nGuaranteed logarithmic performance for search and insert\n**Direct implementation is complicated, because:**\n\nãƒ»Maintaining multiple node types is cumbersome.\n\nãƒ»Need multiple compares to move down tree.\n\nãƒ»Need to move back up the tree to split 4-nodes.\n\nãƒ»Large number of cases for splitting.\n![3-node smaller than E between E and J 2-node larger than J ](media/2-3-Search-Trees-image1.png)\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/AVL-Tree/","title":"AVL Tree"},"frontmatter":{"draft":false},"rawBody":"# AVL Tree\n\nCreated: 2018-08-03 00:10:03 +0500\n\nModified: 2018-08-03 00:14:06 +0500\n\n---\n\nAVL tree (named after inventors**A**delson-**V**elsky and**L**andis) is a self-balancing Binary Search Tree (BST) where the difference between heights of left and right subtrees cannot be more than one for all nodes.\n\n![12 18 ](media/AVL-Tree-image1.jpg)\n\nThe above tree is AVL because differences between heights of left and right subtrees for every node is less than or equal to 1.\n**Why AVL Trees?**\n\nMost of the BST operations (e.g., search, max, min, insert, delete.. etc) take O(h) time where h is the height of the BST. The cost of these operations may become O(n) for a skewed Binary tree. If we make sure that height of the tree remains O(Logn) after every insertion and deletion, then we can guarantee an upper bound of O(Logn) for all these operations. The height of an AVL tree is always O(Logn) where n is the number of nodes in the tree.\n**References**\n\n<https://www.geeksforgeeks.org/avl-tree-set-1-insertion>\n\n<https://en.wikipedia.org/wiki/AVL_tree>\n\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/B-Tree/","title":"B-Tree"},"frontmatter":{"draft":false},"rawBody":"# B-Tree\n\nCreated: 2018-02-06 09:35:06 +0500\n\nModified: 2020-10-13 23:49:41 +0500\n\n---\n\n**Points**\n-   A generalization of 2-3 tees\n-   n-generalization of a binary search tree\nIn computer science, aB-treeis a tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic amortized time. The B-tree is a n-generalization of a binary search tree in that more than two paths diverge from a single node. Unlike self-balancing binary search trees, the B-tree is optimized for systems that read and write large blocks of data. It is commonly used in databases and filesystems.\nB-Trees are a popular index data structure, coming in many variations and used in many databases, including[MySQL InnoDB](https://dev.mysql.com/doc/refman/5.7/en/innodb-physical-structure.html)and[PostgreSQL](https://www.postgresql.org/docs/9.2/static/indexes-types.html). B-Trees areself-balancing, so there's no rotation step required during insertion and deletion, only merges and splits. The reason to use them for indexes, where lookup time is important, is their logarithmic lookup time guarantee.\nB-Tree is a self-balancing search tree. In most of the other self-balancing search trees (like[AVL](https://www.geeksforgeeks.org/avl-tree-set-1-insertion/)and Red Black Trees), it is assumed that everything is in main memory. To understand use of B-Trees, we must think of huge amount of data that cannot fit in main memory.When the number of keys is high, the data is read from disk in the form of blocks. Disk access time is very high compared to main memory access time. The main idea of using B-Trees is to reduce the number of disk accesses. Most of the tree operations (search, insert, delete, max, min, ..etc ) require O(h) disk accesses where h is height of the tree. B-tree is a fat tree. Height of B-Trees is kept low by putting maximum possible keys in a B-Tree node. Generally, a B-Tree node size is kept equal to the disk block size. Since h is low for B-Tree, total disk accesses for most of the operations are reduced significantly compared to balanced Binary Search Trees like AVL Tree, Red Black Tree, ..etc.\nPage - Contiguous block of data\n\nProbe - First access to a page\n\nB-tree - Generalize 2-3 trees by allowing up to M - 1 key-link pairs per node\n-   At least 2 key-link pairs at root\n-   At least M/2 key-link pairs in other nodes\n-   External nodes contain client keys\n-   Internal nodes contain copies of keys to guide search\n![sentinel key external 3-node client keys (black) are in external nodes 2-node each red key is a copy of min key in subtree external 5-node (full) internal 3-node external 4-node mmmmmz all nodes except the root are 3-, 4- or 5-nodes Anatomy of a B-tree set (M = 6) ](media/B-Tree-image1.png)\n![Image for post](media/B-Tree-image2.png)\nComplexity - A search or an insertion in a B-tree of order M with N keys requires between log ~M-1~ N and log ~M/2~ N probes.\nOptimization -\n\n1.  Always keep root page in memory.\n**B-trees (and variants) are widely used for file systems and databases**\n-   Windows: NTFS\n-   Mac: HFS, HFS+\n-   Linux: ReiserFS, XFS, Ext3FS, JFS\n-   Databases: ORACLE, DB2, INGRES, SQL, PostgreSQL\n**Applications**\n-   Implement File Systems\n**Anatomy**\n\nB-Tree have several node types:Root,InternalandLeafnodes.Rootis the node that has no \"parents\" (e.g. is not a child for any other node).Leafnodes are the ones that have no child nodes and carry the data.Internalnodes are ones that have both a parent and children, they're connecting a root node with leaf nodes. Some B-Tree variants allow storing data on internal nodes. B-Trees are characterised by theirbranching factor: the amount (N) of pointers to the child nodes. Root and Internal nodes hold up to N-1 keys.\n\n![](media/B-Tree-image3.png)\n\nB-Tree consists of Root Node (top), Internal Nodes (middle) and Leaf Nodes (bottom). Leaf nodes usually hold the values and internal nodes are connecting Root to Leaves. This depicts a B-Tree with a branching factor of 4 (4 pointers, 3 keys in Internal nodes and 4 key/value pairs stores onleaves).\nEvery non-leaf node in the tree holdsNkeys (index entries), separating the tree into the subtrees andN+1pointers to the children. Pointerifrom an entryKipoints to a subtree in which all the index entries are such thatKi <= K < K+1(whereKis a set of keys). First and the last pointers are the special cases, pointing to subtrees in which all the entries are less than (or equal), and greater thanK, correspondingly. Logically, internal nodes hold keys, representing aminimumkey of the child node they point to. Both Internal and leaf nodes also hold a pointer to the next and previous nodes on the same level, forming a doubly-linked list of sibling nodes.\nIt's usually good to keep the size of B-Tree node to one or two page sizes (4--8K). Considering this and knowing your key size, you can approximate the branching factor and tree height (number of levels). Height should not be too large, as skipping an entire level requires a random seek and performing too many seeks might cause performance degradation. Branching factor is typically in hundreds; it can, however, be lower when keys are large. Setting branching factor too high might also cause performance problems. A good rule of thumb for B-Tree tuning and picking the branching factor is that the the leaf nodes should occupy the vast majority of tree space.\n**Lookups**\n\nRoot and Internal nodes of the B-Trees can often be cached in RAM to facilitate faster lookups. Since on every level, the amount of child nodes grows by branching factor, the amount of space taken by the leaf nodes will be much larger. Searching the leaf node will be done in memory and search and retrieval of the final value from the leaf node will be served from the disk.\n\nWhen performing lookups, the search starts at the root node and proceeds down to the leaf level. On each level, the algorithm finds the two keys, one of which is smaller and the other one is larger the searched term (so the term is between them), following the child pointer between the keys.\n\n![](media/B-Tree-image4.png)\n\nLookups in B-Tree makes a single Root-to-Leaf pass, following the pointers \"between\" the two keys, one of which is larger than (or equal) to the searched term and the other one is less than the searchedterm.\nSearches within the node are usually performed using binary search. It's clear how to perform binary search with a fixed-size keys: knowing the amount of the items in the sorted array, jump to the middle, perform a comparison making a decision whether to traverse left or right and repeat until the item is found.\n\n![3 4 6 7 8 1 0 1 3 1 4 1 8 1 9 21 24 37 40 45 71 ](media/B-Tree-image5.png)\n\nSearches in the B-Trees have logarithmic complexity, since on the node level, keys are stored in order, and the binary search is performed in order to find a match. This is also why it's important to keep the occupancy high and uniform across thetree.\nWith a variable-lengths data, an indirection vector can be prepended to data, holding offsets to the actual records. Pointers in indirection vector have to be sorted by the search key (actual variable-size data doesn't have to follow the sort). The binary search is performed by picking a middle pointer of indirection vector, performing a comparison in order pick the traversal direction and repeating the operation until the searched key is located.\n\n![](media/B-Tree-image6.png)\n\nIn order to allow binary searches and logarithmic time accesses on variable-size data, an indirection vector is used: an indirection vector holds offsets to the actual data. Binary search starts in the middle of the indirection vector, the key at the offset in the middle is compared to the searched term, binary search continues in the direction shown by comparison.\nIn case with point queries, the search is complete after locating a node. When the range scan is performed, keys and values in the current node and then sibling leaf nodes' keys and values are traversed, until the end of the range is reached.\n**Modifications**\n\nWhen performing insertions, first the target leaf has to be located. For that, the aforementioned search algorithm is used. After the target leaf is located, a key and value are appended to it. If the leaf does not have enough free space, the situation is calledoverflow, and the leaf has to be split in two leaves. This is done by allocating a new leaf, moving half the elements to it and appending a pointer to the newly allocated leaf to the parent. If parent doesn't have enough space either, another split is performed. The operation continues until the root is reached. Usually, when the root is split, it's contents are split between the newly allocated nodes and the root node itself is overwritten in order to avoid relocation. This also implies that the tree height is always growing \"from\" the root, by splitting it.\n\n![10 10 20 30 30 40 ](media/B-Tree-image7.png)\n\nWhen the node gets full (left), it gets split. When the root is split, usually two nodes are created and root keys get distributed between new nodes. When the internal node is split, a sibling is created and it's key and pointer get added to theparent.\nUpdates are similar to insertions, except for the existing record is usually overwritten or appended to the write buffer for a delayed write. Deletes can be thought of as a reverse operation: when the leaf occupancy drops below a threshold, the case is consideredunderflow,and either load-balancing (transferring keys from a more-occupied node to less-occupied one) occurs or two sibling nodes are merged, triggering a removal of the key and a pointer from the parent node, which in turn may trigger a cascade of re-balancing and merges upstream.\nWe won't go into deep semantics of B-Tree splits and merges (at what thresholds they occur), since they depend on the concrete B-Tree flavor. What's important here is to mention that the full nodes split and their contents have to be relocated and pointers have to get updated. The node splits and merges may also propagate one or several levels up, since during splits and merges the child pointers are updated on the parent node; tree height grows during the split and shrinks when merging.\nWhile splits and merges are done, a portion of the tree has to be locked: updating all the nodes which will be split during a single operation should look atomic for the readers and writers. There was some research in the field of concurrency control in B-Trees, one of such endeavors is Blink-Trees, promising less locking, an algorithm better suited for high concurrency.\nFanout of tree has direct influence on how much IO operations will be done: smaller nodes may not only cause the tree to have higher depth, but also trigger splits more often.\nHeap files are lists of unordered records of variable size. Since they're often used for mutable storage, file is split into page-sized blocks (4KB or it's multiple). Blocks are numbered sequentially and are referenced from index files.\n**B-Tree variants**\n\nOne of the ways people use B-Trees is ISAM (Indexed Sequential Access Method), a method for creating, maintaining and manipulating trees that allows amortising some of the costs of B-Trees. The ISAM structure is completely static and pre-allocated. Just like with the B+Trees, the data resides only on the leaf pages, which helps to keep the non-leaf nodes static. One of the features of ISAM is the presence overflow pages: the writes will first go into the leaf nodes. When there's not enough space in the leaf nodes, the data will go into the overflow areas, so during search leaf pages are traversed first and overflow pages afterwards. The idea behind it is that there will be be just a few overflow pages and it won't impact the performance.\nAnother example is B+Trees. There are many ways to implement them, but many modern implementations feature a mutable dynamic B+Tree. They're special in the way that the data is stored only on leaves. This may simplify the updates to the tree structure, since the keys are smaller in size and can fit into internal node pages nicely. The leaf pages can grow and even get relocated when necessarily, which will only require an update of a single pointer on an internal node.\nThere many other examples, each one covering a special special case: Cache-Oblivious B-Trees optimizing the memory management, already mentioned Blink-Trees improving the concurrency, Partitioned B-Trees that improve the write performance and many more.\n**B-tree variants - B+ tree, B* tree, B# tree**\n**B+ Tree**\n\nB+Trees are different from the original B-Tree paper in that have an additional level of linked Leaf nodes, which are storing the values.\n**B-Tree Maintenance**\n\nMany modern databases are using B-Tree indexes: they're fast, efficient and there are many optimizations for them that are widely known and used. Most of the implementations are mutable. This means that the data structure is dynamic and is changing on disk: when new nodes are allocated, internal structure is changing. In order to facilitate this, there has to be a certain amount of overhead maintained for the table, so called occupancy factor, that allows some wiggle room for the new writes.\n![](media/B-Tree-image8.png)\n\nRebalancing is a way to compensate for the occupancy factor. The pointers from a more-occupied node are moved to the less-occupied node. This leads to quicker binary searches because of the logarithmic factor of larger numbers. Immutable B-Trees have a hundred percent occupancy factor and require no rebalancing.\nCompaction and load-balancing help with the B-Tree space management. During compaction (the term is really overloaded, and in the context of B-Trees has a different meaning from LSM-Trees one), free space is reclaimed by wiping out the outdated or removed records and re-writing the node contiguously (which may have bigger effect when the tree stores variable length records). Load balancing takes keys from the sibling nodes and moves them from nodes holding more records to ones holding less. Keeping the nodes balanced can also help to reduce an amount of splits. Unfortunately, this technique seems to be somewhat overlooked. Page splits also have some potential for performance optimizations: in order to facilitate key range scans, the nodes can be placed near one another on disk.\nIndex can be formed by bulk-loading (similarly to how SSTable index is created in LSM Trees). When bulk-loading, data can be pre-sorted and the overall algorithm is much simpler: all the appends will be done to the rightmost child in order to avoid traversal. The right-most index page will get split when full which, again, might trigger a cascade of splits on the upper levels. Using an immutable variant of B-Tree with bulk-loading can guarantee a full occupancy, which both saves space and improves read performance, since less nodes are required when occupancy factor is higher.\n<https://medium.com/databasss/on-disk-storage-part-4-b-trees-30791060741>"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Beap-(Bi-Parental-Heap)/","title":"Beap (Bi-Parental Heap)"},"frontmatter":{"draft":false},"rawBody":"# Beap (Bi-Parental Heap)\n\nCreated: 2018-02-04 19:49:19 +0500\n\nModified: 2018-02-04 19:52:44 +0500\n\n---\n\n**Beap**, short for**bi-parental heap**, introduced by Ian Munro and Hendra Suwanda. In this data structure a node usually has two parents (unless it is the first or last on a level) and two children (unless it is on the last level). What separates the beap from Williams' heap is that beap allows sublinear search.\nUnlike a heap, a beap allows[sublinear](https://en.wikipedia.org/wiki/Sublinear)search.\n![](media/Beap-(Bi-Parental-Heap)-image1.png)**Performance**\n\nThe height of the structure is approximately âˆš n.Also, assuming the last level is full, the number of elements on that level is also âˆš n.In fact, because of these properties all basic operations (insert, remove, find) run in 0(âˆš n)time on average. Find operations in the heap can be 0(n)in the worst case. Removal and insertion of new elements involves propagation of elements up or down (much like in a heap) in order to restore the beap invariant. An additional perk is that beap provides constant time access to the smallest element and 0(âˆšn)time for the maximum element.\n\n**Advantages**\n\nThe main advantage of the beap is that it can be fully implemented in-place - only data nodes need to be present (no pointers or other extra information is required). However, this structure should be used with care since it is not 0(logn)or does it perform much better than a vector for small values of n.\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Binary-Heap/","title":"Binary Heap"},"frontmatter":{"draft":false},"rawBody":"# Binary Heap\n\nCreated: 2018-02-03 17:30:38 +0500\n\nModified: 2021-02-21 21:38:45 +0500\n\n---\n\n**Points to remember -**\n\n1.  Common way of implementing priority queues.\n\n2.  Implicit Data Structure (storing keys in an array and using their relative positions within that array to represent child-parent relationships.)\n\n3.  Commonly applied in heapsort sorting algorithms\n**Binary Heap**\n\nA binary heap is defined as a binary tree with two additional constraints:\n-   Shape property: a binary heap is a[*complete binary tree*](https://en.wikipedia.org/wiki/Complete_Binary_Tree); that is, all levels of the tree, except possibly the last one (deepest) are fully filled, and, if the last level of the tree is not complete, the nodes of that level are filled from left to right.\n-   Heap property: the key stored in each node is either greater than or equal to (â‰¥) or less than or equal to (â‰¤) the keys in the node's children, according to some[total order](https://en.wikipedia.org/wiki/Total_order).\nA binary heap is a complete binary tree which satisfies the heap ordering property. The ordering can be one of two types:\n-   the*min-heap property*: the value of each node is greater than or equal to the value of its parent, with the minimum-value element at the root.\n-   the*max-heap property*: the value of each node is less than or equal to the value of its parent, with the maximum-value element at the root.\n\nThroughout this chapter the word \"heap\" will always refer to a min-heap.\n\n![12 15 10 15 17 10 10 ](media/Binary-Heap-image1.png)\n\nIn a heap the highest (or lowest) priority element is always stored at the root, hence the name \"heap\". A heap is not a sorted structure and can be regarded as partially ordered. As you see from the picture, there is no particular relationship among nodes on any given level, even among the siblings.\n\nSince a heap is a complete binary tree, it has a smallest possible height - a heap with N nodes always has O(log N) height.\n\nA heap is useful data structure when you need to remove the object with the highest (or lowest) priority. A common use of a heap is to implement a priority queue.\n**Array Implementation**\n\nA complete binary tree can be uniquely represented by storing its level order traversal in an array.\n\n![](media/Binary-Heap-image2.png)\n\nThe root is the second item in the array. We skip the index zero cell of the array for the convenience of implementation. Consider k-th element of the array, the\n\nits left child is located at 2*k index\n\nits right child is located at 2*k+1. index\n\nits parent is located at k/2 index\nFor 0th indexing of the arr, where k is an element of the array,\n\nparent = (k-1)//2\n\nleft_child = (2*k) + 1\n\nright_child = (2*k) + 2\nArray representation of a heap-ordered complete binary tree\n\n**Heap ordered binary tree -**\n-   Keys in nodes\n-   Parent's key no smaller than children's keys\n\n**Properties -**\n-   Largest key is a[1], which is root of binary tree\n-   Parent of node at k is at k/2\n-   Children of node at k are at 2k and 2k+1\n\n**Promotion in a heap (swim operation / shift-up / unheap / percolate up / bubble-up)**\n\nChild's key becomes larger key than its parent's key\n\nEliminate the violation -\n-   Exchange key in child with key in parent\n-   Repeat until heap order restored\n\n![Promotion in a heap Scenario. Child's key becomes larger key than its parent's key. To eliminate the violation: â€¢ Exchange key in child with key in parent. â€¢ Repeat until heap order restored. s private void swim(int k) while (k > 1&& less(k/2, exch(k, k/ 2) ; k- parent of node at k is at k/2 E 1 2 s E 1 violates heap order (larger key than parent) 1 ](media/Binary-Heap-image3.png)\n**For 0th index array**\n\ndef perc_up(self, k):\n\nwhile(k > 0 and self.arr[k] < self.arr[(k-1)//2]):\n\nself.arr[k], self.arr[(k-1)//2] = self.arr[(k-1)//2], self.arr[k]\n\nk = (k-1)//2\n**Insertion in a heap**\n-   Add node at end, then swim it up\n-   At most 1 + lg N compares\n\n![Insertion in a heap Insert. Add node at end, then swim it up. Cost. At most I +1gN compares. public void insert(Key x) swi m(N) ; insert key to insert add key to heap violates heap order ;wim up ](media/Binary-Heap-image4.png)\n**Demotion in a heap (sink / shift-down / downheap / percolate down / bubble-down)**\n\nParent's key becomes smaller than one (or both) of its children's.\n\nEliminate the violation -\n-   Exchange key in parent with key in larger child\n-   Repeat until heap order restored\n\n![Demotion in a heap Scenario. Parent's key becomes smaller than one (or both) of its children's. To eliminate the violation: why not smaller child? â€¢ Exchange key in parent with key in larger child. â€¢ Repeat until heap order restored. private void sink(int k) children of node at k while (2*k N) violates henp order (smaller than a chilcl) E 1 s 10 E 1 s int j = if (j < if (!less(k, exch(k, j); are 2k and 2k+l less(j, j+l)) j + ; j)) break; Top-down reheapify (sink) Power struggle. Better subordinate promoted. ](media/Binary-Heap-image5.png)\n**Delete the maximum in a heap**\n-   Exchange root with node at end, then sink it down\n-   At most 2 lg N compares\n\n![Delete the maximum in a heap Delete max. Exchange root with node at end, then sink it down. Cost. At most 2 lgN compares. remove the maximum s public Key delMax() Key max = exch(l, N--); si nk (1) ; pq [N+I] = null; return max; key to remove exchange key with root violates heap order E 1 s prevent loitering E 1 sink down E 1 remove node from heap s ](media/Binary-Heap-image6.png)\n**Bottom-up heap construction -**\n\nGoal: Build max heap using bottom-up method\n**Binary Heap considerations**\n-   Immutability of keys (can't change the data type value once created, final keyword is used to make a datatype immutable)\n-   Underflow and overflow\n| **Operation** | [**Binary**](https://en.wikipedia.org/wiki/Binary_heap)[^[6]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-CLRS-6) | [**Leftist**](https://en.wikipedia.org/wiki/Leftist_tree) | [**Binomial**](https://en.wikipedia.org/wiki/Binomial_heap)[^[6]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-CLRS-6) | **Fibonacci**^[[6]](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-CLRS-6)[[2]](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-Fredman_And_Tarjan-2)^ | [**Pairing**](https://en.wikipedia.org/wiki/Pairing_heap)[^[7]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-Iacono-7)                                             | [**Brodal**](https://en.wikipedia.org/wiki/Brodal_queue)^[[8]](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-8)[[a]](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-brodal-10)^ | [**Rank-pairing**](https://en.wikipedia.org/w/index.php?title=Rank-pairing_heap&action=edit&redlink=1)[^[10]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-11) | **Strict Fibonacci**[^[11]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-12) |\n|---------|-------|-------|---------|----------|---------|--------|--------|---------|\n| find-min      | *Î˜*(1)                                                                                                                          | *Î˜*(1)                                                    | *Î˜*(log*n*)                                                                                                                        | *Î˜*(1)                                                                                                                                                                    | *Î˜*(1)                                                                                                                                                                          | *Î˜*(1)                                                                                                                                                                                               | *Î˜*(1)                                                                                                                                                                      | *Î˜*(1)                                                                                    |\n| delete-min    | *Î˜*(log*n*)                                                                                                                    | *Î˜*(log*n*)                                              | *Î˜*(log*n*)                                                                                                                        | *O*(log*n*)[^[b]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-amortized-13)                                                                                | *O*(log*n*)[^[b]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-amortized-13)                                                                                      | *O*(log*n*)                                                                                                                                                                                         | *O*(log*n*)[^[b]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-amortized-13)                                                                                  | *O*(log*n*)                                                                              |\n| insert        | *O*(log*n*)                                                                                                                    | *Î˜*(log*n*)                                              | *Î˜*(1)[^[b]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-amortized-13)                                                | *Î˜*(1)                                                                                                                                                                    | *Î˜*(1)                                                                                                                                                                          | *Î˜*(1)                                                                                                                                                                                               | *Î˜*(1)                                                                                                                                                                      | *Î˜*(1)                                                                                    |\n| decrease-key  | *Î˜*(log*n*)                                                                                                                    | *Î˜*(*n*)                                                  | *Î˜*(log*n*)                                                                                                                        | *Î˜*(1)[^[b]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-amortized-13)                                                                                      | *o*(log*n*)^[[b]](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-amortized-13)[[c]](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-pairingdecreasekey-16)^ | *Î˜*(1)                                                                                                                                                                                               | *Î˜*(1)[^[b]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-amortized-13)                                                                                        | *Î˜*(1)                                                                                    |\n| merge         | *Î˜*(*n*)                                                                                                                        | *Î˜*(log*n*)                                              | *O*(log*n*)[^[d]^](https://en.wikipedia.org/wiki/Fibonacci_heap#cite_note-merge-17)                                              | *Î˜*(1)                                                                                                                                                                    | *Î˜*(1)                                                                                                                                                                          | *Î˜*(1)                                                                                                                                                                                               | *Î˜*(1)                                                                                                                                                                      | *Î˜*(1)                                                                                    |\n**Python 3**\n\n**Python > Documentation > Concurrent Execution**\n\n**Python > Documentation > Data Types**\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Binary-Search-Tree/","title":"Binary Search Tree"},"frontmatter":{"draft":false},"rawBody":"# Binary Search Tree\n\nCreated: 2018-02-03 22:26:46 +0500\n\nModified: 2019-09-15 15:07:52 +0500\n\n---\n\nProperties -\n-   Explicit Data Structure\n-   Can be used to store key value pairs for a symbol table implementation\n\"A Binary Search Tree is sometimes called ordered or sorted binary trees, and it keeps its values in sorted order, so that lookup and other operations can use the principle of binary search\"â€Š---â€Š[Wikipedia](https://en.wikipedia.org/wiki/Binary_search_tree)\nA BST is a binary tree in symmetric order\n\nEach node has a key, and every node's key is:\n-   Larger than all keys in its left subtree\n-   Smaller than all keys in its right subtree\nA BST is a reference to a root Node.\n\nA Node is comprised of four fields:\n-   A Key and a Value\n-   A reference to the left and right subtree\n![pri vate class Node private Key key; private Value val; private Node left, ri ght; Node public Node (Key key, Value val) ke left this. key = this.val key; val; BST with smaller keys BST val right BST with larger keys Binary search tree Key and Value are generic types; Key is Comparable ](media/Binary-Search-Tree-image1.png)\nSearch: If less, go left; if greater, go right; if equal, search hit.\n\nInsert: If less, go left; if greater, go right; if null, insert.\n\nGet: Return value corresponding to given key, or null if no such key\n\nCost: Number of compares is equal to 1 + depth of node\n\n![public Value get(Key key) Node x = root; while (x null) int cmp = key. compareTo(x. key) ; if (cmp else if (cmp else if (cmp = return null ; < 0) x = x. left; > 0) x = x. right; return x. val ; ](media/Binary-Search-Tree-image2.png)\nPut: Associate value with key\n\nSearch for key, then two cases:\n-   Key in tree -> reset value\n-   Key not in tree -> add new node\n\n![public void put(Key key, { root = put(root, key, private Node put(Node x, Value val) val); concise, but tricky, recursive code; read carefully! Key key, Value val) if (x null) return new Node(key, val); int cmp = key. compareTo(x. key if (cmp < 0) x. left = put (x. left, else if (cmp > 0) x. right = put (x. right, if (cmp 0) x. val = val return x; key, key, val); val); ](media/Binary-Search-Tree-image3.png)\nCost: Number of compares is equal to 1 + depth of node\nA BST is a binary tree in symmetric order\n\nEach node has a key, and every node's key is:\n-   Larger than all keys in its left subtree\n-   Smaller than all keys in its right subtree\n**Operations in BST -**\n\na.  Get\n\nb.  Put\n\nc.  Min\n\nd.  Max\n\ne.  Floor\n\nf.  Ceil\n\ng.  Rank (how many keys are less than a given key)\n\nh.  Select (give as the kth largest key)\n\ni.  Subtree count\n\nj.  Inorder traversal\n\nk.  Level order traversal\n\nl.  Deletion in BST\n\n    i.  Lazy Approach (Mark the node as Tombstone)\n\nLeave the key in tree to guide search (but don't consider it equal in search)\n\nii. **Hibbard deletion**\n\nTo delete a node with key k: search for node t containing key k\n\n1.  Case 0 (0 children of the search node)\n\nDelete t by setting parent link to null\n\n2.  Case 1 (1 child of the search node)\n\nDelete t by replacing parent link\n\n3.  Case 2 (2 children of the search node)\n\nFind successor x of t\n\nDelete the minimum in t's right subtree\n\nPut x in t's spot\n![node to delete search for key E successor mi n(t. ri ght) go right, then go left until reaching null left link t. left eleteMin(t. ri ght) update links and node counts after recursive calls ](media/Binary-Search-Tree-image4.png)\nUnsatisfactory solution - Not symmetric\n\nAfter a long random sequence of insert and delete operation, the height of tree becomes sqrt(N)\n\nThe main defect of Hibbard deletion is that it unbalances the tree leading to sqrt(N) height\n\nIf instead of replacing the node to delete with its successor, we flip a coin and choose to replace it with either the successor or predecessor, then in practice the height becomes logarithmic (But nobody has been able to prove this fact mathematically)### Binary Searchtree\n\nAn important property of a Binary Search Tree is that the value of a Binary Search Tree nodeis larger than the value of the offspring of its left child, but smaller than the value of the offspring of its right child.\"\n\n![](media/Binary-Search-Tree-image5.png)\n\nHere is a breakdown of the above illustration:\n-   **A** is inverted. The subtree 7--5--8--6 needs to be on the right side, and the subtree 2--1--3 needs to be on the left.\n-   **B** is the only correct option. It satisfies the Binary Search Tree property.\n-   **C** has one problem: the node with the value 4. It needs to be on the left side of the root because it is smaller than 5.\n\n### Let's code a Binary SearchTree!\n\nNow it's time to code!\n\nWhat will we see here? We will insert new nodes, search for a value, delete nodes, and the balance of the tree.\n\nLet's start.\n\n#### *Insertion: adding new nodes to ourtree*\n\nImagine that we have an empty tree and we want to add new nodes with the following values in this order: 50, 76, 21, 4, 32, 100, 64, 52.\n\nThe first thing we need to know is if 50 is the root of our tree.\n\n![](media/Binary-Search-Tree-image6.png)\n\nWe can now start inserting node by node.\n-   76 is greater than 50, so insert 76 on the right side.\n-   21 is smaller than 50, so insert 21 on the left side.\n-   4 is smaller than 50. Node with value 50 has a left child 21. Since 4 is smaller than 21, insert it on the left side of this node.\n-   32 is smaller than 50. Node with value 50 has a left child 21. Since 32 is greater than 21, insert 32 on the right side of this node.\n-   100 is greater than 50. Node with value 50 has a right child 76. Since 100 is greater than 76, insert 100 on the right side of this node.\n-   64 is greater than 50. Node with value 50 has a right child 76. Since 64 is smaller than 76, insert 64 on the left side of this node.\n-   52 is greater than 50. Node with value 50 has a right child 76. Since 52 is smaller than 76, node with value 76 has a left child 64. 52 is smaller than 64, so insert 54 on the left side of this node.\n\n![](media/Binary-Search-Tree-image7.png)\n\nDo you notice a pattern here?\n\nLet's break it down.\n\n1.  Is the new node value greater or smaller than the current node?\n\n2.  If the value of the new node is greater than the current node, go to the right subtree. If the current node doesn't have a right child, insert it there, or else backtrack to step #1.\n\n3.  If the value of the new node is smaller than the current node, go to the left subtree. If the current node doesn't have a left child, insert it there, or else backtrack to step #1.\n\n4.  We did not handle special cases here. When the value of a new node is equal to the current value of the node, use rule number 3. Consider inserting equal values to the left side of the subtree.\n\nNow let's code it.\n\n| class BinarySearchTree:                        |\n|------------------------------------------------|\n| def __init__(self, value):                 |\n| self.value = value                             |\n| self.left_child = None                         |\n| self.right_child = None                        |\n| def insert_node(self, value):                  |\n| if value <= self.value and self.left_child:   |\n| self.left_child.insert_node(value)             |\n| elif value <= self.value:                     |\n| self.left_child = BinarySearchTree(value)      |\n| elif value > self.value and self.right_child: |\n| self.right_child.insert_node(value)            |\n| else:                                          |\n| self.right_child = BinarySearchTree(value)     |\nIt seems very simple.\n\nThe powerful part of this algorithm is the recursion part, which is on line 9 and line 13. Both lines of code call the insert_node method, and use it for its left and right children, respectively. Lines 11 and 15 are the ones that do the insertion for each child.\n\n#### *Let's search for the node value... Ornot...*\n\nThe algorithm that we will build now is about doing searches. For a given value (integer number), we will say if our Binary Search Tree does or does not have that value.\n\nAn important item to note is how we defined the tree **insertion algorithm**. First we have our root node. All the left subtree nodes will have smaller values than the root node. And all the right subtree nodes will have values greater than the root node.\n\nLet's take a look at an example.\n\nImagine that we have this tree.\n\n![](media/Binary-Search-Tree-image7.png)\n\nNow we want to know if we have a node based on value 52.\n\n![SO 3 2 ](media/Binary-Search-Tree-image8.png)\n\nLet's break it down.\n\n1.  We start with the root node as our current node. Is the given value smaller than the current node value? If yes, then we will search for it on the left subtree.\n\n2.  Is the given value greater than the current node value? If yes, then we will search for it on the right subtree.\n\n3.  If rules #1 and #2 are both false, we can compare the current node value and the given value if they are equal. If the comparison returns true, then we can say, \"Yeah! Our tree has the given value,\" otherwise, we say, \"Nooo, it hasn't.\"\n\nNow let's code it.\n\n| class BinarySearchTree: |                                             |\n|-------------------------|----------------------------------------------|\n|                        | def __init__(self, value):               |\n|                        | self.value = value                           |\n|                        | self.left_child = None                       |\n|                        | self.right_child = None                      |\n|                        | def find_node(self, value):                  |\n|                        | if value < self.value and self.left_child:  |\n|                        | return self.left_child.find_node(value)      |\n|                        | if value > self.value and self.right_child: |\n|                        | return self.right_child.find_node(value)     |\n|                        | return value == self.value                   |\nLet's beak down the code:\n-   Lines 8 and 9 fall under rule #1.\n-   Lines 10 and 11 fall under rule #2.\n-   Line 13 falls under rule #3.\n\nHow do we test it?\n\nLet's create our Binary Search Tree by initializing the root node with the value 15.\n\nbst = BinarySearchTree(15)\n\nAnd now we will insert many new nodes.\n\n| bst.insert_node(10) |                    |\n|---------------------|---------------------|\n|                    | bst.insert_node(8)  |\n|                    | bst.insert_node(12) |\n|                    | bst.insert_node(20) |\n|                    | bst.insert_node(17) |\n|                    | bst.insert_node(25) |\n|                    | bst.insert_node(19) |\nFor each inserted node, we will test if our find_node method really works.\n\n| print(bst.find_node(15)) # True |\n|----------------------------------|\n| print(bst.find_node(10)) # True |\n| print(bst.find_node(8)) # True  |\n| print(bst.find_node(12)) # True |\n| print(bst.find_node(20)) # True |\n| print(bst.find_node(17)) # True |\n| print(bst.find_node(25)) # True |\n| print(bst.find_node(19)) # True |\nYeah, it works for these given values! Let's test for a value that doesn't exist in our Binary Search Tree.\n\nprint(bst.find_node(0)) # False\n\nOh yeah.\n\nOur search is done.\n\n#### *Deletion: removing and organizing*\n\nDeletion is a more complex algorithm because we need to handle different cases. For a given value, we need to remove the node with this value. Imagine the following scenarios for this node: it has no children, has a single child, or has two children.\n-   **Scenario #1**: A node with no children (leaf node).\n\n| # |50| |50| |                                                   |\n|------------------|----------------------------------------------------|\n|                 | # /  /                                        |\n|                 | # |30| |70| (DELETE 20) ---> |30| |70| |\n|                 | # /                                           |\n|                 | # |20| |40| |40|                            |\nIf the node we want to delete has no children, we simply delete it. The algorithm doesn't need to reorganize the tree.\n-   **Scenario #2**: A node with just one child (left or right child).\n\n| # |50| |50| |                                                   |\n|------------------|----------------------------------------------------|\n|                 | # /  /                                        |\n|                 | # |30| |70| (DELETE 30) ---> |20| |70| |\n|                 | # /                                               |\n|                 | # |20|                                          |\nIn this case, our algorithm needs to make the parent of the node point to the child node. If the node is the left child, we make the parent of the left child point to the child. If the node is the right child of its parent, we make the parent of the right child point to the child.\n-   **Scenario #3**: A node with two children.\n\n| # |50| |50| |                                                   |\n|------------------|----------------------------------------------------|\n|                 | # /  /                                        |\n|                 | # |30| |70| (DELETE 30) ---> |40| |70| |\n|                 | # /  /                                          |\n|                 | # |20| |40| |20|                            |\nWhen the node has 2 children, we need to find the node with the minimum value, starting from the node'sright child. We will put this node with minimum value in the place of the node we want to remove.\n\nIt's time to code.\n\n| def remove_node(self, value, parent):                                                     |\n|------------------------------------------------------------------------|\n| if value < self.value and self.left_child:                                               |\n| return self.left_child.remove_node(value, self)                                           |\n| elif value < self.value:                                                                 |\n| return False                                                                              |\n| elif value > self.value and self.right_child:                                            |\n| return self.right_child.remove_node(value, self)                                          |\n| elif value > self.value:                                                                 |\n| return False                                                                              |\n| else:                                                                                     |\n| if self.left_child is None and self.right_child is None and self == parent.left_child:    |\n| parent.left_child = None                                                                  |\n| self.clear_node()                                                                         |\n| elif self.left_child is None and self.right_child is None and self == parent.right_child: |\n| parent.right_child = None                                                                 |\n| self.clear_node()                                                                         |\n| elif self.left_child and self.right_child is None and self == parent.left_child:          |\n| parent.left_child = self.left_child                                                       |\n| self.clear_node()                                                                         |\n| elif self.left_child and self.right_child is None and self == parent.right_child:         |\n| parent.right_child = self.left_child                                                      |\n| self.clear_node()                                                                         |\n| elif self.right_child and self.left_child is None and self == parent.left_child:          |\n| parent.left_child = self.right_child                                                      |\n| self.clear_node()                                                                         |\n| elif self.right_child and self.left_child is None and self == parent.right_child:         |\n| parent.right_child = self.right_child                                                     |\n| self.clear_node()                                                                         |\n| else:                                                                                     |\n| self.value = self.right_child.find_minimum_value()                                        |\n| self.right_child.remove_node(self.value, self)                                            |\n| return True                                                                               |\n1.  **First**: Note the parameters value and parent. We want to find the nodethat has this value, and the node's parent is important to the removal of the node.\n\n2.  **Second**: Note the returning value. Our algorithm will return a boolean value. It returns True if it finds the node and removes it. Otherwise it will return False.\n\n3.  **From line 2 to line 9**: We start searching for the node that has the valuethat we are looking for. If the value is smaller than the current nodevalue, we go to the left subtree, recursively (if, and only if, the current node has a left child). If the value is greater, go to the right subtree, recursively.\n\n4.  **Line 10**: We start to think about the remove algorithm.\n\n5.  **From line 11 to line 13**: We cover the node with no children, and it is the left child from its parent. We remove the node by setting the parent's left child to None.\n\n6.  **Lines 14 and 15**: We cover the node with no children, and it is the right child from it's parent. We remove the node by setting the parent's right child to None.\n\n7.  **Clear node method**: I will show the clear_node code below. It sets the nodes left child, right child, and its value to None.\n\n8.  **From line 16 to line 18**: We cover the node with just one child (left child), and it is the left child from it's parent. We set the parent's left child to the node's left child (the only child it has).\n\n9.  **From line 19 to line 21**: We cover the node with just one child (left child), and it is the right child from its parent. We set the parent's right child to the node's left child (the only child it has).\n\n10. **From line 22 to line 24**: We cover the node with just one child (right child), and it is the left child from its parent. We set the parent's left child to the node's right child (the only child it has).\n\n11. **From line 25 to line 27**: We cover the node with just one child (right child), and it is the right child from its parent. We set the parent's right child to the node's right child (the only child it has).\n\n12. **From line 28 to line 30**: We cover the node with both left and rightchildren. We get the node with the smallest value (the code is shown below) and set it to the value of the current node. Finish it by removing the smallest node.\n\n13. **Line 32**: If we find the node we are looking for, it needs to return True. From line 11 to line 31, we handle this case. So just return True and that's it.-   To use the clear_node method: set the None value to all three attributesâ€Š---â€Š(value, left_child, and right_child)\n\n| def clear_node(self): |                        |\n|-----------------------|-------------------------|\n|                      | self.value = None       |\n|                      | self.left_child = None  |\n|                      | self.right_child = None |-   To use the find_minimum_value method: go way down to the left. If we can't find anymore nodes, we found the smallest one.\n\n| def find_minimum_value(self): |                                            |\n|-------------------------------|-----------------------------------------|\n|                              | if self.left_child:                         |\n|                              | return self.left_child.find_minimum_value() |\n|                              | else:                                       |\n|                              | return self.value                           |\nNow let's test it.\n\nWe will use this tree to test our remove_node algorithm.\n\n| # |15| |                              |\n|-----------|-------------------------------|\n|          | # /                        |\n|          | # |10| |20|              |\n|          | # /  /                   |\n|          | # |8| |12| |17| |25| |\n|          | #                          |\n|          | # |19|                     |\nLet's remove the node with the value 8. It's a node with no child.\n\n| print(bst.remove_node(8, None)) # True |                          |\n|-----------------------------------------|---------------------------|\n|                                        | bst.pre_order_traversal() |\n|                                        | # |15|                 |\n|                                        | # /                    |\n|                                        | # |10| |20|          |\n|                                        | #  /                 |\n|                                        | # |12| |17| |25|   |\n|                                        | #                      |\n|                                        | # |19|                 |\nNow let's remove the node with the value 17. It's a node with just one child.\n\n| print(bst.remove_node(17, None)) # True |                          |\n|------------------------------------------|---------------------------|\n|                                         | bst.pre_order_traversal() |\n|                                         | # |15|                 |\n|                                         | # /                    |\n|                                         | # |10| |20|          |\n|                                         | #  /                 |\n|                                         | # |12| |19| |25|   |\nFinally, we will remove a node with two children. This is the root of our tree.\n\n| print(bst.remove_node(15, None)) # True |                          |\n|------------------------------------------|---------------------------|\n|                                         | bst.pre_order_traversal() |\n|                                         | # |19|                 |\n|                                         | # /                    |\n|                                         | # |10| |20|          |\n|                                         | #                    |\n|                                         | # |12| |25|          |\n**Geometric applications of BSTs**\n\n**Problem -** Intersections among geometric objects (find among a group of rectangles, how many rectangles intersect) - Binary search trees\n\n**Applications -** CAD, games, movies, virtual reality, databases, GIS (Geographic Information System)\n\n![problem Id range search 2d orthogonal line segment intersection kd range search Id interval search 2d orthogonal rectangle intersection example solution BST sweep line reduces to Id range search kd tree interval search tree sweep line reduces to Id interval search ](media/Binary-Search-Tree-image9.png)\n**Further Reading**\n-   2-3 Trees\n-   Red-Black binary search trees\n-   B-trees\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Binary-Tree/","title":"Binary Tree"},"frontmatter":{"draft":false},"rawBody":"# Binary Tree\n\nCreated: 2018-02-03 17:31:27 +0500\n\nModified: 2021-11-07 20:59:16 +0500\n\n---\n\n**Properties**\n-   In Binary tree, number of leaf nodes is always one more than nodes with two children.\n\nL = T + 1\nWhere L = Number of leaf nodes\nT = Number of internal nodes with two children\n**Types of Binary Tree**\n-   A**rooted**binary[tree](https://en.wikipedia.org/wiki/Tree_data_structure)has a[root node](https://en.wikipedia.org/wiki/Root_node)and every node has at most two children.\n-   A**full**binary tree (sometimes referred to as a**proper**or**plane**binary tree)is a tree in which every node has either 0 or 2 children. Another way of defining a full binary tree is a[recursive definition](https://en.wikipedia.org/wiki/Recursive_definition). A full binary tree is either:\n    -   A single vertex.\n    -   A tree whose root node has two subtrees, both of which are full binary trees.\n-   In a**complete**binary tree every level,except possibly the last, is completely filled, and all nodes in the last level are as far left as possible. It can have between 1 and 2hnodes at the last levelh.An alternative definition is a perfect tree whose rightmost leaves (perhaps all) have been removed. Some authors use the termcompleteto refer instead to a perfect binary tree as defined below, in which case they call this type of tree (with a possibly not filled last level) analmost completebinary tree ornearly completebinary tree.A complete binary tree can be efficiently represented using an array.\n-   A**perfect**binary tree is a binary tree in which all interior nodes have two childrenandall leaves have the samedepthor samelevel.An example of a perfect binary tree is the (non-incestuous)[ancestry chart](https://en.wikipedia.org/wiki/Ancestry_chart)of a person to a given depth, as each person has exactly two biological parents (one mother and one father). Provided the ancestry chart always displays the mother and the father on the same side for a given node, their sex can be seen as an analogy of left and right children,childrenbeing understood here as an algorithmic term. A perfect tree is therefore always complete but a complete tree is not necessarily perfect.\n-   In the**infinite complete**binary tree, every node has two children (and so the set of levels is[countably infinite](https://en.wikipedia.org/wiki/Countably_infinite)). The set of all nodes is countably infinite, but the set of all infinite paths from the root is uncountable, having the[cardinality of the continuum](https://en.wikipedia.org/wiki/Cardinality_of_the_continuum). These paths correspond by an order-preserving[bijection](https://en.wikipedia.org/wiki/Bijection)to the points of the[Cantor set](https://en.wikipedia.org/wiki/Cantor_set), or (using the example of a[Stern--Brocot tree](https://en.wikipedia.org/wiki/Stern%E2%80%93Brocot_tree)) to the set of positive[irrational numbers](https://en.wikipedia.org/wiki/Irrational_number).\n-   A**balanced**binary tree is a binary tree structure in which the left and right subtrees of every node differ in height by no more than 1.One may also consider binary trees where no leaf is much farther away from the root than any other leaf. (Different balancing schemes allow different definitions of \"much farther\".)\n-   A**degenerate**(or**pathological**) tree is where each parent node has only one associated child node.This means that the tree will behave like a[linked list](https://en.wikipedia.org/wiki/Linked_list)data structure.\n<https://en.wikipedia.org/wiki/Binary_tree#Types_of_binary_trees>\n\n## Binary Tree**\n\nEmpty or node with links to left and right binary trees.\n\n![](media/Binary-Tree-image1.png)\n\nThis is neither full nor complete\n![2 0 10 12 3 0 4 0 ](media/Binary-Tree-image2.png)\n\nThis is both full and complete\n**Full Binary tree / proper binary tree / 2-tree / strictly binary tree**\n\nTree in which every node other than the leaves has two children.\n\n![20 12 20 30 40 ](media/Binary-Tree-image3.png)\n\nThis is full but not complete\n**Complete tree**\n\nPerfectly balanced, except for bottom level, and all nodes are as far left as possible.\n-   Height of complete tree with N nodes is floor(lg N)\n-   Height only increases when N is a power of 2\n\n![12 20 30 30 ](media/Binary-Tree-image4.png)\n\nThis is complete but not full\n**Binary trees**\n\nIn computer science, a binary tree is a tree data structure in which each node has at the most two children, which are referred to as the left child and the right child.\"â€Š---â€Š[Wikipedia](https://en.wikipedia.org/wiki/Binary_tree)\n![](media/Binary-Tree-image5.png)\nThe first thing we need to keep in mind when we implement a binary tree is that it is a collection of nodes. Each node has three attributes: value, left_child, and right_child.\nHow do we implement a simple binary tree that initializes with these three properties?\nclass BinaryTree:\n\ndef __init__(self, value):\n\nself.value = value\n\nself.left_child = None\n\nself.right_child = None\nWhen we instantiate an object, we pass the value (the data of the node) as a parameter. Look at the left_child and the right_child. Both are set to None.\nBecause when we create our node, it doesn't have any children. We just have the node data.\nWe can pass the string 'a' as the value to our Binary Tree node. If we print the value, left_child, and right_child, we can see the values.\nWe will implement a method to insert a new node to the right and to the left.\nHere are the rules:\n-   If the current node doesn't have a left child, we just create a new nodeand set it to the current node's left_child.\n-   If it does have the left child, we create a new node and put it in the current left child's place. Allocate this left child node to the new node's left child.\ndef insert_left(self, value):\n\nif self.left_child == None:\n\nself.left_child = BinaryTree(value)\n\nelse:\n\nnew_node = BinaryTree(value)\n\nnew_node.left_child = self.left_child\n\nself.left_child = new_node\nAgain, if the current node doesn't have a left child, we just create a new node and set it to the current node's left_child. Or else we create a new node and put it in the current left child's place. Allocate this left child node to the new node's left child.\nAnd we do the same thing to insert a right child node.\ndef insert_right(self, value):\n\nif self.right_child == None:\n\nself.right_child = BinaryTree(value)\n\nelse:\n\nnew_node = BinaryTree(value)\n\nnew_node.right_child = self.right_child\n\nself.right_child = new_node\n<https://www.geeksforgeeks.org/handshaking-lemma-and-interesting-tree-properties>\n\n<https://www.freecodecamp.org/news/how-to-implement-binary-tree-algorithms-in-technical-interviews>"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Binomial-Heap/","title":"Binomial Heap"},"frontmatter":{"draft":false},"rawBody":"# Binomial Heap\n\nCreated: 2018-03-11 14:39:17 +0500\n\nModified: 2018-03-11 16:53:18 +0500\n\n---\n\n**Key Points:**\n-   Faster union or merge operation of two binary heaps\n-   Implementation of mergeable heap abstract data type\nBinomial Heap is to extension of[Binary Heap](http://geeksquiz.com/binary-heap/)that provides faster union or merge operation together with other operations provided by Binary Heap.\n*A Binomial Heap is a collection of Binomial Trees*\n**What is a Binomial Tree?**\n\nA Binomial Tree of order 0 has 1 node. A Binomial Tree of order k can be constructed by taking two binomial trees of order k-1, and making one as leftmost child of other.\n\nA Binomial Tree of order k has following properties.\n\na) It has exactly 2^k^nodes.\n\nb) It has depth as k.\n\nc) There are exactly ^k^Cinodes at depth i for i = 0, 1, . . . , k.\n\nd) The root has degree k and children of root are themselves Binomial Trees with order k-1, k-2,.. 0 from left to right.\n![7-ê±°9 ë‹ˆ1d00 ](media/Binomial-Heap-image1.png)\n![0\"ìŠ¤ì‡  0 ](media/Binomial-Heap-image2.png)\n**Structure of a Binomial Heap:**\n-   A Binomial Heap is a set of Binomial Trees where each Binomial Tree follows Min Heap property.\n-   And there can be at-most one Binomial Tree of any degree.\n\n**Examples Binomial Heap:**\n\n12------------10--------------------20\n/  / | \n15 50 70 50 40\n| / | |\n30 80 85 65\n|\n100\nA Binomial Heap with 13 nodes. It is a collection of 3\nBinomial Trees of orders 0, 2 and 3 from left to right.\n\n10--------------------20\n/  / | \n15 50 70 50 40\n| / | |\n30 80 85 65\n|\n100\n\nA Binomial Heap with 12 nodes. It is a collection of 2\nBinomial Trees of orders 2 and 3 from left to right.\n\n**Binary Representation of a number and Binomial Heaps**\n\nA Binomial Heap with n nodes has number of Binomial Trees equal to the number of set bits in Binary representation of n. For example let n be 13, there 3 set bits in binary representation of n (00001101), hence 3 Binomial Trees. We can also relate degree of these Binomial Trees with positions of set bits. With this relation we can conclude that there are O(Logn) Binomial Trees in a Binomial Heap with 'n' nodes.\n**Operations of Binomial Heap:**\n\nThe main operation in Binomial Heap is union(), all other operations mainly use this operation. The union() operation is to combine two Binomial Heaps into one.\n**Union operation in Binomial Heap:**\n\nGiven two Binomial Heaps H1 and H2, union(H1, H2) creates a single Binomial Heap.\n\n**1)**The first step is to simply merge the two Heaps in non-decreasing order of degrees.\n\n**2)**After the simple merge, we need to make sure that there is at-most one Binomial Tree of any order. To do this, we need to combine Binomial Trees of same order. We traverse the list of merged roots, we keep track of three pointers, prev, x and next-x. There can be following 4 cases when we traverse the list of roots.\n\n-----Case 1: Orders of x and next-x are not same, we simply move ahead.\n\nIn following 3 cases orders of x and next-x are same.\n\n-----Case 2: If order of next-next-x is also same, move ahead.\n\n-----Case 3: If key of x is smaller than or equal to key of next-x, then make next-x as a child of x by linking it with x.\n\n-----Case 4: If key of x is greater, then make x as child of next.\n**1)**insert(H, k): Inserts a key 'k' to Binomial Heap 'H'. This operation first creates a Binomial Heap with single key 'k', then calls union on H and the new Binomial heap.\n\n**2)**getMin(H): A simple way to getMin() is to traverse the list of root of Binomial Trees and return the minimum key. This implementation requires O(Logn) time. It can be optimized to O(1) by maintaining a pointer to minimum key root.\n\n**3)**extractMin(H): This operation also uses union(). We first call getMin() to find the minimum key Binomial Tree, then we remove the node and create a new Binomial Heap by connecting all subtrees of the removed minimum node. Finally we call union() on H and the newly created Binomial Heap. This operation requires O(Logn) time.\n\n**4)**delete(H): Like Binary Heap, delete operation first reduces the key to minus infinite, then calls extractMin().\n\n**5)**decreaseKey(H): decreaseKey() is also similar to Binary Heap. We compare the decreases key with it parent and if parent's key is more, we swap keys and recur for parent. We stop when we either reach a node whose parent has smaller key or we hit the root node. Time complexity of decreaseKey() is O(Logn)."},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Fibonacci-Heap/","title":"Fibonacci Heap"},"frontmatter":{"draft":false},"rawBody":"# Fibonacci Heap\n\nCreated: 2018-03-11 16:58:21 +0500\n\nModified: 2018-03-11 22:33:55 +0500\n\n---\n\n**Key Points:**\n-   Running Time: O(E + V log V)\n-   Find minimum takes constant O(1) amortized time.\n-   Insert and decrease key also take constant amortized time\n-   Deletion takes O(log N) amortized time.\nData Structure for Priority Queue Operations, consisting of a collection of Heap Ordered Trees.\nA Fibonacci heap is a collection of[trees](https://en.wikipedia.org/wiki/Tree_data_structure)satisfying the[minimum-heap property](https://en.wikipedia.org/wiki/Minimum-heap_property), that is, the key of a child is always greater than or equal to the key of the parent. This implies that the minimum key is always at the root of one of the trees. Compared with binomial heaps, the structure of a Fibonacci heap is more flexible. The trees do not have a prescribed shape and in the extreme case the heap can have every element in a separate tree. This flexibility allows some operations to be executed in a[lazy](https://en.wikipedia.org/wiki/Lazy_evaluation)manner, postponing the work for later operations. For example, merging heaps is done simply by concatenating the two lists of trees, and operation*decrease key*sometimes cuts a node from its parent and forms a new tree.\nLazily defer consolidation under next delete-min\n**Running Time:**\n\n1) Find Min: **Î˜(1)** [Same as both Binary and Binomial]\n2) Delete Min: **O(Log n)** [Î˜(Log n) in both Binary and Binomial]\n3) Insert: **Î˜(1)** [Î˜(Log n) in Binary and Î˜(1) in Binomial]\n4) Decrease-Key: **Î˜(1)** [Î˜(Log n) in both Binary and Binomial]\n5) Merge: **Î˜(1)** [Î˜(m Log n) or Î˜(m+n) in Binary and\nÎ˜(Log n) in Binomial]\n**Below are some interesting facts about Fibonacci Heap**\n-   The reduced time complexity of Decrease-Key has importance in Dijkstra and Prim algorithms. With Binary Heap, time complexity of these algorithms is O(VLogV + ELogV). If Fibonacci Heap is used, then time complexity is improved to O(VLogV + E)\n-   Although Fibonacci Heap looks promising time complexity wise, it has been found slow in practice as hidden constants are high.\n-   Fibonacci heap are mainly called so because Fibonacci numbers are used in the running time analysis. Also, every node in Fibonacci Heap has degree at most O(log n) and the size of a subtree rooted in a node of degree k is at least Fk+2, where Fkis the kth Fibonacci number."},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Interval-Search-Tree/","title":"Interval Search Tree"},"frontmatter":{"draft":false},"rawBody":"# Interval Search Tree\n\nCreated: 2018-02-06 20:08:03 +0500\n\nModified: 2018-02-07 08:20:57 +0500\n\n---\n\nOperations\n-   Insert an interval\n-   Search for an interval\n-   Delete an interval\n-   Interval intersection query: Given an interval (lo, hi), find all intervals (or one interval) in data structure that intersects (lo, hi)\n![Interval search trees Create BST, where each node stores an interval (10, hi). â€¢ Use left endpoint as BST key. â€¢ Store max endpoint in subtree rooted at node. (17, 19) 24 (21, 24) 24 binary search tree (left endpoint is key) 8 (7, 10) 18 (15, 18) 10 18 max endpoint in subtree rooted at node ](media/Interval-Search-Tree-image1.png)\n![To insert an interval (10, hi): â€¢ Insert into BST, using 10 as the key. Update max in each node on search path. insert interval (16, 22) (1 7, 19) 24 (21, 24) 24 8 (7, 10) 18 (1 5, 18) 10 18 ](media/Interval-Search-Tree-image2.png)\n![Interval search tree demo To search for any one interval that intersects query interval (10, hi) : â€¢ If interval in node intersects query interval, return it. â€¢ Else if left subtree is null, go right. â€¢ Else if max endpoint in left subtree is less than 10, go right. (16, 22) â€¢ Else go left. interval intersection search for (21, 23) 8 22 10 24 24 22 compare (21 , 23) to (16, 22) 22 (intersection!) (21, 23) ](media/Interval-Search-Tree-image3.png)\n![Node x = root; while (x if else if else if else null) (x. interval . intersects(lo, (x. left --- --- null) (x. left. max < 10) hi)) return x. interval ; x = x. right; x = x. right; x = x. left; return null; ](media/Interval-Search-Tree-image4.png)\n**Implementation -** Use a red-black BST to guarantee performance\n| operation                                          | brute | interval search tree | best in theory |\n|---------------------------------|----------|------------------|-------------|\n| insert interval                                    | 1     | log N                | log N          |\n| find interval                                      | N     | log N                | log N          |\n| delete interval                                    | N     | log N                | log N          |\n| find any one interval that intersects (*lo*, *hi*) | N     | log N                | log N          |\n| find all intervals that intersects (*lo*, *hi*)    | N     | R log N              | R + log N      |"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Kd-trees/","title":"Kd-trees"},"frontmatter":{"draft":false},"rawBody":"# Kd-trees\n\nCreated: 2018-02-06 18:53:06 +0500\n\nModified: 2018-05-12 20:17:07 +0500\n\n---\n\nIn[computer science](https://en.wikipedia.org/wiki/Computer_science), a***k*-d tree**(short for*k-dimensional[tree](https://en.wikipedia.org/wiki/Tree_data_structure)*) is a[space-partitioning](https://en.wikipedia.org/wiki/Space_partitioning)[data structure](https://en.wikipedia.org/wiki/Data_structure)for organizing[points](https://en.wikipedia.org/wiki/Point_(geometry))in a*k*-dimensional[space](https://en.wikipedia.org/wiki/Euclidean_space).*k*-d trees are a useful data structure for several applications, such as searches involving a multidimensional search key (e.g.[range searches](https://en.wikipedia.org/wiki/Range_search)and[nearest neighbor searches](https://en.wikipedia.org/wiki/Nearest_neighbor_search)).*k*-d trees are a special case of[binary space partitioning](https://en.wikipedia.org/wiki/Binary_space_partitioning)trees.\nA non-leaf node in K-D tree divides the space into two parts, called as half-spaces.\n\nPoints to the left of this space are represented by the left subtree of that node and points to the right of the space are represented by the right subtree.\nExtension of BSTs that allow us to do efficient processing of sets of points in space.\n![2d orthogonal range search: grid implementation Grid implementation. â€¢ Divide space into M-by-M grid of squares. â€¢ Create list of points contained in each square. â€¢ Use 2d array to directly index relevant square. â€¢ Insert: add (x, y) to list for corresponding square. â€¢ Range search: examine only squares that intersect 2d range query. LB ](media/Kd-trees-image1.png)\n\nProblem with Grid implementation - **Clustering**\n-   Lists are too long, even though average length is short.\n![2d tree construction Recursively partition plane into two halfplanes. 10 10 ](media/Kd-trees-image2.png)\n![2d tree implementation Data structure. BST, but alternate using x- and y-coordinates as key. â€¢ Search gives rectangle containing point. â€¢ Insert further subdivides the plane. p points left of p 6 3 5 points right of p 2 p even levels 8 9 points below q 4 q 3 6 q points above q odd levels 2 7 7 10 4 10 8 9 ](media/Kd-trees-image3.png)\n**Operations -**\n\na.  Range search in a 2d tree\n\nb.  Nearest neighbor search in a 2d tree\n![Kd tree Recursively partition k-dimensional space into 2 halfspaces. Kd tree. Implementation. BST, but cycle through dimensions ala 2d trees. level points whose ith coordinate is less than ps p i (mod k) points whose ith coordinate is greater than pk Efficient, simple data structure for processing k-dimensional data. â€¢ Widely used. â€¢ Adapts well to high-dimensional and clustered data. â€¢ Discovered by an undergrad in an algorithms class! Jon Bentley ](media/Kd-trees-image4.png)\n**Applications of 2d trees -**\n-   Classifying astronomical objects\n-   Computer animation\n-   Speeding up neural networks\n-   Mining data\n-   Image retrieval\n**Flocking Boids Algorithm**\n\nBoids: Three simple rules lead to complex emergent flocking behavior in birds:\n-   Collision avoidance: point away from k nearest boids\n-   Flock centering: point towards the center of mass of k nearest boids\n-   Velocity matching: update velocity to the average of k nearest boids\n**Appel's algorithm for N-body simulation**\n\nKey idea: suppose particle is far, far away from cluster of particles:\n-   Treat cluster of particles as a single aggregate particle\n-   Compute force between particle and center of mass of aggregate\n\nAlgorithm\n-   Build 3-d tree with N particles as nodes\n-   Store center-of-mass of subtree in each node\n-   To compute total force acting on a particle, traverse tree, but stop as soon as distance from particle to subdivision is sufficiently large\n\nRunning time per step is N log N\n\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Left-Leaning-Red-Black-BSTs-(LLRB-tree)/","title":"Left Leaning Red-Black BSTs (LLRB tree)"},"frontmatter":{"draft":false},"rawBody":"# Left Leaning Red-Black BSTs (LLRB tree)\n\nCreated: 2018-02-05 22:05:48 +0500\n\nModified: 2018-02-06 09:44:20 +0500\n\n---\n\n**Red-Black Tree**\n\n1.  Left-leaning red-black BSTs\n\n    a.  Represent 2-3 tree as a BST\n\n    b.  Use internal left-leaning links as glue for 3-nodes\n![3-node less than a 2-3 tree a etwee a and b greater than b less than a etwee a and b red links \"glue\" nodes within a 3-node c b larger key is root greater than b black links connect 2-nodes and 3-nodes s corresponding red-black BST ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image1.png)\na.  Properties\n    -   No node has two red links connected to it\n    -   Every path from root to null link has the same number of black links\n    -   Red links lean left\n\nb.  Operations\n1.  Search\n![Observation. Search is the same as for elementary BST (ignore color). but runs faster because of better balance public Val get(Key key) Node x = root; while (x null) int cmp = key. compareTo (x. key) ; (cmp < O) x = x. left; > O) x = x. right; return x. val ; x if c s else if (cmp else if (cmp 0) return null; Remark. Most other ops (e.g., floor, iteration, selection) are also identical. ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image2.png)\n2.  Implementation\n![Each node is pointed to by precisely one link (from its parent) can encode color of links in nodes. private static final boolean private static final boolean private cl ass Node Key key; Value val ; Node left, right; boolean color; // color RED BLACK - true; false; h h. 1 eft . color is RED c A D h. ri ght. col or is BLACK of parent link private bool ean isRed(Node x) if (x null) return false; return x. color --- RED; null links are black ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image3.png)\n3.  Left rotation - Orient a (temporarily) right-leaning red link to lean left\n![Left rotation. rotate E left (before) less than E Orient a (temporarily) right-leaning red link to lean left. h s between E and S x greater than S private Node rotateLeft(Node h) assert isRed(h. right); = h. right; Node x h. ri ght = x. left; x. left = h = h. color; x. color h. color = RED return x; Invariants. Maintains symmetric order and perfect black balance. ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image4.png)\n![rotate E left (after) less than E s between E and S x greater than S ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image5.png)\n4.  Right rotation - Orient a left-leaning red link to (temporarily) lean right\n![Right rotation. rotate S right Orient a left-leaning red link to (temporarily) lean right. (befo re) x less than E Invariants. E S between E and S h greater than S private Node rotateRight(Node h) assert isRed(h.1eft); Node x = h. left; h. left = x. right; x. right x. color = h. color; h.color = RED return x; Maintains symmetric order and perfect black balance. ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image6.png)\n![rotate S right (after) x less than E s between E and S greater than S ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image7.png)\n5.  Color flip - Recolor to split a (temporary) 4-node\n![Color flip. flip colors (before) less than A Invariants. Recolor to split a (temporary) 4-node. between A and E s between E and S private void flipCoIors(Node h) assert ; assert isRed(h.1eft); assert isRed(h. right); h. color = RED h. left. color = BLACK; h. right. color = BLACK; greater than S Maintains symmetric order and perfect black balance. ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image8.png)\n![flip colors (after) less than A h between A and E s between E and S greater than S ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image9.png)\n6.  Insertion in a LLRB tree\n\n    a.  Basic Strategy\n\n![Basic strategy. Maintain I-I correspondence with 2-3 trees by applying elementary red-black BST operations. insert C add new node here right link red so rotate left c c s s s A c ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image10.png)\n\nb.  Insert into a tree with exactly 1 node\n\n![Warmup l. left Insert into a tree with exactly I node. root b search ends at this null link root b red link to new node containing a converts 2-node to 3-node right root search ends Fat this null link a attached new node -------with red link b root b rotated left to make a legal 3-node ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image11.png)\n\ni.  Insert into a 2-node at the bottom\n    -   Do standard BST insert; color new link red\n    -   If new red link is a right link, rotate left\n\n![Case l. Insert into a 2-node at the bottom. â€¢ Do standard BST insert; color new link red. â€¢ If new red link is a right link, rotate left. insert C add new node here right link red so rotate left c c s s s ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image12.png)\n\nc.  Insert into a tree with exactly 2 nodes\n\n![Warmup 2. larger b a b a b a Insert into a tree with exactly 2 nodes. search ends at this null link attached new node with red link c colors flipped to black c smaller c b search ends at this null link c b attached new between a a a a c search ends ---at this null link c attached new b red link c b NNrotated left a a b b node with red link rotated right c colors flipped to black c b b rotated right c colors flipped to black c ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image13.png)\n\ni.  Insert into a 3-node at the bottom\n    -   Do standard BST insert; color new link red\n    -   Rotate to balance the 4-node\n    -   Flip colors to pass red link up one level\n    -   Rotate to make lean left\n    -   Repeat case 1 or case 2 up the tree\n\n![Case 2. Insert into a 3-node at the bottom. â€¢ Do standard BST insert; color new link red. â€¢ Rotate to balance the 4-node (if needed). Flip colors to pass red link up one level. â€¢ Rotate to make lean left (if needed). inserting H c add new node here s c c two lefts in a row so rotate right s right link red so rotate left c s c both children red so flip colors s s ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image14.png)\n![Case 2. Insert into a 3-node at the bottom. â€¢ Do standard BST insert; color new link red. â€¢ Rotate to balance the 4-node (if needed). â€¢ Flip colors to pass red link up one level. â€¢ Rotate to make lean left (if needed). â€¢ Repeat case I or case 2 up the tree (if needed). inserting p s c c add new node here right link red so rotate left c s M both ildren red so p flip colors two lefts in a row so rotate right c c s s both children red so flip colors p s c p s both children red so flip colors ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image15.png)\n**Insertion Code**\n\n![Insertion in a LLRB tree: Java implementation Same code for all cases. â€¢ Right child red, left child black: rotate left. â€¢ Left child, left-left grandchild red: rotate right. â€¢ Both children red: flip colors. right rotate pri vate Node put(Node h, Key key, Value val) if (h null) return new int cmp = key. compareTo(h . key) ; Node (key, val , = put(h.left, (cmp < O) else if (cmp > O) else if (cmp if (i sRed(h. right) if (i sRed(h. left) if (isRed(h. left) return h; h. left h. ri ght = put(h.right, h. val = val; && !isRed(h.1eft)) RED) ; key , key , h && isRed(h. left. left)) h val); val); rotateLeft (h) ; rotateRi ght(h) ; && isRed(h. right)) fl i pC01 ors (h) ; h left rotate flip colors insert at bottom (and color it red) lean left balance 4-node split 4-node only a few extra lines of code provides near-perfect balance ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image16.png)\n1.  If implemented properly, the height of a red-black BST with N keys is at most 2 lg N\n![Balance in LLRB trees Proposition. Height of tree is s 2 lgN in the worst case. â€¢ Every path from root to null link has same number of black links. â€¢ Never two red links in-a-row. Property. Height of tree is 1.00 lgN in typical applications. ](media/Left-Leaning-Red-Black-BSTs-(LLRB-tree)-image17.png)\n**Applications -**\n\n**Red-black trees are widely used as system symbol tables -**\n-   Java: Java.util.TreeMap, java.util.TreeSet\n-   C++ STL: map, multimap, multiset\n-   Linux kernel: completely fair scheduler, linux/rbtree.h\n-   Emacs: conservative stack scanning\n\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Problems/","title":"Problems"},"frontmatter":{"draft":false},"rawBody":"# Problems\n\nCreated: 2018-04-25 23:21:16 +0500\n\nModified: 2019-06-23 20:30:19 +0500\n\n---\n\n**Tree Traversals**\n\n1.  Preorder\n\n2.  Inorder\n\n3.  Postorder\n\n4.  Level order traversals\n**Interview Problems**\n\n1.  Validate BST\n\n    a.  Do an inorder traversal of the BST. The output result must be a sorted array\n\n    b.  Another classic solution is to keep track of the minimum and maximum values a node can take. And at each node we will check whether its value is between the min and max values it's allowed to take. The root can take any value between negative infinity and positive infinity. At any node, its left child should be smaller than or equal than its own value, and similarly the right child should be larger than or equal to. So during recursion, we send the current value as the new max to our left child and send the min as it is without changing. And to the right child, we send the current value as the new min and send the max without changing.\n2.  Level Order Traversal\n\nUse queue for pushing items in each level and printing before new items are pushed. Also can have a counter for pushing to queue before the queue had been emptied.\n3.  Trim a BST\n\nWe can do this by performing a post-order traversal of the tree. We first process the left children, then right children, and finally the node itself. So we form the new tree bottom up, starting from the leaves towards the root. As a result while processing the node itself, both its left and right subtrees are valid trimmed binary search trees (may be NULL as well).\nAt each node we'll return a reference based on its value, which will then be assigned to its parent's left or right child pointer, depending on whether the current node is left or right child of the parent. If current node's value is between min and max (min<=node<=max) then there's no action need to be taken, so we return the reference to the node itself. If current node's value is less than min, then we return the reference to its right subtree, and discard the left subtree. Because if a node's value is less than min, then its left children are definitely less than min since this is a binary search tree. But its right children may or may not be less than min we can't be sure, so we return the reference to it. Since we're performing bottom-up post-order traversal, its right subtree is already a trimmed valid binary search tree (possibly NULL), and left subtree is definitely NULL because those nodes were surely less than min and they were eliminated during the post-order traversal. Remember that in post-order traversal we first process all the children of a node, and then finally the node itself.\nSimilar situation occurs when node's value is greater than max, we now return the reference to its left subtree. Because if a node's value is greater than max, then its right children are definitely greater than max. But its left children may or may not be greater than max. So we discard the right subtree and return the reference to the already valid left subtree.\n**def** trimBST(tree, minVal, maxVal):\n\n**if not** tree:\n\n**return**\n\ntree.left=trimBST(tree.left, minVal, maxVal)\n\ntree.right=trimBST(tree.right, minVal, maxVal)\n\n**if** minVal**<**=tree.val**<**=maxVal:\n\n**return** tree\n\n**if** tree.val**<**minVal:\n\n**return** tree.right\n\n**if** tree.val**>**maxVal:\n\n**return** tree.left\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Segment-Tree/","title":"Segment Tree"},"frontmatter":{"draft":false},"rawBody":"# Segment Tree\n\nCreated: 2020-12-06 23:49:38 +0500\n\nModified: 2020-12-06 23:56:10 +0500\n\n---\n\nIn[computer science](https://www.wikiwand.com/en/Computer_science), asegment tree,also known as a statistic tree, is a[tree](https://www.wikiwand.com/en/Tree_(data_structure))[data structure](https://www.wikiwand.com/en/Data_structure)used for storing information about[intervals](https://www.wikiwand.com/en/Interval_(mathematics)), or segments. It allows querying which of the stored segments contain a given point. It is, in principle, a static structure; that is, it's a structure that cannot be modified once it's built. A similar data structure is the[interval tree](https://www.wikiwand.com/en/Interval_tree).\nA Segment Tree is a data structure that allows answering range queries over an array effectively, while still being flexible enough to allow modifying the array. This includes finding the sum of consecutive array elementsa[l...r]a[l...r], or finding the minimum element in a such a range inO(logn)O(logâ¡n)time. Between answering such queries the Segment Tree allows modifying the array by replacing one element, or even change the elements of a whole subsegment (e.g. assigning all elementsa[l...r]a[l...r]to any value, or adding a value to all element in the subsegment).\nIn general a Segment Tree is a very flexible data structure, and a huge number of problems can be solved with it. Additionally it is also possible to apply more complex operations and answer more complex queries.\nIn particular the Segment Tree can be easily generalized to larger dimensions. For instance with a two-dimensional Segment Tree you can answer sum or minimum queries over some subrectangle of a given matrix. However only inO(log2n)O(log2â¡n)time.\nA segment tree is usually represented in an array.\nA segment tree for a setIofnintervals uses[O](https://www.wikiwand.com/en/Big_O_notation)(nlogn) storage and can be built inO(nlogn) time. Segment trees support searching for all the intervals that contain a query point inO(logn+k),kbeing the number of retrieved intervals or segments.\nOne important property of Segment Trees is, that they require only a linear amount of memory. The standard Segment Tree requires4n4nvertices for working on an array of sizenn.\nApplications of the segment tree are in the areas of[computational geometry](https://www.wikiwand.com/en/Computational_geometry), and[geographic information systems](https://www.wikiwand.com/en/Geographic_information_systems).\n<https://cp-algorithms.com/data_structures/segment_tree.html>\n\n<https://www.wikiwand.com/en/Segment_tree>\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Space-partitioning-trees/","title":"Space-partitioning trees"},"frontmatter":{"draft":false},"rawBody":"# Space-partitioning trees\n\nCreated: 2018-02-06 19:23:03 +0500\n\nModified: 2022-12-02 23:57:46 +0500\n\n---\n\nUse a tree to represent a recursive subdivision of 2d space\n**Grid -** Divide space uniformly into squares\n**2d tree -** Recursively divide space into two halfplanes\n**Quadtree / Rtree -** Recursively divide space into four quadrants\n\nIt is a two-dimensional analog of octrees and is most often used to partition a two-dimensional space by recursively subdividing it into four quadrants or regions.\n\nğ™ğ™¨ğ™š ğ™˜ğ™–ğ™¨ğ™š: find nearby interest points\n**BSP tree -** Recursively divide space into two regions\n**Applications**\n-   Ray tracing\n-   2d range search\n-   Flight simulators\n-   N-body simulation\n-   Collision detection\n-   Astronomical databases\n-   Nearest neighbor search\n-   Adaptive mesh generation\n-   Accelerate rendering in Doom (game)\n-   Hidden surface removal and shadow casting\nğ—¥ğ—”ğ—¬ ğ—–ğ—”ğ—¦ğ—§ğ—œğ—¡ğ—š\n\nIt is the most basic of many computer graphics rendering algo that uses geometric algo of ray tracing.\n\nğ™ğ™¨ğ™š ğ™˜ğ™–ğ™¨ğ™š: using longitude and latitude, return the Country of the point.\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/Tree-DS/","title":"Tree DS"},"frontmatter":{"draft":false},"rawBody":"# Tree DS\n\nCreated: 2017-11-29 21:16:37 +0500\n\nModified: 2021-06-11 22:06:50 +0500\n\n---\n\n1.  A Tree with N vertices has N-1 Edges\n\n2.  For a tree withnvertices andn -- 1edges, sum of all degrees should be2 * (n -- 1).3.  ***In a k-ary tree where every node has either 0 or k children, following property is always true.***\n\nL = (k - 1)*I + 1\nWhere L = Number of leaf nodes\nI = Number of internal nodes\n4.  If tree is a complete binary tree, then total number of nodes at last level is (N+1)/2, where N is the total number of nodes.\n\n    a.  If tree is a complete binary tree, then total number of nodes at last level is 2 ^ (N-1), where N is the depth of the tree starting from 1\n**Complete tree -** Perfectly balanced, except for bottom level\n\nProperty\n-   Height of complete tree with N nodes is floor(lg N)\n-   Height only increases when N is a power of 2\nA tree is a collection of entities called nodes. Nodes are connected by edges. Each node contains a value or data, and it may or may not have a child node.\n\n![4de Z noosi ](media/Tree-DS-image1.jpeg)\n\nThe first node of the tree is called the root. If this root node is connected by another node, the root is then a parent node and the connected node is a child.\n\n![Links called ](media/Tree-DS-image2.jpeg)\n\nAll Tree nodes are connected by links called edges. It's an important part of trees, because it's manages the relationship between nodes.\n\n![LEAFS ](media/Tree-DS-image3.jpeg)\n\nLeaves are the last nodes on a tree. They are nodes without children. Like real trees, we have the root, branches, and finally the leaves.\n\n![](media/Tree-DS-image4.png)\n\nOther important concepts to understand are height and depth.\n\nThe height of a tree is the length of the longest path to a leaf.\n\nThe depth of a node is the length of the path to its root.\n\nTerminology summary\n-   Root is the topmost node of the tree\n-   Edge is the link between two nodes\n-   Child is a node that has a parent node\n-   Parent is a node that has an edge to a child node\n-   Leaf is a node that does not have a child node in the tree\n-   Height is the length of the longest path to a leaf\n-   Depth is the length of the path to its root\n**Interview Question**\n-   Find the height of a binary tree\n-   Find kth maximum value in a binary search tree\n-   Find nodes at \"k\" distance from the root\n-   Find ancestors of a given node in a binary tree\n**Fenwick Tree (Binary Indexed Trees)**\n\nA**Fenwick tree**or**binary indexed tree**is a data structure that can efficiently update elements and calculate[prefix sums](https://en.wikipedia.org/wiki/Prefix_sum)in a table of numbers.\nWhen compared with a flat array of numbers, the Fenwick tree achieves a much better balance between two operations: element update and prefix sum calculation. In a flat array ofnnumbers, you can either store the elements, or the prefix sums. In the first case, computing prefix sums requires linear time; in the second case, updating the array elements requires linear time (in both cases, the other operation can be performed in constant time). Fenwick trees allow both operations to be performed inO(log n)time. This is achieved by representing the numbers as a[tree](https://en.wikipedia.org/wiki/Tree_(data_structure)), where the value of each node is the sum of the numbers in that subtree. The tree structure allows operations to be performed using onlyO(log n)node accesses.\n\n![](media/Tree-DS-image5.jpeg)\n![](media/Tree-DS-image6.jpeg)\n![](media/Tree-DS-image7.jpeg)\n<https://en.wikipedia.org/wiki/Fenwick_tree>\n\n<https://www.geeksforgeeks.org/binary-indexed-tree-or-fenwick-tree-2>\n<https://www.geeksforgeeks.org/handshaking-lemma-and-interesting-tree-properties>\n\n"},{"fields":{"slug":"/Data-Structures/Hierarchical-Data-Structure/k-ary-heap---d-ary-heap---d-way-heap/","title":"k-ary heap / d-ary heap / d-way heap"},"frontmatter":{"draft":false},"rawBody":"# k-ary heap / d-ary heap / d-way heap\n\nCreated: 2018-03-11 17:06:47 +0500\n\nModified: 2018-05-14 23:42:20 +0500\n\n---\n\nK-ary heaps are a generalization of binary heap(K=2) in which each node have K children instead of 2.\nProperties:\n\n1.  Nearly complete binary tree, with all levels having maximum number of nodes except the last, which is filled in left to right manner.\n\n2.  Like Binary Heap, it can be divided into two categories:\n\n    a.  Max k-ary heap (key at root is greater than all descendants and same is recursively true for all nodes).\n\n    b.  Min k-ary heap (key at root is greater than all descendants and same is recursively true for all nodes)\n3-ary max heap - root node is maximum\nof all nodes\n10\n/ | \n7 9 8\n/ |  /\n4 6 5 7\n\n3-ary min heap -root node is minimum\nof all nodes\n10\n/ | \n12 11 13\n/ | \n14 15 18\n\nThe height of a complete k-ary tree with n-nodes is given by logkn.\n\n**Applications of K-ary Heap**:\n-   K-ary heap when used in the implementation of[priority queue](http://geeksquiz.com/priority-queue-set-1-introduction/)allows faster decrease key operation as compared to binary heap ( O(log2n)) for binary heap vs O(logkn) for K-ary heap). Nevertheless, it causes the complexity of extractMin() operation to increase to O(k logkn) as compared to the complexity of O(log2n) when using binary heaps for priority queue. This allows K-ary heap to be more efficient in algorithms where decrease priority operations are more common than extractMin() operation.Example:[Dijkstra's](https://www.geeksforgeeks.org/greedy-algorithms-set-6-dijkstras-shortest-path-algorithm/)algorithm for single source shortest path and[Prim's](https://www.geeksforgeeks.org/greedy-algorithms-set-5-prims-minimum-spanning-tree-mst-2/)algorithm for minimum spanning tree\n-   K-ary heap has better memory cache behaviour than a binary heap which allows them to run more quickly in practice, although it has a larger worst case running time of both extractMin() and delete() operation (both being O(k logkn) )."},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Array/","title":"Array"},"frontmatter":{"draft":false},"rawBody":"# Array\n\nCreated: 2018-01-30 21:40:03 +0500\n\nModified: 2020-01-21 16:14:25 +0500\n\n---\n\n**Points to remember-**\n\n1.  Implicit Data Structure\n\n2.  Linear Data Structure\nAn array is collection of items stored at continuous memory locations. The idea is to store multiple items of same type together. This makes it easier to calculate the position of each element by simply adding an offset to a base value, i.e., the memory location of the first element of the array (generally denoted by the name of the array).\n**Types of indexing in array:**\n-   0 (zero-based indexing): The first element of the array is indexed by subscript of 0\n-   1 (one-based indexing): The first element of the array is indexed by subscript of 1\n-   n (n-based indexing): The base index of an array can be freely chosen. Usually programming languages allowing n-based indexing also allow negative index values and other scalar data types like enumerations, or characters may be used as an array index.\n**Advantages of using arrays:**\n-   Arrays allow random access of elements. This makes accessing elements by position faster. Constant time access of elements\n-   Arrays have better[cache locality](https://en.wikipedia.org/wiki/Locality_of_reference)that can make a pretty big difference in performance.\n**Cons of arrays:**\n-   Arrays take O(n) time for insertion and deletion\n-   Array size must be specified at time of creation\n**Referential Arrays (Each element is a reference to an object)**\n-   When computing the slice of a list, the result is a new list instance.\n-   New list has references to the same elements that are in the original list.\n-   temp = primes[3:6]\n![temp: primes: 11 13 15 17 19 ](media/Array-image1.png)**Copying Arrays**\n-   Shallow copy\n    -   Create a new list that has the references to the same elements as in first list.\n-   Deep copy\n    -   If elements are mutable, then changing something in first list can change items from the copied list that is copied using shallow copy, to stop that deep copy should be used. Where every element in the list is copied and then new list references the copied elements. (Can use deepcopy function from the copy module)\ncounters = [0]*8\nAll eight cells references the same object.\n\n![counters: ](media/Array-image2.png)\n\nBut since integers are immutable, therefore changing an item doesn't change values of all the elements in the array\n\n![counters[2] += 1 Does not technically change the valueof the existing integer instance. This computes a new integer counters: ](media/Array-image3.png)-   primes.extend(extras)\n\nExtending a list only copy references of the elements from second list to first list.**Introduction to Arrays**\n\nPython has 3 main sequence classes:\n-   List [1,2,3]\n-   Tuple (1,2,3)\n-   String '123'\n\nAll support indexing (eg. T[0] = 1)\n**Low Level Arrays**\n-   Focus on computer memory\n-   Memory address\n-   Units of memory (bits and bytes)\n-   Memory retrieval\nPython stores each unicode character in 2 bytes of memory. (i.e. 16 bits)\n\nAppropriate memory address can be computed using the calculation, **start + (cellsize)(index)**\n**Dynamic Arrays**\n\nDon't need to specify how large an array is beforehand.\n-   A list instance often has greater capacity than current length.\n-   If elements keep getting appended, eventually this extra space runs out.\n-   When dynamic array is full then it doubles in size.\nAn ArrayList, or a dynamically resizing array, is an array that resizes itself as needed while still providing 0(1) access. A typical implementation is that when the array is full, the array doubles in size. Each doubling takes0(n) time, but happens so rarely that its amortized time is still O(1).\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Behavioral---State/","title":"Behavioral - State"},"frontmatter":{"draft":false},"rawBody":"# Behavioral - State\n\nCreated: 2018-08-20 16:41:15 +0500\n\nModified: 2018-08-21 08:28:43 +0500\n\n---\n\nThe**state pattern**is a[behavioral](https://en.wikipedia.org/wiki/Behavioral_pattern)[software design pattern](https://en.wikipedia.org/wiki/Software_design_pattern)that implements a[state machine](https://en.wikipedia.org/wiki/State_machine)in an[object-oriented](https://en.wikipedia.org/wiki/Object-oriented)way. With the state pattern, a state machine is implemented by implementing each individual state as a derived class of the state pattern interface, and implementing state transitions by invoking[methods](https://en.wikipedia.org/wiki/Method_(computer_programming))defined by the pattern's superclass.\nThe state pattern can be interpreted as a[strategy pattern](https://en.wikipedia.org/wiki/Strategy_pattern)which is able to switch the current strategy through invocations of methods defined in the pattern's interface.\nThis pattern is used in[computer programming](https://en.wikipedia.org/wiki/Computer_programming)to encapsulate varying behavior for the same[object](https://en.wikipedia.org/wiki/Object_(computer_science))based on its internal state. This can be a cleaner way for an object to change its behavior at runtime without resorting to large monolithic conditional statementsand thus improve maintainability.\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Behavioral---Publisher-Subscriber/","title":"Behavioral - Publisher Subscriber"},"frontmatter":{"draft":false},"rawBody":"# Behavioral - Publisher Subscriber\n\nCreated: 2018-05-09 21:15:49 +0500\n\nModified: 2018-08-20 15:55:11 +0500\n\n---\n\nIn 'Publisher-Subscriber' pattern, senders of messages, called publishers, do not program the messages to be sent directly to specific receivers, called subscribers.\nThis means that the publisher and subscriber don't know about the existence of one another. There is a third component, called broker or message broker or event bus, which is known by both the publisher and subscriber, which filters all incoming messages and distributes them accordingly.\npub-sub is a pattern used to communicate messages between different system components without these components knowing anything about each other's identity\n"},{"fields":{"slug":"/Computer-Science/Programming-Paradigms/Behavioral---Observer/","title":"Behavioral - Observer"},"frontmatter":{"draft":false},"rawBody":"# Behavioral - Observer\n\nCreated: 2018-05-09 21:00:15 +0500\n\nModified: 2021-07-27 00:25:47 +0500\n\n---\n\nThe observer pattern is a software design pattern in which an object, called the subject, maintains a list of its dependents, called observers, and notifies them automatically of any state changes, usually by calling one of their methods.\nAll the items that are subscribed, will call the observer whenever an event happens.\n**Example -** in our oopd assignment, we implemented observer pattern in a full stack project where we have to implement facebook, where people can share updates, send friend requests and more. So when a person share an update then every person who is the friend of the current person will get notified of the new post.\n**Real world example -** After you've subscribed to a newspaper or magazine, you no longer need to go to the supermarket and check if the next issue is available. Instead, the publishing house will send new issues by mail directly to your home right after they've been released. The publishing house maintains a list of subscribers and knows which magazine send to whom. You can unsubscribe at any time, and you'll stop receiving this magazine.\n**Pros**\n-   the publishers are independent from specific subscriber classes and vice versa;\n-   you can subscribe and unsubscribe recipients on the fly.\n\n**Cons**\n-   the subscribers are notified at random (it's worth bearing in mind, since this may be important for some situations)\n**References**\n\n<https://py.checkio.org/blog/design-patterns-part-2>\n\n<https://www.youtube.com/watch?v=oNalXg67XEE>\n"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Bag-Data-Structure/","title":"Bag Data Structure"},"frontmatter":{"draft":false},"rawBody":"# Bag Data Structure\n\nCreated: 2018-01-31 10:02:09 +0500\n\nModified: 2018-01-31 10:20:26 +0500\n\n---\n\nApplication - Adding items to a collection and iterating (When order doesn't matter)\nImplementation - Stack (without pop) or queue (without dequeue)\n"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Circular-Buffer/","title":"Circular Buffer"},"frontmatter":{"draft":false},"rawBody":"# Circular Buffer\n\nCreated: 2019-12-03 23:15:17 +0500\n\nModified: 2019-12-03 23:16:10 +0500\n\n---\n\nAcircular buffer,circular queue,cyclic bufferorring bufferis a[data structure](https://en.wikipedia.org/wiki/Data_structure)that uses a single, fixed-size[buffer](https://en.wikipedia.org/wiki/Buffer_(computer_science))as if it were connected end-to-end. This structure lends itself easily to buffering[data streams](https://en.wikipedia.org/wiki/Data_stream).\n![](media/Circular-Buffer-image1.png)\n**Uses**\n\nThe useful property of a circular buffer is that it does not need to have its elements shuffled around when one is consumed. (If a non-circular buffer were used then it would be necessary to shift all elements when one is consumed.) In other words, the circular buffer is well-suited as a[FIFO](https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics))buffer while a standard, non-circular buffer is well suited as a[LIFO](https://en.wikipedia.org/wiki/LIFO_(computing))buffer.\n\nCircular buffering makes a good implementation strategy for a[queue](https://en.wikipedia.org/wiki/Queue_(data_structure))that has fixed maximum size. Should a maximum size be adopted for a queue, then a circular buffer is a completely ideal implementation; all queue operations are constant time. However, expanding a circular buffer requires shifting memory, which is comparatively costly. For arbitrarily expanding queues, a[linked list](https://en.wikipedia.org/wiki/Linked_list)approach may be preferred instead.\n\nIn some situations, overwriting circular buffer can be used, e.g. in multimedia. If the buffer is used as the bounded buffer in the[producer-consumer problem](https://en.wikipedia.org/wiki/Producer-consumer_problem)then it is probably desired for the producer (e.g., an audio generator) to overwrite old data if the consumer (e.g., the[sound card](https://en.wikipedia.org/wiki/Sound_card)) is unable to momentarily keep up. Also, the[LZ77](https://en.wikipedia.org/wiki/LZ77)family of lossless data compression algorithms operates on the assumption that strings seen more recently in a data stream are more likely to occur soon in the stream. Implementations store the most recent data in a circular buffer.\n<https://en.wikipedia.org/wiki/Circular_buffer>\n\n"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Dequeue/","title":"Dequeue"},"frontmatter":{"draft":false},"rawBody":"# Dequeue\n\nCreated: 2018-02-03 14:40:57 +0500\n\nModified: 2019-06-24 05:56:47 +0500\n\n---\n\n**Dequeue.**A*double-ended queue*or*deque*(pronounced \"deck\") is a generalization of a stack and a queue that supports adding and removing items from either the front or the back of the data structure.\nDequeue - double ended queue\n\na.  Insert items from any end\n\nb.  Get items from any end\n"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Indexed-Priority-Queue/","title":"Indexed Priority Queue"},"frontmatter":{"draft":false},"rawBody":"# Indexed Priority Queue\n\nCreated: 2018-03-11 14:08:42 +0500\n\nModified: 2018-03-11 14:09:29 +0500\n\n---\n\n![Indexed priority qt.JeUe Associate an index between 0 and N- I with each key in a priority queue. â€¢ Client can insert and delete-the-minimum. â€¢ Client can change the key by specifying the index. public class voi d voi d boolean int boolean int IndexMi nPQ<Key extends IndexMi nPQ(int N) insert(int i , Key key) decreaseKey(int i , Key contai ns(int i) del Mi n ( ) i sEmpty() si ze ( ) Comparabl key) create indexed priority queue with indices O, l, ..., N-l associate key with index i decrease the key associated with index i is i an index on the priority queue? remove a minimal key and return its associated index is the priority queue empty? number of entries in the priority queue ](media/Indexed-Priority-Queue-image1.png)\n![Indexed priority qveve implementation Implementation. â€¢ Start with same code as MinPQ. â€¢ Maintain parallel arrays keys[], pq[], and qp[] so that: keys[i] is the priority of i --- pq[i] is the index of the key in heap position i qp[i] is the heap position of the key with index i â€¢ Use implement decreaseKey(i , key) â€¢ keys [i pq [i] qp [i] o A 1 1 s O 5 2 o 4 3 7 8 1 4 T 2 7 6 5 1 1 6 6 5 7 4 3 8 3 3 1 ](media/Indexed-Priority-Queue-image2.png)\n**Applications**\n-   Prim's Algorithm"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Linked-List/","title":"Linked List"},"frontmatter":{"draft":false},"rawBody":"# Linked List\n\nCreated: 2018-01-30 19:03:53 +0500\n\nModified: 2020-01-07 22:36:30 +0500\n\n---\n\n**Points to remember -**\n\n1.  Explicit Data Structure\n\n2.  Linear Data Structure\n**Operations**\n\n1.  AddFirst\n\n2.  AddLast\n\n3.  Remove/delete (key)\n\n4.  Iterator\n\n5.  Clonning\n**Variants**\n\n1.  Singly Linked List\n\n2.  Doubly Linked List (Double Ended Linked List)\n\n3.  Circular Linked List\n\n4.  Doubly Circular Linked List\n\n5.  Skip List\n\nThe idea is simple, we create multiple layers so that we can skip some nodes.\n\n![10 20 Express Lane 22 23 27 30 30 Lane 43 45 50 57 58 59 62 65 67 ](media/Linked-List-image1.png)\n\n6.  Intrusive Linked List\n\nIntrusive linked lists are a variation of[linked lists](https://www.data-structures-in-practice.com/linked-lists/)where the links are embedded in the structure that's being linked.\nIn a typical linked list implementation, a list node contains adatapointer to the linked data and anextpointer to the next node in the list.\n\n![data next data next NULL ](media/Linked-List-image2.png)\nIn an intrusive linked list implementation, the list node containsnextpointer to the next list node, but nodatapointer because the list is embedded in the linked object itself.\n\n![next next NULL ](media/Linked-List-image3.png)\n\nThere are two main reasons to use intrusive lists over non-intrusive linked lists:\n-   Fewer memory allocations\n\nWith non-intrusive linked lists, creating a new object and adding it to a list requires two memory allocations: one for the object, and one for the list node. With intrusive linked lists, you only need to allocate one object (since the list node is embedded in the object). This means fewer errors to be handled, because there are half as many cases where memory allocation can fail.-   Less cache thrashing\n\nIntrusive linked lists also suffer less from cache thrashing. Iterating through a non-intrusive list node requires dereferencing a list node, and then dereferencing the list data. Intrusive linked lists only require dereferencing the next list node.\n<https://www.data-structures-in-practice.com/intrusive-linked-lists>\nIn a Linked List the first node is called the**head**and the last node is called the**tail**.\n\n## \n\n## Pros\n-   Linked Lists have constant-time insertions and deletions in any position, in comparison, arrays require O(n) time to do the same thing.\n-   Linked lists can continue to expand without having to specify their size ahead of time (remember our lectures on Array sizing form the Array Sequence section of the course!)\n\n## \n\n## Cons\n-   To access an element in a linked list, you need to take O(k) time to go from the head of the list to the kth element. In contrast, arrays have constant time operations to access elements in an array.\n**Interview Questions**\n-   Reverse a linked list\n-   Detect loop in a linked list\n-   Return Nth node from the end in a linked list\n-   Remove duplicates from a linked list\n"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Priority-Queue/","title":"Priority Queue"},"frontmatter":{"draft":false},"rawBody":"# Priority Queue\n\nCreated: 2018-02-03 14:42:20 +0500\n\nModified: 2022-03-15 14:50:44 +0500\n\n---\n\nA**priority queue**is an[abstract data type](https://en.wikipedia.org/wiki/Abstract_data_type)which is like a regular[queue](https://en.wikipedia.org/wiki/Queue_(abstract_data_type))or[stack](https://en.wikipedia.org/wiki/Stack_(abstract_data_type))data structure, but where additionally each element has a \"priority\" associated with it. In a priority queue, an element with high priority is served before an element with low priority. If two elements have the same priority, they are served according to their order in the queue.\nWhile priority queues are often implemented with[heaps](https://en.wikipedia.org/wiki/Heap_(data_structure)), they are conceptually distinct from heaps. A priority queue is an abstract concept like \"a[list](https://en.wikipedia.org/wiki/List_(abstract_data_type))\" or \"a[map](https://en.wikipedia.org/wiki/Associative_array)\"; just as a list can be implemented with a[linked list](https://en.wikipedia.org/wiki/Linked_list)or an[array](https://en.wikipedia.org/wiki/Array_data_structure), a priority queue can be implemented with a heap or a variety of other methods such as an unordered array.\nMany applications require that we process items having keys in order, but not necessarily in full sorted order and not necessarily all at once. Often, we collect a set of items, then process the one with the largest key, then perhaps collect more items, then process the one with the current largest key, and so forth. An appropriate data type in such an environment supports two operations:*remove the maximum*and*insert*. Such a data type is called a*priority queue*.\n**Goal:** Remove the largest (or smallest) item.\n**Implementation of Priority Queue**\n\n![public](media/Priority-Queue-image1.png)\n\n**Python 3**\n\n**Python > Documentation > Concurrent Execution**\n\n**Python > Documentation > Data Types**\n**Applications -**\n-   Event-driven simulation (customers in a line)\n    -   Simulate the motion of N moving particles that behave according to the laws of elastic collision.\n    -   Collision prediction\n    -   Resolution to calculate velocity\n    -   Particle-wall collision\n    -   Particle-particle collision\n-   Numerical computation (reducing roundoff error)\n-   Data compression (Huffman codes)\n-   Graph searching (Dijkstra's algorithm, Prim's algorithm)\n-   Number theory (sum of powers)\n-   AI (A* search)\n-   Statistics (maintain largest M values in a sequence)\n\nChallenge - Find the largest M items in a stream of N items\n\nConstraint - Not enough memory to store N items\n-   Operating systems (load balancing, interrupt handling)\n-   Discrete optimizations (bin packing, scheduling)\n-   Spam filtering (Bayesian spam filter)\n-   Median Maintanence (Have two heaps and keep invariant that ~i/2 smallest elements in one heap and other ~i/2 elements in another heap)\n**Further Reading**\n-   Heap Sort\n<https://engineering.fb.com/2021/02/22/production-engineering/foqs-scaling-a-distributed-priority-queue>\n\n"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Problems/","title":"Problems"},"frontmatter":{"draft":false},"rawBody":"# Problems\n\nCreated: 2019-06-23 20:31:35 +0500\n\nModified: 2019-06-24 05:58:16 +0500\n\n---\n\n1.  **Anagrams (ABC, CBA are anagrams of each other)**\n\n    a.  Can sort both array and compare both\n\n    b.  Can count all characters of string1 and decrement count by second string2. Check if all values are 0. Use dictionary, defaultdict.\n2.  **Array Pair Sum**\n\n**Problem -** Given an integer array, output all the***unique***pairs that sum up to a specific value**k**.\n**Solution -** The O(N) algorithm uses the set data structure. We perform a linear pass from the beginning and for each element we check whether k-element is in the set of seen numbers. If it is, then we found a pair of sum k and add it to the output. If not, this element doesn't belong to a pair yet, and we add it to the set of seen elements.\n3.  **Find the Missing Element**\n\n**Problem -** Consider an array of non-negative integers. A second array is formed by shuffling the elements of the first array and deleting a random element. Given these two arrays, find which element is missing in the second array.\n**Solution - O(N) -** We can use a hashtable and store the number of times each element appears in the second array. Then for each element in the first array we decrement its counter. Once hit an element with zero count that's the missing element.\n\nwe can achieve linear time and constant space complexity without any problems. Here it is: initialize a variable to 0, thenXOR every element in the first and second arrays with that variable. In the end, the value of the variable is the result, missing element in array2.\n4.  **Largest Continuous Sum**\n\n**Problem -** Given an array of integers (positive and negative) find the largest continuous sum.\n**Solution -** O(N) - Use current_sum and max_sum to track the values and update the values using current_sum = max(current_sum + num, num) and max_sum(current_sum, max_sum)\n5.  **Sentence Reversal**\n\n**Problem -** Given a string of words, reverse all the words.\n**Solution -** Use stack to push the words and then print the stack\n6.  **String Compression**\n\nProblem - Implement Run Length Encoding\nSolution - Use while loop to check if current value is equal to previous value\n7.  **Unique Characters in a String**\n\nSolution - Use dictionary to store seen values\n\nCan also use set\n**Interview Problems (Array)**\n-   Find the second minimum element of an array\n-   First non-repeating integers in an array\n-   Merge two sorted arrays\n-   Rearrange positive and negative values in an array\n**Interview Problems (Linked List)**\n\n1.  Single Linked List Cycle Check\n\nSolution - using two markers, in which marker2 will move two nodes ahead for every one node that marker1 moves, if both markers are equal means there is a cycle\n\nMysolution - add visited as a variable to node, use set to store seen nodes (not efficient)\n2.  Linked List Reversal in-place\n\nSolution - Use previous, current, next pointers (remember to save nextnode before updating the current.nextnode)\n3.  Nth to last node\n\nSolution - Use two pointers - last and nth_last, move last to given n steps away. Move both pointers one step until last becomes last node.\n\nreturn nth_last\n-   Walk one pointer**n**nodes from the head, this will be the right_point\n-   Put the other pointer at the head, this will be the left_point\n-   Walk/traverse the block (both pointers) towards the tail, one node at a time, keeping a distance**n**between them.\n-   Once the right_point has hit the tail, we know that the left point is at the target.\n**Interview Problems (Stacks and Queues)**\n\n1.  Implement a stack\n\n2.  Implement a queue\n\n3.  Implement a deque\n\n4.  Balanced paranthesis\n\nUse stack to store the opening paranthesis. Pop when closing paranthesis found while checking for set. If equal than continue otherwise return False (since not balanced)\n\n5.  Implement a queue using two stacks\n\n    a.  costly enqueue\n\n    b.  costly dequeue\n\nwhenever dequeue is called pop element from second stack, if second stack is empty then push elements from second stack by popping from it. (transfer elements from first stack to second in reverse order)\n"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Queue-FIFO/","title":"Queue FIFO"},"frontmatter":{"draft":false},"rawBody":"# Queue FIFO\n\nCreated: 2018-01-30 21:30:42 +0500\n\nModified: 2018-08-04 12:23:44 +0500\n\n---\n\n**Points to remember -**\n\n1.  Linear Data Structure\nQueue is also an abstract data type or a linear data structure, in which the first element is inserted from one end called**REAR**(also called tail), and the deletion of existing element takes place from the other end called as**FRONT**(also called head). This makes queue as FIFO(First in First Out) data structure, which means that element inserted first will also be removed first.\n\nThe process to add an element into queue is called**Enqueue**and the process of removal of an element from queue is called**Dequeue**.\n**Basic features of Queue**\n\n1.  Like Stack, Queue is also an ordered list of elements of similar data types.\n\n2.  Queue is a FIFO( First in First Out ) structure.\n\n3.  Once a new element is inserted into the Queue, all the elements inserted before the new element in the queue must be removed, to remove the new element.4.  **peek( )**function is oftenly used to return the value of first element without dequeuing it.\n**Applications -**\n\n1.  Serving requests on a single shared resource, like a printer, CPU task scheduling etc.\n\n2.  In real life scenario, Call Center phone systems uses Queues to hold people calling them in an order, until a service representative is free.\n\n3.  Handling of interrupts in real-time systems. The interrupts are handled in the same order as they arrive i.e First come first served.\n\n**Operations -**\n\n1.  Enqueue O(1)\n\n2.  Dequeue O(1) (pronounced as deck)\n\n3.  Peek O(1)\n**Implementation -**\n\n1.  **Using Linked-list**\n\n![public class Li nkedQueueOfStri ngs private Node fi rst, last; private class Node { / * same as i n StackOfStrings public boolean isEmpty() { return fi rst null; public void enqueue(String item) Node oldl ast = last; last = new Node() ; last. item = item; last. next = null; if (i sEmpty()) fi rst = last; special cases for empty queue old 1 ast. next public String dequeue() String item = first. item; = last; fi rst = first. next; if (i sEmpty()) last = null; return item; ](media/Queue-FIFO-image1.png)\n2.  **Using resizing array****Interview Questions**\n-   Implement stack using a queue\n-   Reverse first k elements of a queue\n-   Generate binary numbers from 1 to n using a queue\n\n"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Randomized-Queue/","title":"Randomized Queue"},"frontmatter":{"draft":false},"rawBody":"# Randomized Queue\n\nCreated: 2018-02-03 14:41:24 +0500\n\nModified: 2018-02-03 14:41:31 +0500\n\n---\n\n**Randomized queue.**A*randomized queue*is similar to a stack or queue, except that the item removed is chosen uniformly at random from items in the data structure.\r\n"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Sets/","title":"Sets"},"frontmatter":{"draft":false},"rawBody":"# Sets\n\nCreated: 2018-02-07 18:00:42 +0500\n\nModified: 2018-02-07 18:12:42 +0500\n\n---\n\nApplication of Symbol table.\n**Sets:**\n\nA collection of distinct keys\n\nApplication - Exception filter\n-   Read in a list of words from one file\n-   Print out all words from standard input that are {in, not in} the list\n    -   Whitelist a set of strings or objects\n    -   Blacklist a set of strings or objects\n\nApplication of Exception filter -\n-   Spell checker\n-   Browser\n-   Parental controls\n-   Chess\n-   Spam filter\n-   Credit cards\n"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Skip-Lists/","title":"Skip Lists"},"frontmatter":{"draft":false},"rawBody":"# Skip Lists\n\nCreated: 2018-03-24 13:51:51 +0500\n\nModified: 2018-05-17 22:37:42 +0500\n\n---\n\nIn[computer science](https://en.wikipedia.org/wiki/Computer_science), a**skip list**is a[data structure](https://en.wikipedia.org/wiki/Data_structure)that allows fast searchwithin an[ordered sequence](https://en.wikipedia.org/wiki/Ordered_sequence)of elements. Fast search is made possible by maintaining a[linked](https://en.wikipedia.org/wiki/Linked_list)hierarchy of subsequences, with each successive subsequence skipping over fewer elements than the previous one (see the picture below on the right). Searching starts in the sparsest subsequence until two consecutive elements have been found, one smaller and one larger than or equal to the element searched for. Via the linked hierarchy, these two elements link to elements of the next sparsest subsequence, where searching is continued until finally we are searching in the full sequence. The elements that are skipped over may be chosen probabilisticallyor deterministically,with the former being more common.\n![Skip Lists Sko lists are lir*ed 'Stg that to Sko to the Correct The bottkneck inherent in a sequential scan is avoided. while insertion and deletion remain relatively efficient, Search ig Worst-cage Search time is O(n), tn_lt extremely uNikelÂ» An excellent reference for skip lists is Theory The indexing scheme employed in skip lists is similar in nature to the methcni used to names an TO bokup a name, you index to tab character Of the desired entry. In Figure 3-8. for example. the list represents a simple linked list no tabs. Adcfng tabs (middle figure) facilitates the search. In this Cage, are traversed Once the segment of the list is found. levelâ€¢O pointers are traversed to fnd the specific entry. Figure 3-8: Skip List Construction The indexing scheme may be extended as shown in the bottom figure. we ncnv have an 'Idem to the TO locate an item, level-2 pointers are traversed until the Correct Segment Of the \"st is identified. Subsequently. level-I and level-O pointers are traversed. Our*lg the number Of pointers required for a new node must be determined. This ig easily resolved using a probabilistic technique A rwmber g---atCY is used to togg a computer coin. When inserting a new node. the rnin is tossed to determine fit should be level-I. It you lose. the ig togged again to detemine if Should be level-2 Another 'OSS and the Coin is togged to detemine if the Iuel-3_ This repeats until you \"in. If one (level-Ol IS the data Structwe is a Simple with O(n) search time However. if sufficient levels are implemented. the skip list maybe viewed as a tree with the rcnt at the YÃ¼ghest level, time is O(lg n) The skip list algorithm has a probabilistic component. and thus a probabilistic bwnds on the time requiwd to However, these quite tÃ‰ht in normal For uample, search a list containing 1000 items. the probability that search time will be 5 times the average is about 1 in Implementation in C An fM Skip lists inclL&d_ Typedefg recType, keyType. and Comparison operators compLT and compEQ should be altered to reflect the data stored in the list In addition. MAXLEVEL Should be Set based on the Size Of the dataset ](media/Skip-Lists-image1.png)\n\n![To initialize, initList is called. The list header is allocated and initialized. To indicate an empty list. all leveLS are to point to the insert a Searches the correct insertion paint. and inserts it in the list, While searching. the update array maintains pointers to the upw-leveâ€¢ Ã¦ntered_ This is to establish links for the inserted node. The newLevel is determined using a random number generator. and the node allÃ¦ated_ The links established using fm the update array Function delete deletes andfrees a nt:ule, and is implemented in a similar manner. Function find Searches the list for a partzular value. Implementation in Visual Basic Each ncxie in a skip list varies in size depending on a random number generated at time of insertion. Instantiating a class with dynamic size is a bit of a sticky wicket in Visual Basic. Comparison We have seen several ways to dictionaries: hash tables. unbalanced binary search trees. red-black Skip lists. There are Several factcYS that the Choice Of an algorithm: Sorted Output If is then hash tabkS are a alternative. Entries are stored in the table based on their hashed value. no other ordering. For binary trees. the Story ig different An irFOrder walk produce a Scyted list For example: To examine skip list ncnies in order. simply chain the levelâ€¢O pointers. For example: Node â€¢p = While (p NIL) p P->EÃ„dt01; Space, The amount of memory required to store a value should minimized. This is especially if mar\"' nc&g are to be For hash tables. only one forward pointer per node is required. In addition, the hash table itself m u St be alkJCated For each node hag a left. parent penter_ In additZn, the Of ncxie must be recorded. AJ1hough this requires only one bit. more space may be allcu:ated to ensure that the Size Of ig property Therefore each in a red---Watk tree requires enough space for 34 pointers. For skip lists. each node has a forward pointer. The prcbability ot having a level-I pointer is 1/2. Of having a level-2 is 1/4. In general, the Offorward pointers per ncnie is ](media/Skip-Lists-image2.png)\n\n![Time. The algorithm should be efficient. This is especially true if a large dataset is expected. Table 3-2 the sÃ¦rch for each alwithm Note that for hash tables and skip lists is extremely unlikely. Actual timing tests are descried below Simplicity. If algorithm is and easy to understand. mistakes maybe made. This not only makes your life easy, but with the task Of making repairs will appreciate any efforts pu make in this area. The number of statements required for each ig listed in TaNe 3-2 method table tree tree ski list st atementS a time worst-case time Table 3-2: Comparison Of DictionÃ¼ies time for '1Sert, Search, and dekte operations on a database Of 65,536 (216) randomly input items may be found in Table 3-3. For this test the hash table size was 10.009 and 16 index levels for the Skip list Although there is game variation in the timings for the fcn_lr metht:uis. they are close enough so that other considerations shmâ€¢ld come into play when selecting method hash table insert search delete tree red-black tree ski list Table 3-3: Average Time (us), 65536 Items, Input Table 3-4 Showg the average Search time for Sets Of data a randcm get. where v*ueg are unique. and an ordered set. where values are in ascending order Ordered input creates a vmrstâ€¢ case Scenario for unbalanced tree algorithms. ag the tree ends up being a linked list. The times are for a single search operation. If we were to search for all items in a database of 65,536 values. a red-black tree take _6 Seconds, while an unbalanced tree algorithm would take I hour. ran do m input ordered input hash tabl e 16 65 536 16 256 4,096 65 536 Table 3-4: unbalanced tree red-black tree Sk list 1 033 55019 Average Search Time (us) ](media/Skip-Lists-image3.png)"},{"fields":{"slug":"/Data-Structures/Linear-Data-Structure/Stack-LIFO/","title":"Stack LIFO"},"frontmatter":{"draft":false},"rawBody":"# Stack LIFO\n\nCreated: 2018-01-30 21:19:26 +0500\n\nModified: 2018-08-04 12:23:07 +0500\n\n---\n\n**Points to remember -**\n\n1.  Linear Data Structure\nStack is an abstract data type with a bounded(predefined) capacity. It is a simple data structure that allows adding and removing elements in a particular order. Every time an element is added, it goes on the top of the stack, the only element that can be removed is the element that was at the top of the stack, just like a pile of objects.\n**Basic features of Stack**\n\n1.  Stack is an ordered list of similar data type.\n\n2.  Stack is a**LIFO**structure. (Last in First out).3.  **push()**function is used to insert new elements into the Stack and**pop()**function is used to delete an element from the stack. Both insertion and deletion are allowed at only one end of Stack called**Top**.4.  Stack is said to be in**Overflow**state when it is completely full and is said to be in**Underflow**state if it is completely empty.\n\n5.  Loitering (don't keep the reference of the object when an element is popped)\n**Applications -**\n\n1.  Parsing\n\n2.  Expression Conversion (Infix to Postfix, Postfix to Prefix)\n\n3.  JVM\n\n4.  Back button in a browser\n\n5.  Undo\n\n6.  Implementing function calls in a compiler (ex - gcd of two numbers)\n\n7.  PostScript languages for printer\n\n8.  Arithmetic expression evaluation\n\nDijkstra's Two-stack algorithm (Use - Interpreter)\n**Operations -**\n\n1.  Push (O(1))\n\n2.  Pop (O(1))\n\n3.  Top (O(1))\n\n4.  Search (O(1))\n\n5.  Iterate\n\n6.  Overflow\n\n7.  Underflow\n**Implementation -**\n\n1.  Using Linked List\n\n2.  Using Array (resizing array)\n**Code**\n\n1.  **Linked List Implementation**\n![public class Li nkedStack0fStrings pri vate Node fi rst = null; pri vate class Node String item; Node next; public boolean isEmpty() { return fi rst null; public void push (String item) Node oldfi rst = fi rst; first = new Node() ; fi rst. item = item; fi rst.next = 01 dfi rst; public String pop() String item = first. item; fi rst = first. next; return item; private inner class (access modifiers don't matter) ](media/Stack-LIFO-image1.png)\n**Loitering -**\n\nHolding the reference to an object, when it is no longer needed.\n**Resizing Arrays**\n\n1.  Repeated Doubling\n\nWhen an array fills up, create a new array twice the size of the previous array and copy of the elements in the new array.\n\n2.  Thrashing\n\nIf the client happens to do push-pop-push-pop alternating when the array is full, then it's going to be doubling, halving, doubling, halving, doubling, halving. Creating new arrays on every operation. Take time proportional to N for every operation, and therefore quadratic time for everything.\n2.  **Array Implementation (With resizing array)**\n\n    a.  Push: Double the size of array when array is full (Repeated doubling)\n\n    b.  Pop: halve size of array when array is one-quarter full\n\n    c.  Trashing: Problem if we half the array on pop operation. It can be repeated doubling, halving, doubling, halving\n\n    d.  Invariant: Array is between 25% and 100% full\n\n    e.  Every operation takes constant amortized time.\npublic ResizingArrayStackOfStrings()\n{ s = new String[1]; }\n\npublic void push(String item)\n{\n\nif (N == s.length) resize(2 * s.length);\n\ns[N++] = item;\n}\n\nprivate void resize(int capacity)\n{\n\nString[] copy = new String[capacity];\nfor (int i = 0; i < N; i++)\n\ncopy[i] = s[i];\ns = copy;\n\n}\npublic String pop()\n{\n\nString item = s[--N];\ns[N] = null;\nif (N > 0 && N == s.length/4) resize(s.length/2);\nreturn item;\n\n}Linked List vs Array - Stack Implementation -\n\n1.  If we need perfect constant time operation for every operation, then we should use stack, ex in an airplane or time critical situation\n\n2.  But for ex in a packet router we want very fast operations, so dealing with references will be slow in linked list so we must use array\n**Interview Questions**\n-   Evaluate postfix expression using a stack\n-   Sort values in a stack\n-   Check balanced parentheses in an expression\n\n"},{"fields":{"slug":"/Data-Structures/Others/Bitmap/","title":"Bitmap"},"frontmatter":{"draft":false},"rawBody":"# Bitmap\n\nCreated: 2019-07-14 15:34:08 +0500\n\nModified: 2020-09-05 01:46:52 +0500\n\n---\n\n**Bitmap / Bit Array / Bit Vector / Bitmap index / Bit set / Bit string**\nA**bit array**(also known as**bit map**,**bit set**,**bit string**, or**bit vector**) is an[array data structure](https://en.wikipedia.org/wiki/Array_data_structure)that compactly stores[bits](https://en.wikipedia.org/wiki/Bit). It can be used to implement a simple[set data structure](https://en.wikipedia.org/wiki/Set_data_structure). A bit array is effective at exploiting bit-level parallelism in hardware to perform operations quickly. A typical bit array stores*kw*bits, where*w*is the number of bits in the unit of storage, such as a[byte](https://en.wikipedia.org/wiki/Byte)or[word](https://en.wikipedia.org/wiki/Word_(computer_architecture)), and*k*is some nonnegative integer. If*w*does not divide the number of bits to be stored, some space is wasted due to[internal fragmentation](https://en.wikipedia.org/wiki/Fragmentation_(computing)).\nIn[computing](https://en.wikipedia.org/wiki/Computing), abitmapis a mapping from some domain (for example, a range of integers) to[bits](https://en.wikipedia.org/wiki/Bit). It is also called a[bit array](https://en.wikipedia.org/wiki/Bit_array)or[bitmap index](https://en.wikipedia.org/wiki/Bitmap_index).\nThe more general termpix-maprefers to a map of[pixels](https://en.wikipedia.org/wiki/Pixel), where each one may store more than two colors, thus using more than one bit per pixel. Oftenbitmapis used for this as well. In some contexts, the termbitmapimplies one bit per pixel, whilepixmapis used for images with multiple bits per pixel.\nA bitmap is a type of[memory](https://en.wikipedia.org/wiki/Computer_storage)organization or[image file format](https://en.wikipedia.org/wiki/Image_file_format)used to store[digital images](https://en.wikipedia.org/wiki/Digital_image). The termbitmapcomes from the[computer programming](https://en.wikipedia.org/wiki/Computer_programming)terminology, meaning just amap of bits, a spatially mapped[array of bits](https://en.wikipedia.org/wiki/Bit_array). Now, along withpixmap, it commonly refers to the similar concept of a spatially mapped array of pixels.[Raster](https://en.wikipedia.org/wiki/Raster_graphics)images in general may be referred to as bitmaps or pixmaps, whether synthetic or photographic, in files or memory.\n**When should you use a bitmap?**\n\nSets are a fundamental abstraction in software. They can be implemented in various ways, as hash sets, as trees, and so forth. In databases and search engines, sets are often an integral part of indexes. For example, we may need to maintain a set of all documents or rows (represented by numerical identifier) that satisfy some property. Besides adding or removing elements from the set, we need fast functions to compute the intersection, the union, the difference between sets, and so on.\nTo implement a set of integers, a particularly appealing strategy is the bitmap (also called bitset or bit vector). Using n bits, we can represent any set made of the integers from the range [0,n): the ith bit is set to one if integer i is present in the set. Commodity processors use words of W=32 or W=64 bits. By combining many such words, we can support large values of n. Intersections, unions and differences can then be implemented as bitwise AND, OR and ANDNOT operations. More complicated set functions can also be implemented as bitwise operations.\nWhen the bitset approach is applicable, it can be orders of magnitude faster than other possible implementation of a set (e.g., as a hash set) while using several times less memory.\nHowever, a bitset, even a compressed one is not always applicable. For example, if the you have 1000 random-looking integers, then a simple array might be the best representation. We refer to this case as the \"sparse\" scenario.\n**When should you use compressed bitmaps?**\n\nAn uncompressed BitSet can use a lot of memory. For example, if you take a BitSet and set the bit at position 1,000,000 to true and you have just over 100kB. That's over 100kB to store the position of one bit. This is wasteful even if you do not care about memory: suppose that you need to compute the intersection between this BitSet and another one that has a bit at position 1,000,001 to true, then you need to go through all these zeroes, whether you like it or not. That can become very wasteful.\nThis being said, there are definitively cases where attempting to use compressed bitmaps is wasteful. For example, if you have a small universe size. E.g., your bitmaps represent sets of integers from [0,n) where n is small (e.g., n=64 or n=128). If you are able to uncompressed BitSet and it does not blow up your memory usage, then compressed bitmaps are probably not useful to you. In fact, if you do not need compression, then a BitSet offers remarkable speed.\nThe sparse scenario is another use case where compressed bitmaps should not be used. Keep in mind that random-looking data is usually not compressible. E.g., if you have a small set of 32-bit random integers, it is not mathematically possible to use far less than 32 bits per integer, and attempts at compression can be counterproductive.\n**References**\n\n<https://en.wikipedia.org/wiki/Bitmap>\n\n<https://en.wikipedia.org/wiki/Bit_array>\n\n<https://github.com/RoaringBitmap/RoaringBitmap>\n\n<http://paulbourke.net/dataformats/bitmaps>\n"},{"fields":{"slug":"/Data-Structures/Others/LSM-(Log-Structured-Merge-Trees)/","title":"LSM (Log Structured Merge Trees)"},"frontmatter":{"draft":false},"rawBody":"# LSM (Log Structured Merge Trees)\n\nCreated: 2019-06-21 17:07:15 +0500\n\nModified: 2020-11-18 23:43:09 +0500\n\n---\n\nIn [computer science](https://en.wikipedia.org/wiki/Computer_science), the log-structured merge-tree (or LSM tree) is a[data structure](https://en.wikipedia.org/wiki/Data_structure) with performance characteristics that make it attractive for providing[indexed](https://en.wikipedia.org/wiki/Database_index)access to files with high insert volume, such as [transactional log data](https://en.wikipedia.org/wiki/Transaction_log). LSM trees, like other [search trees](https://en.wikipedia.org/wiki/Search_tree), maintain key-value pairs. LSM trees maintain data in two or more separate structures, each of which is optimized for its respective underlying storage medium; data is synchronized between the two structures efficiently, in batches.\n\nOne simple version of the LSM tree is a two-level LSM tree. As described by [Patrick O'Neil](https://en.wikipedia.org/wiki/Patrick_O%27Neil), a two-level LSM tree comprises two [tree-like](https://en.wikipedia.org/wiki/Tree_(data_structure))structures, called C0and C1. C0 is smaller and entirely resident in memory, whereas C1 is resident on disk. New records are inserted into the memory-resident C0 component. If the insertion causes the C0 component to exceed a certain size threshold, a contiguous segment of entries is removed from C0 and merged into C1 on disk. The performance characteristics of LSM trees stem from the fact that each component is tuned to the characteristics of its underlying storage medium, and that data is efficiently migrated across media in rolling batches, using an algorithm reminiscent of [merge sort](https://en.wikipedia.org/wiki/Merge_sort).\n\nMost LSM trees used in practice employ multiple levels. Level 0 is kept in main memory, and might be represented using a tree. The on-disk data is organized into sorted runs of data. Each run contains data sorted by the index key. A run can be represented on disk as a single file, or alternatively as a collection of files with non-overlapping key ranges. To perform a query on a particular key to get its associated value, one must search in the Level 0 tree and each run.\nA particular key may appear in several runs, and what that means for a query depends on the application. Some applications simply want the newest key-value pair with a given key. Some applications must combine the values in some way to get the proper aggregate value to return. For example, in [Apache Cassandra](https://en.wikipedia.org/wiki/Apache_Cassandra), each value represents a row in a database, and different versions of the row may have different sets of columns.\n\nIn order to keep down the cost of queries, the system must avoid a situation where there are too many runs.\n\nExtensions to the 'leveled' method to incorporate [B+ tree](https://en.wikipedia.org/wiki/B%2B_tree)structures have been suggested, for example bLSMand Diff-Index.\n\nLSM trees are used in data stores such as [Bigtable](https://en.wikipedia.org/wiki/Bigtable), [HBase](https://en.wikipedia.org/wiki/HBase), [LevelDB](https://en.wikipedia.org/wiki/LevelDB), [SQLite4](https://en.wikipedia.org/wiki/SQLite4), [Tarantool](https://en.wikipedia.org/wiki/Tarantool) , [RocksDB](https://en.wikipedia.org/wiki/RocksDB), [WiredTiger](https://en.wikipedia.org/wiki/WiredTiger) (MongoDB Engine), [Apache Cassandra](https://en.wikipedia.org/wiki/Apache_Cassandra), [InfluxDB](https://en.wikipedia.org/wiki/InfluxDB) and[VictoriaMetrics](https://en.wikipedia.org/w/index.php?title=VictoriaMetrics&action=edit&redlink=1).\n\n<table>\n<colgroup>\n<col style=\"width: 100%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>Time complexity in big O notation</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><table>\n<colgroup>\n<col style=\"width: 37%\" />\n<col style=\"width: 29%\" />\n<col style=\"width: 33%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>Algorithm</th>\n<th>Average</th>\n<th>Worst case</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>Insert</td>\n<td>O(1)</td>\n<td>O(1)</td>\n</tr>\n<tr class=\"even\">\n<td>Find-min</td>\n<td>O(N)</td>\n<td>O(N)</td>\n</tr>\n<tr class=\"odd\">\n<td>Delete-min</td>\n<td>O(N)</td>\n<td>O(N)</td>\n</tr>\n</tbody>\n</table></td>\n</tr>\n</tbody>\n</table>\n\n[Log-structured merge-tree - Wikipedia](https://en.wikipedia.org/wiki/Log-structured_merge-tree)\n\n## Anatomy\n\nIn LSM Trees, all the writes are performed against the mutable in-memory data structure (once again, often implemented using a data structure allowing logarithmic time lookup, such as a B-Tree or a [SkipList](http://epaperpress.com/sortsearch/download/skiplist.pdf)). Whenever the size of the tree reaches a certain threshold (or after some predefined time period elapses, whichever comes first), we write the data the disk, creating a new SSTable. This process is sometimes called \"flush\". Retrieving the data may require searching all SSTables on disk, checking the in-memory table and merging their contents together before returning the result.\n\n![Memory key I Reads Writes Flush Disk key2 key4 key4 key7 key8 key5 key6 key8 Reads ](media/LSM-(Log-Structured-Merge-Trees)-image1.png)\n\nStructure of an LSM Tree: a memory-resident table, used for writes. Whenever the memory table is large enough, it's sorted contents are written on disk, becoming an SSTable. Reads are served, hitting all SSTables and the memory-resident table, requiring a merge process to reconcile the data.\nThe merge step during the read is required, since the data can be split in several parts (for example, an insert followed by delete operation, where delete would shadow the originally inserted record; or an insert, followed by the update operation, where a new field is added to the record).\nEvery data item in SSTable has a timestamp associated with it. For inserts it specifies the write time, for updatesâ€Š---â€Šan update time and removal time for deletes.\n**Summary**\n\nLSM Tree databases are typically write-optimized, since all the writes are performed against the write-ahead log (for durability and fail over) and memory resident tables. Reads are usually slower, because of the merge process and a need to check multiple files on disk.\nBecause of the maintenance, LSM-Trees might result into worse latency, since both CPU and IO bandwidth is spent re-reading and merging tables instead of just serving reads and writes. It's also possible, under a write-heavy workload, to saturate IO by just writes and flushes, stalling the compaction process. Lagging compaction results into slower reads, increasing CPU and IO pressure, making the matters worse. This is something to watch out for.\nLSM-Trees cause some write amplification: data has to be written to the write-ahead log, then flushed on disk, where it will be eventually re-read and written again during the compaction process. That said, mutable B-Tree structures also suffer from write amplification, so I'd prefer to leave the cost analysis until after we discuss B-Trees and a conjecture that helps understanding that we are just trading read performance against write performance and memory overhead.\n**Closing Words**\n\nAs you can see all write operations in LSM Trees are sequential: Write-Ahead Log appends, Memtable flushes, Compactions. Using[per-SSTable indexes](https://github.com/apache/cassandra/blob/trunk/doc/SASI.md)or pre-sorting data can also help to make at least some read operations sequential. It can only be done to a certain extend as reads have to be performed against multiple files and then merged together.\n<https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f>\n\n<https://medium.com/databasss/on-disk-io-access-patterns-in-lsm-trees-2ba8dffc05f9>\n\n## LMS Trees (Copy on Write B-Trees)**\n-   Take B-Tree, let's make the pages immutable (every page is a new instance appended to old instance)\n<https://blog.acolyer.org/2014/11/26/the-log-structured-merge-tree-lsm-tree>\n\n"},{"fields":{"slug":"/Data-Structures/Others/SSTables,-Sorted-String-Tables/","title":"SSTables, Sorted String Tables"},"frontmatter":{"draft":false},"rawBody":"# SSTables, Sorted String Tables\n\nCreated: 2019-06-27 15:52:53 +0500\n\nModified: 2019-06-27 16:02:27 +0500\n\n---\n\nThe advantage of the[Sorted String Tables](https://static.googleusercontent.com/media/research.google.com/en/archive/bigtable-osdi06.pdf)is their simplicity: they are easy to write, search and read. SSTables are a persistent ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. They have some nice properties like, for example, the random point-queries (i.e. finding a value by key) can be done quickly by looking up the primary index, sequential scans (i.e. iterating over all key/value pairs in a specified key range) can be done efficiently by just reading the records one after the other.\nUsually the SSTable has two parts: index and data blocks. Data block consists from the key/value pairs concatenated one after another. The index block contains primary keys and offsets, pointing to the offset in the data block where the actual record can be found. Primary index can be implemented using a format optimised for quick searches, like a B-Tree, for example.\n![Index Black keyi :offsetl key2:offset2 key3:offset3 key4:offset4 Data Black ](media/SSTables,-Sorted-String-Tables-image1.png)\n\nSSTable is a persistent ordered immutable data structures. It usually consist of Index and Data blocks, where index block can be represented by a quick lookup data structure, holding offsets to the values in the Data block; the Data block holds concatenated key/value pairs, enabling fast sequential rangescans.\nSince SSTable is immutable, insert, update or delete operations would require rewriting the whole file, since it's optimised for reads, written sequentially and has no reserved empty space that would allow any in-place modifications.\nMany databases use SSTables:[RocksDB](https://github.com/facebook/rocksdb/wiki/Terminology)and[Cassandra](https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlManageOndisk.html), just to name a few, but there are plenty other examples. Cassandra, starting with version 3.4, incorporated[SSTable Attached Secondary Indexes](https://github.com/apache/cassandra/blob/trunk/doc/SASI.md), a concept built on top SSTables and LSM Trees, that simplifies the secondary index maintenance by coupling the index building to memory-resident table flush and SSTable merge process.\n<https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f>\n\n"},{"fields":{"slug":"/Data-Structures/Trie/Compressed-Trie/","title":"Compressed Trie"},"frontmatter":{"draft":false},"rawBody":"# Compressed Trie\n\nCreated: 2018-04-10 22:45:52 +0500\n\nModified: 2018-04-10 22:47:48 +0500\n\n---\n\nCompressed Trie is obtained from standard trie by joining chains of single nodes. The nodes of a compressed trie can be stored by storing index ranges at the nodes\n![](media/Compressed-Trie-image1.png)\n\n"},{"fields":{"slug":"/Data-Structures/Trie/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-12-14 16:17:07 +0500\n\nModified: 2019-12-14 16:17:43 +0500\n\n---\n\n**Hash Array Mapped Tries (HAMT)**\n\nA**hash array mapped trie**[^[1]^](https://en.wikipedia.org/wiki/Hash_array_mapped_trie#cite_note-bagwell-1)(**HAMT**) is an implementation of an[associative array](https://en.wikipedia.org/wiki/Associative_array)that combines the characteristics of a[hash table](https://en.wikipedia.org/wiki/Hash_table)and an array mapped[trie](https://en.wikipedia.org/wiki/Trie).[^[1]^](https://en.wikipedia.org/wiki/Hash_array_mapped_trie#cite_note-bagwell-1)It is a refined version of the more general notion of a[hash tree](https://en.wikipedia.org/wiki/Hash_tree_(persistent_data_structure)).\n<https://en.wikipedia.org/wiki/Hash_array_mapped_trie>\n"},{"fields":{"slug":"/Data-Structures/Trie/Patricia-Trie/","title":"Patricia Trie"},"frontmatter":{"draft":false},"rawBody":"# Patricia Trie\n\nCreated: 2018-04-12 00:52:39 +0500\n\nModified: 2018-04-12 00:52:49 +0500\n\n---\n\n![Patricia trie Patricia trie. [Practical Algorithm to Retrieve Information Coded i â€¢ Remove one-way branching. â€¢ Each node represents a sequence of characters. â€¢ Implementation: one step beyond this course. put shel I fi sh' standard trie Applications. Database search. â€¢ P2P network search. â€¢ IP routing tables: find longest prefix match. Compressed quad-tree for N-body simulation. â€¢ Efficiently storing and querying XML documents. s s h e 1 ](media/Patricia-Trie-image1.png)\n\n"},{"fields":{"slug":"/Data-Structures/Trie/Questions/","title":"Questions"},"frontmatter":{"draft":false},"rawBody":"# Questions\n\nCreated: 2018-08-04 12:27:45 +0500\n\nModified: 2019-12-02 14:01:40 +0500\n\n---\n-   Count total number of words in Trie\n-   Print all words stored in Trie\n-   Sort elements of an array using Trie\n-   Form words from a dictionary using Trie\n-   Build a T9 dictionary\n![Prefix matches Find all keys in a symbol table starting with a given prefix. Ex. Autocomplete in a cell phone, search bar, text editor, or â€¢ User types characters one at a time. â€¢ System reports all matching strings. Googl Emily E:rnrna WebO why is my comÃ… why is my computer so slow why is my computer slow why is my computer so slow all of a why is my computer so loud why is my computer running so slowl why is my computer screen so big why is my computer freezing is my computer beeping ](media/Questions-image1.png)\n\n![Prefix matches in an R-way trie Find all keys in a symbol table starting with a given prefix. keysWi thPrefi sh\") ; find subtriefor all keys beginning with \"sh\" public Iterab1e<Stri keysWithPrefix(String prefix) e 00 ](media/Questions-image2.png)\n\n![Longest prefix Find longest key in symbol table that is a prefix of query stri Ex. To send packet toward destination IP address, router address in routing table that is longest prefix match. \"128\" \"128 .112 \"128.112.055 \"128.112 .055.15\" \"128 .112 .136 \"128 .112 .155 .11\" \"128.112 .155.13\" \"128 . 222 represented as 32-bit binary number for IPv4 (instead of string) 1 ongestPrefi x0f(\" 128.112 .136.11\") 1 ongestPrefi x0f(\" 128.112 .100.16\") 1 ongestPrefi x0f(\" 128.166.123.45\") ](media/Questions-image3.png)\n\n![Longest prefix in an R-way trie Find longest key in symbol table that is a prefix of query stri â€¢ Search for query string. â€¢ Keep track of longest key encountered. \"she\" s s \"shell \" \"shellsorâ€¢ h e a 2 1 s s h e o 1 search ends at end of strine value is null return she (last key on path) a 2 search ends at end of string value is not null s s ](media/Questions-image4.png)\n\n![Longest prefix in an R-way trie: Java implementation Find longest key in symbol table that is a prefix of query stri â€¢ Search for query string. â€¢ Keep track of longest key encountered. public String longestPrefixOf(String query) int length = search(root, query, 0, 0); return query. substring(0, length) ; private int search(Node x, String query, if (x null) return length; if (x. val != null) length = d int d, int ](media/Questions-image5.png)\n\n"},{"fields":{"slug":"/Data-Structures/Trie/R-way-Tries/","title":"R-way Tries"},"frontmatter":{"draft":false},"rawBody":"# R-way Tries\n\nCreated: 2018-04-10 23:06:14 +0500\n\nModified: 2018-05-03 23:28:29 +0500\n\n---\n\n![Tries Tries](media/R-way-Tries-image1.png)\n\n![Search in a trie](media/R-way-Tries-image2.png)\n\n![Search in a trie](media/R-way-Tries-image3.png)\n\n![Search in a trie](media/R-way-Tries-image4.png)\n\n![Search](media/R-way-Tries-image5.png)\n\n![Insertion](media/R-way-Tries-image6.png)\n\n![Trie construction demo trie](media/R-way-Tries-image7.png)\n\n![Trie representation](media/R-way-Tries-image8.png)\n\n![R-way trie](media/R-way-Tries-image9.png)\n\n![R-way trie](media/R-way-Tries-image10.png)\n\n![Trie performance](media/R-way-Tries-image11.png)\n\n![Popular interview](media/R-way-Tries-image12.png)\n\n![Deletion](media/R-way-Tries-image13.png)\n\n![Deletion](media/R-way-Tries-image14.png)\n\n![String symbol](media/R-way-Tries-image15.png)\n"},{"fields":{"slug":"/Data-Structures/Trie/Standard-Trie/","title":"Standard Trie"},"frontmatter":{"draft":false},"rawBody":"# Standard Trie\n\nCreated: 2017-12-10 18:53:32 +0500\n\nModified: 2021-09-05 16:20:45 +0500\n\n---\n\nRadix Tree / Prefix Tree (Because pre-order traversal would give the nodes in lexicographical order)\n-   Autocomplete function\n-   Ordered tree structure having one or more relations between the elements\n**Pros**\n-   M - length of the string\n-   No worry about collision\n**Cons**\n-   Lots of space\nA trie also known as radix tree or prefix tree, is an ordered tree structure that stores sets or arrays having one or more relations between the elements.\nUnlike a binary search tree, an element of a trie isn't an independent entity, but more of a sequence of states following one another.\nThe best example that uses the trie data structure is the autocomplete function of smartphones: the entry node represents the empty character as you press space, then it gives suggestions of words (represented as leaves) that can be obtained extending the current character sequence (path between the current node and a leaf).\n![20 car 19 care 50 ca cat cars ](media/Standard-Trie-image1.png)\nA node that has avalue associatedrepresents a word with a higher or lower chance to be picked by the autocomplete function.\nLooking up data in atrieis fast; the worst case takesO(m), wheremis the length of the string, and you don't have to worry about two different words ending up in the same location(collision).\nThe drawback is thattriescan take up a lot of space because they save each intermediate step. It is also hard to prevent adding long, meaningless sequences of characters to the trie.\nThey are also calledprefix treesbecause apre-order traversalwould visit the nodes inlexicographical order.\n![false false true true Zebra Card false true true true false root false false true false false ](media/Standard-Trie-image2.png)"},{"fields":{"slug":"/Data-Structures/Trie/Suffix-Array/","title":"Suffix Array"},"frontmatter":{"draft":false},"rawBody":"# Suffix Array\n\nCreated: 2018-04-10 22:51:14 +0500\n\nModified: 2021-06-12 09:00:01 +0500\n\n---\n\n***A suffix array is a sorted array of all suffixes of a given string***. The definition is similar to[Suffix Tree which is compressed trie of all suffixes of the given text](https://www.geeksforgeeks.org/pattern-searching-set-8-suffix-tree-introduction/). Any suffix tree based algorithm can be replaced with an algorithm that uses a suffix array enhanced with additional information and solves the same problem in the same time complexity.\nA suffix array can be constructed from Suffix tree by doing a DFS traversal of the suffix tree. In fact Suffix array and suffix tree both can be constructed from each other in linear time.\nAdvantages of suffix arrays over suffix trees include improved space requirements, simpler linear time construction algorithms (e.g., compared to Ukkonen's algorithm) and improved cache locality\n**Applications -**\n-   Full text indices\n-   Data compression algorithms\n-   Field of bibliometrics\n# Suffix Arrays and Suffix Sort\n\n![Keyword-in-context search Given a text of N characters, preprocess it to enable fast sut (find all occurrences of query string context). % more tale . txt it it it it it it it it it it was was was was was was was was was was the the the the the the the the the the best of times worst of times age of wi sdom age of foolishness epoch of belief epoch of incredulity season of light season of darkness spring of hope winter of despair ](media/Suffix-Array-image1.png)\n\n![Keyword-in-context search Given a text of N characters, preprocess it to enable fast sut (find all occurrences of query string context). characters of % java KWIC tale. txt 15 search o st giless to her unavailing le and gone in t provinces in dispersing in n that bed and better thing t is a far far some sense of search search search search search search better better surrounding context for contraband for your fathe of her husband of impoveri she of other carri the straw hold thing that i do than things else forgotte ](media/Suffix-Array-image2.png)\n\n![Suffix sort form suffixes input string o 1 2 3 4 5 6 7 8 9 10 11 i t w a s e s i t w t w a s e s i t w a w a s e s i t w a s a s e s i t w a s w s e s i t w a s w e i t w a s w e s i t w a s w i o s i t w a s w t 1 i t w a s w w 2 i t w a s w a 3 t w a s w s 4 a s w 5 s w e 6 w s 7 8 w a s w i 9 3 12 5 6 o 9 4 7 13 8 1 10 t 10 w 11 a 12 s 13 w 14 sort suffixes to bring rel a a e i i s s t t s s e s t t w i w w w s w w e i t a a e i a a s t w s s s i t s s w a w t t w w i a s e w a t s w S ](media/Suffix-Array-image3.png)\n\n![Keyword-in-context search: suffix-sorting solution suffix sort the text. â€¢ Preprocess: binary search for query; scan until mismatch. â€¢ Query: KWIC search for \"search\" in Tale of Two Cities 632698 713727 660598 67610 4430 42705 499797 182045 143399 411801 S S e e e e e e e e e e a a a 1 m m m r r r r r r e s s c c c c c c d h h h h h h e e e f t s s s s s s i o e i w C y m t s t s h o o p O n r t h V a W w r u w f e a a f s c a n s b a b i h n t a t a s O ](media/Suffix-Array-image4.png)\n\n![Longest repeated substring Given a string of N characters, find the longest repeated sub a g c c a a t c a g a g c c g a a a a c a t g c a g c c c a g a t g t a t a a a a a c g a t c c g g g t g a g c t a a t t c t a t c c a t t c c t a a t a t a t g g a g c a a a t t c g c a c a c a g c a t c t a a c a c a t a a t g t g c g c c a g g a a c c a g c c g g t c c t a a t a t t a a t a g c c c a t a g a a t g g c c g c g g a a t a g g a t t a t c c a t c t c a g a a g a a a g t c a c a g c t t a c a a t g a C a c t g c c c g c g c c a t c t a a a a t a t a t t t a a ](media/Suffix-Array-image5.png)\n\n![](media/Suffix-Array-image6.png)\n**Longest common prefix (LCP)**\n\n![Longest repeated substring Given a string of N characters, find the longest repeated sub Brute-force algorithm. â€¢ Try all indices i and j for start of possible match. â€¢ Compute longest common prefix (LCP) for each pair. a a c a a g t t t a c a a ](media/Suffix-Array-image7.png)\n\n![Longest repeated substring: input string a sorting solution a 4 g 5 t 6 t 7 t 8 form suffixes o 1 2 3 4 5 6 7 8 9 10 11 a o t t a c a 1 t a c a 2 3 c a a 9 o 11 3 9 1 12 4 10 2 13 5 ca agc 10 11 12 13 14 sort suffixes to bring rel a a c a a g t t t a c a a c a g t t t a c a c a a g t t t a c a a a g t t t a c a a g t t t a c a g t t a c a t t t a c a a c a a a a a a a a c c c g a a c c g a a t c g a a t t a t g t t t t a ](media/Suffix-Array-image8.png)\n\n![Longest repeated substring: Java implementation public String Irs(String s) int N = s. length() ; Stri ng[] suffi xes = new String[N] ; for (int i = 0; i < N; i + + ) suffi xes [i ] = s. substring(i, N); Arrays . sort (suffi xes) ; String 1 rs for (int i --- 0; i < N-l; i + + ) int len = lcp(suffixes[i], suffixes[i+l]); if (len > Irs.length()) = suffixes [i] . substri ng(0, len) ; 1 rs return Irs; ](media/Suffix-Array-image9.png)\n\n![Sorting challenge Problem. Five scientists A, B, C, D, and E are looking for Ion substring in a genome with over I billion nucleotides. â€¢ A has a grad student do it by hand. â€¢ B uses brute force (check all pairs). â€¢ C uses suffix sorting solution with insertion sort. â€¢ D uses suffix sorting solution with LSD string sort. â€¢ E uses suffix sorting solution with 3-way string quicksor b ](media/Suffix-Array-image10.png)\n\n![Longest repeated substring: empirical analysis input file LRS . java amendments . txt aesop. txt mobydi Ck . txt chromosomell. txt pi . txt characters 2,162 1 8,369 191 ,945 1.2 million million 7.1 10 million brute 0.6 sec 37 sec 1.2 hours 43 hours 2 months 4 months suffix sort 0.14 sec 0.25 sec I .0 sec 7.6 sec 61 sec 84 sec ](media/Suffix-Array-image11.png)\n\n![Suffix sorting: worst-case input Bad input: longest repeated substring very long. Ex: Ex: o 1 2 3 4 5 6 7 8 9 same letter repeated N times. two copies of the same Java codebase. form suffixes sorted suffixes t i n s t i n s i n s t i n s i n t i n s n s t i n S s t i n s t i n s i n s n s s 9 8 7 6 5 4 3 2 1 o i i n n S s t t n n s s t i s s t 1 i n n t i n n s s i n s s t i n S t n s i s i n n s ](media/Suffix-Array-image12.png)\n\n![Suffix sorting challenge Problem. Suffix sort an arbitrary string of length N. Q. What is worst-case running time of best algorithm for pr â€¢ Quadratic. Linearithmic. Linear. Nobody knows. Manber-Myers algorithm suffix trees (beyond our scope) ](media/Suffix-Array-image13.png)\n\n![Suffix sorting in linearithmic time Manber-Myers MSD algorithm overview. â€¢ Phase 0: sort on first character using key-indexed count â€¢ Phase i: given array of suffixes sorted on first 2i-l chara create array of suffixes sorted on first 2i characters. Worst-case running time. NIgN. â€¢ Finishes after lgN phases. â€¢ Can perform a phase in linear time. (!) [ahead] ](media/Suffix-Array-image14.png)\n\n![Linearithmic suffix sort example: original suffixes phase O key-indexed counting sort (fi 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 b a b a a a a b c b a b a a a a b a a a a b c b a b a a a a b a a a a b c b a b a a a a a a a a b c b a b a a a a a a a b c b a b a a a a a a b c b a b a a a a a a b c b a b a a a a a 0 b c b a b a a a a 0 c b a b a a a a b a b a a a a a b a a a a a b a a a a a 0 a a a a 0 a a a a a a 17 1 16 3 4 5 6 15 14 13 12 10 o 9 11 a a a a a a a a a a b b b b a a a b a a a a b a a a a a a b c 0 a a a a b a a a b c b 0 a a a a a a b c b a a a a a a c b a b a a a b b a b a a a c a b a a a b b a a a c 0 a a a a a a a a a a a a ](media/Suffix-Array-image15.png)\n\n![Linearithmic suffix sort example: original suffixes phase 1 index sort (first two charactel 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 b a b a a a a b c b a b a a a a b a a a a b c b a b a a a a b a a a a b c b a b a a a a a a a a b c b a b a a a a a a a b c b a b a a a a a a b c b a b a a a a a a b c b a b a a a a a 0 b c b a b a a a a 0 c b a b a a a a b a b a a a a a b a a a a a b a a a a a 0 a a a a 0 a a a a a a 17 16 12 3 4 5 13 15 14 6 1 10 o 9 11 a a a a a a a a a a b b b a a a a a a a b b b a a a a a a b a 0 a c a a b b a a a b c 0 b a a a a a b c b a a a a a a c b a b a a a a b a b a b a a abaaa b a a aa aaaaa c b aba ](media/Suffix-Array-image16.png)\n\n![Linearithmic suffix sort example: original suffixes phase 2 index sort (first four characte 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 b a b a a a a b c b a b a a a a b a a a a b c b a b a a a a b a a a a b c b a b a a a a a a a a b c b a b a a a a a a a b c b a b a a a a a a b c b a b a a a a a a b c b a b a a a a a 0 b c b a b a a a a 0 c b a b a a a a b a b a a a a a b a a a a a b a a a a a 0 a a a a 0 a a a a a a 17 16 15 14 3 12 13 4 5 1 10 6 2 11 o a a a a a a a a a a b b b a a a a a a a b b b a a a 0 a a a a a b a a a a b 0 a a b c a a b a a a b c b a a a a a a c b a a a b b a b a b b a c a a b a c a b b b a a b a a c a a a a a b b a a a a a a a a a b ](media/Suffix-Array-image17.png)\n\n![Linearithmic suffix sort example: original suffixes phase 3 index sort (first eight charact 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 b a b a a a a b c b a b a a a a b a a a a b c b a b a a a a b a a a a b c b a b a a a a a a a a b c b a b a a a a a a a b c b a b a a a a a a b c b a b a a a a a a b c b a b a a a a a 0 b c b a b a a a a 0 c b a b a a a a b a b a a a a a b a a a a a b a a a a a 0 a a a a 0 a a a a a a 17 16 15 14 13 12 3 4 5 10 1 6 11 2 9 a a a a a a a a a a b b b a a a a a a a b b b a a a 0 a a a a a b a a a a b 0 a a b c a b a a b c b a a a a c b a a a b b a b a b b a c a a b a c a b a b a a b a a 0 a a a a a b a a a a a a a a ](media/Suffix-Array-image18.png)\n\n![Constant-time string compare by indexing into inverse original suffixes index sort (first four characte 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 b a b a a a a b c b a b a a a a b a a a a b c b a b a a a a b a a a a b c b a b a a a a a a a a b c b a b a a a a a a a b c b a b a a a a a a b c b a b a a a a a a b c b a b a a a a a 0 b c b a b a a a a 0 c b a b a a a a b a b a a a a a b a a a a a b a a a a a 0 a a a a 0 9 a a a a a a 4 - 13 17 16 15 14 3 12 13 4 5 1 10 6 2 11 o a a a a a a a a a a b b a a a a a a a b b b a a 0 a a a a a b a a a b 0 a a b c a a b a a b c b a a a a a a c b a a a b b a b a b b a c a a b a c a b b b a a b a a c a a a a a b b a a a a a a a a a b ](media/Suffix-Array-image19.png)\n\n![String sorting summary We can develop linear-time sorts. â€¢ Key compares not necessary for string keys. â€¢ Use characters as index in an array. We can develop sublinear-time sorts. â€¢ Input size is amount of data in keys (not number of kev â€¢ Not all of the data has to be examined. 3-way string quicksort is asymptotically optimal. â€¢ 1.39 NIgNchars for random data. Long strings are rarely random in practice. ](media/Suffix-Array-image20.png)\n![What is the main reason to use the Manber-Myers algorithm instead of 3-way quicksort in order to suffix sort a string of length N? To use less space. To guarantee N log N performance. Correct While 3-way radix quicksort runs faster than Manber-Myers on typical inputs, the Manber-Myers algorithm has a worst-case running time of N log N, which is superior to that of 3-way radix quicksort. The Manber-Myers algorithm requires several auxiliary arrays and uses more extra space than 3-way radix quicksort. Stability is not a relevant concept for suffix sorting. To guarantee stability. To run faster on typical pieces of English text. ](media/Suffix-Array-image21.png)\n![String challenge: array of SUffixes Q. How to efficiently form array of suffixes? input string 01 2 suffixes a 11 a a g c 12 g c 13 14 c o 1 2 3 4 5 6 7 8 a a c a a g t t t a c a a g t t t a c a a g t t t a c a 3 a g t t t a c a a 4 a t t t a c a a g 5 g t t a c a a g t 6 t t a c a a g c t 7 t t a c a a g c t 8 t a c a a g c a 9 a c a a g c c 10 c a a g c ](media/Suffix-Array-image22.png)\n\n![String vs. StringBuiIder Q. How to efficiently form array of suffixes? A. B. public static String[] suffixes(String s) int N s. length() ; Stri ng[] suffixes = new String [N] ; for (int i --- suffi xes [i ] s.substring(i, N); return suffixes; public int static Stri s) --- s. length(); ](media/Suffix-Array-image23.png)\n![What is the order of growth of the amount of memory used by Algorithm A to form the n suffixes of a string of length n? Linear in both Java 6 and Java 7. Linear in Java 6 and quadratic inJava 7, Update 6. Correct Amazingly, Oracle and OpenJDK changed their representation of the String data type in Java 7, Update 6 so that the underlying char[] array is no longer shared! Now, it takes linear extra space and time to extract a substring (instead of constant extra space and time). See this article for more details. Quadratic in Java 6 and linear in Java 7, Update 6. Quadratic in both Java 6 and Java 7. ](media/Suffix-Array-image24.png)"},{"fields":{"slug":"/Data-Structures/Trie/Suffix-Tree/","title":"Suffix Tree"},"frontmatter":{"draft":false},"rawBody":"# Suffix Tree\n\nCreated: 2018-04-10 22:48:56 +0500\n\nModified: 2019-12-10 00:10:20 +0500\n\n---\n\nIn [computer science](https://en.wikipedia.org/wiki/Computer_science), a suffix tree (also called PAT tree or, in an earlier form, position tree) is a compressed [trie](https://en.wikipedia.org/wiki/Trie) containing all the [suffixes](https://en.wikipedia.org/wiki/Suffix_(computer_science)) of the given text as their keys and positions in the text as their values. Suffix trees allow particularly fast implementations of many important string operations.\nThe construction of such a tree for the string S takes time and space linear in the length of S. Once constructed, several operations can be performed quickly, for instance locating a [substring](https://en.wikipedia.org/wiki/Substring) in S, locating a substring if a certain number of mistakes are allowed, locating matches for a [regular expression](https://en.wikipedia.org/wiki/Regular_expression) pattern etc. Suffix trees also provide one of the first linear-time solutions for the [longest common substring problem](https://en.wikipedia.org/wiki/Longest_common_substring_problem). These speedups come at a cost: storing a string's suffix tree typically requires significantly more space than storing the string itself.\nThe suffix tree for the string S of length n is defined as a tree such that:\n-   The tree has exactly n leaves numbered from 1 to n.\n-   Except for the root, every [internal node](https://en.wikipedia.org/wiki/Tree_(data_structure)#Terminology) has at least two children.\n-   Each edge is labelled with a non-empty substring of S.\n-   No two edges starting out of a node can have string-labels beginning with the same character.\n-   The string obtained by concatenating all the string-labels found on the path from the root to leaf i spells out suffix S[i..n], for i from 1 to n.\n\nSince such a tree does not exist for all strings, S is padded with a terminal symbol not seen in the string (usually denoted $). This ensures that no suffix is a prefix of another, and that there will be n leaf nodes, one for each of the n suffixes of S. Since all internal non-root nodes are branching, there can be at most *n*âˆ’ 1 such nodes, and *n*+(*n*âˆ’1)+1=2*n* nodes in total (*n* leaves, *n*âˆ’1 internal non-root nodes, 1 root).\n\n**Generalized Suffix Tree -**\n\nA [generalized suffix tree](https://en.wikipedia.org/wiki/Generalized_suffix_tree) is a suffix tree made for a set of words instead of a single word. It represents all suffixes from this set of words. Each word must be terminated by a different termination symbol or word.\n**How to build a Suffix Tree for a given text?**\n\nAs discussed above, **Suffix Tree is compressed trie of all suffixes**, so following are very abstract steps to build a suffix tree from given text.\n\n1) Generate all suffixes of given text.\n\n2) Consider all suffixes as individual words and build a compressed trie.\n\nLet us consider an example text \"banana0\" where '0' is string termination character. Following are all suffixes of \"banana0\"\n\nbanana0\nanana0\nnana0\nana0\nna0\na0\n0\n\nIf we consider all of the above suffixes as individual words and build a trie, we get following.\n![](media/Suffix-Tree-image1.png)\nIf we join chains of single nodes, we get the following compressed trie, which is the Suffix Tree for given text \"banana0\"\n\n![Oeu Oeu eu eu e Oeueueq ](media/Suffix-Tree-image2.png)\n![Suffix tree Suffix tree. â€¢ Patricia trie of suffixes of a string. â€¢ Linear-time construction: beyond this course. suffix tree for BANANAS BANANAS s b s s s NAS ](media/Suffix-Tree-image3.png)\n# Suffix Tree and Its Construction\n\nSuffix Tree were first introduced to solve SubString Problem -\n\nThe substring problem: Pre-process text T so that the computation string matching problem is solved in time proportional to m, the length of pattern P.\nUse Case - Performing intensive queries on a big database, which is represented by T. Once the suï¬ƒx tree for T is built each query is proportional to O(m) time.\nProblems -\n\na.  SME - String Matching Existence problem - Given a pattern P and a text T, determine whether there is an occurance of P in T.\n\nb.  SMC - String Matching Computation problem - Given a pattern P and a text T, determine all the occurance of P in T\n# Advanced\n-   Ukkonen's Linear Time Algorithm\n    -   Implicit's Suffix Tree\n\nAn implicit suï¬ƒx tree for a string S is a tree obtained from T(S$) by performing the following operations:\n\n1. Remove all the terminal symbols $.\n\n2. From the resulting tree, remove all edges without label.\n\n3. Finally, from the resulting tree, remove all nodes that do not have at least two children.\n-   Suffix Links\n-   Extension Algorithm\n-   Edge Label Compression\n-   Phase Algorithm\n"},{"fields":{"slug":"/Data-Structures/Trie/Ternary-Search-Tries-(TST)/","title":"Ternary Search Tries (TST)"},"frontmatter":{"draft":false},"rawBody":"# Ternary Search Tries (TST)\n\nCreated: 2018-04-10 23:10:17 +0500\n\nModified: 2018-04-11 20:03:16 +0500\n\n---\n\nSolve the problem of using large amount of data by R-way tries.\nExactly 3 children.\nCreating associative Symbol table\n![Ternary search tries â€¢ Store characters and values in nodes (not keys). â€¢ Each node has 3 children: smaller (left), equal (middle), I Fast Algorithms for Sorting and Searching Strings Jon L. Bentley* Abstract We present theoretical algorithms for sorting and searching multikey data, and derive from them practical C implementations for applications in which keys are charac- ter strings. The sorting algorithm blends Quicksort and radix sort; it is competitive with the best known C sort codes. The searching algorithm blends tries and binary search trees; it is faster than hashing and other commonly used search methods. The basic ideas behind the algo- Robert Sedgewick# that is competitive with the most efficient string sorting programs kiown. The second program is a symbol table implementation that is faster than hashing. which is com- monly regarded as the fastest symbol table implementa- tion. The symbol table implementation is much more space-efficient than multiway trees, and supports more advanced searches. In many application programs, sorts use a Quicksort implementation based on an abstract compare operation, ](media/Ternary-Search-Tries-(TST)-image1.png)\n\n![Ternary search tries â€¢ Store characters and values in nodes (not keys). â€¢ Each node has 3 children: smaller (left), equal (middle), I link to TSTfor all keys that start with a letter before s link to thC1i e 12 14 s e 10 roe 8 e 12 each node has three links 14 s 11 ](media/Ternary-Search-Tries-(TST)-image2.png)\n\n![Search in a TST Follow links corresponding to each character in the key. â€¢ If less, take left link; if greater, take right link. â€¢ If equal, take the middle link and move to the next key c Search hit. Node where search ends has a non-null value. Search miss. Reach a null link or node where search ends ha get (\"sea\") match: take middle link, move to next char mismatch: take left or right link, do not move to next char a e b 12 14 s h e u a 0 O ](media/Ternary-Search-Tries-(TST)-image3.png)\n\n![Search hit in a TST get(\"sea\") return value associated ](media/Ternary-Search-Tries-(TST)-image4.png)\n\n![](media/Ternary-Search-Tries-(TST)-image5.png)\n\n![Ternary search trie construction demo ternary search trie ](media/Ternary-Search-Tries-(TST)-image6.png)\n\n![26-way trie vs. TST 26-way trie. 26 null links in each leaf. n c g n e a la i u g e c) e u w a ko oooeooooooooeoeeooooooooooooo( 26-way trie (1035 null links, not shown) TST. 3 null links in each leaf. O m o u oft o ](media/Ternary-Search-Tries-(TST)-image7.png)\n\n![TST representation in Java A T ST node is five fields: â€¢ A value. â€¢ A character c. â€¢ A reference to a left T ST. â€¢ A reference to a middle T ST. â€¢ A reference to a right T ST. standard array of links (R = 26) s private class Node private Value va private char c; private Node lef ternary search tree (T! link for keys s that start With s o ](media/Ternary-Search-Tries-(TST)-image8.png)\n\n![TST: Java implementation public class TST<Va1 private Node root; private class Node / * see previous slide * / public void put(String key, Value val) { root = put(root, key, van, 0); private Node put(Node x, String key, 1) x.mld char c = if else if else if else key. charAt (d) ; if (x null) { x = new Node() ; Val ue x. 1 eft x. right (d < key. length() val , int d) = put(x.left, = put(x.right, = put(x.mld = val ; key , key , key , x. val ](media/Ternary-Search-Tries-(TST)-image9.png)\n\n![TST: get (x. 1 eft , get(x. ri ght, Java implementation (continued) public bool ean contains(String key) { return get(key) != null; public Value get(String key) Node x = get(root, key, 0) ; if (x null) return null; return x. val ; private Node get(Node x, String key, = null) return null; key. charAt (d) ; int d) char c = if else if return return key , key, d) ](media/Ternary-Search-Tries-(TST)-image10.png)\n\n![String symbol table implementation cost summary character accesses (typical case) implementation red-black BST hashing (linear probing) R-way trie TST search hit L + cig2N search miss c lg2N log RN In N insert clg2N L space (references) 4 N to 16 N ](media/Ternary-Search-Tries-(TST)-image11.png)\nVery easy to speed up and saves a lot of data.\n![TST with R2 branching at root Hybrid of R-way trie and T ST. â€¢ Do IV-way branching at root. â€¢ Each of R2 root nodes points to a T ST. array of 262 roots aa TST ab TST ac TST zy TST ](media/Ternary-Search-Tries-(TST)-image12.png)\n\n![String symbol table implementation cost summary character accesses (typical case) implementation red-black BST hashing (linear probing) R-way trie TST TST with R2 search hit L + cig2N search miss c lg2N log RN insert clg2N L L + In N space (references) 4 N to 16 N ](media/Ternary-Search-Tries-(TST)-image13.png)\n\n![TST vs. hashing Hashing. â€¢ Need to examine entire key. â€¢ Search hits and misses cost about the same. â€¢ Performance relies on hash function. â€¢ Does not support ordered symbol table operations. TSTs. â€¢ Works only for strings (or digital keys). Only examines just enough key characters. Search miss may involve only a few characters. Supports ordered symbol table operations (plus others!). ](media/Ternary-Search-Tries-(TST)-image14.png)"},{"fields":{"slug":"/Databases/Concepts/ACID-and-BASE/","title":"ACID and BASE"},"frontmatter":{"draft":false},"rawBody":"# ACID and BASE\n\nCreated: 2018-04-10 00:20:26 +0500\n\nModified: 2020-12-14 23:50:05 +0500\n\n---\n\n1.  ACID (SQL)\n\n    a.  A - Atomic, Everything in a transaction succeeds or the entire transaction is rolled back\n\n    b.  C - Consistent - A transtion cannot leave the database in an inconsistent state.\n\n    c.  I - Isolated - Transaction cannot interfere with each other\n\n    d.  D - Durable - Completed Transaction persist, even when the server restarts.\n\n2.  BASE - Basically Available, Soft-state Eventual consistency (NoSQL)\n\nIn partitioned databases, trading some consistency for availability can lead to dramatic improvements in scalability. BASE is optimistic and accepts that the database consistency will be in a state of flux. Although this sounds impossible to cope with, in reality it is quite manageable and leads to levels of scalability that cannot be obtained with ACID.\n\ni.  **BA - Basic Availability:** This constraint states that the system does guarantee the availability of the data as regards CAP Theorem; there will be a response to any request. But, that response could still be 'failure' to obtain the requested data or the data may be in an inconsistent or changing state, much like waiting for a check to clear in your bank account.ii. **S - Soft State:** The state of the system could change over time, so even during times without input there may be changes going on due to 'eventual consistency,' thus the state of the system is always 'soft.'iii. **E - Eventual Consistency:** The system will*eventually*become consistent once it stops receiving input. The data will propagate to everywhere it should sooner or later, but the system will continue to receive input and is not checking the consistency of every transaction before it moves onto the next one.\n**BASE in terms of NoSQL Databases**\n\n**Basic Availability.** The NoSQL database approach focuses on the availability of data even in the presence of multiple failures. It achieves this by using a highly distributed approach to database management. Instead of maintaining a single large data store and focusing on the fault tolerance of that store, NoSQL databases spread data across many storage systems with a high degree of replication. In the unlikely event that a failure disrupts access to a segment of data, this does not necessarily result in a complete database outage.\n**Soft State.** BASE databases abandon the consistency requirements of the ACID model pretty much completely. One of the basic concepts behind BASE is that data consistency is the developer's problem and should not be handled by the database.\n**Eventual Consistency.** The only requirement that NoSQL databases have regarding consistency is to require that at some point in the future, data will converge to a consistent state. No guarantees are made, however, about when this will occur. That is a complete departure from the immediate consistency requirement of ACID that prohibits a transaction from executing until the prior transaction has completed and the database has converged to a consistent state.\n**References**\n\nBASE - <https://queue.acm.org/detail.cfm?id=1394128>\n"},{"fields":{"slug":"/Databases/Concepts/Concepts/","title":"Concepts"},"frontmatter":{"draft":false},"rawBody":"# Concepts\n\nCreated: 2019-03-21 19:51:38 +0500\n\nModified: 2021-12-31 13:16:14 +0500\n\n---\n\nCRUD - Create, Read, Update, Delete\n\nINSERT, SELECT, UPDATE, DELETE\n**Idempotent - UUID**\n\nTry to make all statements Idempotent\n**Transaction Log**\n\nIn the field of[databases](https://en.wikipedia.org/wiki/Database)in[computer science](https://en.wikipedia.org/wiki/Computer_science), atransaction log(alsotransaction journal,database log,binary logoraudit trail) is a history of actions executed by a[database management system](https://en.wikipedia.org/wiki/Database_management_system)used to guarantee[ACID](https://en.wikipedia.org/wiki/ACID)properties over[crashes](https://en.wikipedia.org/wiki/Crash_(computing))or hardware failures. Physically, a log is a[file](https://en.wikipedia.org/wiki/Computer_file)listing changes to the database, stored in a stable storage format.\nIf, after a start, the database is found in an[inconsistent](https://en.wikipedia.org/wiki/Consistency_(database_systems))state or not been shut down properly, the database management system reviews the database logs for[uncommitted](https://en.wikipedia.org/wiki/Commit_(data_management))transactions and[rolls back](https://en.wikipedia.org/wiki/Rollback_(data_management))the changes made by these[transactions](https://en.wikipedia.org/wiki/Database_transaction). Additionally, all transactions that are already committed but whose changes were not yet materialized in the database are re-applied. Both are done to ensure [atomicity](https://en.wikipedia.org/wiki/Atomicity_(database_systems)) and[durability](https://en.wikipedia.org/wiki/Durability_(computer_science))of transactions.\n**Types of database log records**\n\n1.  Update log record\n\n2.  Compensation log record\n\n3.  Commit record\n\n4.  Abort record\n\n5.  Checkpoint record\n\n6.  Completion record\n<https://en.wikipedia.org/wiki/Transaction_log>\n\n## Prepared Statement**\n\nIn[database management systems](https://en.wikipedia.org/wiki/Database_management_system)(DBMS), aprepared statementorparameterized statementis a feature used to execute the same or similar database statements repeatedly with high efficiency. Typically used with [SQL](https://en.wikipedia.org/wiki/SQL) statements such as queries or updates, the prepared statement takes the form of a[template](https://en.wikipedia.org/wiki/Template_processor)into which certain constant values are substituted during each execution.\nThe typical workflow of using a prepared statement is as follows:\n\na.  **Prepare:** At first, the application creates the statement template and send it to the DBMS. Certain values are left unspecified, calledparameters,placeholdersorbind variables(labelled \"?\" below):\n\nINSERTINTOproducts(name,price)VALUES(?,?);\n\nb.  Then, the DBMS compiles (parses,[optimizes](https://en.wikipedia.org/wiki/Query_optimization)and translates) the statement template, and stores the result without executing it.\n\nc.  **Execute:** At a later time, the application supplies (orbinds) values for the parameters of the statement template, and the DBMS executes the statement (possibly returning a result). The application may execute the statement as many times as it wants with different values. In the above example, it might supply \"bike\" for the first parameter and \"10900\" for the second parameter.\nAs compared to executing statements directly, prepared statements offer two main advantages:\n-   The overhead of compiling the statement is incurred only once, although the statement is executed multiple times. However not all optimization can be performed at the time the statement template is compiled, for two reasons: the best plan may depend on the specific values of the parameters, and the best plan may change as tables and indexes change over time.\n-   Prepared statements are resilient against[SQL injection](https://en.wikipedia.org/wiki/SQL_injection)because values which are transmitted later using a different protocol are not compiled like the statement template. If the statement template is not derived from external input, SQL injection cannot occur.\nOn the other hand, if a query is executed only once, server-side prepared statements can be slower because of the additional round-trip to the server.Implementation limitations may also lead to performance penalties; for example, some versions of MySQL did not cache results of prepared queries. A[stored procedure](https://en.wikipedia.org/wiki/Stored_procedure), which is also precompiled and stored on the server for later execution, has similar advantages. Unlike a stored procedure, a prepared statement is not normally written in a procedural language and cannot use or modify variables or use control flow structures, relying instead on the declarative database query language. Due to their simplicity and client-side emulation, prepared statements are more portable across vendors.\n<https://en.wikipedia.org/wiki/Prepared_statement>\n\n## Cursor**\n\nCursoris a Temporary Memory or Temporary Work Station. It is Allocated by Database Server at the Time of Performing DML operations on Table by User. Cursors are used to store Database Tables.\nThere are 2 types of Cursors:\n\n1.  **Implicit Cursors**\n\nImplicit Cursors are also known as Default Cursors of SQL SERVER. These Cursors are allocated by SQL SERVER when the user performs DML operations.\n\n2.  **Explicit Cursors**\n\nExplicit Cursors are Created by Users whenever the user requires them. Explicit Cursors are used for Fetching data from Table in Row-By-Row Manner.\n<https://www.geeksforgeeks.org/what-is-cursor-in-sql>\n\n[**How Database Cursors Help in Fetching Large Result sets from your SQL**](https://www.youtube.com/watch?v=C1Y6P6vDFts)\n\n**Pros**\n-   Save on memory on client side\n-   Streaming to websocket/grpc\n-   Easily cancel the query\n-   Paging\n**Cons**\n-   Long running transaction (cannot share cursors)\n-   Stateful\n**LIMIT and OFFSET**\n\nLIMITandOFFSETallow you to retrieve just a portion of the rows that are generated by the rest of the query\n<https://www.postgresql.org/docs/8.0/queries-limit.html>\n[don't use \"offset\" in your SQL](https://youtu.be/WDJRRNCGIRs)\n-   select * from communication_log limit 100 offset 1000000;\n-   offset means fetch and drop the first X number of rows\n-   fetch 110 rows and drop first 100\n-   as the offset increases the database need to do more work, which means offset is very very expensive\n-   You can accidently read duplicate rows, if someone inserted new row after reading 1st page\n\n![select title from news offset 110 neu row Inserted read twice new row row i i row 112 row 121 tow 122 ](media/Concepts-image1.jpeg)\n-   select * from communication_log where id > 140211644 limit 100;\n-   Change offset to where query to filter using id, using index scan\n-   A good alternative for using OFFSET will be the Seek Method\n-   The seek method is all about finding a unique column or set of columns that identifies each row. Then, instead of using the OFFSET clause, we can just use that unique value as a bookmark that presents the position of the last row we've fetched and query the next set of rows by starting from this position in the WHERE clause.\n<https://www.eversql.com/faster-pagination-in-mysql-why-order-by-with-limit-and-offset-is-slow>\n\n## Cursor Pagination vs Offset Pagination**\n\n**Pros**\n-   Superior Real-Time Data Capabilities\n-   No Skipped Data\n-   No Duplicated Data\n-   Handling Big Data Sets Efficiently\n**Cons**\n-   Limited Sort Features\n-   Cursors Can Be a Bit Trickier to Implement\n-   Infinite Scroll Can Be Addicting\n-   The cursor-based pagination presents an opaque \"cursor\" indicator that the client may use to page through the result set. This pagination style only presents forward and reverse controls, and does not allow the client to navigate to arbitrary positions.\n<https://uxdesign.cc/why-facebook-says-cursor-pagination-is-the-greatest-d6b98d86b6c0>\n\n<https://medium.com/swlh/how-to-implement-cursor-pagination-like-a-pro-513140b65f32>\n\n<https://medium.com/swlh/how-to-implement-cursor-pagination-like-a-pro-513140b65f32>\n\n<https://www.django-rest-framework.org/api-guide/pagination/#cursorpagination>\n\n<https://slack.engineering/evolving-api-pagination-at-slack>\n\n<https://betterprogramming.pub/building-apis-a-comparison-between-cursor-and-offset-pagination-88261e3885f8>\n\n"},{"fields":{"slug":"/Databases/Concepts/Concurrency-Control/","title":"Concurrency Control"},"frontmatter":{"draft":false},"rawBody":"# Concurrency Control\n\nCreated: 2019-11-04 00:04:37 +0500\n\nModified: 2021-07-27 23:34:15 +0500\n\n---\n\nThe protocol to allow transactions to access a database in a multi-programmed fashion while preserving the illusion that each of them is executing alone on a dedicated system\n-   Provides Atomicity + Isolation in ACID\n-   The goal is to have the effect of a group of transactions on the database's state is equivalent to any serial execution of all transactions.\nComputer systems, both[software](https://en.wikipedia.org/wiki/Software)and[hardware](https://en.wikipedia.org/wiki/Computer_hardware), consist of modules, or components. Each component is designed to operate correctly, i.e., to obey or to meet certain consistency rules. When components that operate concurrently interact by messaging or by sharing accessed data (in [memory](https://en.wikipedia.org/wiki/Computer_memory) or [storage](https://en.wikipedia.org/wiki/Computer_data_storage)), a certain component's consistency may be violated by another component. The general area of concurrency control provides rules, methods, design methodologies, and[theories](https://en.wikipedia.org/wiki/Scientific_theory)to maintain the consistency of components operating concurrently while interacting, and thus the consistency and correctness of the whole system. Introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction. Operation consistency and correctness should be achieved with as good as possible efficiency, without reducing performance below reasonable levels. Concurrency control can require significant additional complexity and overhead in a[concurrent algorithm](https://en.wikipedia.org/wiki/Concurrent_algorithm)compared to the simpler[sequential algorithm](https://en.wikipedia.org/wiki/Sequential_algorithm).\nFor example, a failure in concurrency control can result in[data corruption](https://en.wikipedia.org/wiki/Data_corruption)from[torn read or write operations](https://en.wikipedia.org/w/index.php?title=Torn_data-access_operation&action=edit&redlink=1).\n**MySQL Concurreny Control**\n-   Lock based concurrency control\n-   Multi version concurrency control (MVCC)\n# Concurrency control mechanisms\n\n**Optimistic vs Pessimistic Locking**\n\na.  Pessimistic Locking\n\nAcquire all the locks beforehand and then commit our transaction\n\nb.  Optimistic Locking\n\nWe do not acquire any locks on data and when commiting a transaction we check to see if any other transaction updated the record we are working on.\n**Optimistic vs Pessimistic Concurrency Control**\n\na.  Pessimistic concurrency control\n\nWidely used by relational databases, this approach assumes that conflicting changes are likely to happen and so blocks access to a resource in order to prevent conflicts. A typical example is locking a row before reading its data, ensuring that only the thread that placed the lock is able to make changes to the data in that row.\n\nb.  Optimistic concurrency control\n\nUsed by Elasticsearch,this approach assumes that conflicts are unlikely to happen and doesn't block operations from being attempted. However, if the underlying data has been modified between reading and writing, the update will fail. It is then up to the application to decide how it should resolve the conflict. For instance, it could reattempt the update, using the fresh data, or it could report the situation to the user.\n**Categories**\n-   **[Optimistic](https://en.wikipedia.org/wiki/Optimistic_concurrency_control)-** Delay the checking of whether a transaction meets the isolation and other integrity rules (e.g.,[serializability](https://en.wikipedia.org/wiki/Serializability)and[recoverability](https://en.wikipedia.org/wiki/Serializability#Correctness_-_recoverability)) until its end, without blocking any of its (read, write) operations (\"...and be optimistic about the rules being met...\"), and then abort a transaction to prevent the violation, if the desired rules are to be violated upon its commit. An aborted transaction is immediately restarted and re-executed, which incurs an obvious overhead (versus executing it to the end only once). If not too many transactions are aborted, then being optimistic is usually a good strategy.\n-   **Pessimistic-** Block an operation of a transaction, if it may cause violation of the rules, until the possibility of violation disappears. Blocking operations is typically involved with performance reduction.\n-   **Semi-optimistic-** Block operations in some situations, if they may cause violation of some rules, and do not block in other situations while delaying rules checking (if needed) to transaction's end, as done with optimistic.\nDifferent categories provide different performance, i.e., different average transaction completion rates (throughput), depending on transaction types mix, computing level of parallelism, and other factors. If selection and knowledge about trade-offs are available, then category and method should be chosen to provide the highest performance.\nThe mutual blocking between two transactions (where each one blocks the other or more) results in a[deadlock](https://en.wikipedia.org/wiki/Deadlock), where the transactions involved are stalled and cannot reach completion. Most non-optimistic mechanisms (with blocking) are prone to deadlocks which are resolved by an intentional abort of a stalled transaction (which releases the other transactions in that deadlock), and its immediate restart and re-execution. The likelihood of a deadlock is typically low.\nBlocking, deadlocks, and aborts all result in performance reduction, and hence the trade-offs between the categories.\n**Methods**\n\nMany methods for concurrency control exist. Most of them can be implemented within either main category above. The major methods,which have each many variants, and in some cases may overlap or be combined, are:\n\n1.  **Locking (e.g.,[Two-phase locking](https://en.wikipedia.org/wiki/Two-phase_locking)- 2PL)**\n    -   Controlling access to data by[locks](https://en.wikipedia.org/wiki/Lock_(computer_science))assigned to the data. Access of a transaction to a data item (database object) locked by another transaction may be blocked (depending on lock type and access operation type) until lock release.\n    -   **Assume transactions will conflict so they must acquire locks on database objects before they are allowed to access them.**\n    -   There are two ways to deal with deadlocks in a two-phase locking (2PL) concurrency control protocol\n        -   **Deadlock Detection**\n            -   Each txn maintains a queue of the txns that hold the locks that it is waiting for\n            -   A separate thread checks these queues for deadlocks\n            -   If deadlock is found, use a heuristic to decide what transaction to kill in order to break deadlock.\n        -   **Deadlock Prevention**\n            -   Check whether another txn already holds a lock when another txn requests it\n            -   If lock is not available, the txn will either\n                -   wait\n                -   commit suicide\n                -   kill the other txn\n![Txn #1 READ(A) LOCX(B) LNLOCK(A) MUTE(B) Growing Phase Shrinking Phase ](media/Concurrency-Control-image1.png)\n2.  **Serialization[graph checking](https://en.wikipedia.org/wiki/Serializability#Testing_conflict_serializability)**(also called Serializability, or Conflict, or Precedence graph checking)\n\nChecking for[cycles](https://en.wikipedia.org/wiki/Cycle_(graph_theory))in the schedule's[graph](https://en.wikipedia.org/wiki/Directed_graph)and breaking them by aborts.\n3.  **[Timestamp ordering](https://en.wikipedia.org/wiki/Timestamp-based_concurrency_control)(TO)**\n    -   Assigning timestamps to transactions, and controlling or checking access to data by timestamp order.\n    -   **Assume that conflicts are rare so transactions do not need to first acquire locks on database objects and instead check for conflicts at commit time.**\n4.  **[Commitment ordering](https://en.wikipedia.org/wiki/Commitment_ordering)(or Commit ordering; CO)**\n\nControlling or checking transactions' chronological order of commit events to be compatible with their respective[precedence order](https://en.wikipedia.org/wiki/Serializability#Testing_conflict_serializability).\nOther major concurrency control types that are utilized in conjunction with the methods above include:\n-   **[Multiversion concurrency control](https://en.wikipedia.org/wiki/Multiversion_concurrency_control)(MVCC) -** Increasing concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object) depending on scheduling method.\n-   **[Index concurrency control](https://en.wikipedia.org/wiki/Index_locking)-** Synchronizing access operations to[indexes](https://en.wikipedia.org/wiki/Index_(database)), rather than to user data. Specialized methods provide substantial performance gains.\n-   **Private workspace model(Deferred update) -** Each transaction maintains a private workspace for its accessed data, and its changed data become visible outside the transaction only upon its commit (e.g.,[Weikum and Vossen 2001](https://en.wikipedia.org/wiki/Concurrency_control#Weikum01)). This model provides a different concurrency control behavior with benefits in many cases.\nThe most common mechanism type in database systems since their early days in the 1970s has been[Strong strict Two-phase locking](https://en.wikipedia.org/wiki/Two-phase_locking)(SS2PL; also calledRigorous schedulingorRigorous 2PL) which is a special case (variant) of both[Two-phase locking](https://en.wikipedia.org/wiki/Two-phase_locking)(2PL) and[Commitment ordering](https://en.wikipedia.org/wiki/Commitment_ordering)(CO). It is pessimistic. In spite of its long name (for historical reasons) the idea of the SS2PL mechanism is simple: \"Release all locks applied by a transaction only after the transaction has ended.\" SS2PL (or Rigorousness) is also the name of the set of all schedules that can be generated by this mechanism, i.e., these are SS2PL (or Rigorous) schedules, have the SS2PL (or Rigorousness) property.\n<https://en.wikipedia.org/wiki/Concurrency_control>\n\n<https://www.geeksforgeeks.org/lock-based-concurrency-control-protocol-in-dbms>\n\n<https://en.wikipedia.org/wiki/Two-phase_locking>\n\n<https://www.geeksforgeeks.org/two-phase-locking-protocol>\n\n<https://vladmihalcea.com/database-deadlock>\n\n## Timestamp Ordering Concurrency Control**\n\nUse timestamps to determine the order of transactions.\n**Basic T/O Protocol**\n-   Every transaction is assigned a unique timestamp when they arrive in the system.\n-   The DBMS maintains separate timestamps in each tuple's header of the last transaction that read that tuple or wrote to it.\n-   Each transaction check for conflicts on each read/write by comparing their timestamp with the timestamp of the tuple they are accessing.\n-   The DBMS needs copy a tuple into the transaction's private workspace when reading a tuple to ensure repeatable reads.\n**Optimistic Concurrency Control (OCC)**\n\nStore all changes in private workspace. Check for conflicts at commit time and then merge.\nThe protocol puts transactions through three phases during its execution:\n\n1.  **Read Phase**\n\nTransaction's copy tuples accessed to private work space to ensure repeatable reads, and keep track of read/write sets.\n2.  **Validation Phase**\n\nWhen the transaction invokes COMMIT, the DBMS checks if it conflicts with other transactions. Parallel validation means that each transaction must check the read/write set of other transactions that are trying to validate at the same time. Each transaction has to acquire locks for its write set records in some global order. Original OCC uses serial validation.\nThe DBMS can proceed with the validation in two directions:\n-   Backward Validation: Check whether the committing transaction intersects its read/write sets with those of any transactions that have already committed.\n-   Forward Validation: Check whether the committing transaction intersects its read/write sets with any active transactions that have not yet committed.\n3.  **Write Phase**\n\nThe DBMS propagates the changes in the transactions write set to the database and makes them visible to other transactions' items. As each record is updated, the transaction releases the lock acquired during the Validation Phase\n**Timestamp Allocation**\n\nThere are different ways for the DBMS to allocate timestamps for transactions. Each have their own performance trade-offs.\n-   **Mutex:** This is the worst option. Mutexes are always a terrible idea.\n-   **Atomic Addition:** Use compare-and-swap to increment a single global counter. Requires cache invalidation on write.\n-   **Batched Atomic Addition:** Use compare-and-swap to increment a single global counter in batches. Needs a back-off mechanism to prevent fast burn.\n-   **Hardware Clock:** The CPU maintains an internal clock (not wall clock) that is synchronized across all cores. Intel only. Not sure if it will exist in future CPUs.\n-   **Hardware Counter:** Single global counter maintained in hardware. Not implemented in any existing CPUs.\n**Observation**\n-   When there is low contention, optimistic protocols perform better because the DBMS spends less time checking for conflicts\n-   At high contention, the both classes of protocols degenerate to essentially the same serial execution\n**Benchmarks**\n\n1.  **Read-only workloads**\n\n![14 12 10 0-0 DL DETECT NO WAIT WAIT DIE 200 TIMESTAMP o. o MVCC occ 400 600 800 1000 Number of Cores ](media/Concurrency-Control-image2.png)\n\n2.  **Write-intensive / Medium-contention**\n\n![4.5 4.0 4-1 3.5 e 3.0 2.5 2.0 1.5 1.0 0.5 0.0 DL DETECT NO WAIT WAIT DIE 200 0-0 TIMESTAMP MVCC occ 400 600 800 1000 Number of Cores ](media/Concurrency-Control-image3.png)\n\n3.  **Write-intensive / High-contention**\n\n![0-0 O 0.25 0.20 0.15 0.10 0.05 0.00 DL DETECT NO WAIT WAIT DIE TIMESTAMP o. o MVCC occ 200 400 600 800 1000 Number of Cores ](media/Concurrency-Control-image4.png)\n**Performance Bottlenecks**\n\nAll concurrency control protocols have performance and scalability problems when there are a large number of concurrent threads and large amount of contention (i.e., the transactions are all trying to read/write to the same set of tuples).-   **Lock Thrashing:**\n    -   DL_DETECT, WAIT_DIE\n    -   Each transaction waits longer to acquire locks, causing other transaction to wait longer to acquire locks.\n    -   Can measure this phenomenon by removing deadlock detection/prevention overhead.\n        -   Force txns to acquire locks in primary key order\n        -   Deadlocks are not possible\n\n![101 100 Q 10 100 theta theta theta -0.6 -0.8 101 102 Number of Cores 103 ](media/Concurrency-Control-image5.png)-   **Timestamp Allocation**\n    -   All T/O algorithms + WAIT_DIE\n    -   Mutex (Worst option)\n    -   Atomic Addition (Requires cache invaliadtion on write)\n    -   Batched Atomic Addition (Needs a back-off mechanism to prevent fast burn)\n    -   Hardware Clock (Not sure if it will exist in future CPUs)\n    -   Hardware Counter (Not implemented in existing CPUs)\n\n![O 10000 1000 100 10 1 o 1 Clock Hardware Atomic batch=16 Atomic batch=8 Atomic Mutex 10 100 1000 Number of Cores ](media/Concurrency-Control-image6.png)-   **Memory Allocation**\n    -   OCC + MVCC\n    -   Copying data on every read/write access slows down the DBMS because of contention on the memory controller.\n        -   In-place updates and non-copying reads are not affected as much\n    -   Default libc malloc is slow. Never use it\n**Operational Transformation**\n\nOperational transformation(OT) is a technology for supporting a range of collaboration functionalities in advanced[collaborative software](https://www.wikiwand.com/en/Collaborative_software)systems. OT was originally invented for consistency maintenance and [concurrency control](https://www.wikiwand.com/en/Concurrency_control) in collaborative editing of plain text documents. Its capabilities have been extended and its applications expanded to include group undo, locking, conflict resolution, operation notification and compression, group-awareness, HTML/XML and tree-structured document editing, collaborative office productivity tools, application-sharing, and collaborative computer-aided media design tools.In 2009 OT was adopted as a core technique behind the collaboration features in[Apache Wave](https://www.wikiwand.com/en/Apache_Wave)and[Google Docs](https://www.wikiwand.com/en/Google_Docs).\n**System architecture**\n\nCollaboration systems utilizing Operational Transformations typically use replicated document storage, where each client has their own copy of the document; clients operate on their local copies in a[lock-free](https://www.wikiwand.com/en/Lock_(computer_science)),[non-blocking](https://www.wikiwand.com/en/Blocking_(computing))manner, and the changes are then propagated to the rest of the clients; this ensures the client high responsiveness in an otherwise high-latency environment such as the Internet. When a client receives the changes propagated from another client, it typically transforms the changes before executing them; the transformation ensures that application-dependent consistency criteria ([invariants](https://www.wikiwand.com/en/Invariant_(computer_science))) are maintained by all sites. This mode of operation results in a system particularly suited for implementing collaboration features, like simultaneous document editing, in a high-latency environment such as[the web](https://www.wikiwand.com/en/Www).\n**Consistency models**\n-   The CC model\n-   The CCI model\n-   The CSM model\n-   The CA model\nOperational Transformation (OT) is an algorithm/technique for the transformation of operations such that they can be applied to documents whose states have diverged, bringing them both back to the same state.\n**How does Operational Transformation work?**\n\nA short overview of how OT works :\n-   Every change (insertion or deletion) is represented as anoperation. An operation can be applied to the current document which results into a new document state.\n-   To handle concurrent operations, we use thetranformfunction that takes two operations that have been applied to the same document state (but on different clients) and computes a new operation that can be applied after the second operation and that preserves the first operation's intended change\n<https://www.wikiwand.com/en/Operational_transformation>\n\n<https://medium.com/coinmonks/operational-transformations-as-an-algorithm-for-automatic-conflict-resolution-3bf8920ea447>\n\n<https://operational-transformation.github.io>\n\n## Differential Synchronization -** <https://neil.fraser.name/writing/sync>\n\nKeeping two or more copies of the same document synchronized with each other in real-time is a complex challenge. **Differential synchronization** offers scalability, fault-tolerance, and responsive collaborative editing across an unreliable network.\n"},{"fields":{"slug":"/Databases/Concepts/Disk-oriented-vs-in-memory-DBs/","title":"Disk oriented vs in-memory DBs"},"frontmatter":{"draft":false},"rawBody":"# Disk oriented vs in-memory DBs\n\nCreated: 2019-12-24 16:00:43 +0500\n\nModified: 2020-04-19 19:36:21 +0500\n\n---\n\n**Background**\n\nThe history of DBMSs development is about dealing with the limitations of hardware. The first DBMSs in the 1970s were designed in environment with the following characteristics:\n-   Uniprocessor (single core CPU)\n-   RAM was severely limited\n-   Database had to be stored on disk\n-   Disk was slow.\nBut now DRAM capacities are large enough that most structured databases will entirely fit in memory. This merits us to rethink all aspects of the DBMS to account for this.\n**Disk-Oriented Database Management Systems**\n\nFor a disk oriented DBMS, the system architecture is predicated on the assumption that data is stored in non-volatile memory. This means that the DBMS may have to read data from disk during query execution.In a disk-based system, only approximately 7% of instructions are done on actual work. The majority of the DBMS's instructions time are in managing three of its key components:\n-   buffer pool\n-   concurrency control\n-   logging/recovery\n![DISK-ORIENTED DBMS OVERHEAD Measured CPU Instructions 16% 12% 16% 34% O BUFFER POOL a LATCHING O LOCKING LOGGING O B-TREE KEYS O REAL WORK H THE LOOKING GLASS, FOUND THERE ](media/Disk-oriented-vs-in-memory-DBs-image1.png)\n**Buffer Pool**\n\nThe DBMS organizes the database as a set of fixed-length blocks called **slotted pages**. The system uses an in-memory (volatile) buffer pool to cache the blocks cached from disk.\n-   When a query accesses a page, the DBMS checks to see if that page is already in memory.\n    -   If it's not, then the DBMS must retrieve it from disk and copy it into a **frame** in its buffer pool\n    -   If there are no free frames, then find a page to evict\n    -   If the page being evicted is dirty, then the DBMS must write it back to disk\n-   Once the page is in memory, the DBMS translates any on-disk addresses to their in-memory addresses.\n-   Every tuple access has to go through the buffer pool manager regardless of whether that data will always be in memory.\n    -   Always translate a tuple's record id to its memory location\n    -   Worker thread must pin pages that it needs to make sure that they are not swapped to disk\n**Concurrency Control**\n\nIn a disk oriented DBMS, the system assumes that a transaction could stall at any time when it tries to access data that is not in memory. The system's concurrency control protocol allows the DBMS to execute other transactions at the same time to improve performance while still preserving atomicity and isolation guarantees.\n-   Set locks to provide ACID guarantees for transactions.\n-   Locks are stored in a separate data structure to avoid being swapped to disk.\n**Logging and Recovery**\n\nMost DBMS use STEAL+NO-FORCE buffer pool policies, so all modifications have to be flushed to the WAL before a transaction can commit. Each log entry contain before and after image of record modified. Lots of work to keep track of LSNs all throughout the DBMS.\n![DISK-ORIENTED DATA ORGANIZATION Index Pageld+ Slot # CARNEGIE MELLON Buffer Pool page6 page4 Database (On-Disk) pageO page 1 page2 Page Tab ](media/Disk-oriented-vs-in-memory-DBs-image2.png)\n**Why not MMaps?**\n\nMemory-map (mmap) a database file into DRAM and let the OS be in charge of swapping data in and out as needed.\n\nUse madvise and msync to give hints to the OS about what data is safe to flush.\nNotable mmap DBMSs\n-   MongoDB(pre WiredTiger)\n-   MonetDB\n-   LMDB\n-   MemSQL\nUsing mmap gives up fine-grained control on the contents of memory.\n-   Cannot perform non-blocking memory access.\n-   The \"on-disk\" representation has to be the same as the \"in-memory\" representation.\n-   The DBMS has no way of knowing what pages are in memory or not.\n-   Various mmap-related syscallsare not portable.\n**In-Memory Database Management Systems**\n\nThe system architecture assumes that the primary storage location of the database is in memory. This means that the DBMS does not need to perform extra steps during execution to handle the case where it has to retrieve data from disk. If disk I/O is no longer the slowest resource, much of the DBMS architecture will have to change to account for other bottlenecks:\n-   Locking/latching\n-   Cache-line misses\n-   Pointer chasing\n-   Predicate evaluation\n-   Data movement and copying\n-   Networking (between application and DBMS)\n**Data Organization**\n\nAn in-memory DBMS splits the data for tuples into fixed-length and variable-length pools. Indexes use pointers to the fixed-length data for each tuple. These tuples then have 64-bit pointers to any variable-length values stored in a separate memory location.\n-   Direct memory pointers vs record ids\n-   Fixed-length vs variable-length data pools\n-   Use checksums to detct software errors from trashing the database\n![IN-MEMORY DATA ORGANIZATION Index Block Id+ Offset Fixed-Length Data Blocks Variable-Length Data Blocks ](media/Disk-oriented-vs-in-memory-DBs-image3.png)\n**Concurrency Control**\n\nIn-memory DBMSs still use either a pessimistic or optimistic concurrency control schemes to interleave transactions. They will use modern variants of these algorithms that are designed for in-memory data stor-age. The new bottleneck is contention caused from transactions trying to access data at the same time.\nOne key difference is that an in-memory DBMS can store locking information about each tuple together with its data. This is because the cost of a transaction acquiring a lock is the same as accessing data. Contrast this with disk-oriented DBMSs where locks are physically stored separate from their tuples because the tuples may get swapped out to disk.\n-   This helps with CPU cache locality\n-   Mutexes are too slow. Need to use compare-and-swap (CAS) instructions.\n**Indexes**\n\nLike with concurrency control schemes, in-memory DBMSs will use data structures for their indexes that are optimized for fast, in-memory access.\nIn-memory DBMSs will not log index updates. Instead, the system will rebuild the indexes upon restart when it loads the database back into memory. This avoids the runtime overhead of logging updates toindexes during transaction execution.\n**Query Processing**\n\nThe best strategy for executing a query plan in a DBMS changes when all the data is already in memory.\n-   Sequential scans are no longer significantly faster than random access.\nThe traditional tuple-at-a-time iterator model is too slow because of function calls.\n-   This problem is more significant in OLAP DBMSs\n**Logging and Recovery**\n\nThe DBMS still needs WAL on non-volatile storage since the system could halt at anytime.\n-   Use group commit to batch log entries and flush them together to amortize fsync cost\n-   In many cases, however, it may be possible to use more lightweight logging schemes (e.g., only store redo information). For example, since there are no \"dirty pages\", the DBMS does not need to maintain LSNs throughout the systems.\n-   In-memory DBMSs still takes checkpoints to reduce the amount of log that the system has to replay during recovery.\n-   Different methods for checkpointing:\n    -   Maintain a second copy of the database in memory that is updated by replaying the WAL.\n    -   Switch to a special \"copy-on-write\" mode and then write a dump of the database to disk.\n    -   Fork the DBMS process and then have the child process write its contents to disk.\n**Bottlenecks**\n\nIf I/O is no longer the slowest resource, much of the DBMS's architecture will have to change account for other bottlenecks:\n-   Locking/latching\n-   Cache-line misses\n-   Pointer chasing\n-   Predicate evaluations\n-   Data movement & copying\n-   Networking (between application & DBMS)\n**Larger than memory databases**\n\nDRAM is fast, but data is not accessed with the same frequency and in the same manner.\n-   Hot Data: OLTP Operations\n-   Cold Data: OLAP Queries\nWe will study techniques for how to bring back disk-resident data without slowing down the entire system.\n**Notable Early In-memory DBMSs**\n-   **TimesTen**: Originally Smallbase from HP Labs. Multi-process, shared memory DBMS. Bought by Oracle in 2005.\n-   **Dali:** Multi-process shared memory storage manager using memory mapped files [? ].\n-   **P*TIME:** Korean in-memory DBMS from the 2000s. Lots of interesting features (e.g., hybrid storage layouts, support for larger-than-memory databases). Sold to SAP in 2005 and is now part of HANA.\n<https://www.youtube.com/watch?v=m72mt4VN9ik>\n"},{"fields":{"slug":"/Databases/Concepts/History/","title":"History"},"frontmatter":{"draft":false},"rawBody":"# History\n\nCreated: 2020-04-19 16:40:35 +0500\n\nModified: 2020-08-18 23:26:22 +0500\n\n---\n\n![39 1980s - RELATIONAL MODEL The relational model wins. ---+ IBM comes out with DB2 in 1983. -9 \"SEQUEL\" becomes the standard (SQL). Many new \"enterprise\" DBMSs but Oracle wins marketplace. IBM DB2 ORACLE , \"TANDEM Informix __TERADA m\" ASE InterBase INGRE Stonebraker creates Postgres. Â±CMU.DB 15-721 (spring 2020) ](media/History-image1.png)\n\n![Â±CMU.DB 1980s - OBJECT-ORIENTED DATABASES Avoid \"relational-object impedance mismatch\" by tightly coupling objects and database. Few of these original DBMSs from the 1980s s exist today but many of the technologies exi in other forms (JSON, XML) ObjectStore. VERSANT 15-721 (spring 2020) ](media/History-image2.png)\n\n![41 OBJECT-ORIENTED MODEL Application Code class Student { int id; String name; String email; String phone[]; Relational Schema STUDENT (id, name, email) STUDENT PHONE (Sid, phone) id 1001 Â±CMU.DB name m.O.p. email ante@up.com Sid leel leel phone 444-444-4444 555-555-5555 15-721 (spring 2020) ](media/History-image3.png)\n\n![41 OBJECT-ORIENTED MODEL Application Code class Student { int id; String name; String email; String phone[]; Â±CMU.DB Student \"id\": leÃ¸l, \"name\": \"M.O.P.\" \"email\": \"ante@u 4444\" , \"555-5 -5555\" 15-721 (spring 2020) ](media/History-image4.png)\n\n![Â±CMU.DB 41 OBJECT-ORIENTED MODEL A Complex Queries \"email\": \"ante@up.com\", A No Standard API 15-721 (spring 2020) ](media/History-image5.png)\n\n![42 1990s - BORING DAYS No major advancements in database systems or application workloads. -9 Microsoft forks Sybase and creates SQL Server. ---+ MySQL is written as a replacement for mSQL. ---+ Postgres gets SQL support. -9 SQLite started in early 2000. QLite Â±CMU.DB PostgreSQL 15-721 (spring 2020) ](media/History-image6.png)\n\n![Â±CMU.DB - INTERNET BOOM All the big players were heavyweight and expensive. Open-source databases were missing important features. Many companies wrote their own custom middleware to scale out database across single- node DBMS instances. 15-721 (spring 2020) ](media/History-image7.png)\n\n![Â±CMU.DB 44 - DATA WAREHOUSES Rise of the special purpose OLAP DBMSs. ---+ Distributed / Shared-Nothing ---9 Relational / SQL ---+ Usually closed-source. Significant performance benefits from using columnar data storage model. N NETÃ‰ZZA PARACCEL monet Greenplum DATAllegrcÂ» VERTION 15-721 (spring 2020) ](media/History-image8.png)\n\n![](media/History-image9.png)\n\n![](media/History-image10.png)\n\n![47 2010s - HYBRID SYSTEMS Hybrid Transactional-Analytical Processing. Execute fast OLTP like a NewSQL system while also executing complex OLAP queries like a data warehouse system. -9 Distributed / Shared-Nothing ---+ Relational / SQL -9 Mixed open/closed-source. -2 Hyper A MEMSQL , HANA JustOneo. ice Peloton Â±CMU.DB 15-721 (spring 2020) ](media/History-image11.png)\n\n![48 - CLOUD SYSTEMS First database-as-a-service (DBaaS) offerings were \"containerized\" versions of existing DBMSs. There are new DBMSs that are designed from scratch explicitly for running in a cloud environment. Â»Â«snowflake amazon REDSHIFT xeround The Cloud Database --- Googl Spanner Â±CMU.DB Amazon + _ â€¢ y FAUNA Aurora SlicingDice Micr 15-721 (spring 2020) ](media/History-image12.png)\n\n![49 201 os - SHARED-DISK ENGINES Instead of writing a custom storage manager, the DBMS leverages distributed storage. -9 Scale execution layer independently of storage. ---+ Favors log-structured approaches. This is what most people think of when they talk about a data lake. clouderÃ¥ APACHE S Â»Â«snowflake MACHINE IMPALA presto Â±CMU.DB + Spar Microsoft 15-721 (spring 2020) IVE ](media/History-image13.png)\n\n![52 201 os - GRAPH SYSTEMS Systems for storing and querying graph data. Their main advantage over other data models is to provide a graph-centric query API -4 Recent research demonstrated that is unclear wheth there is any benefit to using a graph-centric execution engine and storage manager. TigerGraph O Dgraph GRAPH grAphbase.ai --- lerminusDB Â±CMU.DB 15-721 (spring 2020) e J sGraph * IndrÃ…DB ](media/History-image14.png)\n\n![53 2010s - TIMESERIES SYSTEMS Specialized systems that are designed to store timeseries / event data. The design of these systems make deep assumptions about the distribution of datÃ¦an workload query patterns. TIMESCALE VICTORIA V METRICS 15-721 (spring 2020) O influxdb ClickHouse fS Â±CMU.DB ](media/History-image15.png)\n\n![Â±CMU.DB 54 2010s - SPECIALIZED SYSTEMS Embedded DBMSs Multi-Model DBMSs Blockchain DBMSs Hardware Acceleration 15-721 (spring 2020) ](media/History-image16.png)\n[01 - History of Databases (CMU Databases / Spring 2020)](https://www.youtube.com/watch?v=SdW5RKUboKc)\n![u Carnegie Mellon University ADVANCED DATABASE SYSTEMS History of // 15-721 // spriÃ¶g 2020 ](media/History-image17.jpg)"},{"fields":{"slug":"/Databases/Concepts/Indexing/","title":"Indexing"},"frontmatter":{"draft":false},"rawBody":"# Indexing\n\nCreated: 2020-03-14 13:02:08 +0500\n\nModified: 2021-01-04 23:48:37 +0500\n\n---\n\nIndexing is a data structure technique to efficiently retrieve records from the database files based on some attributes on which the indexing has been done. Indexing in database systems is similar to what we see in books - Table of contents.\nIndexing is a way to optimize the performance of a database by minimizing the number of disk accesses required when a query is processed. It is a data structure technique which is used to quickly locate and access the data in a database.\n**Benefits**\n-   Prevent queries from doing full scans of an entire dataset\n-   Access less and therefore lock less rows during queries\n-   Speed up queries drastically\n-   Prevent sorting records post fetching\n-   Impose constraints e.g. data uniqueness\n-   Join datasets efficiently\n**Common problems that surface when applying indexes are**\n-   Having an index for every column in the table\n-   Not utilizing composite (multicolumn) indexes\n-   Using composite indexes but with ineffective column orderings that prevent the index being fully utilized\n-   Creating indexes based on rules of thumb or heuristics, such as indexing all columns that appear in the WHERE clause\n**Indexes are created using a few database columns**\n-   The first column is the**Search key**that contains a copy of the primary key or candidate key of the table. These values are stored in sorted order so that the corresponding data can be accessed quickly.\n    Note: The data may or may not be stored in sorted order.-   The second column is the**Data ReferenceorPointer**which contains a set of pointers holding the address of the disk block where that particular key value can be found.\n\n![Structure of an Index in Database Search Key Key Data Reference Value A single index ](media/Indexing-image1.jpeg)\n\nThe indexing has various attributes:\n-   **Access Types:** This refers to the type of access such as value based search, range access, etc.\n-   **Access Time:** It refers to the time needed to find particular data element or set of elements.\n-   **Insertion Time:** It refers to the time taken to find the appropriate space and insert a new data.\n-   **Deletion Time:** Time taken to find an item and delete it as well as update the index structure.\n-   **Space Overhead:** It refers to the additional space required by the index.\nIndexing is defined based on its indexing attributes. Indexing can be of the following types\n-   **Primary Index**\n\nPrimary index is defined on an ordered data file. The data file is ordered on akey field. The key field is generally the primary key of the relation.-   **Secondary Index**\n\nSecondary index may be generated from a field which is a candidate key and has a unique value in every record, or a non-key with duplicate values.-   **Clustering Index**\n\nClustering index is defined on an ordered data file. The data file is ordered on a non-key field.\nIn general, there are two types of file organization mechanism which are followed by the indexing methods to store the data:\n\n1.  **Sequential File Organization or Ordered Index File:**In this, the indices are based on a sorted ordering of the values. These are generally fast and a more traditional type of storing mechanism. These Ordered or Sequential file organization might store the data in a dense or sparse format:\n    -   **Dense Index:**\n        -   In dense index, there is an index record for every search key value in the database. This makes searching faster but requires more space to store index records itself. Index records contain search key value and a pointer to the actual record on the disk.\n        -   For every search key value in the data file, there is an index record.\n        -   This record contains the search key and also a reference to the first data record with that search key value.\n\n![Dense Index c D c D Data File For every search value in a Data File, There is an Index Record. Hence the name Dense Index. Index Record ](media/Indexing-image2.jpeg)\n-   **Sparse Index:**\n    -   In sparse index, index records are not created for every search key. An index record here contains a search key and an actual pointer to the data on the disk. To search a record, we first proceed by index record and reach at the actual location of the data. If the data we are looking for is not where we directly reach by following the index, then the system starts sequential search until the desired data is found.\n    -   The index record appears only for a few items in the data file. Each item points to a block as shown.\n    -   To locate a record, we find the index record with the largest search key value less than or equal to the search key value we are looking for.\n    -   We start at that record pointed to by the index record, and proceed along with the pointers in the file (that is, sequentially) until we find the desired record.\n\n![Ù‡ ÙŠÙˆ Ù€Ø¨ Ùƒ Ø­ Ø¯ ÙŠ ÙŠ Ù‚ Ø¹ Ù„ Ù‰ E ØŒ Ù‡ Ù† Ø© 00 00 00 00 00000000 ](media/Indexing-image3.jpeg)\n\n2.  **Hash File organization:**Indices are based on the values being distributed uniformly across a range of buckets. The buckets to which a value is assigned is determined by a function called a hash function.\nThere are primarily three methods of indexing:\n-   Clustered Indexing\n-   Non-Clustered or Secondary Indexing\n-   Multilevel Indexing\n1.  **Clustered Indexing**\n\nAclustered indexis collocated with the data in the same table space or same disk file. You can consider that a clustered index is aB-Treeindex whose leaf nodes are the actual data blocks on disk, since the index & data reside together. This kind of index physically organizes the data on disk as per the logical order of the index key.\nWhen more than two records are stored in the same file these types of storing known as cluster indexing. By using the cluster indexing we can reduce the cost of searching reason being multiple records related to the same thing are stored at one place and it also gives the frequent joining of more than two tables(records).\n\nClustering index is defined on an ordered data file. The data file is ordered on a non-key field. In some cases, the index is created on non-primary key columns which may not be unique for each record. In such cases, in order to identify the records faster, we will group two or more columns together to get the unique values and create index out of them. This method is known as the clustering index. Basically, records with similar characteristics are grouped together and indexes are created for these groups.\n\nFor example, students studying in each semester are grouped together. i.e. 1stSemester students, 2ndsemester students, 3rdsemester students etc are grouped.\n\n![cluster_index](media/Indexing-image4.png)\n\nClustered index sorted according to first name (Search key)\n\n**Primary Indexing**\n\nThis is a type of Clustered Indexing wherein the data is sorted according to the search key and the primary key of the database table is used to create the index. It is a default format of indexing where it induces sequential file organization. As primary keys are unique and are stored in a sorted manner, the performance of the searching operation is quite efficient.\n2.  **Non-clustered or Secondary Indexing**\n    A non clustered index just tells us where the data lies, i.e. it gives us a list of virtual pointers or references to the location where the data is actually stored. Data is not physically stored in the order of the index. Instead, data is present in leaf nodes. For eg. the contents page of a book. Each entry gives us the page number or location of the information stored. The actual data here(information on each page of the book) is not organized but we have an ordered reference(contents page) to where the data points actually lie. We can have only dense ordering in the non-clustered index as sparse ordering is not possible because data is not physically organized accordingly.\n    It requires more time as compared to the clustered index because some amount of extra work is done in order to extract the data by further following the pointer. In the case of a clustered index, data is directly present in front of the index.\n\n![indexing3](media/Indexing-image5.png)\n\n3.  **Multilevel Indexing**\n    With the growth of the size of the database, indices also grow. As the index is stored in the main memory, a single-level index might become too large a size to store with multiple disk accesses. The multilevel indexing segregates the main block into various smaller blocks so that the same can be stored in a single block. The outer blocks are divided into inner blocks which in turn are pointed to the data blocks. This can be easily stored in the main memory with fewer overheads.\n\n![](media/Indexing-image6.png)\n\nIndex records comprise search-key values and data pointers. Multilevel index is stored on the disk along with the actual database files. As the size of the database grows, so does the size of the indices. There is an immense need to keep the index records in the main memory so as to speed up the search operations. If single-level index is used, then a large size index cannot be kept in memory which leads to multiple disk accesses.\n\n![Multi-level Index](media/Indexing-image7.png)\n\nMulti-level Index helps in breaking down the index into several smaller indices in order to make the outermost level so small that it can be saved in a single disk block, which can easily be accommodated anywhere in the main memory.\n<https://www.geeksforgeeks.org/indexing-in-databases-set-1>\n\n<https://www.tutorialspoint.com/dbms/dbms_indexing.htm>\n<https://www.toptal.com/database/sql-indexes-explained-pt-1>\nWith a clustered index the rows are stored physically on the disk in the same order as the index. Therefore, there can be only one clustered index.\nWith a non clustered index there is a second list that has pointers to the physical rows. You can have many non clustered indices, although each new index will increase the time it takes to write new records.\nIt is generally faster to read from a clustered index if you want to get back all the columns. You do not have to go first to the index and then to the table.\nWriting to a table with a clustered index can be slower, if there is a need to rearrange the data.\n<https://stackoverflow.com/questions/1251636/what-do-clustered-and-non-clustered-index-actually-mean>\n\n## KEY** is the synonym for INDEX . You use the KEY when you want to create an index for a column or a set of columns that is not the part of aprimary keyorunique key. AUNIQUEindex ensures that values in a column must beunique. Unlike the PRIMARY index, MySQL allows NULL values in theUNIQUEindex.\n\nKey is synonymous to an index. If you want to create an index for a column, then use 'Key'.\n<https://www.mysqltutorial.org/mysql-primary-key>\n\n## Cardinality**\n\nIndex cardinality refers to the uniqueness of values stored in a specified column within an index.\nMySQL generates the index cardinality based on statistics stored as integers, therefore, the value may not be necessarily exact.\nThe query optimizer uses the index cardinality to generate an optimal query plan for a given query. It also uses the index cardinality to decide whether to use the index or not in the[join](https://www.mysqltutorial.org/mysql-join/)operations.\n\nIf the query optimizer chooses the index with a low cardinality, it may be more effective than scan rows without using the index.\nDo not reverse the order of the columns. You should always index lower cardinality columns first.\n-   statehas 22\n-   sourcehas 1122\n\nThat being the case, you need to run the following\n\nALTER TABLE tbl DROP INDEX state;\nALTER TABLE tbl ADD INDEX state_source_index (state,source);\n<https://dba.stackexchange.com/questions/63047/why-is-mysql-not-using-the-index-with-the-higher-cardinality>\nIndex selectivity is the ratio of the number of distinct indexed values (the â€‹cardinality) to the total number of rows in the table (#T).\n-   It ranges from 1/#T to 1. A unique index has a selectivity of 1, which is as good as it gets.\n-   Using the phonebook example: an index of(first_name, last_name)might be less effective than(last_name, first_name)because first names are much less distinct when compared to last_names, meaning it narrow down less results.\n**Why low cardinality indexes negatively impact performance**\n\n<https://www.ibm.com/developerworks/data/library/techarticle/dm-1309cardinal>\n\n"},{"fields":{"slug":"/Databases/Concepts/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2020-02-24 21:44:07 +0500\n\nModified: 2022-01-10 16:33:38 +0500\n\n---\n\n**Datastructures > Others > Database Index**\n**JDBC - Java Database Connectivity**\n\n**Java Database Connectivity**(**JDBC**) is an[application programming interface](https://en.wikipedia.org/wiki/Application_programming_interface)(API) for the programming language[Java](https://en.wikipedia.org/wiki/Java_(programming_language)), which defines how a client may access a[database](https://en.wikipedia.org/wiki/Database). It is a Java-based data access technology used for Java database connectivity. It is part of the[Java Standard Edition](https://en.wikipedia.org/wiki/Java_Standard_Edition)platform, from[Oracle Corporation](https://en.wikipedia.org/wiki/Oracle_Corporation). It provides methods to query and update data in a database, and is oriented towards[relational databases](https://en.wikipedia.org/wiki/Relational_database). A JDBC-to-[ODBC](https://en.wikipedia.org/wiki/ODBC)bridge enables connections to any ODBC-accessible data source in the[Java virtual machine](https://en.wikipedia.org/wiki/Java_virtual_machine)(JVM) host environment.\n<https://en.wikipedia.org/wiki/Java_Database_Connectivity>\n\n## ODBC - Opensource Database Connectivity**\n\nIn[computing](https://en.wikipedia.org/wiki/Computing),Open Database Connectivity(ODBC) is a standard[application programming interface](https://en.wikipedia.org/wiki/Application_programming_interface)(API) for accessing[database management systems](https://en.wikipedia.org/wiki/Database_management_system)(DBMS) . The designers of ODBC aimed to make it independent of database systems and[operating systems](https://en.wikipedia.org/wiki/Operating_system). An application written using ODBC can be ported to other platforms, both on the client and server side, with few changes to the data access code.\nODBC accomplishes DBMS independence by using anODBC driveras a translation layer between the application and the DBMS. The application uses ODBC functions through anODBC driver managerwith which it is linked, and the driver passes the[query](https://en.wikipedia.org/wiki/Query_language)to the DBMS. An ODBC driver can be thought of as analogous to a printer driver or other driver, providing a standard set of functions for the application to use, and implementing DBMS-specific functionality. An application that can use ODBC is referred to as \"ODBC-compliant\". Any ODBC-compliant application can access any DBMS for which a driver is installed. Drivers exist for all major DBMSs, many other data sources like[address book](https://en.wikipedia.org/wiki/Address_book)systems and[Microsoft Excel](https://en.wikipedia.org/wiki/Microsoft_Excel), and even for text or[comma-separated values](https://en.wikipedia.org/wiki/Comma-separated_values)(CSV) files.\nODBC was originally developed by[Microsoft](https://en.wikipedia.org/wiki/Microsoft)and[Simba Technologies](https://en.wikipedia.org/wiki/Simba_Technologies)during the early 1990s, and became the basis for the[Call Level Interface](https://en.wikipedia.org/wiki/Call_Level_Interface)(CLI) standardized by[SQL Access Group](https://en.wikipedia.org/wiki/SQL_Access_Group)in the [Unix](https://en.wikipedia.org/wiki/Unix) and [mainframe](https://en.wikipedia.org/wiki/Mainframe_computer) field. ODBC retained several features that were removed as part of the CLI effort. Full ODBC was later ported back to those platforms, and became a[de facto standard](https://en.wikipedia.org/wiki/De_facto_standard)considerably better known than CLI. The CLI remains similar to ODBC, and applications can be ported from one platform to the other with few changes.\n<https://en.wikipedia.org/wiki/Open_Database_Connectivity>\n\n## Data Integrity**\n\nIt's vital to users that the data they interact with is secure, correct and sensible. Examples are making sure that age isn't a negative number, or that no two students have the same information. We refer to this asdata integrity.\nData integrity takes several forms and can be divided into four categories:\n-   **Entity Integrity:** No duplicate rows exist in a table. For example, we can't insert Ron Weasley twice in the database.\n-   **Domain Integrity:** Restricting the type of values that one can insert in order to enforce correct values. For example, a House can only be Gryffindor, Ravenclaw, Slytherin, or Hufflepuff.\n-   **Referential Integrity:** Records that are used by other records cannot be deleted. A teacher cannot be deleted if they are currently teaching a course.\n-   **User-Defined Integrity:**An \"other\" category that consists of business-related logic and rules to the database.\n**Referential Integrity Constraints**\n\nReferential integrity constraints are one of the features of DBMSs we become acquainted with. We use them because they make us feel comfortable. These **constraints guarantee that you will never mistakenly insert a value that doesn't exist in the referenced column.** It also prevents you from deleting a row whose column is referenced on the other side of the constraint.\nIf you have seconds thoughts about using referential integrity constraints, their use is not tied to business requirements for your running software. They act as safety nets, preventing your code from sustaining mistaken behavior and beginning to write uncorrelated data to your database. Because we are applying zero-downtime migrations to our schema and none of our migrations are destructive, you still have another safety net: you never lose any data, and you are always capable of deploying your previous version into production without the fear of losing or corrupting anything. In this sense, we believe that it is safe to simply drop your referential integrity constraints before applying your series of migrations. Later, when everything is deployed into production and you know that you are done with the refactorings, you will recreate the constraints using the new columns\n**Data Deduplication (Dedup)**\n\n**Data Deduplication**is a specialized[data compression](https://en.wikipedia.org/wiki/Data_compression)technique for eliminating duplicate copies of repeating data. Related and somewhat synonymous terms are**intelligent (data) compression**and[**single-instance (data) storage**](https://en.wikipedia.org/wiki/Single-instance_storage). This technique is used to improve storage utilization and can also be applied to network data transfers to reduce the number of bytes that must be sent. In the deduplication process, unique chunks of data, or byte patterns, are identified and stored during a process of analysis. As the analysis continues, other chunks are compared to the stored copy and whenever a match occurs, the redundant chunk is replaced with a small reference that points to the stored chunk. Given that the same byte pattern may occur dozens, hundreds, or even thousands of times (the match frequency is dependent on the chunk size), the amount of data that must be stored or transferred can be greatly reduced.\nToday, you usually head aboutdeduplicationin the contest of storage devices or architectures. It is a way of, for example, saving disk space when large amounts of duplicate data is present (imagine, for example, having 100 VM images on a SAN - there is likely to be a lot of duplication among the operating system and other common files on each VM).\nDeduplication is a way of storing this redundant data only once.\n<https://en.wikipedia.org/wiki/Data_deduplication>\n\n<https://searchstorage.techtarget.com/definition/data-deduplication>\n\n## General Data Types**\n-   **Scalar Types**\n\nA scalar type can represent exactly one value. The scalar types are number, string, binary, Boolean, and null.-   **Document Types**\n\nA document type can represent a complex structure with nested attributes, such as you would find in a JSON document. The document types are list and map.-   **Set Types**\n\nA set type can represent multiple scalar values. The set types are string set, number set, and binary set.\n**Autoincrement**\n\nAUTOINCREMENT'ing is a common way of generating primary keys. It's not uncommon to see cases where databases are used as ID generators and there are ID-generation designated tables in a database.\nThere are a few reasons why generating primary keys via auto-incrementing may not be not ideal:\n-   In distributed database systems, auto-incrementing is a hard problem. A global lock would be needed to be able to generate an ID. If you can generate a UUID instead, it would not require any collaboration between database nodes. Auto-incrementing with locks may introduce contention and may significantly downgrade the performance for insertions in distributed situations. Some databases like MySQL may require specific[configuration](https://www.percona.com/blog/2011/01/12/conflict-avoidance-with-auto_increment_incremen-and-auto_increment_offset/)and more attention to get things right in master-master replication. The configuration is easy to mess up and can lead to write outages.\n-   Some databases have partitioning algorithms based on primary keys. Sequential IDs may cause unpredictable hotspots and may overwhelm some partitions while others stay idle.\n-   The fastest way to access to a row in a database is by its primary key. If you have better ways to identify records, sequential IDs may make the most significant column in tables a meaningless value. Please pick a globally unique natural primary key (e.g. a username) where possible.\nPlease consider the impacts of auto-incremented IDs vs UUIDs on indexing, partitioning and sharding before you decide on what works better for you.\n<https://medium.com/@rakyll/things-i-wished-more-developers-knew-about-databases-2d0178464f78>\n\n## Database Storage Engines**\n\n<https://medium.com/yugabyte/a-busy-developers-guide-to-database-storage-engines-the-basics-6ce0a3841e59>\n\n![DATABASE STORAGE ENGINES B-TREE Server DB2 LSM TREE HBRSE cassandra O Google Cloud Bigtable O influxdb elasticsearch O LEVELDB DB mongo MySQL. Couchbase z RocksDB Byte DB ](media/Others-image1.png)\n**Performance Benchmarks**\n\n**SQL Databases**\n\nWhen you make a lot of inserts or updates do it one statement or disable autocommit and use explicit transactions.\n<https://docs.google.com/spreadsheets/d/1fbrqhC83l8GNb_WuNu_53_QgAXAi661wsv7bun-IMt0/edit?usp=sharing>\n\n![0.100 0075 0050 0025 Red,hift Databases insert comparision 10 100 ](media/Others-image2.png)\n\n<https://github.com/harryho/db-samples>"},{"fields":{"slug":"/Databases/Concepts/RUM-Conjecture/","title":"RUM Conjecture"},"frontmatter":{"draft":false},"rawBody":"# RUM Conjecture\n\nCreated: 2019-06-27 16:31:44 +0500\n\nModified: 2020-12-07 00:37:46 +0500\n\n---\n\nRUM - Read Update Memory\nThere are many ways to read and write the data: data structures, access patterns, optimizations: all of it contributes to the performance of the resulting system. But it's hard to make system that'll be simultaneously optimized in all directions. In an ideal world we would have data structures that can guarantee the best read and write performance and have no storage overhead, but of course in practice this is not possible. Researchers from Harvard DB lab summarized the three parameters people working on database systems are trying to optimize for:**ReadOverhead,UpdateOverhead, and Memory Overhead**. Deciding which overhead to optimize for will influence the choice of data structures, access methods and even suitability for certain workloads. RUM Conjecture states that setting an upper bound for two of the mentioned overheads also sets a lower bound for the third one.\n\n![Read Optimised DS Write Optimised Space Optimised ](media/RUM-Conjecture-image1.png)\n\nRUM Conjecture separates data structures in Read, Update and Memory optimized.\nTree-based data structures are typically optimized for read performance, which is traded for the space overhead and write amplification coming from node splits/merges, relocation, and fragmentation/misbalance-related maintenance. In terms of read amplification, Tree-based structures help to minimize the ratio between the total data accessed and the data that's intended to be read. Using adaptive data structures gives better read performance at the price of higher maintenance costs. Adding metadata facilitating traversals (like fractional cascading) will have an impact on write time and take space, but can improve the read time.\nAs we have already discussed, LSM-Trees optimize for Write Performance. Both updates are deletes do not require searching for the data on disk and guarantee sequential writes by deferring and buffering all insert, update and delete operations. This comes at a price of higher maintenance costs and a need for compaction (which is just a way to mitigate the ever-growing price of reads) and more expensive reads (as the data has to be read from multiple files and merged together). At the same time, LSM Trees help to improve the memory efficiency by avoiding reserving empty space (a source of overhead in some read-optimised data structures) and allowing block compression due to the better occupancy and immutability of the end file.\nOptimizing for memory efficiency might involve using compression (for example, algorithms such as[Gorilla compression](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf), delta encoding and many others), which will add some read and write overhead. Sometimes you can trade functionality for efficiency. For example, heap files and hash indexes can give great performance guarantees and smaller space overhead due to the file format simplicity for the price of not being able to perform anything but point queries. You can also trade precision for efficiency, by using approximate data structures, such as[Bloom Filter](https://www.jasondavies.com/bloomfilter/),[HyperLogLog](https://research.neustar.biz/2012/10/25/sketch-of-the-day-hyperloglog-cornerstone-of-a-big-data-infrastructure/),[Count Min Sketch](https://redislabs.com/blog/count-min-sketch-the-art-and-science-of-estimating-stuff/)and many others.\nThese three tunables:Read,UpdateandMemoryoverheads can help you to evaluate the database and deeper understand the workloads it's best suitable for. All of them are quite intuitive and it's often easy to sort the storage system into one of the buckets and guess how it's going to perform.\n\n![Read Update Memory ](media/RUM-Conjecture-image2.png)\n\nRUM Conjecture: Setting an upper bound on two tunables also sets a lower bound on the thirdone.\nThat said, using the best data structures is just a half of job: there are enough parts in database systems where performance may still be a bottleneck. But we assume that database developers always aspire to eliminate problems and make their product perform best under the workloads it's designed for and well enough for all other use cases.\n<https://medium.com/databasss/on-disk-storage-part-4-b-trees-30791060741>"},{"fields":{"slug":"/Databases/Concepts/Types-of-Databases/","title":"Types of Databases"},"frontmatter":{"draft":false},"rawBody":"# Types of Databases\n\nCreated: 2018-04-01 23:05:17 +0500\n\nModified: 2022-04-23 00:53:12 +0500\n\n---\n\n**Types of databases**\n\n1.  Relational database (ACID)\n    -   MySQL Cluster\n    -   PostgreSQL\n    -   VoltDB\n    -   Clustrix\n    -   ScaleBase\n    -   NimbusDB\n    -   Megastore over BigTable\n    -   MariaDB\n    -   SQLite\n2.  NoSQL Databases (Scales better, Higher availability)\n    -   While the traditional SQL can be effectively used to handle large amount of structured data, we need NoSQL (Not Only SQL) to handle unstructured data\n    -   NoSQL databases store unstructured data with no particular schema\n    -   Each row can have its own set of column values. NoSQL gives better performance in storing massive amount of data\n1.  Key-Value\n    -   Project Voldemort\n    -   Riak\n    -   Redis\n    -   Aerospike\n    -   Scalaris\n    -   Tokyo cabinet\n    -   Memcached, membrain, and membase\n    -   LF (fully decentralized fully replicated key/value store.)\n    -   Etcd\n\n2.  Wide Column / Extensible Record Stores / Column-family\n\nCan have many many different types of column\n-   HBase\n-   HyperTable\n-   Cassandra\n3.  Column Oriented Database\n\nNot to be confused with column-family databases, column-oriented databases are very similar to relational databases, but store data on disk by column instead of by row. This means that all of the data for a single column is together, allowing for faster aggregation on larger data sets. Since the columns are separate from each other, inserting or updating values is a performance intensive task, so column-oriented databases are primarily used for analytical work where entire data sets can be preloaded at one time.-   Druid\n4.  Object Oriented Database\n\nObject-oriented databases store data items as objects, seeking to bridge the gap between the representations used by objected-oriented programming languages and databases. Although this solves many problems with translating between different data paradigms, historically, adoption has suffered due to increased complexity, lack of standardization, and difficulty decoupling the data from the original application.\n5.  Document Oriented Database / Document Stores\n    -   Semi-structured data (XML, JSON)\n    -   Flat File Database\n**Databases**\n-   SimpleDB\n-   CouchDB\n-   MongoDB\n-   Terrastore\n-   SQLite\n-   RethinkDB\n\n<https://rethinkdb.com>\n6.  Hierarchical database / Graph based database (Entities, Relationships)\n    -   Dgraph\n    -   Nebula-graph\n\n<https://nebula-graph.io>\n-   [Alibaba Graph Database](https://cn.aliyun.com/product/gdb)- A real-time, reliable, cloud-native graph database service that supports property graph model.\n-   [Amazon Neptune](https://aws.amazon.com/neptune/)- Fully-managed graph database service.\n-   [ArangoDB](https://github.com/ArangoDB-Community/arangodb-tinkerpop-provider)- OLTP Provider for ArangoDB.\n-   [Bitsy](https://github.com/lambdazen/bitsy/wiki)- A small, fast, embeddable, durable in-memory graph database.\n-   [Blazegraph](https://github.com/blazegraph/tinkerpop3)- RDF graph database with OLTP support.\n-   [CosmosDB](https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction)- Microsoft's distributed OLTP graph database.\n-   [ChronoGraph](https://github.com/MartinHaeusler/chronos/tree/master/org.chronos.chronograph)- A versioned graph database.\n-   [DSEGraph](https://www.datastax.com/products/datastax-enterprise-graph)- DataStax graph database with OLTP and OLAP support.\n-   [GRAKN.AI](https://grakn.ai/)- Distributed OLTP/OLAP knowledge graph system.\n-   [Hadoop (Spark)](https://tinkerpop.apache.org/docs/current/reference/#sparkgraphcomputer)- OLAP graph processor using Spark.\n-   [HGraphDB](https://github.com/rayokota/hgraphdb)- OLTP graph database running on Apache HBase.\n-   [Huawei Graph Engine Service](https://www.huaweicloud.com/en-us/product/ges.html)- Fully-managed, distributed, at-scale graph query and analysis service that provides a visualized interactive analytics platform.\n-   [IBM Graph](https://console.ng.bluemix.net/catalog/services/ibm-graph/)- OLTP graph database as a service.\n-   [JanusGraph](http://janusgraph.org/)- Distributed OLTP and OLAP graph database with BerkeleyDB, Apache Cassandra and Apache HBase support.\n-   [JanusGraph (Amazon)](https://github.com/awslabs/dynamodb-janusgraph-storage-backend/)- The Amazon DynamoDB Storage Backend for JanusGraph.\n\n<https://medium.com/terminusdb/graph-fundamentals-part-1-rdf-60dcf8d0c459>\n-   [Neo4j](https://tinkerpop.apache.org/docs/current/reference/#neo4j-gremlin)- OLTP graph database (embedded and high availability) (open source, noSQL graph database)\n-   [neo4j-gremlin-bolt](https://github.com/SteelBridgeLabs/neo4j-gremlin-bolt)- OLTP graph database (using Bolt Protocol).\n-   [OrientDB](https://github.com/orientechnologies/orientdb-gremlin)- OLTP graph database\n-   [Apache S2Graph](https://s2graph.apache.org/)- OLTP graph database running on Apache HBase.\n-   [Sqlg](https://github.com/pietermartin/sqlg)- OLTP implementation on SQL databases.\n-   [Stardog](https://stardog.com/)- RDF graph database with OLTP and OLAP support.\n-   [TinkerGraph](https://tinkerpop.apache.org/docs/current/reference/#tinkergraph-gremlin)- In-memory OLTP and OLAP reference implementation.\n-   [Titan](https://thinkaurelius.github.io/titan/)- Distributed OLTP and OLAP graph database with BerkeleyDB, Apache Cassandra and Apache HBase support.\n-   [Titan (Amazon)](https://github.com/awslabs/dynamodb-titan-storage-backend)- The Amazon DynamoDB storage backend for Titan.\n-   [Titan (Tupl)](https://github.com/classmethod/tupl-titan-storage-backend)- The Tupl storage backend for Titan.\n-   [Unipop](https://github.com/rmagen/unipop)- OLTP Elasticsearch and JDBC backed graph.\n<http://tinkerpop.apache.org>\nExamples\n-   Filesystems\n-   DNS\n-   LDAP directories\n7.  Network databases\n    -   IDMS\n8.  Time-Series databases\n    -   TimeScale DB (TSDB)\n    -   InfluxDB\n    -   OpenTSDB\n    -   Prometheus\n9.  In-memory databases\n    -   Redis\n    -   RocksDB\n    -   Memcached (a distributed memory object caching system)\n10. Cloud databases / on-line databases / Managed services\n    -   Google Firebase\n    -   Facebook Parse\n    -   Amazon DynamoDB\n    -   Amazon Aurora\n11. Object Storage\n\nObject storage(also known asobject-based storage) is a[computer data storage](https://en.wikipedia.org/wiki/Computer_data_storage)architecture that manages data as objects, as opposed to other storage architectures like[file systems](https://en.wikipedia.org/wiki/File_systems)which manages data as a file hierarchy, and[block storage](https://en.wikipedia.org/wiki/Block_storage)which manages data as blocks within sectors and tracks.Each object typically includes the data itself, a variable amount of[metadata](https://en.wikipedia.org/wiki/Metadata), and a[globally unique identifier](https://en.wikipedia.org/wiki/Globally_unique_identifier). Object storage can be implemented at multiple levels, including the device level (object-storage device), the system level, and the interface level. In each case, object storage seeks to enable capabilities not addressed by other storage architectures, like interfaces that can be directly programmable by the application, a namespace that can span multiple instances of physical hardware, and data-management functions like[data replication](https://en.wikipedia.org/wiki/Data_replication)and data distribution at object-level granularity.\nObject storage systems allow retention of massive amounts of[unstructured data](https://en.wikipedia.org/wiki/Unstructured_data). Object storage is used for purposes such as storing photos on[Facebook](https://en.wikipedia.org/wiki/Facebook), songs on[Spotify](https://en.wikipedia.org/wiki/Spotify), or files in online collaboration services, such as[Dropbox](https://en.wikipedia.org/wiki/Dropbox_(service)).-   S3\n-   Azure Blob Storage\n<https://en.wikipedia.org/wiki/Object_storage>\n12. NewSQL databases\n\nNewSQL databases follow the relational structure and semantics, but are built using more modern, scalable designs. The goal is to offer greater scalability than relational databases and greaterconsistency guaranteesthan NoSQL alternatives. They achieve this by sacrificing certain amounts of availability in the event of a networking partition. The trade offs between consistency and availability is a fundamental problem of distributed databases described by theCAP theorem.-   MemSQL\n-   VoltDB\n-   Spanner\n-   Calvin\n-   CockroachDB\n-   FaunaDB\n\n<https://www.youtube.com/watch?v=2CipVwISumA&t=661s&ab_channel=Fireship>\n-   yugabyteDB\n12. Multi-model databases\n\nMulti-model databases are databases that combine the functionality of more than one type of database. The benefits of this approach are clear --- the same system can use different representations for different types of data.-   ArangoDB\n-   OrientDB\n-   Couchbase\n13. Semantic RDF graph database\n\nSemantic RDF graph databases are databases that map objects using the Resource Description Framework. This framework a way to describe, in detail, objects and their relationships by categorizing pieces of data and connections. The idea is to map subjects, actions, and objects like you would in a sentence (for example, \"Bill calls Sue\"). For most use cases, labeled property graphs, usually just called[graph databases](https://www.prisma.io/blog/comparison-of-database-models-1iz9u29nwn37#graph-databases-mapping-relationships-by-focusing-on-how-connections-between-data-are-meaningful), can express relationships more flexibly and concisely.\n14. Ledger Databases\n\n15. Embedded databases\n\n<https://jsondb.io>\n\n## Comparision**\n\n![](media/Types-of-Databases-image1.png)\n**Relational vs Non-Relational Databases**\n\nA**relational database**is one where data is stored in the form of a table. Each table has a**schema**, which is the columns and types a record is required to have. Each schema must have at least one primary key that uniquely identifies that record. In other words, there are no duplicate rows in your database. Moreover, each table can be related to other tables using foreign keys.\nOne important aspect of relational databases is that a change in a schema must be applied to all records. This can sometimes cause breakages and big headaches during migrations.**Non-relational databases**tackle things in a different way. They are inherently schema-less, which means that records can be saved with different schemas and with a different, nested structure. Records can still have primary keys, but a change in the schema is done on an entry-by-entry basis.\n<https://www.prisma.io/blog/comparison-of-database-models-1iz9u29nwn37>\n\n<http://www.cattell.net/datastores/Datastores.pdf>\n\n## AWS Databases**\n\n1.  Amazon Aurora\n\nHigh Performance Managed Relational Database\n2.  Amazon RDS\n\nManaged Relational Database Service for MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB\nAmazon RDS Proxy\n\nAmazon RDS Proxy is a fully managed, highly available database proxy for[Amazon Relational Database Service (RDS)](https://aws.amazon.com/rds/)that makes applications more scalable, more resilient to database failures, and more secure.\nMany applications, including those built on modern[serverless architectures](https://aws.amazon.com/serverless/), can have a large number of open connections to the database server, and may open and close database connections at a high rate, exhausting database memory and compute resources. Amazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. With RDS Proxy, failover times for Aurora and RDS databases are reduced by up to 66% and database credentials, authentication, and access can be managed through integration with AWS Secrets Manager and AWS Identity and Access Management (IAM).\n<https://aws.amazon.com/rds/proxy>\n3.  Amazon RDS on VMWare\n\nAutomate on-premises database management\n4.  Amazon DynamoDB\n\nManaged NoSQL Database\n5.  Amazon DocumentDB (With MongoDB Compatibility)\n\nFully Managed Document Database\n6.  Amazon ElastiCache\n\nIn-memory Data Store and Cache-   Query Caching\n-   Delayed Transactions\n-   Read / Write Splitting\n-   Connection Polling\nElasticache between rds and application - <https://aws.amazon.com/blogs/database/automating-sql-caching-for-amazon-elasticache-and-amazon-rds>\n\n<https://www.heimdalldata.com/aws>\n\n<http://demoa.heimdalldata.com:8087/docs/theory/caching.html>\n\n## ElastiCache using Redis**\n\n<https://aws.amazon.com/redis>\n7.  Amazon Redshift\n\nFast, Simple, Cost-effecitive Data Warehousing\n8.  Amazon Neptune\n\nFully Managed Graph Database Service\n9.  AWS Database Migration Service (DMS)\n\nMigrate Databases with Minimal Downtime\n<https://aws.amazon.com/dms>\n\n<https://aws.amazon.com/blogs/database/archiving-data-from-relational-databases-to-amazon-glacier-via-aws-dms>\n\n<https://aws.amazon.com/blogs/database/automating-aws-dms-migration-tasks>\n10. AWS Managed Apache Cassandra Service\n\nAmazon Managed Apache Cassandra Service is a scalable, highly available, and managed Apache Cassandra--compatible database service. With Amazon Managed Cassandra Service, you can run your Cassandra workloads on AWS using the same Cassandra application code and developer tools that you use today. You don't have to provision, patch, or manage servers, and you don't have to install, maintain, or operate software. Amazon Managed Cassandra Service is serverless, so you pay for only the resources you use and the service automatically scales tables up and down in response to application traffic. You can build applications that serve thousands of requests per second with virtually unlimited throughput and storage.\n<https://aws.amazon.com/mcs>\n11. Amazon TimeStream\n\nFully Managed TimeSeries Database\n12. Amazon Managed Apache Cassandra Service\n\nManaged Cassandra-compatible database\n13. Amazon Quantum Ledger Database (QLDB)\n\nFully managed ledged database\n| **Database Type** | **Use Cases**                                                                                  | **AWS Service**                                                                                                                                                                                |\n|--------------|--------------------------------|---------------------------|\n| Relational        | Traditional applications, ERP, CRM, e-commerce                                                 | [Amazon Aurora](https://aws.amazon.com/rds/aurora/?c=db&sec=srv)|[Amazon RDS](https://aws.amazon.com/rds/?c=db&sec=srv)|[Amazon Redshift](https://aws.amazon.com/redshift/?c=db&sec=srv) |\n| Key-value         | High-traffic web apps, e-commerce systems, gaming applications                                 | [Amazon DynamoDB](https://aws.amazon.com/dynamodb/?c=db&sec=srv)                                                                                                                               |\n| In-memory         | Caching, session management, gaming leaderboards, geospatial applications                      | [Amazon ElastiCache for Memcached](https://aws.amazon.com/elasticache/memcached/?c=db&sec=srv)|[Amazon ElastiCache for Redis](https://aws.amazon.com/elasticache/redis/?c=db&sec=srv)       |\n| Document          | Content management, catalogs, user profiles                                                    | [Amazon DocumentDB](https://aws.amazon.com/documentdb/?c=db&sec=srv)                                                                                                                           |\n| Wide-column       | High scale industrial apps for equipment maintenance, fleet management, and route optimization | [Amazon Managed Apache Cassandra Service](https://aws.amazon.com/mcs/?c=db&sec=srv)                                                                                                            |\n| Graph             | Fraud detection, social networking, recommendation engines                                     | [Amazon Neptune](https://aws.amazon.com/neptune/?c=db&sec=srv)                                                                                                                                 |\n| Time series       | IoT applications, DevOps, industrial telemetry                                                 | [Amazon Timestream](https://aws.amazon.com/timestream/?c=db&sec=srv)                                                                                                                           |\n| Ledger            | Systems of record, supply chain, registrations, banking transactions                           | [Amazon Quantum Ledger Database](https://aws.amazon.com/qldb/?c=db&sec=srv)                                                                                                                    |\n<https://aws.amazon.com/products/databases>\n\n## Row Oriented vs Column Oriented (columnar) Databases**\n\nHere is an example: Say we have a table that stores the following data for 1M users:user_id, name, # logins, last_login. So we effectively have 1M rows and 4 columns. A row-oriented data store will physically store each user's data (i.e., each row) contiguously on disk. By contrast, a columnar store will store all of the user_id's together, all of the names together, and so forth, so that each column's data is stored contiguously on disk.\nAs a result, shallow-and-wide queries will be faster on a row store (e.g., \"fetch all data for user X\"), while deep-and-narrow queries will be faster on a column store (e.g., \"calculate the average number of logins for all users\").\nIn particular, columnar stores do really well with narrow queries over very wide data. With such storage, only the designated columns need to be read from disk (rather than bringing in pages of data from disk with the entire rows, then selecting one or a few columns just in memory). \nAdditionally, because individual columns of data are typically the same type and are often drawn from a more limited domain or range, they typically compress better than an entire wide row of data comprising many different data types and ranges. For example, our column of number of logins would all be of an integer type and may cover a small range of numeric values.\nYet columnar stores are not without trade-offs. First of all, inserts take much longer: the system needs to split each record into the appropriate columns and write it to disk accordingly. Second, it is easier for row-based stores to take advantage of an index (e.g., B-tree) to quickly find the appropriate records. Third, with a row-store it is easier to normalize your dataset, such that you can more efficiently store related datasets in other tables. \nAs a result, the choice of row-oriented vs. columnar database greatly depends on your workload. Typically, row-oriented stores are used with transactional (OLTP) workloads, while columnar stores are used with analytical (OLAP) workloads.\n<https://blog.timescale.com/blog/building-columnar-compression-in-a-row-oriented-database>\n\n## Benefits of Columnar Formats**\n\n**READ-OPTIMIZATION**\n\nLet me pretend I want to run a SQL query against this data, for example:\n\nSELECT COUNT(1) from people where last_name = \"Rathbone\"\n\nWith a regular CSV a SQL engine would have to scan every row, parse each column, extract thelast_namevalue, then count allRathbonevalues that it sees.\nIn CCSV, the SQL engine can skip past the first two fields and simply scan line 3, which contains all the last name values available.\nWhy is that good? Well now the SQL engine is only processing around 1/6 of the data, so CCSV just delivered a (theoretical and totally unsubstantiated)600% performance improvement vs regular CSV files.\nImagine the same gains against a petabyte-scale dataset. It is not hard to imagine columnar file format optimizations saving a tonne of processing power (and money) compared to regular JSON datasets. This is the core value of columnar file formats.\nOf course, in reality there is more work that CCSV would need to do to be a viable file format, but that is getting a little into the weeds, so I will not cover those topics here.\n**COMPRESSION IMPROVEMENTS**\n\nStoring like-data together also has advantages for compression codecs. Many compression codecs (including GZIP and Snappy) have a higher compression-ratio when compressing sequences of similar data. By storing records column-by-column, in many cases each section of column data will contain similar values --- that makes it ripe for compression. In fact, each column could be compressed independently of the others to optimize this further.\nThe final benefit is that compression and dense-packing in columnar databases free up space --- space that may be used to sort and index data within the columns. In other words,**columnar databases have higher sorting and indexing efficiency**, which comes more as a side benefit of having some leftover space from strong compression. It is also, in fact, mutually beneficial: researchers who study columnar databases point out that sorted data compress better than unsorted data, because sorting lowers entropy.\n**Negatives of Columnar Formats**\n\nThe biggest negative of columnar formats is that re-constructing a complete record is slower and requires reading segments from each row, one-by-one. It is for this reason that columnar-file-formats initially hit their groove for analytics-style workflows, rather than Map/Reduce style workflows --- which by default operate on whole rows of data at a time.\nFor real columnar file formats (like[Parquet](http://parquet.apache.org/)), this downside is minimized by some clever tricks like breaking the file up into 'row groups' and building extensive metadata, although for particularly wide datasets (like 200+ columns), the speed impact can be fairly significant.\nThe other downside, is that they are more CPU and ram intensive to write, as the file writer needs to collect a whole bunch of metadata, and reorganize the rows before it can write the file.\n<https://blog.matthewrathbone.com/2019/11/21/guide-to-columnar-file-formats.html>\n\n## Choosing the Database**\n\n1.  Instant performance (respond in less than 1ms)\n\n2.  Scalability (Linear and horizontal scaling)\n\n3.  High availability (quickly recover from database failure without loss of data, replication)\n\n4.  Tiered memory support (hottest data in DRAM and warm data in persistent memory)\n\n5.  Simplicity and extensibility\n\n6.  Developer tools\n\n7.  Cloud native\n\n8.  Open source\n\n9.  NoSQL for the future\n**In-Memory Databases (IMDB) and In-Memory Data Grids (IMDG)**\n\nOne of the crucial differences between In-Memory Data Grids and In-Memory Databases lies in the ability to scale to hundreds and thousands of servers. That is the In-Memory Data Grid's**inherent capability**for such scale due to their MPP (Massively Parallel Processing) architecture, and the In-Memory Database's**explicit inability**to scale due to fact that SQL joins, in general, cannot be efficiently performed in a distribution context.\n<https://www.gridgain.com/resources/blog/in-memory-database-vs-in-memory-data-grid-revisited>\n\n## RDBMS**\n\nA relational database management system (RDBMS) is a program that allows you to create, update, and administer a relational database. Most relational database management systems use the SQL language to access the database.\nArelational databaseis a type of database. It uses a structure that allows us to identify and access datain relationto another piece of data in the database. Often, data in a relational database is organized into tables.\n**Columns -** Tables can have hundreds, thousands, sometimes even millions of columns of data. Columns are labeled with a descriptive name (say,age) and have a specificdata type.\n**Rows/Records -** Tables can also have manyrowsof data. These rows are often calledrecords.\n**Resources**\n\n<https://www.toptal.com/database/database-migrations-caterpillars-butterflies>\n\n<https://www.toptal.com/database/database-design-bad-practices>\n<https://dbdb.io>\n\n<https://www.sciencedirect.com/science/article/pii/S1319157816300453>\n[Rust at speed --- building a fast concurrent database](https://www.youtube.com/watch?v=s19G6n0UjsM)\n\n<https://www.youtube.com/watch?v=Cym4TZwTCNU>\n<https://www.freecodecamp.org/news/learn-nosql-in-3-hours>\n\n"},{"fields":{"slug":"/Databases/Modeling/Data-Modeling/","title":"Data Modeling"},"frontmatter":{"draft":false},"rawBody":"# Data Modeling\n\nCreated: 2020-02-04 17:34:41 +0500\n\nModified: 2021-07-24 17:22:24 +0500\n\n---\n\n**Data modeling**in[software engineering](https://en.wikipedia.org/wiki/Software_engineering)is the process of creating a[data model](https://en.wikipedia.org/wiki/Data_model)for an[information system](https://en.wikipedia.org/wiki/Information_system)by applying certain formal techniques.\n![Activity Model Detailed Data Requirements Technical Environment Performance Considerations Business Data Create/U pd ate Logical Data Model Create/Update Physical Data Model Create/Update Data Conceptual Data Model Entities/Subtypes Attributes Relationships Integrity Rules Physical Data Model Tables Columns Keys/lndices Triggers Data ](media/Data-Modeling-image1.png)\n\nThe data modeling process. The figure illustrates the way data models are developed and used today . A[conceptual data model](https://en.wikipedia.org/wiki/Conceptual_schema)is developed based on the data[requirements](https://en.wikipedia.org/wiki/Requirement)for the application that is being developed, perhaps in the context of an[activity model](https://en.wikipedia.org/wiki/Activity_diagram). The data model will normally consist of entity types, attributes, relationships, integrity rules, and the definitions of those objects. This is then used as the start point for interface or database design.\n<https://en.wikipedia.org/wiki/Data_modeling>\nData modeling (data modelling)is the process of creating a data model for the data to be stored in a Database. This data model is a conceptual representation of Data objects, the associations between different data objects and the rules. Data modeling helps in the visual representation of data and enforces business rules, regulatory compliances, and government policies on the data. Data Models ensure consistency in naming conventions, default values, semantics, security while ensuring quality of the data.\nData model emphasizes on what data is needed and how it should be organized instead of what operations need to be performed on the data. Data Model is like architect's building plan which helps to build a conceptual model and set the relationship between data items.\nThe two types of Data Models techniques are\n\n1.  Entity Relationship (E-R) Model\n\n2.  UML (Unified Modelling Language)\n**Why use Data Model?**\n\nThe primary goal of using data model are:\n-   Ensures that all data objects required by the database are accurately represented. Omission of data will lead to creation of faulty reports and produce incorrect results.\n-   A data model helps design the database at the conceptual, physical and logical levels.\n-   Data Model structure helps to define the relational tables, primary and foreign keys and stored procedures.\n-   It provides a clear picture of the base data and can be used by database developers to create a physical database.\n-   It is also helpful to identify missing and redundant data.\n-   Though the initial creation of data model is labor and time consuming, in the long run, it makes your IT infrastructure upgrade and maintenance cheaper and faster.\n**Types of Data Models**\n\nThere are mainly three different types of data models:\n\na.  **Conceptual:**This Data Model defines**WHAT**the system contains. This model is typically created by Business stakeholders and Data Architects. The purpose is to organize, scope and define business concepts and rules.\n\nb.  **Logical:**Defines**HOW**the system should be implemented regardless of the DBMS. This model is typically created by Data Architects and Business Analysts. The purpose is to developed technical map of rules and data structures.\n\nc.  **Physical:** This Data Model describes**HOW**the system will be implemented using a specific DBMS system. This model is typically created by DBA and developers. The purpose is actual implementation of the database.\n\n![Process Models Data Requirements Technical Requirements Performance Requirements Business Data Logical Data Modeling Physical Data Modeling Create/Update Data iCal ata Mod Physical ta Mod Data ](media/Data-Modeling-image2.png)\n<https://www.guru99.com/data-modelling-conceptual-logical.html>\n\n## Tenets of NoSQL Data Modeling**\n-   Understand the use case\n    -   Nature of the application\n        -   OLTP/OLAP/DSS (Decision Support System)\n        -   Define the Entity-Relationship Model\n        -   Identify Data Life Cycle\n            -   TTL, Backup/Archival, etc\n-   Define the access patterns\n    -   Read/Write workloads\n        -   Identify data sources\n        -   Define query aggregations\n        -   Document all workflows\n-   NoSQL Data Model is not flexible (it's efficient)\n-   Data-modeling\n    -   Avoid relational design patterns, use one table\n        -   1 application service = 1 table\n            -   Reduce round trips\n            -   Simplify access patterns\n        -   Identify primary keys\n            -   How will items be inserted and read?\n            -   Overload items into partitions\n        -   Define indexes for secondary access patterns\n-   Review -> Repeat -> Review\n<https://www.toptal.com/data-modeling/interview-questions>\n\n## Schema**\n\nFrom a very simple point of view, a relational database comprises *tables* with multiple *columns* and *rows*, and *relationships* between them. The collection of database objects' definitions associated within a certain namespace is called a *schema*. You can also consider a schema to be the definition of your data structures within your database.\nJust as our data changes over time with Data Manipulation Language (DML) statements, so does our schema. We need to add more tables, add and remove columns, and so on. The process of evolving our database structure over time is called *schema evolution*.\nSchema evolution uses Data Definition Language (DDL) statements to transition the database structure from one version to the other. The set of statements used in each one of these transitions is called *database migrations*, or simply *migrations*.-   Migrating to Microservices Databases, Chapter 3, Evolving your Relational Database\n**Extensible Data Modeling**\n\nDesigning an extensible, flexible schema that supports user customization is a common requirement, but it's easy to paint yourself into a corner.\nExamples of extensible database requirements:\n-   A database that allows users to declare new fields on demand.\n-   Or an e-commerce catalog with many products, each with distinct attributes.\n-   Or a content management platform that supports extensions for custom data.\nThe solutions we use to meet these requirements is overly complex and the performance is terrible. How should we find the right balance between schema and schemaless database design?\nI'll briefly cover the disadvantages of Entity-Attribute-Value (EAV), a problematic design that's an example of the antipattern called the Inner-Platform Effect, That is, modeling an attribute-management system on top of the RDBMS architecture, which already provides attributes through columns, data types, and constraints.\nThen we'll discuss the pros and cons of alternative data modeling patterns, with respect to developer productivity, data integrity, storage efficiency and query performance, and ease of extensibility.\n-   Class Table Inheritance\n-   Serialized BLOB\n-   Inverted Indexing\n**Extensibility**\n-   How can we add new attributes without the pain of schema changes?\n-   Especially to support user-defined attributes at runtime or after deployment\n-   Solution\n    -   **Extra Columns**\n\n![TABLE id int. (11) title text imdb index Title ( NOT NULL AUTO INCREMENT PRIMARY KEY, NOT NULL, varchar(12) DEFAULT NULL, kind_id int(ll) NULL, production_year int ( II ) DEFAULT NULL, imdb_id int(ll) DEFAULT NULL, phonetic code varchar( 5 ) extra extra extra extra extra extra dat a 1 data2 data3 data4 data5 data6 TEXT DEFAULT TEXT DEFAULT TEXT DEFAULT TEXT DEFAULT TEXT DEFAULT TEXT DEFAULT DEPAtLT NULL, NULL, NULL, NULL, NULL, NULL, NULL, use for whatever comes up that we ddnt think of at the Start of the ](media/Data-Modeling-image3.jpg)\n-   **Entity-Attribute-Value**\n\nStore each attribute in a row instead of a column\n\n![CREATE TABLE Attributes ( entity INT NOT NULL, attribute VARCHAR(20) NOT NULL, value TEXT , FOREIGN KEY (entity) REFERENCES Title (id) ](media/Data-Modeling-image4.jpeg)\n![SELECT * entity 207468 207468 207468 207468 FROM Attributes ; attribute title production_year rating length value Gold f in ger 1964 7.8 110 min ](media/Data-Modeling-image5.jpeg)\n-   **Class Table Inheritance**\n    -   Some attributes apply to all, other attributes apply to one subtype or the other\n    -   Title table\n\n![CREATE TABLE id int (11) title text imdb index Title ( NOT NULL AUTO INCREMENT PRIMARY KEY, NOT NULL, varchar(12) DEFAULT NULL, kind id int(ll) NOT NULL, production _ year int (11 ) DEFAULT NULL, imdb id int(ll) DEFAULT NULL, phonetic code varchar( 5 ) DEFAULT NULL , episode of_id int( 11) DEFAULT NULL, season nr int( 11) DEFAULT NULL, only for tv shows episode_nr int( 11) DEFAULT NULL, series_years varchar( 49) DEFAULT NULL, title crc32 int( 10) unsigned DEFAULT NULL ](media/Data-Modeling-image6.jpeg)\n-   Title table with subtype tables\n\n![CREATE TABLE Title ( id int(ll) NOT NULL AUTO INCREMENT PRIMARY KEY, title text NOT NULL, imdb index varehar(12) DEFAULT NULL, kind id int(ll) NOT NULL, production_year int(ll) DEFAULT NULL, imdb id int(ll) DEFAULT NULL, phonetie_eode varchar( 5) DEFAULT NULL, title_erc32 int(10) unsigned DEFAULT NULL, PRIMARY KEY (id) i; CREATE TABLE Film ( id int(ll) NOT NULL PRIMARY KEY , aspect _ ratio varchar(20) , FOREIGN KEY (id) REFERENCES CREATE TABLE TVShow ( id int(ll) NOT NULL PRIMARY KEY episode_of id int(ll) DEFAULT NULL, season_nr int(ll) DEFAULT NULL, episode nr int(ll) DEFAULT NULL, series_years varchar(49) DEFAULT NULL, FOREIGN KEY (id) REFERENCES Title( {d) i; ](media/Data-Modeling-image7.jpeg)\n-   Pros\n    -   Best to support a finite set of subtypes, which are likely unchanging after creation\n    -   Data types and contraints work normally\n    -   Easy to create or drop subtype tables\n    -   Easy to query attributes common to all subtypes\n    -   Subtype tables are shorter, indexes are smaller\n-   Cons\n    -   Adding one entry takes two INSERT statements\n    -   Querying attributes of subtypes requires a join\n    -   Querying all types with subtype attributes requires multiple joins (as many as subtypes)\n    -   Adding a common attribute locks a large table\n    -   Ading an attribute to a populated subtype locks a smaller table-   Serialized LOB & Inverted Indexes\n-   Online Schema Changes\n-   Non-relational databases\n\n"},{"fields":{"slug":"/Databases/Modeling/Database-Workloads/","title":"Database Workloads"},"frontmatter":{"draft":false},"rawBody":"# Database Workloads\n\nCreated: 2018-02-06 10:56:25 +0500\n\nModified: 2020-03-01 00:04:42 +0500\n\n---\n\n**OLTP (On-line Transaction Processing)**\n\n**Transactions**\n\n**groups of read and write requests that occur together as atomic units**\nOLTP is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). The main emphasis for OLTP systems is put on very fast query processing, maintaining data integrity in multi-access environments and an effectiveness measured by number of transactions per second. In OLTP database there is detailed and current data, and schema used to store transactional databases is the entity model (usually 3NF).-   Most likely to have data from the past hour\n-   Data is inserted and updated more often\n-   Typically uses an operational database\n**OLAP (On-line Analytical Processing)**\n\nis characterized by relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems a response time is an effectiveness measure. OLAP applications are widely used by Data Mining techniques. In OLAP database there is aggregated, historical data, stored in multi-dimensional schemas (usually star schema).-   Typically uses a data warehouse\n-   Help business with decision making and problem solving\n-   Queries a large amount of data\nThe following table summarizes the major differences between OLTP and OLAP system design.\n\n<table>\n<colgroup>\n<col style=\"width: 14%\" />\n<col style=\"width: 40%\" />\n<col style=\"width: 44%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th><p><strong>OLTP System</strong></p>\n<p><strong>Online Transaction Processing</strong></p>\n<p><strong>(Operational System)</strong></p></th>\n<th><p><strong>OLAP System</strong></p>\n<p><strong>Online Analytical Processing</strong></p>\n<p><strong>(Data Warehouse)</strong></p></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>Source of data</td>\n<td>Operational data; OLTPs are the original source of the data.</td>\n<td>Consolidation data; OLAP data comes from the various OLTP Databases</td>\n</tr>\n<tr class=\"even\">\n<td>Purpose of data</td>\n<td>To control and run fundamental business tasks</td>\n<td>To help with planning, problem solving, and decision support</td>\n</tr>\n<tr class=\"odd\">\n<td>What the data</td>\n<td>Reveals a snapshot of ongoing business processes</td>\n<td>Multi-dimensional views of various kinds of business activities</td>\n</tr>\n<tr class=\"even\">\n<td>Inserts and Updates</td>\n<td>Short and fast inserts and updates initiated by end users</td>\n<td>Periodic long-running batch jobs refresh the data</td>\n</tr>\n<tr class=\"odd\">\n<td>Queries</td>\n<td>Relatively standardized and simple queries Returning relatively few records</td>\n<td>Often complex queries involving aggregations</td>\n</tr>\n<tr class=\"even\">\n<td>Processing Speed</td>\n<td>Typically very fast</td>\n<td>Depends on the amount of data involved; batch data refreshes and complex queries may take many hours; query speed can be improved by creating indexes</td>\n</tr>\n<tr class=\"odd\">\n<td>Space Requirements</td>\n<td>Can be relatively small if historical data is archived</td>\n<td>Larger due to the existence of aggregation structures and history data; requires more indexes than OLTP</td>\n</tr>\n<tr class=\"even\">\n<td>Database Design</td>\n<td>Highly normalized with many tables</td>\n<td>Typically de-normalized with fewer tables; use of star and/or snowflake schemas</td>\n</tr>\n<tr class=\"odd\">\n<td>Backup and Recovery</td>\n<td>Backup religiously; operational data is critical to run the business, data loss is likely to entail significant monetary loss and legal liability</td>\n<td>Instead of regular backups, some environments may consider simply reloading the OLTP data as a recovery method</td>\n</tr>\n</tbody>\n</table>\n1.  ClickHouse\n\n2.  Druid\n\n3.  Pinot\n<https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7>\n\n## RTAP - Real Time Analytic Processing**\n\nStream processing\n**HTAP - Hybrid Transaction + Analytical Processing**\n\nOLTP + OLAP together on the same database instance\n"},{"fields":{"slug":"/Databases/Modeling/ER---Tools/","title":"ER - Tools"},"frontmatter":{"draft":false},"rawBody":"# ER - Tools\n\nCreated: 2021-12-02 23:32:44 +0500\n\nModified: 2022-01-27 23:19:24 +0500\n\n---\n\n**<https://drawsql.app> (Great)**\n\n<https://drawsql.app/templates>\n\n<https://dbdiagram.io>\n\n<https://dbdiagram.io/d/60354600fcdcb6230b212562>\n\n<https://dataedo.com>\n\n[~~https://drawerd.com/~~](https://drawerd.com/)\n\n<https://dbdocs.io>\n\n<https://erdplus.com>\n\n<https://dbschema.com> (local, no web)\n\n<https://sqldbm.com> (Best but too costly)\n\n<https://www.quickdatabasediagrams.com>\n\n## Oracle EBS docs (Oracle E-Business Suite)**\n\nOracle E-Business Suite (EBS) applications technical documentation. EBS is an ERP application for enterprises with huge database (24 thousand tables). Its documentation is broken down to application (a module), each having its own document. Each document hasHigh Level Design and Detailed Design.\nHigh Level Design has a list of small diagrams each showing entities and relationships between them of one aspect/topic of the module. Diagrams only include primary key columns.\nLow level design section includes a list of tables with a description of their purpose, definition of foreign keys and a list of columns with the description of their purpose.\n![suO ä¸€ ä¸€ åŒ• 5a9 uo ä¸” ä¸€ åŒ• 5a9 Ã¶lqe.L 083 ](media/ER---Tools-image1.png)\nData warehouse included many tables from multiple modules (accounts payable, accounts receivable, general ledger, projects, purchasing, inventory and a few more) and this documentation can save weeks or months of figuring out where the data was, the purpose of columns, how to join tables, or which views to use.\n<https://www.dbml.org>\n\nDBML (Database Markup Language)is an open-source DSL language designed to define and document database schemas and structures. It is designed to be simple, consistent and highly-readable.\n\n"},{"fields":{"slug":"/Databases/Modeling/ER-Diagrams-(Entity-Relationships)/","title":"ER Diagrams (Entity Relationships)"},"frontmatter":{"draft":false},"rawBody":"# ER Diagrams (Entity Relationships)\n\nCreated: 2020-02-04 17:41:30 +0500\n\nModified: 2021-12-02 23:32:42 +0500\n\n---\n\nAn**entity--relationship model**(or**ER model**) describes interrelated things of interest in a specific domain of knowledge. A basic ER model is composed of entity types (which classify the things of interest) and specifies relationships that can exist between[entities](https://en.wiktionary.org/wiki/entity)(instances of those entity types).\n![AcctName Password LastSignedOn SbscrbrName SbscrbrAddress SbscrbrEMail SbscrbrPhone AcctCreatedOn CreepName HitPoints Mana Attack LastPlayed CreatedOn GharName Level ExpPoints Type n Has MaxHitPoints MaxMana CurrHitPoints CurrMana n 1 Contains Account m Ranlnto n Creep 1 1 IDNum Modifier n IsType Character 1 Carrying Item Instantiation Can-ying 1 Creep Instantiation IDNum n IsType WhenCreated 1 ReqionName Climate Precipitation Foliage Playersln Region ItemName Item Item Type ItemDamage 1 Contains ](media/ER-Diagrams-(Entity-Relationships)-image1.png)\n\nAn entity--relationship diagram for a[MMORPG](https://en.wikipedia.org/wiki/MMORPG)using Chen's notation.\nIn[software engineering](https://en.wikipedia.org/wiki/Software_engineering), an ER model is commonly formed to represent things a business needs to remember in order to perform business processes. Consequently, the ER model becomes an abstract[data model](https://en.wikipedia.org/wiki/Data_modeling), that defines a data or information structure which can be implemented in a[database](https://en.wikipedia.org/wiki/Database), typically a[relational database](https://en.wikipedia.org/wiki/Relational_database).\nEntity--relationship modeling was developed for database and design by[Peter Chen](https://en.wikipedia.org/wiki/Peter_Chen)and published in a 1976 paper.However, variants of the idea existed previously.Some ER models show super and subtype entities connected by generalization-specialization relationships,and an ER model can be used also in the specification of domain-specific[ontologies](https://en.wikipedia.org/wiki/Ontology_(computer_science)).\n<https://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model>\nThe ER model defines the conceptual view of a database. It works around real-world entities and the associations among them. At view level, the ER model is considered a good option for designing databases.\n**Entity**\n\nAn entity can be a real-world object, either animate or inanimate, that can be easily identifiable. For example, in a school database, students, teachers, classes, and courses offered can be considered as entities. All these entities have some attributes or properties that give them their identity.\nAn entity set is a collection of similar types of entities. An entity set may contain entities with attribute sharing similar values. For example, a Students set may contain all the students of a school; likewise a Teachers set may contain all the teachers of a school from all faculties. Entity sets need not be disjoint.\n**Attributes**\n\nEntities are represented by means of their properties, calledattributes. All attributes have values. For example, a student entity may have name, class, and age as attributes.\nThere exists a domain or range of values that can be assigned to attributes. For example, a student's name cannot be a numeric value. It has to be alphabetic. A student's age cannot be negative, etc.\nTypes of Attributes\n-   Simple attributeâˆ’ Simple attributes are atomic values, which cannot be divided further. For example, a student's phone number is an atomic value of 10 digits.\n-   Composite attributeâˆ’ Composite attributes are made of more than one simple attribute. For example, a student's complete name may have first_name and last_name.\n-   Derived attributeâˆ’ Derived attributes are the attributes that do not exist in the physical database, but their values are derived from other attributes present in the database. For example, average_salary in a department should not be saved directly in the database, instead it can be derived. For another example, age can be derived from data_of_birth.\n-   Single-value attributeâˆ’ Single-value attributes contain single value. For example âˆ’ Social_Security_Number.\n-   Multi-value attributeâˆ’ Multi-value attributes may contain more than one values. For example, a person can have more than one phone number, email_address, etc.\nThese attribute types can come together in a way like âˆ’\n-   simple single-valued attributes\n-   simple multi-valued attributes\n-   composite single-valued attributes\n-   composite multi-valued attributes\n**Entity-Set and Keys**\n\nKey is an attribute or collection of attributes that uniquely identifies an entity among entity set.\nFor example, the roll_number of a student makes him/her identifiable among students.\n-   Super Keyâˆ’ A set of attributes (one or more) that collectively identifies an entity in an entity set.\n-   Candidate Keyâˆ’ A minimal super key is called a candidate key. An entity set may have more than one candidate key.\n-   Primary Keyâˆ’ A primary key is one of the candidate keys chosen by the database designer to uniquely identify the entity set.\n**Relationship**\n\nThe association among entities is called a relationship. For example, an employeeworks_ata department, a studentenrollsin a course. Here, Works_at and Enrolls are called relationships.\n**Relationship Set**\n\nA set of relationships of similar type is called a relationship set. Like entities, a relationship too can have attributes. These attributes are calleddescriptive attributes.\n**Degree of Relationship**\n\nThe number of participating entities in a relationship defines the degree of the relationship.\n-   Binary = degree 2\n-   Ternary = degree 3\n-   n-ary = degree\n**Mapping Cardinalities**\n\nCardinalitydefines the number of entities in one entity set, which can be associated with the number of entities of other set via relationship set.\n-   One-to-oneâˆ’ One entity from entity set A can be associated with at most one entity of entity set B and vice versa.\n\n![One-to-one relation](media/ER-Diagrams-(Entity-Relationships)-image2.png)\n-   One-to-manyâˆ’ One entity from entity set A can be associated with more than one entities of entity set B however an entity from entity set B, can be associated with at most one entity.\n\n![One-to-many relation](media/ER-Diagrams-(Entity-Relationships)-image3.png)\n-   Many-to-oneâˆ’ More than one entities from entity set A can be associated with at most one entity of entity set B, however an entity from entity set B can be associated with more than one entity from entity set A.\n\n![Many-to-one relation](media/ER-Diagrams-(Entity-Relationships)-image4.png)\n-   Many-to-manyâˆ’ One entity from A can be associated with more than one entity from B and vice versa.\n\n![Many-to-many relation](media/ER-Diagrams-(Entity-Relationships)-image5.png)\n![[User should be able to view/edit his profile containing following information: -pic cm â€¢ On top left we should show user's Profile Photo â€¢ On the right side of Profile pic the name and contact information should be displayed. uses ](media/ER-Diagrams-(Entity-Relationships)-image6.jpeg)<https://www.tutorialspoint.com/dbms/er_model_basic_concepts.htm>\n\n<https://www.geeksforgeeks.org/introduction-of-er-model>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/AWS-DynamoDB/","title":"AWS DynamoDB"},"frontmatter":{"draft":false},"rawBody":"# AWS DynamoDB\n\nCreated: 2020-02-13 22:19:56 +0500\n\nModified: 2021-06-19 10:04:51 +0500\n\n---\n\nDynamoDB is a managed NoSQL database service provided by Amazon Web Services. As it is managed by Amazon, users do not have to worry about operations such as hardware provisioning, configuration, and scaling. The offering primarily targets key-value and document storage.\nAmazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second.\n**History**\n\nAmazon runs an e-commerce platform for millions of concurrent users, so the underlying services powering the platform must be highly reliable and scalable. Failures in infrastructure would have significant financial consequences and would degrade consumer trust in Amazon's platform. Thus, the motivating purpose behind DynamoDB was a data-store used by these services that could keep up in both availability and scalability such that infrastructure failures would not affect user experience.\nThe original concept and implementation of DynamoDB was first introduced in 2007 with a paper titled: \"Dynamo: Amazon's Highly Available Key-value Store\". As DynamoDB is a proprietary, managed service from AWS, they have not published detailed information regarding the internals of the platform since the original paper.\nThe managed DynamoDB service was launched by AWS in January of 2012, with pricing being based on throughput rather than storage.\n**Compression**\n\n[Dictionary Encoding](https://dbdb.io/browse?compression=dictionary-encoding)\nDynamoDB does not natively support compression. However, users can compress large attributes into binary data using compression algorithms like LZO or GZIP.\nThese compression algorithms are available via the AWS SDK.\n**Concurrency Control**\n\n[Optimistic Concurrency Control (OCC)](https://dbdb.io/browse?concurrency-control=optimistic-concurrency-control-occ)\nUsers have the ability to enable Optimistic Locking for DynamoDB using the AWS Java SDK. For example, they can specify a property with the annotation:@DynamoDBVersionAttributewhich is then designated to store the version number used for optimistic locking.\nFor conflict resolution, DynamoDB employs a \"last writer wins\" policy.\n**Data Model**\n\n[Key/Value](https://dbdb.io/browse?data-model=keyvalue)[Document / XML](https://dbdb.io/browse?data-model=document-xml)\nDynamoDB is a schemaless, NoSQL database. Each table requires a primary key to identify its corresponding data item. However, there are no such constraints on non-key attributes.\nDynamoDB can hold both structured or semi-structured data, such as JSON or XML.\n**Foreign Keys**\n\n[Not Supported](https://dbdb.io/browse?foreign-keys=not-supported)\nDynamoDB does not natively support foreign keys as it is not a relational database.\n**Isolation Levels**\n\n[Read Uncommitted](https://dbdb.io/browse?isolation-levels=read-uncommitted)[Read Committed](https://dbdb.io/browse?isolation-levels=read-committed)[Repeatable Read](https://dbdb.io/browse?isolation-levels=repeatable-read)\nAlthough DynamoDB was originally not designed for transactions, AWS has subsequently published a[library](https://github.com/awslabs/dynamodb-transactions)that supports them. With the library, DynamoDB has three configurable isolation levels for transactions: Read Uncommitted, Read Committed, and Repeatable Reads. However, neither DynamoDB nor the library support locking for range queries, thus phantom reads can potentially result in phantom reads.\n**Joins**\n\n[Not Supported](https://dbdb.io/browse?joins=not-supported)\nDynamoDB does not support joins natively. One can join DynamoDB tables using Amazon's Elastic MapReduce (EMR) service.\n**Logging**\n\n[Not Supported](https://dbdb.io/browse?logging=not-supported)\nAlthough there are no public implementation details on DynamoDB's internal logging schema, external users can log DynamoDB API operations via AWS CloudTrail.\nWhen enabled, AWS CloudTrail logs the DynamoDB API calls made by the user in a JSON format, then stores it in an Amazon S3 bucket.\n**Query Interface**\n\n[Custom API](https://dbdb.io/browse?query-interface=custom-api)[SQL](https://dbdb.io/browse?query-interface=sql)[Command-line / Shell](https://dbdb.io/browse?query-interface=command-line-shell)\nAs DynamoDB is hosted by AWS and is a NoSQL database, users must query the database through the internet using the DynamoDB (RESTful) API or SQL statements.\nUsers can send SQL statements over the network or use the API endpoints via the AWS Management Console, the AWS CLI, or the AWS SDKs.\n**Storage Architecture**\n\n[Hybrid](https://dbdb.io/browse?storage-architecture=hybrid)\nDynamoDB supports a pluggable local persistence engine that can range from in-memory buffers with persistent backing to purely disk-oriented ones. This allows it to be flexible with an application's access patterns.\nKnown storage engines used by DynamoDB include the Berkeley Database Transactional Data Store and MySQL.\n**Storage Model**\n\n[N-ary Storage Model (Row/Record)](https://dbdb.io/browse?storage-model=n-ary-storage-model-rowrecord)\nDynamoDB uses a key-value storage model.\n**Stored Procedures**\n\n[Not Supported](https://dbdb.io/browse?stored-procedures=not-supported)\nAlthough DynamoDB doesn't natively support stored procedures, users can use AWS Lambdas for a similar effect.\nFor example, users can have Lambda functions process incoming DynamoDB stream data and have it be triggered when a specific action is performed on DynamoDB.\n**System Architecture**\n\n[Shared-Nothing](https://dbdb.io/browse?system-architecture=shared-nothing)\nThe DynamoDB system architecture is tailored for high availability and scalability. The system is distributed across multiple nodes for availability, and it employs[consistent hashing](https://en.wikipedia.org/wiki/Consistent_hashing)to partition data across these nodes.\nFor membership and failure detection, DynamoDB utilizes a[gossip-based protocol](https://dbdb.io/db/(https:/en.wikipedia.org/wiki/Gossip_protocol). This allows DynamoDB to be decentralized and require minimal manual administration. To handle temporary failures, DynamoDB uses a Sloppy Quorum with hinted hand-off. This technique allows DynamoDB to be available and durable despite network partitions and server failures.\n**Views**\n\n[Not Supported](https://dbdb.io/browse?views=not-supported)\nDynamoDB does not natively support materialized or virtual views. However, customers can use DynamoDB Streams for Cross-Region Replication for similar use cases as materialized views.\nFurthermore, AWS encourages the use of DynamoDB as a materialized view for relational systems.\n<https://dbdb.io/db/dynamodb>\n\n<https://medium.com/expedia-group-tech/dynamodb-data-modeling-c4b02729ac08>\n\n<https://aws.amazon.com/about-aws/whats-new/2021/05/learn-how-to-develop-applications-with-amazon-dynamodb>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Cassandra/","title":"Cassandra"},"frontmatter":{"draft":false},"rawBody":"# Cassandra\n\nCreated: 2019-03-17 19:09:02 +0500\n\nModified: 2021-11-19 17:58:08 +0500\n\n---\n\nApache Cassandrais a[free and open-source](https://en.wikipedia.org/wiki/Free_and_open-source_software),[distributed](https://en.wikipedia.org/wiki/Distributed_database),[wide column store](https://en.wikipedia.org/wiki/Wide_column_store), [NoSQL](https://en.wikipedia.org/wiki/NoSQL)[database](https://en.wikipedia.org/wiki/Database) management system designed to handle large amounts of data across many[commodity servers](https://en.wikipedia.org/wiki/Commodity_computing), providing high availability with no[single point of failure](https://en.wikipedia.org/wiki/Single_point_of_failure). Cassandra offers robust support for [clusters](https://en.wikipedia.org/wiki/Computer_cluster) spanning multiple datacenters,with asynchronous masterless replication allowing low latency operations for all clients.\n-   Intented to run in a datacenter (and also across DCs)\n-   Originally designed at Facebook\n-   Open-sourced later, today an Apache project\n-   Is a highly scalable, distributed and high-performance NoSQL database. Cassandra is designed to handle a huge amount of data\n-   Cassandra handles the huge amount of data with its distributed architecture\n-   Data is placed on different machines with more than one replication factor that provides high availability and no single point of failure\nUnder hood cassandra uses **consistent hashing to shard your data** and also use **gossiping to keep all the nodes informed about the cluster.**\n**Features**\n\n1.  Distributed columnar data store\n\nEvery node in the cluster has the same role. There is no single point of failure. Data is distributed across the cluster (so each node contains different data), but there is no master as every node can service any request.\n\n2.  Supports replication and multi data center replication\n\nReplication strategies are configurable. Cassandra is designed as a distributed system, for deployment of large numbers of nodes across multiple data centers. Key features of Cassandra's distributed architecture are specifically tailored for multiple-data center deployment, for redundancy, for failover and disaster recovery.\n\n3.  Scalability\n\nDesigned to have read and write throughput both increase linearly as new machines are added, with the aim of no downtime or interruption to applications.\n\n4.  Fault-tolerant (No single point of failure)\n\nData is automatically replicated to multiple nodes for[fault-tolerance](https://en.wikipedia.org/wiki/Fault-tolerance).[Replication](https://en.wikipedia.org/wiki/Replication_(computer_science))across multiple data centers is supported. Failed nodes can be replaced with no downtime.\n\n5.  Tunable consistency\n\nCassandra is typically classified as an[AP system](https://en.wikipedia.org/wiki/CAP_theorem), meaning that availability and partition tolerance are generally considered to be more important than consistency in Cassandra, Writes and reads offer a tunable level of[consistency](https://en.wikipedia.org/wiki/Consistency_(database_systems)), all the way from \"writes never fail\" to \"block for all replicas to be readable\", with the[quorum level](https://en.wikipedia.org/wiki/Quorum_(distributed_computing))in the middle.\n\n**Can provide both eventual and strong consistency. (Local Quorum -** This many nodes must be written out to transaction to be successful**)**\n\n6.  MapReduce support\n\nCassandra has[Hadoop](https://en.wikipedia.org/wiki/Hadoop)integration, with[MapReduce](https://en.wikipedia.org/wiki/MapReduce)support. There is support also for[Apache Pig](https://en.wikipedia.org/wiki/Pig_(programming_tool))and[Apache Hive](https://en.wikipedia.org/wiki/Apache_Hive).\n\n7.  Query language\n\nCassandra introduced the Cassandra Query Language (CQL). CQL is a simple interface for accessing Cassandra, as an alternative to the traditional[Structured Query Language](https://en.wikipedia.org/wiki/SQL)(SQL).\n\n8.  Time-series data\n\n9.  Optimized for availability (through tunable consistency)\n\n10. Optimized for writes\n\n11. Easily maintainable\n\n12. Almost infinitely scalable\n**Cassandra Data Model**\n\n1.  KeySpace\n\nA keyspace in Cassandra is a namespace that defines data replication on nodes. A cluster contains one keyspace per node.\n\n2.  Column Family\n\n3.  Rows\n\n4.  Column\n\n5.  SuperColumn\n<https://www.tutorialspoint.com/cassandra/cassandra_data_model.htm>\nCassandra is fundamentally a key-value store and distributes data around the cluster by a **PARTITION KEY**, then sorts the data on that partition (or row) by the **CLUSTERING key**. Adding new data to that row is almost free, and updates are handled by marking the previous cell value with a **tombstone** and adding the new value to the row. Eventually, you will need to **compact** these partitions as data becomes fragmented over multiple files, but remember that you are amortizing your INSERTs and UPDATEs over time with almost instantaneous commits. This makes scanning a single partition or row very fast as the disk head only performs a single seek operation. However, if you want more than a single Cassandra partition, performance goes south fairly quickly as **scatter/gather** queries are an **anti-pattern**, and[secondary indexes](https://www.datastax.com/dev/blog/cassandra-native-secondary-index-deep-dive)are only useful in extremely rare and specific occasions. Therefore, when you know what partition you want to scan, and you don't want to do any aggregations, GROUPBYs, or any other more analytical operations, then you are in good shape. The result is that Cassandra is great for small, tightly constrained, well-known queries and high-volume inserts and updates.\n**Node**\n-   6000 - 12000 transactions/second/core\n-   2 - 4 TB\nCassandra vs RDBMS\n-   On > 50 GB data\n-   MySQL\n    -   Writes 300 ms avg\n    -   Reads 350 ms avg\n-   Cassandra\n    -   Writes 0.12 ms avg\n    -   Reads 15 ms avg\nAlternative - ScyllaDB\n\n![Client ](media/Cassandra-image1.jpg)\n\nIn the image above, circles are Cassandra nodes and lines between the circles shows distributed architecture, while the client is sending data to the node\n**References**\n\n[**https://academy.datastax.com/**](https://academy.datastax.com/)\n\n<https://en.wikipedia.org/wiki/Apache_Cassandra>\n\n<http://cassandra.apache.org>\n\n<https://www.tutorialspoint.com/cassandra/index.htm>\n\n<https://www.freecodecamp.org/news/the-apache-cassandra-beginner-tutorial>\n\n[Introduction to Apache Cassandraâ„¢ + What's New in 4.0 by Patrick McFadin | DataStax Presents](https://www.youtube.com/watch?v=d7o6a75sfY0)\n\n[Cassandra Tutorial Videos](https://www.youtube.com/playlist?list=PL9ooVrP1hQOGJ4Yz9vbytkRmLaD6weg8k)\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Column-family/","title":"Column family"},"frontmatter":{"draft":false},"rawBody":"# Column family\n\nCreated: 2019-12-22 20:42:20 +0500\n\nModified: 2020-03-01 00:11:00 +0500\n\n---\n\nColumn-family databases, also called non-relational column stores, wide-column databases, or simply column databases, are perhaps the NoSQL type that, on the surface, looks most similar to relational databases. Like relational databases, wide-column databases store data using concepts like rows and columns. However, in wide-column databases, the association between these elements is very different from how relational databases use them.\nIn relational databases, a schema defines the column layout in a table by specifying what columns the table will have, their respective data types, and other criteria. All of the rows in a table must conform to this fixed schema.\n\nInstead of tables, column-family databases have structures calledcolumn families. Column families contain rows of data, each of which define their own format. A row is composed of a unique row identifier --- used to locate the row --- followed by sets of column names and values.\nWith this design, each row in a column family defines its own schema. That schema can be easily modified because it only affects that single row of data. Each row can have different numbers of columns with different types of data. Sometimes it helps to think of column family databases as key-value databases where each key (row identifier) returns a dictionary of arbitrary attributes and their values (the column names and their values).\n\n![Diagram of column-family database structure](media/Column-family-image1.png)\n\nColumn-family databases are good when working with applications that requires great performance for row-based operations and highly scalability. Since all of the data and metadata for an entry is accessible with a single row identifier, no computationally expensive joins are required to find and pull the information. The database system also typically makes sure all of the data in a row is collocated on the same machine in a cluster, simplifying data sharding and scaling.\nHowever, column-family databases do not work well in all scenarios. If you have highly relational data that requires joins, this is not the right type of database for your application. Column-family databases are firmly oriented around row-based operations. This means that aggregate queries like summing, averaging, and other analytics-oriented processes can be difficult or impossible. This can have a great impact on how you design your applications and what types of usage patterns you can use.\nExamples:\n-   [Cassandra](https://cassandra.apache.org/)\n-   [HBase](https://hbase.apache.org/)\n\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Druid/","title":"Druid"},"frontmatter":{"draft":false},"rawBody":"# Druid\n\nCreated: 2019-02-06 11:04:11 +0500\n\nModified: 2021-01-29 02:06:32 +0500\n\n---\n\nApache Druid (incubating) is a real-time analytics database designed for fast slice-and-dice analytics (\"[OLAP](http://en.wikipedia.org/wiki/Online_analytical_processing)\" queries) on large data sets. Druid is most often used as a database for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.-   **High performance, column oriented, distributed data store**\n-   Druid is primarily used to store, query, and analyze large event streams\n-   Druid is optimized for sub-second queries to slice-and-dice, drill down, search, filter, and aggregate this data. Druid is commonly used to power interactive applications where performance, concurrency, and uptime are important.\n**Features**\n-   Column-oriented storage\n-   Native search indexes\n\nDruid uses[CONCISE](https://arxiv.org/pdf/1004.0403)or[Roaring](https://roaringbitmap.org/)compressed bitmap indexes to create indexes that power fast filtering and searching across multiple columns.\n-   Scalable distributed system\n-   Massively parallel processing\n-   Streaming and batch ingest\n-   Self-healing, self-balancing, easy to operate\n\nAs an operator, to scale the cluster out or in, simply add or remove servers and the cluster will rebalance itself automatically, in the background, without any downtime. If any Druid servers fail, the system will automatically route around the damage until those servers can be replaced. Druid is designed to run 24/7 with no need for planned downtimes for any reason, including configuration changes and software updates.\n-   Cloud-native, fault-tolerant architecture that won't lose data\n\nOnce Druid has ingested your data, a copy is stored safely in[deep storage](http://druid.io/docs/latest/design/index.html#deep-storage)(typically cloud storage, HDFS, or a shared filesystem). Your data can be recovered from deep storage even if every single Druid server fails. For more limited failures affecting just a few Druid servers, replication ensures that queries are still possible while the system recovers.\n-   Approximation algorithms\n-   Automatic summarization at ingest time\n-   Flexible schemas\n-   Time-optimized partitioning\n-   SQL support\n-   Horizontally scalable\n**Applications**\n-   Clickstream analytics\n-   Network telemetry analytics (network performance monitoring)\n-   Server metrics storage\n-   Supply chain analytics (manufacturing metrics)\n-   Application performance metrics\n-   Digital marketing/advertising analytics\n-   Business intelligence / OLAP\n**Druid Important Points**\n\nDruid does not natively support nested data, so, we need to flatten arrays in our JSON events by providing a[flattenspec](https://druid.apache.org/docs/latest/ingestion/index.html#flattenspec), or by doing some preprocessing before the event lands in it.\nDruid assigns types to columns --- string, long, float, complex, etc. The type enforcement at the column level can be restrictive if the incoming data presents with mixed types for a particular field/fields. Each column except the timestamp can be of type dimension or metric.\nOne can filter and group by on dimension columns, but not on metric columns. This needs some forethought when picking which columns to pre-aggregate and which ones will be used for slice-and-dice analyses.\nPartition keys must be picked carefully for load-balancing and scaling up. Streaming new updates to the table after creation requires using one of the[supported ways of ingesting](https://druid.apache.org/docs/latest/ingestion/index.html#streaming)--- Kafka, Kinesis, or Tranquility.\nDruid works well for event analytics in environments where data is somewhat predictable and rollups and pre-aggregations can be defined a priori. It involves some maintenance and tuning overhead in terms of engineering, but for event analytics that doesn't involve complex joins, it can serve queries with low latency and scale up as required.\nDruid is fundamentally a column store, and is designed for analytical queries (GROUPBYs with complex WHERE clauses) that need to scan across multiple partitions. Druid stores its index in[segment files](http://druid.io/docs/latest/design/segments.html), which are partitioned by time. Segment files are columnar, with the data for each column laid out in **separate data structures**. By storing each column separately, Druid can decrease query latency by scanning only those columns that are required for a query. There are different column types for different data types (string, numbers, etc.). Different columns can have different encoding and compression algorithms applied. For example, string columns will be dictionary encoded, LZF compressed, and have search indexes created for faster filtering. Numeric columns will have completely different compression and encoding algorithms applied. Druid segments are immutable once finalized, so updates in Druid have limitations. Although more recent versions of Druid have added \"lookups\", or the ability to join a mutable table external to Druid with an immutable one in Druid, I would not recommend Druid for any workflows where the same underlying data is frequently updated and those updates need to complete in less than a second (say, powering a social media profile page). Druid supports bulk updates, which are more commonly seen with analytic workloads.\n**Druid File Format**\n\n**Segments**\n-   Datasources are broken into files called segments\n-   Segments are grouped into time chunks and potentially, further partitioned at creation time\n    -   This allows Druid to parallelize ingestion and querying of data\n-   Segments are immutable\n    -   Each segment is versioned to allow new versions to replace old versions\n    -   This can be done as a background task without blocking queries\n-   Druid segments have their own file format\n-   The file format is columnar\n    -   Data is laid out into columns and each column is packed and compressed individually\n    -   String columns are dictionary encoded instead of having the full string value in every row\n    -   Numeric columns use various kinds of numeric compression based on the data\n-   Columnar formats are the standard for analytical workloads due to superior scan performance\n-   String columns in Druid are indexed with bitmap indexes\n-   Columns are further compressed with general-purpose algorithms like LZ4 (Lossless compression)\n-   Example\n\n![Take this table's data as an exam le Timestamp 2019-01-@ITOI : 00: ooz 2019-01 -OITOI : 00:01Z 2019-01-OITOI : 00: loz Druid would save this out on disk as: Domain example.com example.com druid . io Amount 10 20 10 Field Timestamp [2019-01-OITOI Values Array Domain Amount example. com, example.com, druid. iol [10, 20, 10] ](media/Druid-image1.png)\n-   This on-disk format has several benefits:\n    -   Filtering if a domain exists require reading less data\n    -   Compression of like data performs better than a row-oriented format\n    -   Druid only needs to read the columns involved in a query, eliminating extraneous fetches from disk and memory\n-   ![Query Broker Deep Storage Masters Coordinator Overlord If deep storage is not available, the files are already copied to Drud, but new files won't be. Data Historical Only the Historical and MiddleManager nodes interact with deep storage. MiddeManager Segments deleted On Druid servers wont be lost, but data deleted in deep storage wil be lost. Deep Storage (S3, HDFS) ](media/Druid-image2.png)\n**Druid Data Modelling**\n-   Designing a data model for Druid is different than a RDBMS\n-   Most RDBMS design focuses on normalization\n    -   Each piece of data should only appear once in the database\n-   Databases like Druid favor some denormalization\n    -   Each piece of data may appear many different times in the database\n-   Druid favors denormalized data because this is faster than doing a join with another large table\n    -   Flat schemas increase performance by allowing Druid to operate directly on compressed dictionary-encoded data\n-   When designing a table, you must\n    -   Choose the datasource name\n    -   The source for the input data\n    -   The columns you want to store-   **Columns**\n-   Creating a data model is more than just copying the table structure of a RDBMS table\n-   When transitioning from a RDBMS, you will need to choose\n    -   Which columns should be included\n    -   Which types to use for each column (Druid doesn't support all RDBMS types)\n    -   Whether you will be totally flattening your tables, or using Druid's query-time lookups to partially normalize data\n-   In Druid, columns will become\n    -   Dimensions (stored as-is)\n    -   Metrics (partially aggregated)\n-   **Supported Types**\n\n![Type tong float double string complex Description 64 bit signed int 32-bit float 64-bit double UT F-8 string Represents column types provided by sketches and Druid extensions, including hyperUnique, approxHistogram, DataSketches types. Timestamps use the long type and are milliseconds since epoch â€¢ Each row has a primary timestamp that is stored as a long complex types are often used to power approximate computations, such as approximate count distinct, histograms, and quantiles. ](media/Druid-image3.png)\n-   **Multi-value and Nested Dimensions**\n\n![Druid supports multi-value arrays A Druid column can have a value that is an array, like: \"mykey\": [\"valueone\", \"valuetwo\"] Druid does not support nested dimensions Nested data beyond simple lists should be flattened out, either before ingestion, or during ingestion using Druid flattenspecs. ](media/Druid-image4.png)\n-   **Druid Native Batch**\n    -   Native batch is a built-in way to index data in Druid\n    -   Native batch ingestion is useful when you have a file that you want to load into Druid\n    -   There has to be a method to convert your file to Druid's file format and make segments (Native batch performs this conversion)\n    -   Druid also supports batching ingestion with Hadoop\n    -   **Native Batch Architecture**\n\n![Query Broker Masters Overlord 2. The Overlord will instruct MiddleManagers to run the submitted task. 4. MiddleManagers push the segments to deep storage, where they can be loaded by Historicals. 3. MiddleManagers load the data file and create Druid segments. ay happen on MiddleManagers 1. An ingestion spec that uses Native Batch is submitted to the Overlord process. 5. Once Historicals load the segments, they are available for querying. D ta Historical Middle Manager MiddleManager ](media/Druid-image5.png)\n-   **Druid SQL Query API**\n    -   Data stored in Druid can be queried with a SQL API\n    -   These calls go over a HTTP REST API\n        -   The payload of the REST call is JSON\n        -   The query results come back as JSON or CSV\n    -   Not every SQL feature is supported\n    -   The query engine is implemented with Apache Calcite\n    -   The SQL queries are translated into Druid's native queries\n    -   There is a native JSON query API available too\n        -   The SQL API is virtually at parity with the JSON and is easier to use\n    -   Where clauses\n\n![SELECT FROM datasourcename WHERE mydimension = SELECT FROM datasourcename WHERE mydimension = \" somet hing\" \" somet hing\" AND myotherdimension = The timestamp column in Druid is named _ _ time Timestamps can be written out in SQL with TIMESTAMP YYYY-m-DD SELECT * FROM datasourcename WHERE __time = TIMESTAMP '2019-01-01 ](media/Druid-image6.png)\n-   Limiting\n\n![Druid can further optimize queries when a LIMIT is applied â€¢ This makes Druid use less resources while querying data This is a general best practice SELECT * FROM datasourcename LIMIT 10 SELECT * FROM datasourcename WHERE mydimension = \" something\" LIMIT 10 ](media/Druid-image7.png)\n-   Group By Query\n\n![SELECT mydimension, as itemcount FROM datasourcename GROUP BY mydimension SELECT mydimension, anotherdimension, as itemcount FROM datasourcename GROUP BY mydimension, anotherdimension SELECT mydtmenston, SIN( mydimension) as mysum, AVG( mydtmension) as myaverage, COUNT( * ) as itemcount FROM datasourcename GROUP BY mydimens ion ](media/Druid-image8.png)\n**Druid kafka ingestion**\n\n<http://druid.io/docs/latest/development/extensions-core/kafka-ingestion.html>\n\n<http://druid.io/docs/latest/tutorials/tutorial-kafka.html>\n\n## Kafka Indexing Service - Exactly once ingestion**\n\n<https://imply.io/post/exactly-once-streaming-ingestion>\n\n![An indexing task reads a subset of the Kafka topic partitions](media/Druid-image9.png)\n\n![Supervisors live on the Overlord and manage indexing tasks which run on Middle Managers](media/Druid-image10.png)\n![1. All real-time data is initially ingested into Kafka from a producer or Kafka Connect. Kafka Query Broker Masters Coordinator Overlord 2. Data may optionally be enriched or joined by stream processors. 3. Data is then consumed by MiddleManager processes, which will create Druid segments. Data Historical MiddleManager ](media/Druid-image11.png)\n\n**References**\n\n<http://druid.io/docs/latest/design/index.html>\n<https://medium.com/@leventov/the-problems-with-druid-at-large-scale-and-high-load-part-1-714d475e84c9>\n\n<https://medium.com/@leventov/the-challenges-of-running-druid-at-large-scale-and-future-directions-part-2-ef594ce298f2>\n<https://druid.apache.org/docs/latest/ingestion/data-management.html>\n<https://imply.io/druid-university/intro-to-druid-university>\n\n## Imply Druid**\n-   tiered data\n-   data volumes-   Plywood\n    -   queries\n    -   expressions\n-   Pivot - alerts\n-   Measures\n-   Roll up\n-   Virtual columns (middle manager)\n-   Clarity\n-   Imply Manager\n**Others**\n-   Ingestion Spec can handle flattening of schema\n-   Primary partition keys\n-   Secondary partition keys\n-   2 threads in middlemanager can handle 25 GB/hour of ingestion (2 HyperThreads per task)\n-   Per tasks per partition ingest from Kafka\n-   Segment scan rate\n    -   4 GB/s/HT for data sitting in RAM\n    -   0.5 GB/s/HT for data sitting in disk\n-   Druid is not good for computing moving averages\n-   No ML library in Druid\n-   Duplicate data removal using HyperLogLog- <https://cleanprogrammer.net/getting-unique-counts-from-druid-using-hyperloglog>\n-   Druid not build for historical storage\n-   MiddleManager creates segments, and then put it in deep storage, Historical pulls data from deep storage, create segment cache and serves it. Local Cache is there until the retention policy.\n-   Middle manager will skip the data if it's not in the right format. There's no way\n\nto know which data was skipped.\n**Ingestion spec tuning**\n\n1.  Task duration - 60 minutes (PT60M) (current - 5 minutes)\n\n2.  Task completion time - 60 minutes (current - 5 minutes)\n\n3.  Segment size - 5 Million\n\n4.  Segment granularity - 1 day/1hour (current - 1 hour)\n\n5.  Handoff period - currently too large (tune it)"},{"fields":{"slug":"/Databases/NoSQL-Databases/MongoDB/","title":"MongoDB"},"frontmatter":{"draft":false},"rawBody":"# MongoDB\n\nCreated: 2019-05-06 22:11:22 +0500\n\nModified: 2022-02-05 01:24:21 +0500\n\n---\n\n**MongoDB / CouchBase**\n\nJson like structure and you want to persist that than mongodb works fine, provide ACID properties at a document level and scale pretty well\nMongoDB is an open-source document database and leading NoSQL database. MongoDB is written in C++\n**COMPONENTS**\n\nmongod - The database server.\nmongos - Sharding router.\nmongo - The database shell (uses interactive javascript).\n**UTILITIES**\n\nmongodump - Create a binary dump of the contents of a database.\nmongorestore - Restore data from the output created by mongodump.\nmongoexport - Export the contents of a collection to JSON or CSV.\nmongoimport - Import data from JSON, CSV or TSV.\nmongofiles - Put, get and delete files from GridFS.\nmongostat - Show the status of a running mongod/mongos.\nbsondump - Convert BSON files into human-readable formats.\nmongoreplay - Traffic capture and replay tool.\nmongotop - Track time spent reading and writing data.\ninstall_compass - Installs MongoDB Compass for your platform.\n**Storage Engine**\n\nA storage engine is the part of a database that is responsible for managing how data is stored, both in memory and on disk. Many databases support multiple storage engines, where different engines perform better for specific workloads. For example, one storage engine might offer better performance for read-heavy workloads, and another might support a higher throughput for write operations.\nThe[storage engine](https://docs.mongodb.com/manual/reference/glossary/#term-storage-engine)is the component of the database that is responsible for managing how data is stored, both in memory and on disk. MongoDB supports multiple storage engines, as different engines perform better for specific workloads. Choosing the appropriate storage engine for your use case can significantly impact the performance of your applications.\n-   WiredTiger Storage Engine (Default)\n\n[WiredTiger](https://docs.mongodb.com/manual/core/wiredtiger/)is the default storage engine starting in MongoDB 3.2. It is well-suited for most workloads and is recommended for new deployments. WiredTiger provides a document-level concurrency model, checkpointing, and compression, among other features.\n<https://docs.mongodb.com/manual/faq/storage>\n\n<https://docs.mongodb.com/manual/core/storage-engines>\n\n## ODM (Object Document Mapper)**\n**Commands**\n\nuse admin\ndb.createUser(\n{\nuser: 'admin',\npwd: 'password',\nroles: [ { role: 'root', db: 'admin' } ]\n}\n);\nexit;\nConnect to mongodb database\n\nmongo admin --username root --password YOURPASSWORD\n**Mongo vs SQL**\n\n**Pros**\n-   Document oriented\n-   High performance\n-   High availability --- Replication\n-   High scalability -- Sharding\n-   Dynamic --- No rigid schema.\n-   Flexible -- field addition/deletion have less or no impact on the application\n-   Heterogeneous Data\n-   No Joins\n-   Distributed\n-   Data Representation in JSON or BSON\n-   Geospatial support\n-   Easy Integration with BigData Hadoop\n-   Document-based query language that's nearly as powerful as SQL\n-   Cloud distributions such as AWS, Microsoft, RedHat,dotCloud and SoftLayer etc:-. In fact, MongoDB is built for the cloud. Its native scale-out architecture, enabled by 'sharding,' aligns well with the horizontal scaling and agility afforded by cloud computing.\n\n**Cons**\n-   A downside of NoSQL is that most solutions are not as strongly ACID-compliant (Atomic, Consistency, Isolation, Durability) as the more well-established RDBMS systems.\n-   Complex transaction\n-   No function or stored procedure exists where you can bind the logic\n**Implementation**\n\n**Good For**\n-   E-commerce product catalog.\n-   Blogs and content management.\n-   Real-time analytics and high-speed logging, caching, and high scalability.\n-   Configuration management.\n-   Maintaininglocation-based data --- Geospatial data.\n-   Mobile and social networking sites.\n-   Evolving data requirements.\n-   Loosely coupled objectives --- the design may change by over time.\n**Not so Good For**\n-   Highly transactional systems or where the data model is designed up front.\n-   Tightly coupled systems-   MongoDB is not magically faster. If you store the same data, organised in basically the same fashion, and access it exactly the same way, then you really shouldn't expect your results to be wildly different. After all, MySQL and MongoDB are both GPL, so if Mongo had some magically better IO code in it, then the MySQL team could just incorporate it into their codebase.\n\n<https://stackoverflow.com/questions/9702643/mysql-vs-mongodb-1000-reads>\n\nPerformance metrics - <https://stackoverflow.com/a/37654411/5424888>\n-   MongoDB doesn't write the data to the disk straight away which is why it \"looks\" faster, but if your computer crashes, the data is lost.\n**Questions**\n-   How to add access control for some fields in a document (like some users cannot access first_name, last_name in a customer_id document\n**GridFS**\n\n[GridFS](https://docs.mongodb.com/manual/reference/glossary/#term-gridfs)is a specification for storing and retrieving files that exceed the[BSON](https://docs.mongodb.com/manual/reference/glossary/#term-bson)-document[size limit](https://docs.mongodb.com/manual/reference/limits/#limit-bson-document-size)of 16 MB.\n<https://docs.mongodb.com/manual/core/gridfs>\n\n## UI / Tools**\n\n<https://www.mongodb.com/products/compass>\n\n<https://studio3t.com>\n\n## Others**\n\n<https://www.ferretdb.io> - A truly Open Source MongoDB alternative\n**References**\n\n<https://www.tutorialspoint.com/mongodb>\n\n<https://github.com/mongodb/mongo>\n\n<https://www.toptal.com/mongodb/interview-questions>\n\n<https://github.com/ramnes/awesome-mongodb>\n[**https://university.mongodb.com/**](https://university.mongodb.com/)\n[**https://medium.com/swlh/mongodb-developer-roadmap-for-2021-bec3eb10891d**](https://medium.com/swlh/mongodb-developer-roadmap-for-2021-bec3eb10891d)\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/NoSQL-Databases/","title":"NoSQL Databases"},"frontmatter":{"draft":false},"rawBody":"# NoSQL Databases\n\nCreated: 2020-04-04 15:18:07 +0500\n\nModified: 2021-08-08 09:17:03 +0500\n\n---\n\n**Aerospike**\n\nAerospike is a distributed key-value DBMS. It is mainly targeted at OLTP workloads with large number of transactions.\nDrop-in replacement for redis\n<https://dbdb.io/db/aerospike>\n\n<https://www.aerospike.com>\n\n## bbolt / BoltDB**\n\nAn embedded key/value database for Go.\n\n<https://dbdb.io/db/boltdb>\n\n<https://github.com/etcd-io/bbolt>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/","title":"Redis"},"frontmatter":{"draft":false},"rawBody":"# Redis\n\nCreated: 2019-05-08 16:24:51 +0500\n\nModified: 2022-12-11 13:47:19 +0500\n\n---\n\n**Redis (Remote Dictionary Service)**\nRedis is an open source (BSD licensed), extremely fast, in-memory data structure store, used as a database, cache and message broker. It can optionally persist to a disk also. It supports different data structures like simple key-value pairs, sets, queues, strings, hashes, lists, sorted sets with range queries, bitmaps, hyprloglogs and geospatial indexes with radis queries. Redis has built-in replication, Lua scripting, LRU eviction, transactions and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster\nYou can runatomic operationson these types, like[appending to a string](https://redis.io/commands/append);[incrementing the value in a hash](https://redis.io/commands/hincrby);[pushing an element to a list](https://redis.io/commands/lpush);[computing set intersection](https://redis.io/commands/sinter),[union](https://redis.io/commands/sunion)and[difference](https://redis.io/commands/sdiff); or[getting the member with highest ranking in a sorted set](https://redis.io/commands/zrangebyscore).\nIn order to achieve its outstanding performance, Redis works with anin-memory dataset. Depending on your use case, you can persist it either by[dumping the dataset to disk](https://redis.io/topics/persistence#snapshotting)every once in a while, or by[appending each command to a log](https://redis.io/topics/persistence#append-only-file). Persistence can be optionally disabled, if you just need a feature-rich, networked, in-memory cache.\nRedis also supports trivial-to-setup[master-slave asynchronous replication](https://redis.io/topics/replication), with very fast non-blocking first synchronization, auto-reconnection with partial resynchronization on net split.\n**Features**\n-   Distributed Cache\n-   Holds all data in-memory\n-   Can also flush data into hard-drive\n-   Master-slave, slaves can hold same data as master\n-   Redis can also do key-value storage\n**Other features include**\n-   [Transactions](https://redis.io/topics/transactions)\n-   [Pub/Sub](https://redis.io/topics/pubsub)\n    -   Channels\n    -   PUSH model\n-   [Lua scripting](https://redis.io/commands/eval)\n-   [Keys with a limited time-to-live](https://redis.io/commands/expire)\n-   [LRU eviction of keys](https://redis.io/topics/lru-cache)\n-   [Automatic failover](https://redis.io/topics/sentinel)\nRedis is written inANSI Cand works in most POSIX systems like Linux, *BSD, OS X without external dependencies.\n**Advantages**\n-   **Exceptionally fastâˆ’** Redis is very fast and can perform about 110000 SETs per second, about 81000 GETs per second.\n-   **Supports rich data typesâˆ’** Redis natively supports most of the datatypes that developers already know such as list, set, sorted set, and hashes. This makes it easy to solve a variety of problems as we know which problem can be handled better by which data type.\n-   **Operations are atomicâˆ’** All Redis operations are atomic, which ensures that if two clients concurrently access, Redis server will receive the updated value.\n-   **Multi-utility toolâˆ’** Redis is a multi-utility tool and can be used in a number of use cases such as caching, messaging-queues (Redis natively supports Publish/Subscribe), any short-lived data in your application, such as web application sessions, web page hit counts, etc.\n**Optional Durability**\n-   Journaling (append only log AOL)\n-   Snapshotting\n-   Both happens asynchronously in the background\n**Transport Protocol**\n-   TCP\n-   Request / response just like HTTP\n-   Message format is RESP (REdis Serialization Protocol)\n**Replication/Clustering**\n\n**Default: Master-Slave**\n\nWhen installing the chart withcluster.enabled=true, it will deploy a Redis master StatefulSet (only one master node allowed) and a Redis slave StatefulSet. The slaves will be read-replicas of the master. Two services will be exposed:\n-   Redis Master service: Points to the master, where read-write operations can be performed\n-   Redis Slave service: Points to the slaves, where only read operations are allowed.\nIn case the master crashes, the slaves will wait until the master node is respawned again by the Kubernetes Controller Manager.\n**Master-Slave with Sentinel**\n\nWhen installing the chart withcluster.enabled=trueandsentinel.enabled=true, it will deploy a Redis master StatefulSet (only one master allowed) and a Redis slave StatefulSet. In this case, the pods will contain en extra container with Redis Sentinel. This container will form a cluster of Redis Sentinel nodes, which will promote a new master in case the actual one fails. In addition to this, only one service is exposed:\n-   Redis service: Exposes port 6379 for Redis read-only operations and port 26379 for accesing Redis Sentinel.\nFor read-only operations, access the service using port 6379. For write operations, it's necessary to access the Redis Sentinel cluster and query the current master using the command below (using redis-cli or similar:\n\nSENTINEL get-master-addr-by-name mymaster\nThis command will return the address of the current master, which can be accessed from inside the cluster.\nIn case the current master crashes, the Sentinel containers will elect a new master node.\n<https://redis.io/topics/sentinel>\n\n## Redis Cluster**\n\n<https://redis.io/topics/cluster-tutorial>\n\n<https://redis.io/topics/cluster-spec>\nReplication - One leader many followers model\n\nClustering - Shard data across multiple nodes\n\n<https://redis.io/topics/cluster-tutorial>\n\nHybrid - Replication + Clustering\nSentinel - <https://redis.io/topics/sentinel>\n\nkeydb - Multithreaded fork of redis\n\n<https://docs.keydb.dev/blog/2019/10/07/blog-post>\n\n## twenproxy (by twitter)**\n\ntwemproxy(pronounced \"two-em-proxy\"), akanutcrackeris a fast and lightweight proxy for[memcached](http://www.memcached.org/)and[redis](http://redis.io/)protocol. It was built primarily to reduce the number of connections to the caching servers on the backend. This, together with protocol pipelining and sharding enables you to horizontally scale your distributed caching architecture.\n\n<https://github.com/twitter/twemproxy>\n\n## codis**\n\nProxy based Redis cluster solution supporting pipeline and scaling dynamically\n\n![architecture](media/Redis-image1.png)\n\n<https://github.com/CodisLabs/codis>\n|                                      | Codis       | Twemproxy   | Redis Cluster                                                               |\n|--------------------------|-----------|------------|-------------------------|\n| resharding without restarting cluster | Yes         | No          | Yes                                                                         |\n| pipeline                              | Yes         | Yes         | No                                                                          |\n| hash tags for multi-key operations    | Yes         | Yes         | Yes                                                                         |\n| multi-key operations while resharding | Yes         | -          | No([details](http://redis.io/topics/cluster-spec#multiple-keys-operations)) |\n| Redis clients supporting              | Any clients | Any clients | Clients have to support cluster protocol                                    |\n**Other in-memory database**\n\n1.  facebook rocksdb\n\n2.  memcached\n**Redis vs Memcached**\n\nBoth Redis and MemCached are in-memory, open-source data stores. Memcached, a high-performance distributed memory cache service, is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases.\n\n| ****                                                                                                                                                       | **Memcached** | **Redis** |\n|------------------------------------------------|---------------|---------|\n| [Sub-millisecond latency](https://aws.amazon.com/elasticache/redis-vs-memcached/#Sub-millisecond_latency)                                                   | Yes           | Yes       |\n| [Developer ease of use](https://aws.amazon.com/elasticache/redis-vs-memcached/#Developer_ease_of_use)                                                       | Yes           | Yes       |\n| [Data partitioning](https://aws.amazon.com/elasticache/redis-vs-memcached/#Data_partitioning)                                                               | Yes           | Yes       |\n| [Support for a broad set of programming languages](https://aws.amazon.com/elasticache/redis-vs-memcached/#Support_for_a_broad_set_of_programming_languages) | Yes           | Yes       |\n| [Advanced data structures](https://aws.amazon.com/elasticache/redis-vs-memcached/#Advanced_data_structures)                                                 | -            | Yes       |\n| [**Multithreaded architecture**](https://aws.amazon.com/elasticache/redis-vs-memcached/#Multithreaded_architecture)                                         | **Yes**       | **-**     |\n| [Snapshots](https://aws.amazon.com/elasticache/redis-vs-memcached/#Snapshots)                                                                               | -            | Yes       |\n| [Replication](https://aws.amazon.com/elasticache/redis-vs-memcached/#Replication)                                                                           | -            | Yes       |\n| [Transactions](https://aws.amazon.com/elasticache/redis-vs-memcached/#Transactions)                                                                         | -            | Yes       |\n| [Pub/Sub](https://aws.amazon.com/elasticache/redis-vs-memcached/#Pub.2FSub)                                                                                 | -            | Yes       |\n| [Lua scripting](https://aws.amazon.com/elasticache/redis-vs-memcached/#Lua_scripting)                                                                       | -            | Yes       |\n| [Geospatial support](https://aws.amazon.com/elasticache/redis-vs-memcached/#Geospatial_support)                                                             | -            | Yes       |\n<https://aws.amazon.com/elasticache/redis-vs-memcached>\n\n## Books**\n\n<https://redislabs.com/redis-in-action>\n\n## Redis 6**\n\nRedis 6.0 open source\n-   Access control lists (ACLs)\n-   Improved evictions\n-   Threaded I/O\n-   RESP3 protocol\n\nRedis Enterprise 6.0\n-   Role-based access control (RBAC)\n-   Extending active-active\n-   HyperLogLog\n-   Streams\n**Memory Footprint**\n-   An empty instance uses ~ 3MB of memory\n-   1 Million small Keys -> String Value pairs use ~ 85MB of memory\n-   1 Million Keys -> Hash value, representing an object with 5 fields, use ~ 160 MB of memory\n<https://redis.io/topics/faq>\n\n## References**\n\n[https://redis.io/](https://redis.io/commands)\n\n<https://redis.io/commands>\n\n<https://www.tutorialspoint.com/redis>\n\n<https://redis.io/topics/rediscli>\n\n<https://dzone.com/articles/introduction-to-redis-data-structures-bitmaps>\n\n[Redis Crash Course](https://www.youtube.com/watch?v=sVCZo5B8ghE)\n\n<https://www.youtube.com/channel/UCybK6TMZFQeSN74jzTiDWfg>\n\n<https://university.redislabs.com/courses/course-v1:redislabs+RU202+2020_03/about>\n\n<https://university.redislabs.com/courses/course-v1:redislabs+RU201+2020_03/about>\n\n<https://github.com/antirez/redis>\n\n"},{"fields":{"slug":"/Databases/Others/Course---AWS-Certified-Database---Specialty/","title":"Course - AWS Certified Database - Specialty"},"frontmatter":{"draft":false},"rawBody":"# Course - AWS Certified Database - Specialty\n\nCreated: 2020-03-11 18:00:26 +0500\n\nModified: 2022-12-10 22:04:10 +0500\n\n---\n\n1.  Workload-specific database design\n\n    a.  Select appropriate database services for specific types of data and workloads\n\n    b.  Determine strategies for disaster recovery and high availability\n\n    c.  Design database solutions for performance, compliance, and scalability\n\n    d.  Compare the costs of database solutions\n\n2.  Deployment and migration\n\n    a.  Automate database solution deployments\n\n    b.  Determine data preparation and migration strategies\n\n    c.  Execute and validate data migration\n\n3.  Management and operations\n\n    a.  Determine maintenance tasks and processes\n\n    b.  Determine backup and restore strategies\n\n    c.  Manage the operational environment of a database solution\n\n4.  Monitoring and troubleshooting\n\n    a.  Determine monitoring and alerting strategies\n\n    b.  Troubleshoot and resolve common database issues\n\n    c.  Optimize database performance\n\n5.  Database security\n\n    a.  Encrypt data at rest and in transit\n\n    b.  Evaluate auditing solutions\n\n    c.  Determine access control and authentication mechanisms\n\n    d.  Recognize potential security vulnerabilities within database solutions\n<https://www.aws.training/Details/eLearning?id=47245>\n\n## Purpose Built Databases: Match your workload to the right database**\n\nFactors while choosing a db\n\n1.  Transactional compliance requirements of your workload\n2.  Data longevity\n3.  How strict are you with invalid data being sent to your database? (Ideally you are very strict and do server side data validation before persisting it to your database)\n4.  Structure of data\n\nThe structure of the data basically decides how we need to store and retrieve it. As our applications deal with data present in a variety of formats, selecting the right database should include picking the right data structures for storing and retrieving the data. If we do not select the right data structures for persisting our data, our application will take more time to retrieve data from the database, and will also require more development efforts to work around any data issues.\n5.  Size of data to be stored\n\nThis factor takes into consideration the quantity of data we need to store and retrieve as critical application data. The amount of data we can store and retrieve may vary depending on a combination of the data structure selected, the ability of the database to differentiate data across multiple file systems and servers, and even vendor-specific optimisations. So we need to choose our database keeping in mind the overall volume of data generated by the application at any specific time and also the size of data to be retrieved from the database.\n6.  Speed and scalability\n\nThis decides the speed we require for reading the data from the database and writing the data to the database. It addresses the time taken to service all incoming reads and writes to our application. Some databases are actually designed to optimise read-heavy applications, while others are designed in a way to support write-heavy solutions. Selecting a database that can handle our application's input/output needs can actually go a long way to making a scalable architecture.\n7.  Accessibility of data\n\nThe number of people or users concurrently accessing the database and the level of computation involved in accessing any specific data are also important factors to consider while choosing the right database. The processing speed of the application gets affected if the database chosen is not good enough to handle large loads.\n8.  Data modelling\n\nThis helps map our application's features into the data structure and we will need to implement the same. Starting with a conceptual model, we can identify the entities, their associated attributes, and the entity relationships that we will need. As we go through the process, the type of data structures we will need in order to implement the application will become more apparent. We can then use these structural considerations to select the right category of database that will serve our application the best.\n9.  Scope for multiple databases\n\nDuring the modelling process, we may realise that we need to store our data in a specific data structure, where certain queries cannot be optimised fully. This may be because of various reasons such as some complex search requirements, the need for robust reporting capabilities, or the requirement for a data pipeline to accept and analyse the incoming data. In all such situations, more than one type of database may be required for our application. When choosing more than one database, it's quite important to select one database that will own any specific set of data. This database acts as the canonical database for those entities. Any additional databases that work with this same set of data may have a copy, but will not be considered as the owner of this data.\n10. Safety and security of data\n\nWe should also check the level of security that any database provides to the data stored in it. In scenarios where the data to be stored is highly confidential, we need to have a highly secured database. The safety measures implemented by the database in case of any system crash or failure is quite a significant factor to keep in mind while choosing a database.\n11. Understand the data structure(s) you require, the amount of data you need to store/retrieve, and the speed/scaling requirements\n12. Model your data to determine if a relational, document, columnar, key/value, or graph database is most appropriate for your data\n13. During the modeling process, consider things such as the ratio of reads-to-writes, along with the throughput you will require to satisfy reads and writes\n14. Consider the use of multiple databases to manage data under different contexts/usage patterns\n15. Always use a master database to store and retrieve canonical data, with one or more additional databases to support additional features such as searching, data pipeline processing, and caching\n"},{"fields":{"slug":"/Databases/Others/Course---Advanced-Database-Systems/","title":"Course - Advanced Database Systems"},"frontmatter":{"draft":false},"rawBody":"# Course - Advanced Database Systems\n\nCreated: 2020-04-19 23:43:01 +0500\n\nModified: 2020-04-26 00:23:08 +0500\n\n---\n\n1.  Course Introduction and History of Databases\n\n2.  In-Memory Databases\n\n3.  Multi-Version Concurrency Control (Design Decisions)\n\n4.  Multi-Version Concurrency Control (Protocols)\n\n5.  Multi-Version Concurrency Control (Garbage Collection)\n\n6.  OLTP Indexes (B+Tree Data Structures)\n\n7.  OLTP Indexes (Trie Data Structures)\n\n8.  Storage Models, Data Layout, & System Catalogs\n\n9.  Database Compression\n\n10. Recovery Protocols\n\n11. Networking Protocols\n\n12. Scheduling\n\n13. Query Execution & Processing\n\n14. Query Compilation\n\n15. Vectorized Execution\n\n16. Vectorization vs. Compilation\n\n17. Parallel Join Algorithms (Hashing)\n\n18. Parallel Join Algorithms (Sorting)\n\n19. Optimizer Implementation (Overview)\n\n20. Optimizer Implementation (Top-Down vs. Bottom-Up)\n\n21. Optimizer Implementation (Alternative Approaches)\n\n22. Cost Models\n\n23. Larger-than-Memory Databases\n\n24. Server-side Logic Execution\n\n25. Databases on New Hardware\n**Query Optimizer**\n\nBuilding a really good query optimizer and query execution system in a distributed database system is hard. It requires a number of sophisticated components including statistics, cardinality estimation, plan space search, the right storage structures, fast query execution operators, intelligent shuffle, both broadcast and point-to-point data transmission, and more.\n[15-721 Advanced Database Systems (Spring 2020)](https://www.youtube.com/playlist?list=PLSE8ODhjZXjasmrEd2_Yi1deeE360zv5O)\n![](media/Course---Advanced-Database-Systems-image1.jpg)\n\n"},{"fields":{"slug":"/Databases/Others/Data-Lake/","title":"Data Lake"},"frontmatter":{"draft":false},"rawBody":"# Data Lake\n\nCreated: 2018-10-13 16:12:14 +0500\n\nModified: 2020-10-18 14:07:51 +0500\n\n---\n\nAlso called Data Swamp\nThe idea is to have a single store for all of the raw data that anyone in an organization might need to analyze. Commonly people use Hadoop to work on the data in the lake.\nThe data lake stores*raw*data, in whatever form the data source provides. There is no assumptions about the schema of the data, each data source can use whatever schema it likes. It's up to the consumers of that data to make sense of that data for their own purposes.\n![With a data warehouse, incoming data is cleaned and organized into a single consistent schema before being put into the warehouse... analysis is done directly on the curated warehouse data With a data lake, incoming data goes into the lake in its raw form... we select and organize data for each need ](media/Data-Lake-image1.png)\n\nIt is important that all data put in the lake should have a clear provenance in place and time. Every data item should have a clear trace to what system it came from and when the data was produced.\nThe data lake is schemaless\n|                 | **Data Lake**                                                                   | **Data Warehouse**                                                                |\n|----------|-------------------------------|--------------------------------|\n| **Type of data** | Unstructured and structured data from various company data sources              | Historical data that has been structured to fit a relational database schema      |\n| **Purpose**      | Cost-effective big data storage                                                 | Analytics for business decisions                                                  |\n| **Analytics**    | Machine learning, predictive analytics, data discovery, and profiling           | Batch reporting, BI, and visualizations                                           |\n| **Users**        | Data scientists and engineers                                                   | Data analysts and business analysts                                               |\n| **Tasks**        | Storing data and big data analytics, like deep learning and real-time analytics | Typically read-only queries for aggregating and summarizing data                  |\n| **Size**         | Stores all data that might be used---can take up petabytes!                     | Only stores data relevant to analysis                                             |\n|                 | Seperation of compute and storage                                               | Tightly coupled compute and storage                                               |\n|                 | Designed prior to the data warehouse implementation **(Schema on read**)        | Written at the time of analysis **(Schema on write**)                             |\n|                 | Great for storing granular data; raw as well as processed data                  | Great for storing frequently accessed data as well as data aggregates and summary |\n| **Data Quality** | Any data that may or may not be curated (i.e., raw data)                        | Highly curated data that serves as the central version of the truth               |\n**References**\n\n<https://martinfowler.com/bliki/DataLake.html>\n\n"},{"fields":{"slug":"/Databases/Others/Data-Warehousing/","title":"Data Warehousing"},"frontmatter":{"draft":false},"rawBody":"# Data Warehousing\n\nCreated: 2018-02-18 23:43:07 +0500\n\nModified: 2021-12-08 23:23:20 +0500\n\n---\n\n**Lifecycle Data**\nIn[computing](https://en.wikipedia.org/wiki/Computing), adata warehouse(DWorDWH), also known as anenterprise data warehouse(EDW), is a system used for[reporting](https://en.wikipedia.org/wiki/Business_reporting)and[data analysis](https://en.wikipedia.org/wiki/Data_analysis), and is considered a core component of[business intelligence](https://en.wikipedia.org/wiki/Business_intelligence).DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single placethat are used for creating analytical reports for workers throughout the enterprise.\nThe data stored in the warehouse is[uploaded](https://en.wikipedia.org/wiki/Upload)from the[operational systems](https://en.wikipedia.org/wiki/Operational_system)(such as marketing or sales). The data may pass through an[operational data store](https://en.wikipedia.org/wiki/Operational_data_store)and may require[data cleansing](https://en.wikipedia.org/wiki/Data_cleansing)for additional operations to ensure[data quality](https://en.wikipedia.org/wiki/Data_quality)before it is used in the DW for reporting.\n[Extract, transform, load](https://en.wikipedia.org/wiki/Extract,_transform,_load)(ETL) and[Extract, load, transform](https://en.wikipedia.org/wiki/Extract,_load,_transform)(E-LT) are the two main approaches used to build a data warehouse system.\n\n# \n\n<https://en.wikipedia.org/wiki/Data_warehouse>\n\n## Data Warehouse vs Database**\n\nA data warehouse is specially designed for data analytics, which involves reading large amounts of data to understand relationships and trends across the data. A database is used to capture and store data, such as recording details of a transaction.\n\n| **Characteristics** | **Data Warehouse**                                                                         | **Transactional Database**                                                                            |\n|-------------|-----------------------------|------------------------------|\n| Suitable workloads  | Analytics, reporting,[big data](https://aws.amazon.com/big-data/what-is-big-data/)       | Transaction processing                                                                                |\n| Data source         | Data collected and normalized from many sources                                            | Data captured as-is from a single source, such as a transactional system                              |\n| Data capture        | Bulk write operations typically on a predetermined batch schedule                          | Optimized for continuous write operations as new data is available to maximize transaction throughput |\n| Data normalization  | Denormalized schemas, such as the Star schema or Snowflake schema                          | Highly normalized, static schemas                                                                     |\n| Data storage        | Optimized for simplicity of access and high-speed query performance using columnar storage | Optimized for high throughout write operations to a single row-oriented physical block                |\n| Data access         | Optimized to minimize I/O and maximize data throughput                                     | High volumes of small read operations                                                                 |\n**Data Warehouse vs Data Lake**\n\nUnlike a data warehouse, a data lake is a centralized repository foralldata, including structured and unstructured. A data warehouse utilizes a pre-defined schema optimized for analytics. In a data lake, the schema is not defined, enabling additional types of analytics like big data analytics, full text search, real-time analytics, and machine learning.\n| **Characteristics** | **Data Warehouse**                                                                                   | **Data Lake**                                                                                                         |\n|---------------|----------------------------|-----------------------------|\n| Data                | Relational data from transactional systems, operational databases, and line of business applications | Non-relational and relational data from IoT devices, web sites, mobile apps, social media, and corporate applications |\n| Schema              | Designed prior to the data warehouse implementation (schema-on-write)                                | Written at the time of analysis (schema-on-read)                                                                      |\n| Price/Performance   | Fastest query results using higher cost storage                                                      | Query results getting faster using low-cost storage                                                                   |\n| Data Quality        | Highly curated data that serves as the central version of the truth                                  | Any data that may or may not be curated (i.e. raw data)                                                               |\n| Users               | Business analysts, data scientists, and data developers                                              | Data scientists, data developers, and business analysts (using curated data)                                          |\n| Analytics           | Batch reporting, BI, and visualizations                                                              | Machine learning, predictive analytics, data discovery, and profiling                                                 |\n**Data Warehouse vs Data Mart**\n\nA data mart is a data warehouse that serves the needs of a specific team or business unit, like finance, marketing, or sales. It is smaller, more focused, and may contain summaries of data that best serve its community of users.\n| **Characteristics** | **Data Warehouse**                                      | **Data Mart**                                                                         |\n|-------------|--------------------------|----------------------------------|\n| Scope               | Centralized, multiple subject areas integrated together | Decentralized, specific subject area                                                  |\n| Users               | Organization-wide                                       | A single community or department                                                      |\n| Data source         | Many sources                                            | A single or a few sources, or a portion of data already collected in a data warehouse |\n| Size                | Large, can be 100's of gigabytes to petabytes          | Small, generally up to 10's of gigabytes                                             |\n| Design              | Top-down                                                | Bottom-up                                                                             |\n| Data detail         | Complete, detailed data                                 | May hold summarized data                                                              |\n**Reference**\n\n<https://dzone.com/refcardz/data-warehousing>\n\n<http://www.oracle.com/webfolder/technetwork/tutorials/obe/db/10g/r2/owb/owb10gr2_gs/owb/lesson3/slowlychangingdimensions.htm>\n\n<https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-ii-47c4e7cbda71>\n\n<https://medium.com/@rchang/a-beginners-guide-to-data-engineering-the-series-finale-2cc92ff14b0>\n\n<https://www.tutorialspoint.com/dwh/index.htm>\n\n<https://www.guru99.com/data-warehouse-architecture.html>\n\n<https://aws.amazon.com/data-warehouse>\n"},{"fields":{"slug":"/Databases/Others/Databases---Others/","title":"Databases - Others"},"frontmatter":{"draft":false},"rawBody":"# Databases - Others\n\nCreated: 2018-08-31 18:45:14 +0500\n\nModified: 2022-12-11 13:53:23 +0500\n\n---\n\n1.  **Flat file database**\n\nA**flat file database**is a[database](https://en.wikipedia.org/wiki/Database)stored as an ordinary unstructured file called a \"flat file\". To access the structure of the data and manipulate it on a computer system, the file must be read in its entirety into the computer's memory. Upon completion of the database operations, the file is again written out in its entirety to the host's file system. In this stored mode the database is said to be \"flat\", meaning that it has no structure for indexing and there are usually no structural relationships between the records. A flat file can be a[plain text](https://en.wikipedia.org/wiki/Plain_text)file or a[binary file](https://en.wikipedia.org/wiki/Binary_file).\n<https://en.wikipedia.org/wiki/Flat_file_database>\n2.  **ScyllaDB**\n    -   **Real-time big data database**\n    -   **Shared nothing architecture**\n    -   **NoSQL Database**\n    -   **Written in C++**\n    -   **Alternative - Cassandra**\n<https://www.scylladb.com>\n3.  **RocksDB: A Persistent Key-Value Store for Flash and RAM Storage** by Facebook Database Engineering Team\nThis code is a library that forms the core building block for a fast key value server, especially suited for storing data on flash drives. It has a Log-Structured-Merge-Database (LSM) design with flexible tradeoffs between Write-Amplification-Factor (WAF), Read-Amplification-Factor (RAF) and Space-Amplification-Factor (SAF). It has multi-threaded compactions, making it specially suitable for storing multiple terabytes of data in a single database.\n**Features**\n\n1.  **High Performance**\n\nRocksDB uses a log structured database engine, written entirely in C++, for maximum performance. Keys and values are just arbitrarily-sized byte streams.\n\n2.  **Optimized for Fast Storage**\n\nRocksDB is optimized for fast, low latency storage such as flash drives and high-speed disk drives. RocksDB exploits the full potential of high read/write rates offered by flash or RAM.\n\n3.  **Adaptable**\n\nRocksDB is adaptable to different workloads. From database storage engines such as[MyRocks](https://github.com/facebook/mysql-5.6)to[application data caching](http://techblog.netflix.com/2016/05/application-data-caching-using-ssds.html)to embedded workloads, RocksDB can be used for a variety of data needs.\n\n4.  **Basic and Advanced Database Operations**\n\nRocksDB provides basic operations such as opening and closing a database, reading and writing to more advanced operations such as merging and compaction filters.\n<https://github.com/facebook/rocksdb>\n3.  **IndexedDB**\n\nIndexedDB is a large-scale, NoSQL storage system. It lets you store just about anything in the user's browser. In addition to the usual search, get, and put actions, IndexedDB also supports transactions. Here is the definition of IndexedDB on MDN:\n\n\"IndexedDB is a low-level API for client-side storage of significant amounts of structured data, including files/blobs. This API uses indexes to enable high performance searches of this data. While DOM Storage is useful for storing smaller amounts of data, it is less useful for storing larger amounts of structured data. IndexedDB provides a solution.\"\n\nEach IndexedDB database is unique to an origin (typically, this is the site domain or subdomain), meaning it cannot access or be accessed by any other origin.[Data storage limits](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API/Browser_storage_limits_and_eviction_criteria)are usually quite large, if they exist at all, but different browsers handle limits and data eviction differently.\n4.  **Mnesia**\n\n**Mnesia**is a[distributed](https://en.wikipedia.org/wiki/Distributed_computing),[soft real-time](https://en.wikipedia.org/wiki/Real-time_computing)[database management system](https://en.wikipedia.org/wiki/Database_management_system)written in the[Erlang programming language](https://en.wikipedia.org/wiki/Erlang_(programming_language)). It is distributed as part of the[Open Telecom Platform](https://en.wikipedia.org/wiki/Open_Telecom_Platform).\nEmqtt and RabbitMQ uses Mnesia database\nA distributed telecommunications DBMS.\n\nThe following are some of the most important and attractive capabilities provided by Mnesia:\n-   A relational/object hybrid data model that is suitable for telecommunications applications.\n-   A DBMS query language, Query List Comprehension (QLC) as an add-on library.\n-   Persistence. Tables can be coherently kept on disc and in the main memory.\n-   Replication. Tables can be replicated at several nodes.\n-   Atomic transactions. A series of table manipulation operations can be grouped into a single atomic transaction.\n-   Location transparency. Programs can be written without knowledge of the actual data location.\n-   Extremely fast real-time data searches.\n-   Schema manipulation routines. The DBMS can be reconfigured at runtime without stopping the system.\n**References**\n\n<https://en.wikipedia.org/wiki/Mnesia>\n\n<http://erlang.org/doc/man/mnesia.html>\n\n<https://github.com/erlang/otp/tree/master/lib/mnesia>\n5.  **LevelDB**\n\nLevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.\n**Features**\n-   Keys and values are arbitrary byte arrays.\n-   Data is stored sorted by key.\n-   Callers can provide a custom comparison function to override the sort order.\n-   The basic operations arePut(key,value),Get(key),Delete(key).\n-   Multiple changes can be made in one atomic batch.\n-   Users can create a transient snapshot to get a consistent view of data.\n-   Forward and backward iteration is supported over the data.\n-   Data is automatically compressed using the[Snappy compression library](http://google.github.io/snappy/).\n-   External activity (file system operations etc.) is relayed through a virtual interface so users can customize the operating system interactions.\n**References**\n\n<https://github.com/google/leveldb>\n6.  **Gorilla TSDB (Used by Prometheus for storing metrics)**\n\n<https://blog.acolyer.org/2016/05/03/gorilla-a-fast-scalable-in-memory-time-series-database>\n\n<https://fabxc.org/tsdb>\n7.  **CockroachDB**\n\nCockroachDB is a cloud-native SQL database for building global, scalable cloud services that survive disasters.\nCockroachDB is a distributed SQL database built on a transactional and strongly-consistent key-value store. Itscales horizontally;survivesdisk, machine, rack, and even datacenter failures with minimal latency disruption and no manual intervention; supportsstrongly-consistentACID transactions; and provides a familiarSQLAPI for structuring, manipulating, and querying data.\n<https://github.com/cockroachdb/cockroach>\n8.  **AresDB**\n\nA GPU-powered real-time analytics storage and query engine. It features low query latency, high data freshness and highly efficient in-memory and on disk storage management.\n<https://eng.uber.com/aresdb>\n\n<https://github.com/uber/aresdb>\n9.  **Riak**\n\n**Riak**(pronounced \"ree-ack\"[^[2]^](https://en.wikipedia.org/wiki/Riak#cite_note-Riak_1.0_Release_Party-2)) is a distributed[NoSQL](https://en.wikipedia.org/wiki/NoSQL)key-value[data store](https://en.wikipedia.org/wiki/Data_store)that offers high availability, fault tolerance, operational simplicity, and scalability.[^[3]^](https://en.wikipedia.org/wiki/Riak#cite_note-Datamation-3)In addition to the[open-source](https://en.wikipedia.org/wiki/Open-source_software)version, it comes in a supported enterprise version and a[cloud storage](https://en.wikipedia.org/wiki/Cloud_storage)version.[^[3]^](https://en.wikipedia.org/wiki/Riak#cite_note-Datamation-3)Riak implements the principles from Amazon's[Dynamo](https://en.wikipedia.org/wiki/Dynamo_(storage_system))paper[^[4]^](https://en.wikipedia.org/wiki/Riak#cite_note-4)with heavy influence from the[CAP Theorem](https://en.wikipedia.org/wiki/CAP_Theorem). Written in[Erlang](https://en.wikipedia.org/wiki/Erlang_(programming_language)), Riak has fault tolerant data replication and automatic data distribution across the cluster for performance and resilience.\nThe Riak product line of distributed databases is built on a set of core services providing a highly reliable, scalable distributed systems framework.[RiakKV](https://riak.com/products/riak-kv/index.html)is a distributed NoSQL database.[RiakTS](https://riak.com/products/riak-ts/index.html)is builton the same core foundation as RiakKV and is highly optimized for IoT and time series data.Riak also integrates with[RiakS2](https://riak.com/products/riak-s2/index.html?p=6196.html)to optimize large object storage, and integrates with other data services including[Apache Spark](https://riak.com/products/apache-spark/index.html),[Redis Caching](https://riak.com/products/redis/index.html?p=6927.html),[Apache Solr](https://riak.com/products/solr/index.html), and[Apache Mesos](https://riak.com/products/apache-mesos/index.html?p=11511.html).\n<https://riak.com/products>\n\n<https://en.wikipedia.org/wiki/Riak>\n10. **JanusGraph (Opensource distributed graph database)**\n\nJanusGraph is a highly scalable[graph database](https://en.wikipedia.org/wiki/Graph_database)optimized for storing and querying large graphs with billions of vertices and edges distributed across a multi-machine cluster. JanusGraph is a transactional database that can support thousands of concurrent users, complex traversals, and analytic graph queries.\n<https://github.com/janusgraph/janusgraph>\n\n[https://docs.janusgraph.org](https://docs.janusgraph.org/)\n11. **ClickHouse**\n\nClickHouse is anopensourcecolumn-oriented database management system capable ofrealtime generation of analytical data reports usingSQLqueries.\nKey Features\n-   True column-oriented storage\n-   Vectorized query execution\n-   Data compression\n-   Parallel and distributed query execution\n-   Real time query processing\n-   Real time data ingestion\n-   On-disk locality of reference\n-   Cross-datacenter replication\n-   High availability\n-   SQL support\n-   Local and distributed joins\n-   Pluggable external dimension tables\n-   Arrays and nested data types\n-   Approximate query processing\n-   Probabilistic data structures\n-   Full support of IPv6\n-   Features for web analytics\n-   State-of-the-art algorithms\n-   Detailed documentation\n-   Clean documented code\n<https://clickhouse.yandex>\n\n<https://github.com/yandex/ClickHouse>\nUsed by - Zerodha\n12. **tidb**\n\nTiDB (\"Ti\" stands for Titanium) is an open-source NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. It is MySQL compatible and features horizontal scalability, strong consistency, and high availability.\n![TiDB platform architecture](media/Databases---Others-image1.png)\nInside the TiDB platform, the main components are as follows:\n-   [TiDB server](https://github.com/pingcap/tidb)is a stateless SQL layer that processes users' SQL queries, accesses data in the storage layer, and returns the corresponding results to the application. It is MySQL-compatible and sits on top of TiKV.\n-   [TiKV server](https://github.com/pingcap/tikv)is the distributed transactional key-value storage layer where the data persists. It uses the[Raft](https://raft.github.io/) consensus protocol for replication to ensure strong data consistency and high availability.\n-   [TiSpark](https://github.com/pingcap/tispark)cluster also sits on top of TiKV. It is an Apache Spark plugin that works with the TiDB platform to support complex Online Analytical Processing (OLAP) queries for business intelligence (BI) analysts and data scientists.\n-   [Placement Driver (PD) server](https://github.com/pingcap/pd)is a metadata cluster powered by[etcd](https://github.com/etcd-io/etcd)that manages and schedules TiKV.\n<https://github.com/pingcap/tidb>\n\n<https://pingcap.com/success-stories/lesson-learned-from-queries-over-1.3-trillion-rows-of-data-within-milliseconds-of-response-time-at-zhihu>\n\n<https://dzone.com/articles/building-a-large-scale-distributed-storage-system>\n13. **tikv**\n\nTiKV (\"Ti\" stands for Titanium) is an open source distributed transactional key-value database. Unlike other traditional NoSQL systems, TiKV not only provides classical key-value APIs, but also transactional APIs with ACID compliance. Built in Rust and powered by Raft, TiKV was originally created to complement[TiDB](https://github.com/pingcap/tidb), a distributed HTAP database compatible with the MySQL protocol.\n<https://github.com/tikv/tikv>\n14. **ObjectBox**\n\nObjectBox is a super fast database and sychronization solution, built uniquely for Mobile and IoT devices. We bring edge computing to small devices, allowing data to be stored and processed from sensor to server for reliable, fast and secure data management. ObjectBox is smaller than 1MB, so it is the ideal solution across hardware from Mobile Apps, to IoT Devices and IoT Gateways. We are the first high-performance NoSQL, ACID-compliant on-device edge database. All of our products are built with developers in mind, so they are easy to use and take minimal code to implement.\n<https://objectbox.io>\n15. **LF**\n\nLF (pronounced \"aleph\") is a fully decentralized fully replicated key/value store.\nFully decentralized means anyone can run a node without obtaining special permission and all nodes are effectively equal. Fully replicated means every node stores the entire data set.\nLF is built on a[directed acyclic graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph)data model that makes synchronization easy and allows many different security and conflict resolution strategies to be used. One way to think of LF's DAG is as a gigantic[conflict-free replicated data type](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type)(CRDT).\nProof of work is used to rate limit writes to the shared data store on public networks and as one thing that can be taken into consideration for conflict resolution. Other things that can be considered (at the querying client's discretion) are local subjective heuristics at the node and certificates issued by a certificate authority.\n<https://github.com/zerotier/lf>\n16. **SQLite**\n    -   SQLite is a popular open source SQL database. It can store an entire database in a single file. One of the most significant advantages this provides is that all of the data can be stored locally without having to connect your database to a server.\n    -   SQLite is a popular choice for databases in cellphones, PDAs, MP3 players, set-top boxes, and other electronic gadgets.\napt-get install sqlite3 libsqlite3-dev\n\ncd /data\n\nsqlite3 db.sqlite3\n\n.help\n\n.databases\n\n.tables\n\n>>> .schema\n\n>>> .exit\n\n>>> PRAGMA table_info(table_name); # show all columns of a table\n**# Run this query to find the names of the tables in this database**\n\nSELECT name FROM sqlite_master where type = 'table';\n**# Run this query to find the structure of the `crime_scene_report` table**\n\nSELECT sql FROM sqlite_master where name = 'crons_cron';\n\nSELECT sql_query FROM crons_cron;\n\nSELECT cron_name FROM crons_cron where sql_query like '%st_email_sms_exception_sources%';\nSELECT cron_name, sql_query FROM crons_cron where sql_query like '%\"goo.gl%';\n\nSELECT cron_name, sql_query FROM crons_cron where sql_query like '%\"goo.gl%' and cron_name = \"credit_bldr_elig_not_assign_1\";\n\nUPDATE `crons_cron` SET `sql_query` =replace(sql_query, '\"goo.gl', '\"\"credit_bldr_elig_not_assign_1\";\n\nUPDATE `crons_cron` SET `sql_query` =replace(sql_query, '\"goo.gl', '\"<https://goo.gl>');\n**SQLite database**\n\ndb = sqlite3.connect(':memory:') # Using an in-memory database\n\ncur = db.cursor()\ncur.execute('''SELECT itemid, AVG(price) FROM BoughtItem GROUP BY itemid''')\n\nprint(cur.fetchall())\n<https://realpython.com/data-engineer-interview-questions-python>\n17. **Supersqllite**\n\nA feature-packed Python package and for utilizing SQLite in Python by[Plasticity](https://www.plasticity.ai/). It is intended to be a drop-in replacement to Python's built-in[SQLite API](https://docs.python.org/3/library/sqlite3.html), but without any limitations. It offers unique features like[remote streaming over HTTP](https://github.com/plasticityai/supersqlite#remote-streaming-over-http)and[bundling of extensions like JSON, R-Trees (geospatial indexing), and Full Text Search](https://github.com/plasticityai/supersqlite#extensions). SuperSQLite is also packaged with pre-compiled native binaries for SQLite and all of its extensions for nearly every platform as to avoid any C/C++ compiler errors during install.\n<https://github.com/plasticityai/supersqlite>\n18. **MilliDB**\n\nA full-text search database based on the fast LMDB key-value store\n<https://github.com/meilisearch/MeiliDB>\n\n<https://www.meilisearch.com>\n19. **Lightning Memory-Mapped Database(LMDB)**\n\nLMDB is a[software library](https://en.wikipedia.org/wiki/Software_library)that provides a high-performance embedded transactional database in the form of a[key-value store](https://en.wikipedia.org/wiki/Key-value_store). LMDB is written in[C](https://en.wikipedia.org/wiki/C_(programming_language))with[API bindings](https://en.wikipedia.org/wiki/Lightning_Memory-Mapped_Database#API_and_uses) for several[programming languages](https://en.wikipedia.org/wiki/Programming_language). LMDB stores arbitrary key/data pairs as byte arrays, has a range-based search capability, supports multiple data items for a single key and has a special mode for appending records at the end of the database (MDB_APPEND) which gives a dramatic write performance increase over other similar stores.LMDB is not a[relational database](https://en.wikipedia.org/wiki/Relational_database), it is strictly a key-value store like[Berkeley DB](https://en.wikipedia.org/wiki/Berkeley_DB)and[dbm](https://en.wikipedia.org/wiki/DBM_(computing)).\nLMDB may also be used[concurrently](https://en.wikipedia.org/wiki/Lightning_Memory-Mapped_Database#Concurrency)in a multi-threaded or multi-processing environment, with read performance scaling linearly by design. LMDB databases may have only one writer at a time, however unlike many similar key-value databases, write transactions donotblock readers, nor do readers block writers. LMDB is also unusual in that multiple applications on the same system may simultaneously open and use the same LMDB store, as a means to scale up performance. Also, LMDB does not require a transaction log (thereby increasing write performance by not needing to write data twice) because it maintains data integrity inherently by design.\nLMDB is a tiny database with some excellent properties:\n-   Ordered map interface (keys are always lexicographically sorted).\n-   Reader/writer transactions: readers don't block writers, writers don't block readers. Each environment supports one concurrent write transaction.\n-   Read transactions are extremely cheap.\n-   Environments may be opened by multiple processes on the same host, making it ideal for working around Python's[GIL](http://wiki.python.org/moin/GlobalInterpreterLock).\n-   Multiple named databases may be created with transactions covering all named databases.\n-   Memory mapped, allowing for zero copy lookup and iteration. This is optionally exposed to Python using the[buffer()](https://docs.python.org/2.7/library/functions.html#buffer)interface.\n-   Maintenance requires no external process or background threads.\n-   No application-level caching is required: LMDB fully exploits the operating system's buffer cache.\nLMDB is a Btree-based database management library modeled loosely on the BerkeleyDB API, but much simplified. The entire database is exposed in a memory map, and all data fetches return data directly from the mapped memory, so no malloc's or memcpy's occur during data fetches. As such, the library is extremely simple because it requires no page caching layer of its own, and it is extremely high performance and memory-efficient. It is also fully transactional with full ACID semantics, and when the memory map is read-only, the database integrity cannot be corrupted by stray pointer writes from application code.\nThe library is fully thread-aware and supports concurrent read/write access from multiple processes and threads. Data pages use a copy-on- write strategy so no active data pages are ever overwritten, which also provides resistance to corruption and eliminates the need of any special recovery procedures after a system crash. Writes are fully serialized; only one write transaction may be active at a time, which guarantees that writers can never deadlock. The database structure is multi-versioned so readers run with no locks; writers cannot block readers, and readers don't block writers.\nUnlike other well-known database mechanisms which use either write-ahead transaction logs or append-only data writes, LMDB requires no maintenance during operation. Both write-ahead loggers and append-only databases require periodic checkpointing and/or compaction of their log or database files otherwise they grow without bound. LMDB tracks free pages within the database and re-uses them for new write operations, so the database size does not grow without bound in normal use.\nThe memory map can be used as a read-only or read-write map. It is read-only by default as this provides total immunity to corruption. Using read-write mode offers much higher write performance, but adds the possibility for stray application writes thru pointers to silently corrupt the database. Of course if your application code is known to be bug-free (...) then this is not an issue.\n<https://en.wikipedia.org/wiki/Lightning_Memory-Mapped_Database>\n\n<http://www.lmdb.tech/doc>\n20. **etcd**\n\netcd is a distributed key value store that provides a reliable way to store data across a cluster of machines. It's open-source and available on GitHub. etcd gracefully handles leader elections during network partitions and will tolerate machine failure, including the leader.\n\nYour applications can read and write data into etcd. A simple use-case is to store database connection details or feature flags in etcd as key value pairs. These values can be watched, allowing your app to reconfigure itself when they change.\n\nAdvanced uses take advantage of the consistency guarantees to implement database leader elections or do distributed locking across a cluster of workers.-   Used by kubernetes, emqx, openshift\n\netcdis a distributed key-value store. In fact,etcdis the primary datastore of **Kubernetes;** storing and replicating allKubernetescluster state. As a critical component of aKubernetescluster having a reliable automated approach to its configuration and management is imperative.\n-   As a distributed consensus-based system, the cluster configuration of etcd can be complicated. Bootstrapping, maintaining quorum, reconfiguring cluster membership, creating backups, handling disaster recovery, and monitoring critical events are tedious work, and require etcd-specific expertise.\n-   Uses Raft consensus algorithm\n    -   <https://raft.github.io>\n-   etcd is pronounced/ËˆÉ›tsiËdiË/, and means distributedetcdirectory.\n-   Operator\n\nAn Operator builds upon the basic Kubernetes resource and controller concepts but includes application domain knowledge to take care of common tasks. They reduce the complexity of running distributed systems and help you focus on the desired configuration, not the details of manual deployment and lifecycle management.-   **References**\n\n<https://coreos.com/etcd>\n\n<https://github.com/etcd-io/etcd>\n\n<https://etcd.readthedocs.io/en/latest/faq.html#what-is-etcd>\n\n<https://jepsen.io/analyses/etcd-3.4.3>\n21. **KsqlDB**\n\nThe event streaming database purpose-built for stream processing applications.\n<https://ksqldb.io>\n\n<https://www.confluent.io/blog/intro-to-ksqldb-sql-database-streaming>22. **Memcached**\n    -   **Distributed cache and hold the data in-memory**\n    -   **Memcached is simple, fast key-value storage**\n    -   **Memcached functions like a large hash table and offers a simple API to store and retrieve arbitrarily shaped objects by key**\n    -   **Can also be set up as a cluster so can provide availability and data application**\n    -   **Can also flush data on the hard drive**\n    -   **Memcached is a distributed system that allows its hash table capacity to scale horizontally across a pool of servers. Each Memcached server operates in complete isolation from the other servers in the pool. Therefore, the routing and load balancing between the servers must be done at the client level. Memcached clients apply a consistent hashing scheme to appropriately select the target servers. This scheme guarantees the following conditions:**\n        -   **The same server is always selected for the same key.**\n        -   **Memory usage is evenly balanced between the servers.**\n        -   **A minimum number of keys are relocated when the pool of servers is reduced or expanded.**\n23. **Minio**\n\nMinio is a high performance distributed object storage server, designed for\n\nlarge-scale private cloud infrastructure.\n<https://minio.io>\n\n## Others**\n-   **GlusterFS**\n24. **Ehcache**\n\nEhcache is an open source, standards-based cache that boosts performance, offloads your database, and simplifies scalability. It's the most widely-used Java-based cache because it's robust, proven, full-featured, and integrates with other popular libraries and frameworks. Ehcache scales from in-process caching, all the way to mixed in-process/out-of-process deployments with terabyte-sized caches.\n<https://www.ehcache.org>\n\n<https://github.com/ehcache/ehcache3>25. **Realm**\n\nRealm is a mobile database: an alternative to SQLite & key-value stores\n<https://github.com/realm/realm-js>\n\n<https://realm.io>\n26. **Datomic**\n\nA transactional database with a flexible data model, elastic scaling, and rich queries.\n[Datomic](https://en.wikipedia.org/wiki/Datomic)is a distributed database designed to enable scalable, flexible and intelligent applications, running on new cloud architectures. It uses Datalog as the query language.\n<https://www.datomic.com>\n\n<https://docs.datomic.com/on-prem/index.html>\n\n<https://dbdb.io/db/datomic>\n\n[**https://www.datomic.com/cloud-faq.html**](https://www.datomic.com/cloud-faq.html)\n"},{"fields":{"slug":"/Databases/Others/ETL-(Extract-Transform-Load)/","title":"ETL (Extract Transform Load)"},"frontmatter":{"draft":false},"rawBody":"# ETL (Extract Transform Load)\n\nCreated: 2018-02-19 00:01:22 +0500\n\nModified: 2019-09-20 11:58:53 +0500\n\n---\n\nIn[computing](https://en.wikipedia.org/wiki/Computing),**extract, transform, load**(**ETL**) refers to a process in[database](https://en.wikipedia.org/wiki/Database)usage and especially in[data warehousing](https://en.wikipedia.org/wiki/Data_warehouse).\n-   Data extraction is where data is extracted from homogeneous or heterogeneous data sources\n-   Data transformationis where the data is transformed for storing in the proper format or structure for the purposes of querying and analysis\n-   Data loadingwhere the data is loaded into the final target database, more specifically, an[operational data store](https://en.wikipedia.org/wiki/Operational_data_store),[data mart](https://en.wikipedia.org/wiki/Data_mart), or data warehouse.\nSince the data extraction takes time, it is common to execute the three phases in parallel. While the data is being extracted, another transformation process executes while processing the data already received and prepares it for loading while the data loading begins without waiting for the completion of the previous phases.\n![ETL process overview](media/ETL-(Extract-Transform-Load)-image1.png)\n<https://www.toptal.com/etl/interview-questions>\n\n"},{"fields":{"slug":"/Databases/Others/MemSQL/","title":"MemSQL"},"frontmatter":{"draft":false},"rawBody":"# MemSQL\n\nCreated: 2020-04-26 00:31:37 +0500\n\nModified: 2020-04-26 00:33:51 +0500\n\n---\n\nMemSQLis a[distributed](https://en.wikipedia.org/wiki/Distributed_database),[in-memory](https://en.wikipedia.org/wiki/In-memory_database),[SQL](https://en.wikipedia.org/wiki/Structured_Query_Language)[database](https://en.wikipedia.org/wiki/Database)management system.\nIt is a[relational database management system](https://en.wikipedia.org/wiki/Relational_database_management_system)(RDBMS). It[compiles](https://en.wikipedia.org/wiki/Compiler)Structured Query Language ([SQL](https://en.wikipedia.org/wiki/SQL)) into machine code, via[termed code generation](https://en.wikipedia.org/w/index.php?title=Termed_code_generation&action=edit&redlink=1).\nMemSQL combines lock-free data structures and a[just-in-time compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation)(JIT) to process highly volatile workloads. More specifically, MemSQL implements lock-free[hash tables](https://en.wikipedia.org/wiki/Hash_table)and lock-free[skip lists](https://en.wikipedia.org/wiki/Skip_list)in memory for fast random access to data. SQL queries sent to the MemSQL server are converted into byte code and compiled through [LLVM](https://en.wikipedia.org/wiki/LLVM) into [machine code](https://en.wikipedia.org/wiki/Machine_code). Queries are then stripped of their parameters and the query template is stored as a shared object which is subsequently matched against incoming queries to the system. Executing pre-compiled query plans removes interpretation along hot code paths, providing highly efficient code paths that minimize the number of[central processing unit](https://en.wikipedia.org/wiki/Central_processing_unit)(CPU) instructions required to process SQL statements.\nMemSQL is wire-compatible with[MySQL](https://en.wikipedia.org/wiki/MySQL).This means that applications can connect to MemSQL through MySQL clients and drivers, as well as standard[Open Database Connectivity](https://en.wikipedia.org/wiki/Open_Database_Connectivity)(ODBC) and[Java Database Connectivity](https://en.wikipedia.org/wiki/Java_Database_Connectivity)(JDBC) connectors.\nIn addition to MySQL syntax and functionality, MemSQL can also store columns in[JSON](https://en.wikipedia.org/wiki/JSON)format, and supports[geospatial](https://en.wikipedia.org/wiki/Geospatial)datatypes and operations.\n**Distributed Architecture**\n\nA MemSQL database is a distributed database implemented with aggregators and leaf nodes.MemSQL binaries used for aggregator and leaf nodes are nearly the same, with the only difference being the user identifying the node as an aggregator or leaf. An aggregator is responsible for receiving SQL queries, breaking them up across leaf nodes, and aggregating results back to the client. A leaf node stores MemSQL data and processes queries from the aggregator. All communication between aggregators and leaf nodes is done over the network through SQL syntax. MemSQL uses hash partitioning to distribute data uniformly across the number of leaf nodes.\n**Durability**\n\nMemSQL durability is slightly different for its in-memory rowstore and an on-disk columnstore.\nDurability for the in-memory rowstore is implemented with a write-ahead log and snapshots, similar to checkpoints. With default settings, as soon as a transaction is acknowledged in memory, the database will asynchronously write the transaction to disk as fast as the disk allows.\nThe on-disk columnstore is actually fronted by an in-memory rowstore-like structure (skiplist). This structure has the same durability guarantees as the MemSQL rowstore. Apart from that, the columnstore is durable since its data is stored on disk.\n**Replication**\n\nA MemSQL cluster can be configured in \"High Availability\" mode, where every data partition is automatically created with master and slave versions on two separate leaf nodes. In High Availability mode, aggregators send transactions to the master partitions, which then send logs to the slave partitions. In the event of an unexpected master failure, the slave partitions take over as master partitions in a fully online operation.\nEffortlessly set up streaming ingest feeds from[Apache Kafka](http://docs.memsql.com/docs/kafka-extractor),[Amazon S3](http://docs.memsql.com/docs/s3-pipelines-overview), and[HDFS](https://docs.memsql.com/docs/hdfs-pipelines-overview/) using a single[CREATE PIPELINE](http://docs.memsql.com/docs/create-pipeline)command\n<https://www.memsql.com>\n\n<https://en.wikipedia.org/wiki/MemSQL>\n\n<https://www.memsql.com/blog/a-technical-introduction-to-memsql>\n"},{"fields":{"slug":"/Databases/Others/Technologies---Tools/","title":"Technologies / Tools"},"frontmatter":{"draft":false},"rawBody":"# Technologies / Tools\n\nCreated: 2019-04-23 12:36:18 +0500\n\nModified: 2022-02-08 18:52:13 +0500\n\n---\n\n**Vitess**\n\nVitess is a database clustering system for horizontal scaling of MySQL through generalized sharding.\nBy encapsulating shard-routing logic, Vitess allows application code and database queries to remain agnostic to the distribution of data onto multiple shards. With Vitess, you can even split and merge shards as your needs grow, with an atomic cutover step that takes only a few seconds.\n**Traditional Transactional Architecture**\n\n![Begin Select Select Select Insert Database Select Update Commit User Module Campaign Module Matching Module Client ](media/Technologies---Tools-image1.png)\n<https://github.com/vitessio/vitess>\n\n<https://vitess.io>\n\n<https://www.planetscale.com/blog/videos-intro-to-vitess-its-powerful-capabilities-and-how-to-get-started>\n\n## MySQL Workbench / MySQLWorkbench**\n\n<https://dev.mysql.com/doc/workbench/en/wb-performance-explain.html>\n<https://www.mysql.com/products/workbench>\n<https://dev.to/realtrevorfaux/8-new-sql-tools-that-will-change-how-you-work-in-2020-n63>\n\n## Settings**\n-   Preferences > SQL Editor > DBMS connection read timeout interval (in seconds) > 3000\n**Shortcuts**\n\nEdit>Format>Beautify Query\n\nShortcut:CMD+B\n**Search**\n\n***.*test**\n\nfind all objects that include test in their name\n\n***.test**\n\nfind all objects that names start with test\n**Reverse Engineer**\n**Data Grip**\n**NoSQL Workbench**\n\n<https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/workbench.html>\n\n## DBeaver**\n\n**Universal Database Tool**\nDBeaveris an SQL client and a database administration tool. For[relational databases](https://en.wikipedia.org/wiki/Relational_database)it uses the[JDBC](https://en.wikipedia.org/wiki/JDBC)API to interact with databases via a JDBC driver. For other databases ([NoSQL](https://en.wikipedia.org/wiki/NoSQL)) it uses proprietary database drivers. It provides an editor that supports[code completion](https://en.wikipedia.org/wiki/Autocomplete)and[syntax highlighting](https://en.wikipedia.org/wiki/Syntax_highlighting). It provides a plugin architecture (based on the[Eclipse](https://en.wikipedia.org/wiki/Eclipse_(software))plugins architecture) that allows users to modify much of the application's behavior to provide database-specific functionality or features that are database-independent. This is a desktop application written in[Java](https://en.wikipedia.org/wiki/Java_platform)and based on[Eclipse](https://en.wikipedia.org/wiki/Eclipse_(software))platform.\nFree multi-platform database tool for developers, database administrators, analysts and all people who need to work with databases. Supports all popular databases: MySQL, PostgreSQL, SQLite, Oracle, DB2, SQL Server, Sybase, MS Access, Teradata, Firebird, Apache Hive, Phoenix, Presto, etc.\nWith DBeaver you are able to manipulate with your data like in a regular spreadsheet, create analytical reports based on records from different data storages, export information in an appropriate format. For advanced database users DBeaver suggests a powerful SQL-editor, plenty of administration features, abilities of data and schema migration, monitoring database connection sessions, and a lot more.\n\nOut-of-the box DBeaver supports more than 80 databases.\n\nHaving usability as its main goal, DBeaver offers:\n-   Carefully designed and implemented User Interface\n-   Support of Cloud datasources\n-   Support for Enterprise security standard\n-   Capability to work with various extensions for integration with Excel, Git and others.\n-   Great number of features\n-   Multiplatform support\n<https://dbeaver.io>\n\n<https://github.com/dbeaver/dbeaver>\n\n## Shortcuts**\n-   Ctrl + ] - New sql script\n**Teradata**\n\nAnalytics, Data Lakes and Data Warehouses Unified in the Cloud\n**Datastage**\n\nDatastage is an ETL tool which extracts data, transform and load data from source to the target. The data sources might include sequential files, indexed files, relational databases, external data sources, archives, enterprise applications, etc. DataStage facilitates business analysis by providing quality data to help in gaining business intelligence.\n<https://www.guru99.com/datastage-tutorial.html>\n\n## Liquibase**\n\nLiquibase helps millions of teams track, version, and deploy database schema changes.\nLiquibase Hub - Visualize all changes-   Changelogs\n    -   **Formats**\n        -   XML\n        -   YAML\n        -   JSON\n        -   SQL\n-   Tracking Tables\n    -   DATABASECHANGELOG\n\nTrack which changes have or have not been deployed\n-   DATABASECHANGELOGLOCK\n**Commands**\n\nliquibase update\n<https://www.liquibase.org>\n\n## Fixing Forward**\n\nSince rolling back database changes is complicated, time-consuming, and error-prone, the fixing forward approach is very quickly getting very popular.\n**Fixing forward is lower-risk.**\n\nDBAs aren't trying to get the database back to the old state. Instead, the focus is on getting to a good working state (with all that updated data).\n**Fixing forward eliminates overhead.**\n\nTeams can become more agile with their development for database code, making it very popular with the DevOps, CI/CD, and Agile development communities.\nThe fix forward method works best when changes are broken into small chunks that are deployed independently and automatically. If you're starting from a software development environment where you have one years' worth of work about to deploy, this may not be the approach you adopt right now. However, there are tools that can help you break up your database scripts and schema changes into small, trackable chunks that make this approach much more accessible to companies that are ready to try this out.\n<https://www.liquibase.com/blog/roll-back-database-fix-forward>\n\n"},{"fields":{"slug":"/Databases/Others/YugabyteDB/","title":"YugabyteDB"},"frontmatter":{"draft":false},"rawBody":"# YugabyteDB\n\nCreated: 2020-07-25 17:13:10 +0500\n\nModified: 2020-07-25 17:26:19 +0500\n\n---\n\nYugabyteDB is a high-performance, cloud-native distributed SQL database that aims to support all PostgreSQL features. It is best fit for cloud-native OLTP (i.e. real-time, business critical) applications that need absolute data correctness and require at least one of the following: scalability, high tolerance to failures, globally-distributed deployments.\nThe core features of YugabyteDB include:\n-   **Powerful RDBMS capabilities**Yugabyte SQL (YSQLfor short) reuses the query layer of PostgreSQL (similar to Amazon Aurora PostgreSQL), thereby supporting most of its features (datatypes, queries, expressions, operators and functions, stored procedures, triggers, extensions, etc). Here is a detailed[list of features currently supported by YSQL](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/YSQL-Features-Supported.md).\n-   **Distributed transactions**The transactions design is based on the Google Spanner architecture. Strongly consistency of writes is achieved by using Raft consensus for replication and cluster-wide distributed ACID transactions usinghybrid logical clocks. *Snapshot * andserializableisolation levels are supported. Reads (queries) have strong consistency by default, but can be tuned dynamically to read from followers and read-replicas.\n-   **Continuous availability**YugabyteDB is extremely resilient to common outages with native failover and repair. YugabyteDB can be configured to tolerate disk, node, zone, region and cloud failures automatically. For a typical deployment where a YugabyteDB cluster is deployed in one region across multiple zones on a public cloud, the RPO is 0 (meaning no data is lost on a failure) and the RTO is 3 seconds (meaning the data being served by the failed node is available in 3 seconds).\n-   **Horizontal scalability**Scaling a YugabyteDB cluster in order to achieve more IOPS or data storage is as simple as adding nodes to the cluster.\n-   **Geo-distributed, multi-cloud**YugabyteDB can be deployed in public clouds and natively inside Kubernetes. It supports deployments that span three or more fault domains, such as multi-zone, multi-region and multi-cloud deployments. It also supports xCluster asynchronous replication with unidirectional master-slave and bidirectional multi-master configurations that can be leveraged in two-region deployments. To serve (stale) data with low latencies, read replicas are also a supported feature.\n-   **Multi API design**The query layer of YugabyteDB is built to be extensible. Currently, YugabyteDB supports two distributed SQL APIs[Yugabyte SQL (YSQL)](https://docs.yugabyte.com/latest/api/ysql/), a fully relational API that re-uses query layer of PostgreSQL, and[Yugabyte Cloud QL (YCQL)](https://docs.yugabyte.com/latest/api/ycql/), a semi-relational SQL-like API with documents/indexing support with Apache Cassandra QL roots.\n-   **100% open source**YugabyteDB is fully open-source under the[Apache 2.0 license](https://github.com/yugabyte/yugabyte-db/blob/master/LICENSE.md). The open-source version has powerful enterprise features distributed backups, encryption of data at-rest, in-flight TLS encryption, change data capture, read replicas and others.\n<https://github.com/yugabyte/yugabyte-db>\n\n<https://docs.yugabyte.com/latest/comparisons>\n\n<https://docs.yugabyte.com/latest/architecture/design-goals>\n"},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Aurora/","title":"AWS Aurora"},"frontmatter":{"draft":false},"rawBody":"# AWS Aurora\n\nCreated: 2020-02-13 23:20:13 +0500\n\nModified: 2021-09-03 20:52:33 +0500\n\n---\n\nFor OLTP\nHigh Performance Managed Relational Database\nAmazon Aurora is a relational database offered as a service in Amazon's AWS. Based on the open source version of MySQL, it is a commercial database that claims to be compatible with MySQL and PostgreSQL while providing superior throughput. Being provided as a cloud service, Aurora promises high availability by using distributed replications of backend storage. The system is being actively maintained and updated by Amazon.\n**History**\n\nAurora was announced on Nov. 12, 2014 in Amazon's re:Invent conference in Las Vegas. It was officially released and ready to use as a service in AWS on July 27, 2015 by being added into the Amazon Relational Database Service. A major patch of Aurora was added in October 24, 2017, where Aurora was extended with PostgreSQL compatibility.\n**Concurrency Control**\n\n[Multi-version Concurrency Control (MVCC)](https://dbdb.io/browse?concurrency-control=multi-version-concurrency-control-mvcc)\nAurora decouples the storage engine from its database engine, and the concurrency control protocol is entirely decided by the database engine it used. In the paper that introduced Aurora, the concurrency control model was stated to be exactly the same as the database engine it inherited from. So Aurora has the same concurrency control protocol, MVCC, as MySQL/InnoDB does. InnoDB's MVCC protocol stores a separate data structure for \"rollback segments\", which are actually undo logs. In the situation of a consistent read (for isolation levels beyond read committed), the logs will be applied in place to reconstruct the requested earlier versions of a row.\n**Data Model**\n\n[Relational](https://dbdb.io/browse?data-model=relational)\nAurora is stated to be a relational database engine. That can also be inferred from its full inheritance of MySQL/InnoDB's database engine and storage layout.\n**Foreign Keys**\n\n[Supported](https://dbdb.io/browse?foreign-keys=supported)\nAurora supports foreign keys, just as MySQL/InnoDB does. Best practice guide on Aurora's documentation provided some insights on how to properly use foreign keys, please refer to the citation for more details.\n**Indexes**\n\n[B+Tree](https://dbdb.io/browse?indexes=btree)[Hash Table](https://dbdb.io/browse?indexes=hash-table)\nAurora uses the same indexes as MySQL/InnoDB. In MySQL, both b-tree and hash indexes are used. The default index choice for MySQL is b-tree unless explicitly specified. Aurora also supports spatial indexes, and its implementation utilizes a b-tree.\n**Isolation Levels**\n\n[Serializable](https://dbdb.io/browse?isolation-levels=serializable)\nIn the paper that introduced Aurora, the authors stated that Aurora has exactly the same isolation levels as MySQL. The supported isolation levels includes the standard ANSI levels and Snapshot Isolation.\n[Amazon Aurora](https://aws.amazon.com/rds/aurora/)with MySQL compatibility supports the ANSI READ COMMITTED isolation level on read replicas. This isolation level enables long-running queries on an Aurora read replica to execute without impacting the throughput of writes on the writer node.\n>>> SELECT @@TX_ISOLATION;\n\nREPEATABLE-READ\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Reference.html#AuroraMySQL.Reference.IsolationLevels>\n\n## Joins**\n\n[Hash Join](https://dbdb.io/browse?joins=hash-join)\nAurora has different join algorithm compared to MySQL. It supports hash join in addition to the already-existing nested loop join in MySQL. When the hash join option is enabled, Aurora's optimizer will automatically choose a join method as it evaluates the query plan. However, there are several restrictions for hash join in Aurora. To be more specific, left-right outer joins, semijoins such as subqueries and multiple-table updates or deletes are not supported.\n**Logging**\n\n[Logical Logging](https://dbdb.io/browse?logging=logical-logging)\nAurora's logging design is logical logging. The system separates the database engine from the backend storage, where the database engine propagates logs continuously to the backend storage. Such logs are then asynchronously processed by the distributed storage servers to bring the database to its latest state.\n**Query Compilation**\n\n[Not Supported](https://dbdb.io/browse?query-compilation=not-supported)\nSimilar to MySQL, Aurora doesn't support query compilation.\n**Query Execution**\n\n[Tuple-at-a-Time Model](https://dbdb.io/browse?query-execution=tuple-at-a-time-model)\nAurora uses the same query execution engine as MySQL.\n**Query Interface**\n\n[SQL](https://dbdb.io/browse?query-interface=sql)\nAurora supports the standard SQL query interface. It inherits the SQL compatibility from MySQL, as well as the extensions MySQL made to the SQL standard.\n**Storage Architecture**\n\n[Disk-oriented](https://dbdb.io/browse?storage-architecture=disk-oriented)\nAurora is a disk-oriented database.\n**Storage Model**\n\n[N-ary Storage Model (Row/Record)](https://dbdb.io/browse?storage-model=n-ary-storage-model-rowrecord)\nSame as MySQL/InnoDB, Aurora is a row-storage DBMS. The tuples are stored row-by-row in the distributed storage servers.\n**Stored Procedures**\n\n[Supported](https://dbdb.io/browse?stored-procedures=supported)\nSame as MySQL/InnoDB, Aurora supports stored procedures.\n**System Architecture**\n\n[Shared-Nothing](https://dbdb.io/browse?system-architecture=shared-nothing)\nAurora decouples its database engine from the storage backend. The database engine is a modified version with MySQL/InnoDB, where the storage backend consists of distributed replicas that span across different availability zones in AWS. The database engine will propagates logs to the backend storage, and the backend storage utilizes a quorum based synchronization scheme to ensure the consistency of the database.\nAurora architecture works on the basis of a cluster volume that manages the data for all the database instances in that particular cluster. A cluster volume spans across multiple availability zones and is effectively virtual database storage. The underlying storage volume is on top of multiple cluster nodes which are distributed across different availability zones. Separate from this, Aurora database can also have read-replicas. Only one instance usually serves as the primary instance and it supports reads as well as writes. The rest of the instances serve as read-replicas and load balancing needs to be handled by the user. This is different from the multiAZ deployment, where instances are located across the availability zone and support automatic failover.\n[Amazon Aurora](https://aws.amazon.com/rds/aurora/)is a relational database that was designed to take full advantage of the abundance of networking, processing, and storage resources available in the cloud. While maintaining compatibility with MySQL and PostgreSQL on the user-visible side, Aurora makes use of a modern, purpose-built distributed storage system under the covers. Your data is striped across hundreds of storage nodes distributed over three distinct AWS Availability Zones, with two copies per zone, on fast SSD storage. Here's what this looks like:\n\n![Availability Zone 1 SQL Transactions Caching Availability Zone 2 SQL Transactions Caching Shared storage volume Storage nodes with SSDs Availability Zone 3 SQL Transactions Caching ](media/AWS-Aurora-image1.png)-   Asynchronous Key Fetch\n-   Batched Scans\n**Views**\n\n[Materialized Views](https://dbdb.io/browse?views=materialized-views)\nSame as MySQL, Aurora supports materialized views\n<https://dbdb.io/db/aurora>\n\n<https://aws.amazon.com/rds/aurora/faqs>\n\n## Aurora vs RDS MySQL**\n\n<https://www.percona.com/blog/2018/07/17/when-should-i-use-amazon-aurora-and-when-should-i-use-rds-mysql>\n\n<https://www.actifio.com/company/blog/post/comparing-aws-rds-for-aurora-vs-mysql-vs-postgresql>\n\n"},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Redshift/","title":"AWS Redshift"},"frontmatter":{"draft":false},"rawBody":"# AWS Redshift\n\nCreated: 2020-02-13 22:47:08 +0500\n\nModified: 2021-12-18 12:32:30 +0500\n\n---\n-   OLAP\n-   Columnar data storage structure\n-   Fully managed data warehouse service\n-   Its datasets range from 100s of gigabytes to a petabyte\n-   **Based on industry-standard PostgreSQL and ParAccel's Massive Parallel Processing technology,** leveraging its distributed architecture, columnar storage, and column compression to execute exploratory queries\n-   Due to being based off of PostgreSQL, Redshift allows clients to make connections and execute DDL and DML SQL statements using JDBC or ODBC\n-   Very efficient, targeted data compression encoding schemes\n-   **Redshift spectrum** which basically allows the customers to use the computing power of Redshift cluster on data stored in S3 by creating external tables.\nThe initial process to create a data warehouse is to launch a set of compute resources callednodes, which are organized into groups calledcluster. After that you can process your queries.\n**History**\n\nIn July 2011, Amazon invested in ParAccel, a software company that developed a shared-nothing architecture relational database system for analytics and business intelligence. In exchange for its investment, Amazon acquired license rights to ParAccel's database system which would form the foundation of Amazon's own data warehouse solution: Amazon Redshift. After subsequent development by Amazon and integration with AWS, Amazon Redshift was officially announced at the AWS re:invent 2012 conference and, after a limited preview, was released to the general public in February 2013.\nRewrote whole storage engine for columnar, MPP\n**Compression**\n\n[Dictionary Encoding](https://dbdb.io/browse?compression=dictionary-encoding)[Delta Encoding](https://dbdb.io/browse?compression=delta-encoding)[Run-Length Encoding](https://dbdb.io/browse?compression=run-length-encoding)[NaÃ¯ve (Page-Level)](https://dbdb.io/browse?compression=naive-page-level)[Bit Packing / Mostly Encoding](https://dbdb.io/browse?compression=bit-packing-mostly-encoding)\nRedshift allows for the columns to be compressed, reducing data size and storing more data within each disk block. This allows for reduced disk I/O and improves query performance. Column compression will be automatically applied when loading data into Redshift using theCOPYcommand but can also be selected manually.\nRedshift allows for the following possible compression options.\n**Raw**\n\nRaw encoding stores the data as-is. None of the values are compressed. By default, no compression is applied to values of columns defined as the sort key and values ofBOOLEAN,REAL, orDOUBLEdatatypes.\n**Byte-Dictionary**\n\nFor each 1MB block on disk, a dictionary a created which maps the first 256 unique column values to a single byte. In the original data, those values are replaced with the corresponding single byte. If there are more than 256 unique column data values in a block, any unique data values beyond the first 256 are stored raw. This encoding is primarily suited for columns containing a limited number of character values and does not supportBOOLEANdatatypes.\n**Delta**\n\nFor each 1MB block on disk, data is stored as the difference relative to the previous value in series. Redshift supports two delta variations, DELTA (supports SMALLINT, INT, BIGINT, DATE, TIMESTAMP, DECIMAL) which stores difference as 1-byte values and DELTA32K (INT,BIGINT,DATE,TIMESTAMP,DECIMAL) which stores the difference as 2-byte values. Any difference greater than the delta representable is stored raw along with a 1 byte flag.\n**Mostly**\n\nThis encoding utilizes packing to reduce storage. In the event that the value cannot be compressed, the original raw value is stored. MOSTLY8 supportsSMALLINT,INT,BIGINT, andDECIMAL. MOSTLY16 supportsINT,BIGINT, andDECIMAL. MOSTLY32 supportsBIGINTandDECIMAL.\n**Runlength**\n\nFor each 1MB block on disk, consecutive values are replaced with a corresponding token that indicates the number of repetitions and the value repeated. A separate dictionary of unique values is also created for each 1MB block. Runlength is supported for all datatypes.\n**Naive Block-level**\n\nUnder this encoding, each block is compressed with a standard compression algorithm. Particular choices includeLZOandZTSD. For all columns other than the sort key or with typesBOOLEAN,REAL, orDOUBLE,LZOis the default compression.-   Compress ratio: 2 to 4 times less\n-   By default COPY automatically analyzes and compresses data on first load into an empty table\n-   ANALYZE COMPRESSION is a built-in command that will find the optimal compression for each column on an existing table\n-   Compression can return raw if there are very less number of rows in a column or data is very sparse (i.e. column has a lot of NULL)\n-   Changing column encoding requires a table rebuild\n-   SELECT \"column\", type, encoding FROM pg_table_def WHERE tablename = 'deep dive';\n![Compression: Example CREATE TABLE deep_dive ( aid aid INT ENCODE , loc CHAR(3) ENCODE , dt DATE ENCODE ZSTD BYTE-DICT RUNLENGTH dt loc SFO JFK SFO JFK dt 2017-10-20 2017-10-20 2017-04-01 2017-05-14 aid Ã¦:lnvent loc More efficient compression is due to storing the same data type in the columnar architecture Columns grow and shrink independently Reduces storage requirements Reduces I/O 2018, Amazon Web Services. or its affiliates, All rights reserved, aws ](media/AWS-Redshift-image1.png)\n**Concurrency Control**\n\n[Multi-version Concurrency Control (MVCC)](https://dbdb.io/browse?concurrency-control=multi-version-concurrency-control-mvcc)\nAlthough not explicitly stated, Redshift utilizes Multi Version Concurrency Control. In particular, transactions capture a snapshot of the latest committed version of the data at the time aSELECTquery, a DML statement,ALTER TABLEstatement,CREATE TABLEstatement,DROP TABLEstatement, orTRUNCATE TABLEstatement is executed. Redshift prevents write-write conflicts from happening by forcing a transaction to obtain a table-level write lock and only allowing a transaction to release all its write locks when the transaction either commits or aborts. Furthermore, aVACUUMoperation is required in order to remove all records marked for deletion and also perform any resorting operations that may or may not be required.\nHowever, Redshift does not offer deadlock prevention or deadlock detection. As such, Redshift warns users to schedule transaction operations in a way that would prevent any deadlocks from arising in the first place, such as by updating tables in the same order or taking locks in the same order.\n**Data Model**\n\n[Relational](https://dbdb.io/browse?data-model=relational)\nRedshift is a relational database even though it is built upon PostgreSQL. Particular reasons are that Redshift does not support many features considered to be part of the \"object-relational\" definition, such as but not limited to inheritance and definition of custom structured types.\n**Foreign Keys**\n\n[Supported](https://dbdb.io/browse?foreign-keys=supported)\nRedshift supports the concept of foreign keys but does not actually enforce the foreign key constraint. Redshift utilizes foreign keys as pieces of information during the query planning and optimization stage. However, Redshift does not spend computational resources to ensure that the constraint holds, instead relying on the application where the data originated from to ensure that the foreign key constraint is satisfied.\n**Indexes**\n\n[Not Supported](https://dbdb.io/browse?indexes=not-supported)\nRedshift does not support indexes. As such, Redshift does not suffer any computational overhead from the creation, maintenance, or concurrent use of index data structures.\n**Isolation Levels**\n\n[Serializable](https://dbdb.io/browse?isolation-levels=serializable)\nRedshift only supports serializable isolation, which provides each transaction with the illusion that they are the only transaction operating on a table at a given time (utilizing table-level locks) and ensures that the end-result is equivalent to some serial execution of the transactions. In the event that a transaction executes an operation which would violate serializability, the violating transaction would be aborted and rolled back.\n**Joins**\n\n[Nested Loop Join](https://dbdb.io/browse?joins=nested-loop-join)[Hash Join](https://dbdb.io/browse?joins=hash-join)[Sort-Merge Join](https://dbdb.io/browse?joins=sort-merge-join)[Broadcast Join](https://dbdb.io/browse?joins=broadcast-join)[Shuffle Join](https://dbdb.io/browse?joins=shuffle-join)\nThe query planner and optimizer picks the best join and distributed joining algorithm possible. The three join algorithms utilized by Redshift are nested join, hash join which is used for inner and left/right outer joins, and merge join which is used for inner and outer joins. Redshift only uses merge join if the join column is both the distribution and sort key and if the percentage of unsorted data in the two tables is less than 20%.\nIn the event that the query planner needs to move data around, Redshift will either perform a redistribution (shuffle) or broadcast one side of the join to all other nodes. If redistribution is needed, Redshift may move table data between slices of a single node or between nodes, utilizing the distribution key if the distribution key is part of the join.\n**Logging**\n\n[Logical Logging](https://dbdb.io/browse?logging=logical-logging)\nRedshift provides logging for both audit purposes and also for all operations executed by transactions on the system. In particular, Redshift logs the rawSQLstatements that are executed by users and transactions in the system. Furthermore, with the auditing functionality built-in to Redshift, administrators can also track all the SQL statements executed by a specific user.\n**Query Compilation**\n\n[Code Generation](https://dbdb.io/browse?query-compilation=code-generation)\nThe query execution plan is generated at the leader node of a particular Redshift cluster. The leader (coordinator) node is responsible for evaluating all the possible execution plans and cost effectiveness of each plan. The leader node rewrites the query, generates compiled C++ code, and sends the compiled binaries to the compute nodes for execution.\n**Query Execution**\n\n[Materialized Model](https://dbdb.io/browse?query-execution=materialized-model)\nRedshift utilizes the materialized query processing model, where each processing step emits the entire result at a time. The leader node is responsible for coordinating query execution with the compute nodes and stitching together the results of all the compute nodes into a final result that is returned to the user.\nTo improve performance, Redshift utilizes late materialization where row reconstruction, utilizing the value's block position, is delayed until later steps in the process. Furthermore, Redshift utilizes zone map optimization for its sequential scan, storing a min and max value in the header of each disk block to allow the executor to determine which blocks can be skipped.\n**Query Interface**\n\n[SQL](https://dbdb.io/browse?query-interface=sql)[PartiQL](https://dbdb.io/browse?query-interface=partiql)\nRedshift supports a majority of the standard DDL statements to create and define tables and DML statements to manipulate the data stored within the database. Furthermore, Redshift supports scalar User-Defined Functions that can be constructed either via a SQLSELECTclause or a Python program.\nIn addition, there are certain functions that can only be executed on the leader node, primarily functions to query the database schema. Furthermore, Redshift implements various extensions to SQL, such as aggregate functions, string functions, and JSON functions although careful care must be taken to the many PostgreSQL features that became unsupported in Redshift.\nIn 2019, Amazon announced that Redshift Spectrum supports their own SQL dialect called PartiQL.\n**Storage Architecture**\n\n[Disk-oriented](https://dbdb.io/browse?storage-architecture=disk-oriented)\nRedshift uses disk storage. In the distributed system, all the data is stored at the compute node layer. Based on the particular distribution style elected for a particular table, the leader node will either duplicate the data across all the compute nodes or partition the data across all the compute nodes. Furthermore, each compute node will partition its data across its CPU slices in order to achieve maximum parallel computation. To tolerate node or slice failures, data is also duplicated at other nodes in the system.\n**Storage Model**\n\n[Decomposition Storage Model (Columnar)](https://dbdb.io/browse?storage-model=decomposition-storage-model-columnar)\nRedshift utilizes columnar storage as opposed to row storage. Instead of storing the entire row record together, Redshift stores the values of each table column together. This allows Redshift to pack data together and apply compression in order to minimize disk I/O during query execution. A row can be stitched together by utilizing the offset of a specific value.\n**Storage Organization**\n\n[Heaps](https://dbdb.io/browse?storage-organization=heaps)[Sorted Files](https://dbdb.io/browse?storage-organization=sorted-files)\nOn tables without a sort key specified and that remains unsorted, Redshift preserves the order in which the records were originally inserted in. In the unsorted scenario, Redshift stores data in 1MB blocks on disk where new records are simply appended to the end.\nOn tables with a sort key specified, Redshift stores the \"sorted\" portion of the data in sorted blocks on disk. Adding new data to the table except for the special case listed below will result in the data being appended to the \"unsorted\" portion that will only be merged into the \"sorted\" potion upon aVACUUM. In addition, Redshift provides that theCOPYcommand will automatically sort the incoming data.\nIn the event that the following conditions are met, adding data to a table will preserve the sortedness of the data and not require an additionalVACUUMoperation to sort the table.\n\na.  The sort column isNOT NULLand there is only 1 sort column\n\nb.  UsingCOPYinto an empty table or the table is 100% sorted\n\nc.  The data can be appended in sort-order to the end of any existing data\n**Stored Procedures**\n\n[Not Supported](https://dbdb.io/browse?stored-procedures=not-supported)\nStored procedures, from PostgreSQL, are not supported.\n**System Architecture**\n\n[Shared-Nothing](https://dbdb.io/browse?system-architecture=shared-nothing)\nRedshift clusters take a two-tiered architecture approach. The leader node serves the role as the coordinator and handles accepting client query requests, generating the query plan, dispatching query fragments to compute nodes, and coalescing results from compute nodes.\nThe compute nodes utilize a shared-nothing architecture, with each node having dedicated CPU, memory, and disk storage. The leader node determines the data each node stores and will only dispatch a query fragment to a node if the query requires data from that node. For performance reasons, each compute node assigns a portion of memory and disk to each CPU slice for parallel processing.\nTo ensure data availability during disk or node failure, Redshift utilizes synchronous replication to save redundant copies of the data on other nodes in the system. To provide further durability, Redshift provides complete automated backups to S3 which can then later be used to restore the entire database or a particular table to a cluster.\n**Leader Node**\n-   SQL endpoint\n-   Stores metadata\n-   Coordinates parallel SQL processing\n**Compute Nodes**\n-   Local, columnar storage\n-   Executes query in parallel\n-   Load, unload, backup, restore\n**Amazon Redshift Spectrum Nodes**\n-   Execute queries directly again Amazon S3\n![Compute ---node --- SQL clients/Bl tools JDBC/ODBC Leader ---node --- Compute ---node--- Compute ---node --- Load Query Amazon Redshift Spectrum Amazon S3 ](media/AWS-Redshift-image2.png)\n**Views**\n\n[Virtual Views](https://dbdb.io/browse?views=virtual-views)\nRedshift supports virtual views. The contents of the view are not directly materialized and the query defining the view is rerun every time the view is being used in another query.DELETEorUPDATEstatements cannot be used against the view.\n<https://docs.aws.amazon.com/redshift/latest/dg/c_high_level_system_architecture.html>\n\n<https://docs.aws.amazon.com/redshift/latest/dg/c_challenges_achieving_high_performance_queries.html>\n<https://dbdb.io/db/redshift>\n\n## Table Locks**\n\nAmazon Redshift uses table-level locks\nAmazon Redshift has three lock modes:\n-   **AccessExclusiveLock:**Acquired primarily during DDL operations, such as ALTER TABLE, DROP, or TRUNCATE. AccessExclusiveLock blocks all other locking attempts.\n-   **AccessShareLock:**Acquired during UNLOAD, SELECT, UPDATE, or DELETE operations. AccessShareLock blocks only AccessExclusiveLock attempts. AccessShareLock doesn't block other sessions that are trying to read or write on the table.\n-   **ShareRowExclusiveLock:**Acquired during COPY, INSERT, UPDATE, or DELETE operations. ShareRowExclusiveLock blocks AccessExclusiveLock and otherShareRowExclusiveLock attempts, but doesn't block AccessShareLock attempts.\nWhen a query or transaction acquires a lock on a table, the lock remains for the duration of the query or transaction. Other queries or transactions that are waiting to acquire the same lock are blocked.\n<https://aws.amazon.com/premiumsupport/knowledge-center/prevent-locks-blocking-queries-redshift>\nIt is worth mentioning that[Merge join](https://docs.aws.amazon.com/redshift/latest/dg/c-the-query-plan.html)(typically the fastest join ; for inner joins and outer joins) is used when joining tables where the join columns are[both distribution keysandsort keys]{.underline}, and[when less than 20 percent of the joining tables are unsorted]{.underline}.The merge join is not used for full joins.\n<https://aws.amazon.com/blogs/big-data/whats-new-in-amazon-redshift-2021-a-year-in-review>\n-   Query Editor V2 with charts and jupyter notebooks"},{"fields":{"slug":"/Databases/SQL-Databases/MySQL/","title":"MySQL"},"frontmatter":{"draft":false},"rawBody":"# MySQL\n\nCreated: 2020-02-08 10:39:01 +0500\n\nModified: 2022-11-09 19:08:52 +0500\n\n---\n\n**MySQL gotchas**\n-   select * from table_name where column != 'abc'; # doesn't return NULL rows\nMany tables and relationship between tables\n\nFull set of ACID properties\n\nMaster slave architecture, so scales up pretty well\nMySQL is an open-source relational database management system. It is considered a \"fast, stable, and true multi-user, multi-threaded sql server.\" MySQL is used in many web applications and websites, and there are several commercial databases that are compatible with it. Users interact with the database through the Structured Query Language (SQL). The default version of MySQL uses InnoDB as a storage engine. However, the MySQL storage engine architecture is pluggable, allowing for a specialized storage engine to be used. MySQL has been actively updated and supported since 1995, with new versions being released every 1-3 years. Several MySQL forks exist, most notably MariaDB, which is led by the original MySQL developers. Oracle, which acquired MySQL in 2010, offers several paid editions of the DBMS that offer additional features.\n**History**\n\nMySQL was first released on May 23, 1995. It was created by MySQL AB, and first developed by two of MySQL AB's founders, David Axmark and Monty Widenius. Sun Microsystems acquired MySQL AB in 2008, and thus Oracle acquired MySQL through their 2010 acquisition of Sun. In response to the Oracle purchase, Michael Widenius forked MySQL, creating MariaDB.\nInnoDB became the default storage engine for MySQL at version 5.5, in 2010. Prior to this, the default storage engine had been MyISAM, which lacked transaction support and foreign key support. Some MySQL forks also used a fork of InnoDB.\n**Checkpoints**\n\n[Fuzzy](https://dbdb.io/browse?checkpoints=fuzzy)\nInnoDB takes fuzzy checkpoints during normal operation. When the database shuts down, it takes a sharp checkpoint.\n**Compression**\n\n[NaÃ¯ve (Page-Level)](https://dbdb.io/browse?compression=naive-page-level)\nInnoDB supports table compression and page compression. Specifically, InnoDB supports transparent page compression. The algorithms used for compression are Zlib and LZ4. In order to compress or uncompress an existing table, the table must be rebuilt. The compressed data is held in small pages in the buffer pool, but in order to extract or update column values, MySQL must also create an uncompressed page in the buffer pool, containing the uncompressed data.\n**Concurrency Control**\n\n[Multi-version Concurrency Control (MVCC)](https://dbdb.io/browse?concurrency-control=multi-version-concurrency-control-mvcc)[Two-Phase Locking (Deadlock Detection)](https://dbdb.io/browse?concurrency-control=two-phase-locking-deadlock-detection)\nMySQL/InnoDB utilizes Multi-version Concurrency Control. In doing so, it seeks to combine the benefits of a multi-versioning database with two phase locking. InnoDB stores information about the old versions of changed rows in a data structure called a rollback segment. This information is used to perform undo operations in the event that a transaction rollback is required. InnoDB uses shared, exclusive, intention, record, gap, next-key, insert intention, and AUTO-INC locks. In addition, when handling spatial indexes, InnoDB utilizes predicate locks.\n**Data Model**\n\n[Relational](https://dbdb.io/browse?data-model=relational)\nMySQL supports a relational model.\n**Foreign Keys**\n\n[Supported](https://dbdb.io/browse?foreign-keys=supported)\nMySQL supports foreign keys and utilizes foreign key constraints to keep the data consistent. InnoDB does not support foreign keys for tables with user-defined partitioning, though certain NDB tables can.\n**Indexes**\n\n[B+Tree](https://dbdb.io/browse?indexes=btree)\nThe default InnoDB indexes are B+Tree data structures. MySQL also supports spatial indexes, which use R-trees. Earlier versions supported hash indexes, but current versions do not and hash indexes have been entirely replaced by B+Trees.\nMost MySQL indexes (PRIMARY KEY,UNIQUE,INDEX, andFULLTEXT) are stored in[B-trees](https://dev.mysql.com/doc/refman/8.0/en/glossary.html#glos_b_tree). Exceptions: Indexes on spatial data types use R-trees;MEMORYtables also support[hash indexes](https://dev.mysql.com/doc/refman/8.0/en/glossary.html#glos_hash_index);InnoDBuses inverted lists forFULLTEXTindexes.\n<https://dev.mysql.com/doc/refman/8.0/en/mysql-indexes.html>\n\n## Isolation Levels**\n\n[Read Uncommitted](https://dbdb.io/browse?isolation-levels=read-uncommitted)[Read Committed](https://dbdb.io/browse?isolation-levels=read-committed)[Serializable](https://dbdb.io/browse?isolation-levels=serializable)[Repeatable Read](https://dbdb.io/browse?isolation-levels=repeatable-read)\nMySQL/InnoDB supports all four of the isolation levels defined by the ANSI/ISO SQL standard - read uncommitted, read committed, repeatable read, and serializable. The default isolation level for InnoDB is repeatable read isolation.\n**Joins**\n\n[Nested Loop Join](https://dbdb.io/browse?joins=nested-loop-join)\nMySQL uses the Nested-Loop Join Algorithm and the Block Nested-Loop Join Algorithm to execute joins between tables. The Block Nested-Loop Join Algorithm reduces the number of times the inner table must be read by buffering rows read in the outer loops, making the join faster.\n**Logging**\n\n[Physical Logging](https://dbdb.io/browse?logging=physical-logging)[Physiological Logging](https://dbdb.io/browse?logging=physiological-logging)\nMySQL uses the InnoDB transaction log to support data durability. The InnoDB transaction log handles REDO logging using a physiological logging scheme, which maintains atomicity, consistency, and durability. MySQL also has several server logs, though by default, none of them are enabled. The server logs use physical logging.\n**Query Compilation**\n\n[Not Supported](https://dbdb.io/browse?query-compilation=not-supported)\n**Query Execution**\n\n[Tuple-at-a-Time Model](https://dbdb.io/browse?query-execution=tuple-at-a-time-model)\nMySQL supports an iterator query processing model. Its iterator interface is tuple-at-a-time, and it does not support intra-query parallelism.\n**Query Interface**\n\n[SQL](https://dbdb.io/browse?query-interface=sql)\nMySQL supports the standard SQL query interface to load data and execute queries. However, there are some slight differences between the MySQL implementation and the standard interface. These differences are found in the utilization of certain commands in the Sybase SQL extension and in the behavior of UPDATE. There are also a few foreign key differences, primarily due to InnoDB. MySQL has also removed the \"--\" start-comment sequence allowed by SQL, and instead uses only the C syntax comment style used by SQL. In addition, MySQL supports several extensions to the standard SQL query interface.\n**Storage Architecture**\n\n[Disk-oriented](https://dbdb.io/browse?storage-architecture=disk-oriented)\nMySQL/InnoDB is a disk-oriented DBMS. InnoDB uses disk storage and a buffer pool divided into pages in main memory to cache table and index data as it is accessed.The buffer pool uses an LRU replacement policy, moving older pages toward the tail of a sublist as other pages are used, until they are evicted.\n**Storage Model**\n\n[N-ary Storage Model (Row/Record)](https://dbdb.io/browse?storage-model=n-ary-storage-model-rowrecord)\nMySQL/InnoDB is a row-storage DBMS. InnoDB supports four row formats, which determine how the data is stored within the B-trees. The formats are redundant, compact, dynamic, and compressed.\n**Storage Organization**\n\n[Copy-on-Write / Shadow Paging](https://dbdb.io/browse?storage-organization=copy-on-write-shadow-paging)\nInnoDB tables arrange the data on disk based on primary keys, in order to optimize queries. Each table has a clustered index, which is a primary key index that organizes the data. InnoDB primarily uses a buffer pool, change buffer, adaptive hash index, and log buffer as in memory structures, while the disk structures used are primarily tables, indexes, tablespaces, and the doublewrite buffer. InnoDB uses a doublewrite buffer, one of the disk structures, to write pages after they have been flushed, prior to writing pages to the actual data file.\n**Stored Procedures**\n\n[Supported](https://dbdb.io/browse?stored-procedures=supported)\nMySQL supports stored procedures. Supported stored procedures must be written in SQL.\n**System Architecture**\n\n[Shared-Everything](https://dbdb.io/browse?system-architecture=shared-everything)\nMySQL is a shared-everything DBMS. The architecture of the system is comprised of three layers.\n\nThe topmost layer is the client application, which contains services such as connection handling, authentication, and security. Each client receives its own thread for connecting to the server. All of that client's queries will be executed within that thread.\nThe middle layer is the MySQL server layer, which handles the logic of the DBMS. The second layer handles query parsing, analysis, optimization, caching, and built-in functions. This is the layer that handles stored procedures and views.\nThe third and final layer is responsible for storing and retrieving all of the data. MySQL utilizes a pluggable storage engine architecture, and thus communicates with the selected storage engine - the default of which is InnoDB - using thestorage engine API. Thestorage engine APIallows for different storage engines to be used with MySQL while allowing the DBMS to provide similar levels of functionality.\n**Views**\n\n[Virtual Views](https://dbdb.io/browse?views=virtual-views)\nMySQL supports views, including updatable and insertable views. It does not natively support materialized views\n<https://dbdb.io/db/mysql>\n\n## Architecture**\n\nMySQLs design supports a wide range of underlying storage engines, here's a simple picture to illustrate that.\n\n![Image for post](media/MySQL-image1.png)\nClients connect to MySQL and issues queries (which may or may not already be cached), these queries are parsed, optimized, and then MySQL through a defined API will interact with a chosen storage engine to retrieve/persist data. Each storage engine has different properties that make it suitable for different use cases.\n**Important Paths**\n-   datadir - where the data lives by default - default location for:\n    -   Data and indexes for non-InnoDB tables\n    -   InnoDB file-per-table, general, syste, and undo log tablespaces\n-   innodb_data_home_dir - default location for the InnoDB system tablespace\n-   innodb_log_group_home_dir - path to the InnoDB redo log files\n-   innodb_undo_directory - path to the InnoDB undo tablespaces\n-   log_bin - dual functionality: enable binary logging and set path/file name prefix\n-   log_error\n<https://www.mysqltutorial.org/advanced-mysql>\n\n## InnoDB internals**\n\n![critical parameter Thumb rule: 60-80% of RAM que buffer pool Page level cache far tables and indexes Table 1 _ihd Table2ibd Table3â€ibd XtraDB, NOOB BUFFER POOL PAGES sfu_nvs content of que Insert buffer parl of buffer_paol â€¢ rnay e o XtraDB: innodb ibuf max size Disable: innodb chan bufferin ackgrour,d thread â€¢merges \" tAJffer with indexes Adaptive hash search --- speeds up search by secondary indexes lookups and range scans Check sizes in SHOW INNODB STATUS OperÂ«f tabks into can unlimitedly XtraDB: innodb dict Size limit Misc internal memory: Page Nash Check sizes in File system SHOW INNODB STATUS Lcxk system Re-an ery system i nn uffer_size 4M-16M is value Reads are dale in fcreground. are Pagos are Mitten in bac*ground innodb write io threads. innodb flush method -O DIRECT TO avoid '*hing in OS cadie Writes are done via 'double Mite buffet' to prevent corruptims Changes are Exed in file via buffer Log r Usually changes are fixed In background \"log thread\" on disk with fsync() commancf cm trols to fsync Tablgl_ihd Primary key Secondary indexes Table2â€ihd Table3.ibd Disk Abd are placed separetely if Otherwise in Files are divided by 16K pages 4K. 8K. 16K XtraDB: you can view content in I S]NNOOB sys TABLES, I S.INNODB sys INDEXES lbdatal ( system table space ) Data dictiona ty C%uble Mite buffer Insert buffer Changes to secondary non- indexes are buffered there. Rollback segments Undo Llndo UNDO Llndo file file InnoDB-std' rollback segment is I innodb_extra_rsegments 1023 shts per segment Pointers to undo space NOO space may grow unlimitedly use separate threads fot cleaning REDO LOGS InnoDd-std: max size 4GB XtraDB: max size 2 TB (usually 2-3) ](media/MySQL-image2.png)"},{"fields":{"slug":"/Databases/SQL-Databases/Normalization/","title":"Normalization"},"frontmatter":{"draft":false},"rawBody":"# Normalization\n\nCreated: 2020-01-27 19:22:52 +0500\n\nModified: 2021-04-27 23:59:44 +0500\n\n---\n-   Normalization is the process of organizing the data in the database.\n-   Normalization is used to minimize the redundancy from a relation or set of relations. It is also used to eliminate the undesirable characteristics like Insertion, Update and Deletion Anomalies.\n-   Normalization divides the larger table into the smaller table and links them using relationship.\n-   The normal form is used to reduce redundancy from the database table.\n**Why Data Normalization?**\n\n1.  **Increased consistency.** Information is stored in one place and one place only, reducing the possibility of inconsistent data.\n\n2.  **Easier object-to-data mapping.** Highly-normalized data schemas in general are closer conceptually to object-oriented schemas because the object-oriented goals of promoting high cohesion and loose coupling between classes results in similar solutions (at least from a data point of view).\nYou typically want to have highly normalized operational data stores (ODSs) and data warehouses (DWs).\nThe primary disadvantage of normalization is slower reporting performance. You will want to have a[denormalized](http://agiledata.org/essays/dataNormalization.html#Denormalization)schema to support reporting, particularly in data marts.\n**Anamolies of DB**\n\n**Example:** Suppose a manufacturing company stores the employee details in a table named employee that has four attributes: emp_id for storing employee's id, emp_name for storing employee's name, emp_address for storing employee's address and emp_dept for storing the department details in which the employee works. At some point of time the table looks like this:\n\n| emp_id | emp_name | emp_address | emp_dept |\n|--------|----------|-------------|----------|\n| 101    | Rick     | Delhi       | D001     |\n| 101    | Rick     | Delhi       | D002     |\n| 123    | Maggie   | Agra        | D890     |\n| 166    | Glenn    | Chennai     | D900     |\n| 166    | Glenn    | Chennai     | D004     |\n\nThe above table is not normalized. We will see the problems that we face when a table is not normalized.\n**Update anomaly:** In the above table we have two rows for employee Rick as he belongs to two departments of the company. If we want to update the address of Rick then we have to update the same in two rows or the data will become inconsistent. If somehow, the correct address gets updated in one department but not in other then as per the database, Rick would be having two different addresses, which is not correct and would lead to inconsistent data.\n**Insert anomaly:** Suppose a new employee joins the company, who is under training and currently not assigned to any department then we would not be able to insert the data into the table if emp_dept field doesn't allow nulls.\n**Delete anomaly:** Suppose, if at a point of time the company closes the department D890 then deleting the rows that are having emp_dept as D890 would also delete the information of employee Maggie since she is assigned only to this department.\n**Normalization**\n-   First normal form(1NF)\n-   Second normal form(2NF)\n-   Third normal form(3NF)\n-   Boyce & Codd normal form (BCNF)\n-   Fourth normal form (4NF)\n-   Fifth normal form (5NF)\n**First Normal Form (1NF)**\n\nAn attribute (column) of a table cannot hold multiple values. It should hold only atomic values.\nSample Employee table, it displays employees are working with multiple departments.\n\n| Employee | Age | Department        |\n|----------|-----|-------------------|\n| Melvin   | 32  | Marketing, Sales  |\n| Edward   | 45  | Quality Assurance |\n| Alex     | 36  | Human Resource    |\nEmployee table following 1NF:\n\n| Employee | Age | Department        |\n|----------|-----|-------------------|\n| Melvin   | 32  | Marketing         |\n| Melvin   | 32  | Sales             |\n| Edward   | 45  | Quality Assurance |\n| Alex     | 36  | Human Resource    |\n**Second Normal Form (2NF)**\n-   Should be in 1NF\n-   All non-key attributes are fully functional dependent on the primary key\nSample Products table:\n\n| productID | product    | Brand   |\n|-----------|------------|---------|\n| 1         | Monitor    | Apple   |\n| 2         | Monitor    | Samsung |\n| 3         | Scanner    | HP      |\n| 4         | Head phone | JBL     |\nProduct table following 2NF:\n\nProducts Category table:\n\n| productID | product    |\n|-----------|------------|\n| 1         | Monitor    |\n| 2         | Scanner    |\n| 3         | Head phone |\n\nBrand table:\n\n| brandID | brand   |\n|---------|---------|\n| 1       | Apple   |\n| 2       | Samsung |\n| 3       | HP      |\n| 4       | JBL     |\n\nProducts Brand table:\n\n| pbID | productID | brandID |\n|------|-----------|---------|\n| 1    | 1         | 1       |\n| 2    | 1         | 2       |\n| 3    | 2         | 3       |\n| 4    | 3         | 4       |\n![Second Normal Form (2NF)](media/Normalization-image1.png)\n**Third Normal Form (3NF)**\n-   Should be already in 2NF\n-   No transition dependency exists\n\nno column entry should be dependent on any other entry (value) other than the key for the table.\n\nIf such an entity exists, move it outside into a new table.\n**Boyce-Codd Normal Form (BCNF)**\n\n3NF and all tables in the database should be only one primary key.\nIt is an advance version of 3NF that's why it is also referred as 3.5NF. BCNF is stricter than 3NF. A table complies with BCNF if it is in 3NF and for every[**functional dependency**](https://beginnersbook.com/2015/04/functional-dependency-in-dbms/)X->Y, X should be the super key of the table.\n**Fourth Normal Form (4NF)**\n\nTables cannot have multi-valued dependencies on a Primary Key.\n**Fifth Normal Form (5NF)**\n\nA composite key shouldn't have any cyclic dependencies.\n<https://www.w3schools.in/dbms/database-normalization>\n\n<http://www.bkent.net/Doc/simple5.htm>\n\n## Denormalization**\n\nThe majority of modern applications need to be able to retrieve data in the shortest time possible. And that's when you can consider denormalizing a relational database. As the name suggests, denormalization is the opposite of normalization. When you normalize a database, you organize data to ensure integrity and eliminate redundancies. Database denormalization means you deliberately put the same data in several places, thus increasing redundancy.\n**The main purpose of denormalization is to significantly speed up data retrieval.**\n**Techniques**\n-   Storing derivable data\n-   Using pre-joined tables\n-   Using hardcoded values\n-   Keeping details with the master\n-   Repeating a single detail with its master\n-   Adding short-circuit keys\n<https://rubygarage.org/blog/database-denormalization-with-examples>\n\n"},{"fields":{"slug":"/Databases/SQL-Databases/Partitioning---Sharding/","title":"Partitioning / Sharding"},"frontmatter":{"draft":false},"rawBody":"# Partitioning / Sharding\n\nCreated: 2018-04-10 00:20:45 +0500\n\nModified: 2020-10-25 22:31:09 +0500\n\n---\n\n**Partitioning/Sharding Data**\n\nWe cannot store 1 Trillion entries in a database, so we can shard / split / divide databases into parts where one part is responsible for that amount of data.\n**Partitioning vs Sharding**\n\n**Shard is also commonly used to mean \"shared nothing\" partitioning. But it's also possible to have a \"shared nothing\" architecture without partitioning.**\nA partition is a physically separate file that comprises a subset of rows of a logical file, which occupies the same CPU+memory+storage node as its peer partitions.\nA shard is a physical compute node comprised of CPU+memory+storage. A shard's schema (and integrity constraints) may be replicated across as many shards as needed. Shards may contain unpartioned and partitioned tables.\nWhen using shards and partitions together we effectively have two keys which we can use to chunk out the data. How we decide to choose those keys depends on the query biases of the main applications reading and writing data.\nFor example, Facebook could shard its data by User Key (so you might live on one MySQL node, and I might live on another MySQL node). But within those nodes, they could also partition data based on Create Date of the timeline item (e.g. items posted could be broken down by month).\nThis sharding and partitioning scheme would make sense for Facebook because of Facebook's natural query biases during operations. People normally look at the stuff that pertains to them (which can live on the same shard - even if some of that data is replicated from other user shards), and they will then dig into stuff that is recent (which will live within a small physical file, holding the current month's partition). So the partition key could be based on User ID + Month ID (since presumably within the shard multiple users would still exist). Keep in mind that both shard keys and partition keys can be composite keys based on multiple columns.\nAnother thing to keep in mind is that within a shard, the RDBMS can protect data integrity. So Facebook could configure foreign key constraints to other local tables (e.g. the local MySQL instance can guarantee that a photo has to belongs to a valid Facebook User). If Facebook were to instead shard based on Date instead of User ID, then they would not be able to provide these integrity guarantees for user items, like photos.\nSo it is important to recognize that sharding and partitioning keys are not necessarily interchangeable.\nFor data warehouse design, integrity constraints are of little concerns (since consistency should be maintained in the operational database). So in that case the only thing to consider is the analyst's query bias. For example, Facebook's data warehouse may decide to shard on Advertiser ID and partition on Advertiser ID + date.\n<https://www.quora.com/Whats-the-difference-between-sharding-DB-tables-and-partitioning-them>\nSharding is a database architecture pattern related to**horizontal partitioning--- the practice of separating one table's rows into multiple different tables, known as partitions.** Each partition has the same schema and columns, but also entirely different rows. Likewise, the data held in each is unique and independent of the data held in other partitions.\nIt can be helpful to think of horizontal partitioning in terms of how it relates tovertical partitioning. **In a vertically-partitioned table, entire columns are separated out and put into new, distinct tables.** The data held within one vertical partition is independent from the data in all the others, and each holds both distinct rows and columns. The following diagram illustrates how a table could be partitioned both horizontally and vertically:\n\n![Example tables showing horizontal and vertical partitioning](media/Partitioning---Sharding-image1.png)\n\n**Logical vs Physical Shard**\n\nSharding involves breaking up one's data into two or more smaller chunks, calledlogical shards. The logical shards are then distributed across separate database nodes, referred to asphysical shards, which can hold multiple logical shards. Despite this, the data held within all the shards collectively represent an entire logical dataset.\n**ShardorPartition Key**is a portion of primary key which determines how data should be distributed. A partition key allows you to retrieve and modify data efficiently by routing operations to the correct database. Entries with the same partition key are stored in the same node. A**logical shard**is a collection of data sharing the same partition key. A database node, sometimes referred as a**physical shard**, contains multiple logical shards.\nDatabase shards exemplify a[shared-nothing architecture](https://en.wikipedia.org/wiki/Shared-nothing_architecture). This means that the shards are autonomous; they don't share any of the same data or computing resources. In some cases, though, it may make sense to replicate certain tables into each shard to serve as reference tables. For example, let's say there's a database for an application that depends on fixed conversion rates for weight measurements. By replicating a table containing the necessary conversion rate data into each shard, it would help to ensure that all of the data required for queries is held in every shard.\nOftentimes, sharding is implemented at the application level, meaning that the application includes code that defines which shard to transmit reads and writes to. However, some database management systems have sharding capabilities built in, allowing you to implement sharding directly at the database level.\n**Benefits of Sharding**\n\nThe main appeal of sharding a database is that it can help to facilitatehorizontal scaling, also known asscaling out. Horizontal scaling is the practice of adding more machines to an existing stack in order to spread out the load and allow for more traffic and faster processing. This is often contrasted withvertical scaling, otherwise known asscaling up, which involves upgrading the hardware of an existing server, usually by adding more RAM or CPU.\nIt's relatively simple to have a relational database running on a single machine and scale it up as necessary by upgrading its computing resources. Ultimately, though, any non-distributed database will be limited in terms of storage and compute power, so having the freedom to scale horizontally makes your setup far more flexible.\nAnother reason why some might choose a sharded database architecture is to speed up query response times. When you submit a query on a database that hasn't been sharded, it may have to search every row in the table you're querying before it can find the result set you're looking for. For an application with a large, monolithic database, queries can become prohibitively slow. By sharding one table into multiple, though, queries have to go over fewer rows and their result sets are returned much more quickly.\nSharding can also help to make an application more reliable by mitigating the impact of outages. If your application or website relies on an unsharded database, an outage has the potential to make the entire application unavailable. With a sharded database, though, an outage is likely to affect only a single shard. Even though this might make some parts of the application or website unavailable to some users, the overall impact would still be less than if the entire database crashed.\n**Drawbacks of Sharding**\n\nWhile sharding a database can make scaling easier and improve performance, it can also impose certain limitations. Here, we'll discuss some of these and why they might be reasons to avoid sharding altogether.\nThe first difficulty that people encounter with sharding is the sheer complexity of properly implementing a sharded database architecture. If done incorrectly, there's a significant risk that the sharding process can lead to lost data or corrupted tables. Even when done correctly, though, sharding is likely to have a major impact on your team's workflows. Rather than accessing and managing one's data from a single entry point, users must manage data across multiple shard locations, which could potentially be disruptive to some teams.\nOne problem that users sometimes encounter after having sharded a database is that the shards eventually become unbalanced. By way of example, let's say you have a database with two separate shards, one for customers whose last names begin with letters A through M and another for those whose names begin with the letters N through Z. However, your application serves an inordinate amount of people whose last names start with the letter G. Accordingly, the A-M shard gradually accrues more data than the N-Z one, causing the application to slow down and stall out for a significant portion of your users. The A-M shard has become what is known as adatabase hotspot. In this case, any benefits of sharding the database are canceled out by the slowdowns and crashes. The database would likely need to be repaired and resharded to allow for a more even data distribution.\nAnother major drawback is that once a database has been sharded, it can be very difficult to return it to its unsharded architecture. Any backups of the database made before it was sharded won't include data written since the partitioning. Consequently, rebuilding the original unsharded architecture would require merging the new partitioned data with the old backups or, alternatively, transforming the partitioned DB back into a single DB, both of which would be costly and time consuming endeavors.\nA final disadvantage to consider is that sharding isn't natively supported by every database engine. For instance, PostgreSQL does not include automatic sharding as a feature, although it is possible to manually shard a PostgreSQL database. There are a number of Postgres forks that do include automatic sharding, but these often trail behind the latest PostgreSQL release and lack certain other features. Some specialized database technologies --- like MySQL Cluster or certain database-as-a-service products like MongoDB Atlas --- do include auto-sharding as a feature, but vanilla versions of these database management systems do not. Because of this, sharding often requires a \"roll your own\" approach. This means that documentation for sharding or tips for troubleshooting problems are often difficult to find.\n**Sharding Architectures**\n\n**Algorithmic vs Dynamic Sharding**\n\nIn algorithmic sharding, the client can determine a given partition's database without any help. In dynamic sharding, a separate locator service tracks the partitions amongst the nodes.\n![1 id % 4 Cluster 2 3 ](media/Partitioning---Sharding-image2.png)\n\nAn algorithmically sharded database, with a simple sharding function\n![range DB ã€Œ 50, 90) [30, 50) ã€Œ 10, 30) ã€Œ 90, 8 ) | 10) Cluster ](media/Partitioning---Sharding-image3.png)\n\nA dynamic sharding scheme using range based partitioning.\n**Entity Groups**\n\n![Cluster ](media/Partitioning---Sharding-image4.png)\n\nEntity Groups partitions all related tables together\nThe concept of entity groups is very simple. Store related entities in the same partition to provide additional capabilities within a single partition. Specifically:\n\n1.  Queries within a single physical shard are efficient.\n\n2.  Stronger consistency semantics can be achieved within a shard.\n<https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6>\n1.  **Key/Hash based sharding**\n\nKey based sharding, also known ashash based sharding, involves using a value taken from newly written data --- such as a customer's ID number, a client application's IP address, a ZIP code, etc. --- and plugging it into ahash functionto determine which shard the data should go to. A hash function is a function that takes as input a piece of data (for example, a customer email) and outputs a discrete value, known as ahash value. In the case of sharding, the hash value is a shard ID used to determine which shard the incoming data will be stored on. Altogether, the process looks like this:\n\n![Key based sharding example diagram](media/Partitioning---Sharding-image5.png)\n\nTo ensure that entries are placed in the correct shards and in a consistent manner, the values entered into the hash function should all come from the same column. This column is known as ashard key. In simple terms, shard keys are similar to[primary keys](https://en.wikipedia.org/wiki/Primary_key)in that both are columns which are used to establish a unique identifier for individual rows. Broadly speaking, a shard key should be static, meaning it shouldn't contain values that might change over time. Otherwise, it would increase the amount of work that goes into update operations, and could slow down performance.\nWhile key based sharding is a fairly common sharding architecture, it can make things tricky when trying to dynamically add or remove additional servers to a database. As you add servers, each one will need a corresponding hash value and many of your existing entries, if not all of them, will need to be remapped to their new, correct hash value and then migrated to the appropriate server. As you begin rebalancing the data, neither the new nor the old hashing functions will be valid. Consequently, your server won't be able to write any new data during the migration and your application could be subject to downtime.\nThe main appeal of this strategy is that it can be used to evenly distribute data so as to prevent hotspots. Also, because it distributes data algorithmically, there's no need to maintain a map of where all the data is located, as is necessary with other strategies like range or directory based sharding.\nHash-based sharding processes keys using a hash function and then uses the results to get the sharding ID, as shown in Figure 3 (source:[MongoDB uses hash-based sharding to partition data](https://docs.mongodb.com/manual/core/hashed-sharding/)).\nContrary to range-based sharding, where all keys can be put in order, hash-based sharding has the advantage that keys are distributed almost randomly, so the distribution is even. As a result, it is more friendly to systems with heavy write workloads and read workloads that are almost all random. This is because the write pressure can be evenly distributed in the cluster. But apparently, operations likerange scanare almost impossible.\n\n![Figure 3. Hash-based sharding for data partitioning](media/Partitioning---Sharding-image6.png)\n\nSome typical examples of hash-based sharding are[Cassandra Consistent hashing](https://docs.datastax.com/en/archived/cassandra/2.1/cassandra/architecture/architectureDataDistributeHashing_c.html), presharding of Redis Cluster and[Codis](https://github.com/CodisLabs/codis), and[Twemproxy consistent hashing](https://github.com/twitter/twemproxy/blob/master/README.md#features).\n2.  **Range based sharding**\n\nRange based shardinginvolves sharding data based on ranges of a given value. To illustrate, let's say you have a database that stores information about all the products within a retailer's catalog. You could create a few different shards and divvy up each products' information based on which price range they fall into, like this:\n\n![Range based sharding example diagram](media/Partitioning---Sharding-image7.png)\n\nThe main benefit of range based sharding is that it's relatively simple to implement. Every shard holds a different set of data but they all have an identical schema as one another, as well as the original database. The application code just reads which range the data falls into and writes it to the corresponding shard.\nOn the other hand, range based sharding doesn't protect data from being unevenly distributed, leading to the aforementioned database hotspots. Looking at the example diagram, even if each shard holds an equal amount of data the odds are that specific products will receive more attention than others. Their respective shards will, in turn, receive a disproportionate number of reads.\nRange-based sharding assumes that all keys in the database system can be put in order, and it takes a continuous section of keys as a sharding unit.\nIt's very common to sort keys in order. HBase keys are sorted in byte order, while MySQL keys are sorted in auto-increment ID order. For some storage engines, the order is natural. In the case of both log-structured merge-tree (LSM-Tree) and B-Tree, keys are naturally in order.\n\n![Figure 2. Range-based sharding for data partitioning](media/Partitioning---Sharding-image8.png)\nIn Figure 2 (source:[MongoDB uses range-based sharding to partition data](https://docs.mongodb.com/manual/core/ranged-sharding/)), the key space is divided into (minKey, maxKey). Each sharding unit (chunk) is a section of continuous keys. The advantage of range-based sharding is that the adjacent data has a high probability of being together (such as the data with a common prefix), which can well support operations likerange scan. For example, HBase Region is a typical range-based sharding strategy.\nHowever, range-based sharding is not friendly to sequential writes with heavy workloads. For example, in the time series type of write load, the write hotspot is always in the last Region. This occurs because the log key is generally related to the timestamp, and the time is monotonically increasing. But relational databases often need to executetable scan(orindex scan), and the common choice is range-based sharding.\n3.  **Hash-Range combination sharding**\n\nNote that hash-based and range-based sharding strategies are not isolated. Instead, you can flexibly combine them. For example, you can establish a multi-level sharding strategy, which uses hash in the uppermost layer, while in each hash-based sharding unit, data is stored in order.\n4.  **Directory based sharding**\n\nTo implementdirectory based sharding, one must create and maintain alookup tablethat uses a shard key to keep track of which shard holds which data. In a nutshell, a lookup table is a table that holds a static set of information about where specific data can be found. The following diagram shows a simplistic example of directory based sharding:\n\n![Directory based sharding example diagram](media/Partitioning---Sharding-image9.png)\n\nHere, theDelivery Zonecolumn is defined as a shard key. Data from the shard key is written to the lookup table along with whatever shard each respective row should be written to. This is similar to range based sharding, but instead of determining which range the shard key's data falls into, each key is tied to its own specific shard. Directory based sharding is a good choice over range based sharding in cases where the shard key has a low cardinality and it doesn't make sense for a shard to store a range of keys. Note that it's also distinct from key based sharding in that it doesn't process the shard key through a hash function; it just checks the key against a lookup table to see where the data needs to be written.\nThe main appeal of directory based sharding is its flexibility. Range based sharding architectures limit you to specifying ranges of values, while key based ones limit you to using a fixed hash function which, as mentioned previously, can be exceedingly difficult to change later on. Directory based sharding, on the other hand, allows you to use whatever system or algorithm you want to assign data entries to shards, and it's relatively easy to dynamically add shards using this approach.\nWhile directory based sharding is the most flexible of the sharding methods discussed here, the need to connect to the lookup table before every query or write can have a detrimental impact on an application's performance. Furthermore, the lookup table can become a single point of failure: if it becomes corrupted or otherwise fails, it can impact one's ability to write new data or access their existing data.\n5.  **Others**\n\n    a.  Initial Implementation in Cassandra -- Linear Hash Sharding\n\n    b.  DynamoDB and Cassandra -- Consistent Hash Sharding\n\n    c.  Google Spanner and HBase -- Range Sharding\n<https://blog.yugabyte.com/four-data-sharding-strategies-we-analyzed-in-building-a-distributed-sql-database>\n\n## Should I Shard?**\n\nWhether or not one should implement a sharded database architecture is almost always a matter of debate. Some see sharding as an inevitable outcome for databases that reach a certain size, while others see it as a headache that should be avoided unless it's absolutely necessary, due to the operational complexity that sharding adds.\nBecause of this added complexity, sharding is usually only performed when dealing with very large amounts of data. Here are some common scenarios where it may be beneficial to shard a database:\n-   The amount of application data grows to exceed the storage capacity of a single database node.\n-   The volume of writes or reads to the database surpasses what a single node or its read replicas can handle, resulting in slowed response times or timeouts.\n-   The network bandwidth required by the application outpaces the bandwidth available to a single database node and any read replicas, resulting in slowed response times or timeouts.\nBefore sharding, you should exhaust all other options for optimizing your database. Some optimizations you might want to consider include:\n-   Setting up a remote database. If you're working with a monolithic application in which all of its components reside on the same server, you can improve your database's performance by moving it over to its own machine. This doesn't add as much complexity as sharding since the database's tables remain intact. However, it still allows you to vertically scale your database apart from the rest of your infrastructure.\n-   Implementing[caching](https://en.wikipedia.org/wiki/Database_caching). If your application's read performance is what's causing you trouble, caching is one strategy that can help to improve it. Caching involves temporarily storing data that has already been requested in memory, allowing you to access it much more quickly later on.\n-   Creating one or more read replicas. Another strategy that can help to improve read performance, this involves copying the data from one database server (theprimary server) over to one or moresecondary servers. Following this, every new write goes to the primary before being copied over to the secondaries, while reads are made exclusively to the secondary servers. Distributing reads and writes like this keeps any one machine from taking on too much of the load, helping to prevent slowdowns and crashes. Note that creating read replicas involves more computing resources and thus costs more money, which could be a significant constraint for some.\n-   Upgrading to a larger server. In most cases, scaling up one's database server to a machine with more resources requires less effort than sharding. As with creating read replicas, an upgraded server with more resources will likely cost more money. Accordingly, you should only go through with resizing if it truly ends up being your best option.\n<https://www.digitalocean.com/community/tutorials/understanding-database-sharding>\n\nHigh Scalability - <http://highscalability.com/unorthodox-approach-database-design-coming-shard>\n\n<https://docs.oracle.com/cd/B19306_01/server.102/b14220/partconc.htm>\n<https://dev.mysql.com/doc/refman/5.7/en/partitioning-overview.html>\n<https://www.vertabelo.com/blog/everything-you-need-to-know-about-mysql-partitions>\n\nMySQL partitioning is about altering -- ideally, optimizing -- the way the database engine physically stores data. It allows you to distribute portions of table data (a.k.a. partitions) across the file system based on a set of user-defined rules (a.k.a. the \"partitioning function\"). In this way, if the queries you perform access only a fraction of table data and the partitioning function is properly set, there will be less to scan and queries will be faster.\nIt is important to note that partitioning makes the most sense when dealing with large data sets. If you have fewer than a million rows or only thousands of records, partitioning will not make a difference.\n**Advantages**\n-   **Storage:**It is possible to store more data in one table than can be held on a single disk or file system partition.\n-   **Deletion:**Dropping a useless partition is almost instantaneous, but a classicalDELETEquery run in a very large table could take minutes.\n-   **Partition Pruning:**This is the ability to exclude non-matching partitions and their data from a search; it makes querying faster. Also, MySQL 5.7 supports explicit partition selection in queries, which greatly increases the search speed. (Obviously, this only works if you know in advance which partitions you want to use.) This also applies forDELETE,INSERT,REPLACE, andUPDATEstatements as well asLOAD DATAandLOAD XML.\n**Partition Types**\n\nFour partition types available:RANGE,LIST,HASHandKEY\n"},{"fields":{"slug":"/Databases/SQL-Databases/Postgres/","title":"Postgres"},"frontmatter":{"draft":false},"rawBody":"# Postgres\n\nCreated: 2018-12-15 00:10:32 +0500\n\nModified: 2022-03-12 12:05:00 +0500\n\n---\n\n**Introduction**\n\nIs a relational database management system with an object-oriented approach, meaning that information can be represented as objects or classes in PostgreSQL schemas.\nPostgreSQL is an open-source SQL database that is not controlled by any corporation. It is typically used for web application development.\nPostgreSQL shares many of the same advantages of MySQL. It is easy to use, inexpensive, reliable and has a large community of developers. It also provides some additional features such as foreign key support without requiring complex configuration.-   SQL Query Planner\n    -   Lexing\n    -   Parsing\n    -   Rewriter\n    -   Optimizer\n-   Two types of SQL Commands\n    -   Full blown query (SELECT, INSERT, UPDATE, DELETE)\n    -   Utility queries (GRANT, DROP table, REVOKE)\n-   JOIN\n    -   Bring data back together from different tables\n    -   Merge join\n    -   Hash join\n-   SQL is a declarative language, you tell the system what do you want, and system figures it out how to give it to you\n-   Query Plan\n    -   Optimal set of instructions to get your data from the database\n    -   Output of optimizer\n    -   Cost out different options using prunning process\n-   Scan\n    -   Sequential scan\n    -   Index scan\n-   Up to 12 tables, postgres tries all possible join operations and prunning for optimization query\n-   Generic query optimizer\n**Functions in Postgres**\n\nPostgres provides a powerful server-side function environment in multiple programming languages.\nTry to pre-process as much data as you can on the Postgres server with server-side functions. That way, you can cut down on the latency that comes from passing too much data back and forth between your application servers and your database. This approach is particularly useful for large aggregations and joins.\nWhat's even better is your development team can use its existing skill set for writing Postgres code. Other than the default PL/pgSQL (Postgres' native procedural language), Postgres functions and triggers can be written in PL/Python, PL/Perl, PL/V8 (JavaScript extension for Postgres) and PL/R.\nHere is an example of creating a PL/Python function for checking string lengths:\n\nCREATE FUNCTION longer_string_length (string1 string, string2 string)\nRETURNS integer\nAS $$\na=len(string1)\nb-len(string2)\nif a > b:\nreturn a\nreturn b\n$$ LANGUAGE plpythonu;\n**Advanced Features**\n-   in-memory caching\n-   full text search\n\n<https://rob.conery.io/2019/10/29/fine-tuning-full-text-search-with-postgresql-12>\n-   specialized indexing\n-   key-value storage\n-   Partial indexes\n\nA partial index can save space by specifying which values get indexed\nApartial indexis an index built over a subset of a table; the subset is defined by a conditional expression (called thepredicateof the partial index). The index contains entries only for those table rows that satisfy the predicate. Partial indexes are a specialized feature, but there are several situations in which they are useful.\nOne major reason for using a partial index is to avoid indexing common values. Since a query searching for a common value (one that accounts for more than a few percent of all the table rows) will not use the index anyway, there is no point in keeping those rows in the index at all. This reduces the size of the index, which will speed up those queries that do use the index. It will also speed up many table update operations because the index does not need to be updated in all cases.\nCREATE INDEX orders_completed_user_id\n\nON orders (user_id)\n\nWHERE completed IS TRUE;\n<https://www.postgresql.org/docs/12/indexes-partial.html>\n\n## JSON Types**\n\nPostgreSQLoffers two types for storing JSON data:jsonandjsonb.\nThejsonandjsonbdata types accept*almost*identical sets of values as input. The major practical difference is one of efficiency. Thejsondata type stores an exact copy of the input text, which processing functions must reparse on each execution; whilejsonbdata is stored in a decomposed binary format that makes it slightly slower to input due to added conversion overhead, but significantly faster to process, since no reparsing is needed.jsonbalso supports indexing, which can be a significant advantage.\n<https://www.postgresql.org/docs/current/datatype-json.html>\n\n<https://severalnines.com/database-blog/overview-json-capabilities-within-postgresql>\n\n## Containment**\n\nContainment tests whether one document (a set or an array) is contained within another.\n**Postgres Extensions**\n\n1.  **PostGIS**\n\nused for geospatial data manipulation and running location queries in SQL\n<https://medium.com/@tjukanov/why-should-you-care-about-postgis-a-gentle-introduction-to-spatial-databases-9eccd26bc42b>\n2.  **Key-Value data type (hstore)**\n\n3.  **Semi-structured data types**\n\n4.  **pg_timetable - Advanced postgres job scheduling**\n\n<https://www.cybertec-postgresql.com/en/pg_timetable-advanced-postgresql-job-scheduling>\nSELECT * FROM pg_available_extensions;\n| tablefunc          | functions that manipulate whole tables, including crosstab           |\n|-------------------|-----------------------------------------------------|\n| adminpack          | administrative functions for PostgreSQL                              |\n| amcheck            | functions for verifying relation integrity                           |\n| tsm_system_rows    | TABLESAMPLE method which accepts number of rows as a limit           |\n| isn                | data types for international product numbering standards             |\n| pageinspect        | inspect the contents of database pages at a low level                |\n| btree_gist         | support for indexing common datatypes in GiST                        |\n| moddatetime        | functions for tracking last modification time                        |\n| insert_username    | functions for tracking who changed a table                           |\n| intagg             | integer aggregator and enumerator (obsolete)                         |\n| pg_buffercache     | examine the shared buffer cache                                      |\n| fuzzystrmatch      | determine similarities and distance between strings                  |\n| cube               | data type for multidimensional cubes                                 |\n| uuid-ossp          | generate universally unique identifiers (UUIDs)                      |\n| dict_int           | text search dictionary template for integers                         |\n| seg                | data type for representing line segments or floating-point intervals |\n| dict_xsyn          | text search dictionary template for extended synonym processing      |\n| earthdistance      | calculate great-circle distances on the surface of the Earth         |\n| pgcrypto           | cryptographic functions                                              |\n| sslinfo            | information about SSL certificates                                   |\n| pg_prewarm         | prewarm relation data                                                |\n| tcn                | Triggered change notifications                                       |\n| lo                 | Large Object maintenance                                             |\n| pgrowlocks         | show row-level locking information                                   |\n| tsm_system_time    | TABLESAMPLE method which accepts time in milliseconds as a limit     |\n| dblink             | connect to other PostgreSQL databases from within a database         |\n| pg_trgm            | text similarity measurement and index searching based on trigrams    |\n| citext             | data type for case-insensitive character strings                     |\n| xml2               | XPath querying and XSLT                                              |\n| plpgsql            | PL/pgSQL procedural language                                         |\n| autoinc            | functions for autoincrementing fields                                |\n| refint             | functions for implementing referential integrity (obsolete)          |\n| unaccent           | text search dictionary that removes accents                          |\n| timetravel         | functions for implementing time travel                               |\n| pgstattuple        | show tuple-level statistics                                          |\n| postgres_fdw       | foreign-data wrapper for remote PostgreSQL servers                   |\n| file_fdw           | foreign-data wrapper for flat file access                            |\n| pg_freespacemap    | examine the free space map (FSM)                                     |\n| hstore             | data type for storing sets of (key, value) pairs                     |\n| btree_gin          | support for indexing common datatypes in GIN                         |\n| pg_stat_statements | track execution statistics of all SQL statements executed            |\n| intarray           | functions, operators, and index support for 1-D arrays of integers   |\n| bloom              | bloom access method - signature file based index                     |\n| ltree              | data type for hierarchical tree-like structures                      |\n| pg_visibility      | examine the visibility map (VM) and page-level visibility info       |\n**Indexes in Postgres**\n-   [B-tree indexes](https://www.postgresql.org/docs/current/btree-intro.html)\n\nB-tree indexes are binary trees that are used to sort data efficiently. They're the default if you use theINDEXcommand. Most of the time, a B-tree index suffices. As you scale, inconsistencies can be a larger problem, so use the[amcheck](https://www.postgresql.org/docs/11/amcheck.html)extension periodically.\n-   [BRIN indexes](https://www.postgresql.org/docs/11/brin-intro.html)\n\nA Block Range INdex (BRIN) can be used when your table is naturally already sorted by a column, and you need to sort by that column. For example, for a log table that was written sequentially, setting a BRIN index on the timestamp column lets the server know that the data is already sorted.\n-   [Bloom filter index](https://www.postgresql.org/docs/11/bloom.html)\n\nA bloom index is perfect for multi-column queries on big tables where you only need to test for equality. It uses a special mathematical structure called a bloom filter that's based on probability and uses significantly less space.\n-   [GIN and GiST indexes](https://www.postgresql.org/docs/11/textsearch-indexes.html)\n\nUse a GIN or GiST index for efficient indexes based on composite values like text, arrays, and JSON.\n<https://habr.com/en/company/postgrespro/blog/448746>\n\n## pgagroal**\n\npgagroalis a high-performance protocol-native connection pool for[PostgreSQL](https://www.postgresql.org/).\n**Features**\n-   High performance\n-   Connection pool\n-   Limit connections for users and databases\n-   Prefill support\n-   Remove idle connections\n-   Perform connection validation\n-   Graceful / fast shutdown\n-   Daemon mode\n-   User vault\n<https://github.com/agroal/pgagroal>\n\n## PgBouncer -** PgBouncer is a lightweight connection pooler for PostgreSQL.\n\n<https://github.com/pgbouncer/pgbouncer/blob/master/etc/pgbouncer.ini>\n\n## DATABASES_HOST**:**\"zpg-postgresql-headless.zenalytix\"**\n\n**DATABASES_PORT**:**\"5432\"**\n\n**DATABASES_USER**:**\"postgres\"**\n\n**DATABASES_PASSWORD**:**\"xitanez123\"**\n\n**DATABASES_DBNAME**:**\"zenalytix_db_new\"**\n\n**PGBOUNCER_LISTEN_PORT**:**\"5432\"**\n\n**PGBOUNCER_MAX_CLIENT_CONN**:**\"10000\"**\n\n**PGBOUNCER_DEFAULT_POOL_SIZE**:**\"100\"**\n\n**PGBOUNCER_MAX_DB_CONNECTIONS**:**\"100\"**\n\n**PGBOUNCER_MAX_USER_CONNECTIONS**:**\"100\"**\n\n**PGBOUNCER_MIN_POOL_SIZE**:**\"10\"**\n\n**PGBOUNCER_SERVER_IDLE_TIMEOUT**:**\"600\"**\n\n**PGBOUNCER_CLIENT_IDLE_TIMEOUT**:**\"600\"**\n**Odyssey**\n\nAdvanced multi-threaded PostgreSQL connection pooler and request router.\n<https://github.com/yandex/odyssey>\n\n## Postgres on Kubernetes**\n\n<https://github.com/zalando/patroni>\n\n## Streaming replication asynchronous and synchronous**\n\n<https://severalnines.com/database-blog/converting-asynchronous-synchronous-replication-postgresql>\n\n## pg_trgm**\n\nTrigram (Trigraph) concepts\n\nA trigram is a group of three consecutive characters taken from a string. We can measure the similarity of two strings by counting the number of trigrams they share. This simple idea turns out to be very effective for measuring the similarity of words in many natural languages.\n**Note:**pg_trgmignores non-word characters (non-alphanumerics) when extracting trigrams from a string. Each word is considered to have two spaces prefixed and one space suffixed when determining the set of trigrams contained in the string. For example, the set of trigrams in the string\"cat\"is\"c\",\"ca\",\"cat\", and\"at\". The set of trigrams in the string\"foo|bar\"is\"f\",\"fo\",\"foo\",\"oo\",\"b\",\"ba\",\"bar\", and\"ar\".\n<https://www.postgresql.org/docs/9.6/pgtrgm.html>\n\n## Postgres database tunable parameters to optimize performance (configurations)**\n\n1.  shared_buffer\n\nPostgreSQL uses its own buffer and also uses kernel buffered IO. That means data is stored in memory twice, first in PostgreSQL buffer and then kernel buffer. Unlike other databases, PostgreSQL does not provide direct IO. This is called double buffering. The PostgreSQL buffer is called**shared_buffer**which is the most effective tunable parameter for most operating systems. This parameter sets how much dedicated memory will be used by PostgreSQL for cache.\n\n2.  wal_buffers\n\nPostgreSQL writes its WAL (write ahead log) record into the buffers and then these buffers are flushed to disk. The default size of the buffer, defined bywal_buffers, is 16MB, but if you have a lot of concurrent connections then a higher value can give better performance.\n\n3.  effective_cache_size\n\nTheeffective_cache_sizeprovides an estimate of the memory available for disk caching. It is just a guideline, not the exact allocated memory or cache size. It does not allocate actual memory but tells the optimizer the amount of cache available in the kernel. If the value of this is set too low the query planner can decide not to use some indexes, even if they'd be helpful. Therefore, setting a large value is always beneficial.\n\n4.  work_mem\n\nThis configuration is used for complex sorting. If you have to do complex sorting then increase the value ofwork_memfor good results. In-memory sorts are much faster than sorts spilling to disk. Setting a very high value can cause a memory bottleneck for your deployment environment because this parameter is per user sort operation. Therefore, if you have many users trying to execute sort operations, then the system will allocatework_mem*totalsortoperationsfor all users. Setting this parameter globally can cause very high memory usage. So it is highly recommended to modify this at the session level.\n\n5.  maintenance_work_mem\n\nmaintenance_work_memis a memory setting used for maintenance tasks. The default value is 64MB. Setting a large value helps in tasks like[VACUUM](https://www.percona.com/blog/2018/08/06/basic-understanding-bloat-vacuum-postgresql-mvcc/), RESTORE, CREATE INDEX, ADD FOREIGN KEY and ALTER TABLE.\n\n6.  synchronous_commit\n\nThis is used to enforce that commit will wait for WAL to be written on disk before returning a success status to the client. This is a trade-off between performance and reliability. If your application is designed such that performance is more important than the reliability, then turn offsynchronous_commit. This means that there will be a time gap between the success status and a guaranteed write to disk. In the case of a server crash, data might be lost even though the client received a success message on commit. In this case, a transaction commits very quickly because it will not wait for a WAL file to be flushed, but reliability is compromised.\n\n7.  checkpoint_timeout, checkpoint_completion_target\n\nPostgreSQL writes changes into WAL. The checkpoint process flushes the data into the data files. This activity is done when CHECKPOINT occurs. This is an expensive operation and can cause a huge amount of IO. This whole process involves expensive disk read/write operations. Users can always issue CHECKPOINT whenever it seems necessary or automate the system by PostgreSQL's parameterscheckpoint_timeoutandcheckpoint_completion_target.\nThe checkpoint_timeout parameter is used to set time between WAL checkpoints. Setting this too low decreases crash recovery time, as more data is written to disk, but it hurts performance too since every checkpoint ends up consuming valuable system resources. The checkpoint_completion_target is the fraction of time between checkpoints for checkpoint completion. A high frequency of checkpoints can impact performance. For smooth checkpointing,checkpoint_timeoutmust be a low value. Otherwise the OS will accumulate all the dirty pages until the ratio is met and then go for a big flush.\n<https://www.percona.com/blog/2018/08/31/tuning-postgresql-database-parameters-to-optimize-performance>\n\n<https://postgresqlco.nf/en/doc/param>\n\n## Caching**\n\n<https://madusudanan.com/blog/understanding-postgres-caching-in-depth>\n\n## Database Physical Storage**\n\n<https://www.postgresql.org/docs/current/storage.html>\n\n## Others**\n\n**pgbackrest**\n\npgBackRest is a reliable and simple to configure backup and restore solution for PostgreSQL, which provides a powerful solution for any PostgreSQL database; be it a small project, or scaled up to enterprise-level use cases.\nMany powerful features are included in pgBackRest, including parallel backup and restore, local or remote operation, full, incremental, and differential backup types, backup rotation, archive expiration, backup integrity, page checksums, backup resume, streaming compression and checksums, delta restore, and much more.\n<https://info.crunchydata.com/blog/how-to-get-started-with-pgbackrest-and-postgresql-12>\n[**https://www.kubegres.io/**](https://www.kubegres.io/)\n**Advanced**\n\nLow level working - <https://erthalion.info/2019/12/06/postgresql-stay-curious>\n\nLocking Tuples internals - <https://github.com/postgres/postgres/blob/master/src/backend/access/heap/README.tuplock>\n\nYoutube - [Breaking PostgreSQL at Scale --- Christophe Pettus](https://www.youtube.com/watch?v=XUkTUMZRBE8)\n**Configuration**\n\n<https://tightlycoupled.io/my-goto-postgres-configuration-for-web-services>\n\n## Tools**\n\n**pgadmin**\n**References**\n\n<http://www.postgresqltutorial.com>\n\n<https://dev.to/digitalocean/-an-introduction-to-queries-in-postgresql-44la>\n\n<https://postgrescheatsheet.com/#/databases>\n\nSE Radio - 328: Postgres Query Planner (Robert Blumen with Bruce Momjian)\n\n<https://dev.to/heroku/postgres-is-underrated-it-handles-more-than-you-think-4ff3>\n\n<https://sql-performance-explained.com>\n\n<https://wiki.postgresql.org/wiki/Don%27t_Do_This>\n\n[Scaling Postgres Episodes](https://www.youtube.com/playlist?list=PLdTaEgcmPg9Kl539gyIFtWL0-cqk3m7v9)\n\n[PostgreSQL Tutorials](https://www.youtube.com/playlist?list=PLdTaEgcmPg9KiTCPWh-K961tiZrvhgfFu)\n\n<https://zerodha.tech/blog/working-with-postgresql>\n"},{"fields":{"slug":"/Databases/SQL-Databases/RDBMS/","title":"RDBMS"},"frontmatter":{"draft":false},"rawBody":"# RDBMS\n\nCreated: 2019-12-18 19:47:23 +0500\n\nModified: 2021-12-28 13:54:11 +0500\n\n---\n\nArelational databaseis a database that organizes information into one or more tables. Here, the relational database contains one table.\nAtableis a collection of data organized into rows and columns. Tables are sometimes referred to asrelations. Here the table iscelebs.\nAcolumnis a set of data values of a particular type. Here,id,name, andageare the columns.\nArowis a single record in a table. The first row in thecelebstable has:\n-   Anidof1\n-   AnameofJustin Bieber\n-   Anageof22\nAll data stored in a relational database is of a certain data type. Some of the most common data types are:\n-   INTEGER, a positive or negative whole number\n-   TEXT, a text string\n-   DATE, the date formatted as YYYY-MM-DD\n-   REAL, a decimal value\n**SQL Databases**\n\n1.  Oracle Database\n\n2.  MySQL\n\n3.  Microsoft SQL Server\n\n4.  IBM DB2\n\n5.  Postgres\n1.  MySQL\n\n    a.  MySQL is the most popular open source SQL database. It is typically used for web application development, and often accessed using PHP.\n\n    b.  The main advantages of MySQL are that it is easy to use, inexpensive, reliable (has been around since 1995) and has a large community of developers who can help answer questions.\n\n    c.  Some of the disadvantages are that it has been known to suffer from poor performance when scaling, open source development has lagged since Oracle has taken control of MySQL, and it does not include some advanced features that developers may be used to.\n2.  Oracle DB\n\n    a.  Oracle corporation owns Oracle DB, and the code is not open source.\n\n    b.  Oracle DB is for large applications, particularly in the banking industry. Most of the world's top banks run Oracle applications because Oracle offers a powerful combination of technology and comprehensive, pre-integrated business applications, including essential functionality built specifically for banks.\n\n    c.  The main disadvantage of using Oracle is that it is not free to use like its open source competitors and can be quite expensive.\n3.  SQL Server\n\n    a.  Microsoft owns SQL Server. Like Oracle DB, the code is also close sourced.\n\n    b.  Large enterprise applications mostly use SQL Server. The key difference between Oracle and SQL Server is that SQL Server only supports the Windows Operating System.\n\n    c.  Microsoft offers a free entry-level version calledExpressbut can become very expensive as you scale your application.\n**SQL Server Integration Service (SSIS)**\n\nSQL Server Integration Services(SSIS) is a component of the[Microsoft SQL Server](https://en.wikipedia.org/wiki/Microsoft_SQL_Server)database software that can be used to perform a broad range of[data migration](https://en.wikipedia.org/wiki/Data_migration)tasks.\nSSIS is a platform for[data integration](https://en.wikipedia.org/wiki/Data_integration)and[workflow applications](https://en.wikipedia.org/wiki/Workflow_application). It features a[data warehousing](https://en.wikipedia.org/wiki/Data_warehouse)tool used for data[extraction, transformation, and loading (ETL)](https://en.wikipedia.org/wiki/Extract,_transform,_load). The tool may also be used to automate maintenance of SQL Server databases and updates to multidimensional[cube data](https://en.wikipedia.org/wiki/OLAP_cube).\nFirst released with Microsoft SQL Server 2005, SSIS replaced[Data Transformation Services](https://en.wikipedia.org/wiki/Data_Transformation_Services), which had been a feature of SQL Server since Version 7.0. Unlike DTS, which was included in all versions, SSIS is only available in the \"Standard\", \"Business Intelligence\" and \"Enterprise\" editions.With Microsoft \"Visual Studio Dev Essentials\" it is now possible to use SSIS with Visual Studio 2017 free of cost so long as it is for development and learning purposes only.\n<https://en.wikipedia.org/wiki/SQL_Server_Integration_Services>\n\n## SQL Server Reporting Service (SSRS)**\n\nSQL Server Reporting Services (SSRS) provides a set of on-premises tools and services that create, deploy, and manage mobile and paginated reports.\n<https://www.toptal.com/sql/oracle-sql-server-differences>\n\n<https://www.toptal.com/sql/oracle-sql-server-migrations-pt-2>\n\n<https://www.toptal.com/sql/oracle-sql-server-migrations-pt-3>\n\n## Codd's 12 rules**\n\nCodd's twelve rulesare a set of thirteen rules ([numbered zero to twelve](https://www.wikiwand.com/en/Zero-based_numbering)) proposed by[Edgar F. Codd](https://www.wikiwand.com/en/Edgar_F._Codd), a pioneer of the[relational model](https://www.wikiwand.com/en/Relational_model)for[databases](https://www.wikiwand.com/en/Database), designed to define what is required from a[database management system](https://www.wikiwand.com/en/Database_management_system)in order for it to be consideredrelational, i.e., a[relational database management system](https://www.wikiwand.com/en/Relational_database_management_system)(RDBMS).They are sometimes jokingly referred to as \"Codd's Twelve Commandments\".\nRule 0: The foundation rule\n\nRule 1: The information rule\n\nRule 2: The guaranteed access rule\n\nRule 3: Systematic treatment of null values\n\nRule 4: Dynamic online catalog based on the relational model\n\nRule 5: The comprehensive data sublanguage rule\n\nRule 6: The view updating rule\n\nRule 7: Possible for high-level insert, update, and delete\n\nRule 8: Physical data independence\n\nRule 9: Logical data independence\n\nRule 10: Integrity independence\n\nRule 11: Distribution independence\n\nRule 12: The nonsubversion rule\n<https://www.wikiwand.com/en/Codd%27s_12_rules>\n"},{"fields":{"slug":"/Databases/Time-Series-DB/InfluxDB/","title":"InfluxDB"},"frontmatter":{"draft":false},"rawBody":"# InfluxDB\n\nCreated: 2018-05-18 14:46:16 +0500\n\nModified: 2020-01-17 19:40:33 +0500\n\n---\n\nTag sets are indexed, field sets are not. InfluxDB's speed is based on the fact that tag sets are stored in-memory, whereas the field sets are stored on-disk.\n[Key concepts](https://docs.influxdata.com/influxdb/v1.7/concepts/key_concepts/)\n\n[Glossary of terms](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/)\n\n[Comparing InfluxDB to SQL databases](https://docs.influxdata.com/influxdb/v1.7/concepts/crosswalk/)\n\n[InfluxDB design insights and tradeoffs](https://docs.influxdata.com/influxdb/v1.7/concepts/insights_tradeoffs/)\n\n[Schema design and data layout](https://docs.influxdata.com/influxdb/v1.7/concepts/schema_and_data_layout/)\n\n[In-memory indexing with TSM](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/)\n\n[Time Series Index (TSI) overview](https://docs.influxdata.com/influxdb/v1.7/concepts/time-series-index/)\n\n[Time Series Index (TSI) details](https://docs.influxdata.com/influxdb/v1.7/concepts/tsi-details/)\n# InfluxDB design insights and tradeoffs\n\nInfluxDB is a time series database. Optimizing for this use case entails some tradeoffs, primarily to increase performance at the cost of functionality. Below is a list of some of those design insights that lead to tradeoffs:\n\n1.  For the time series use case, we assume that if the same data is sent multiple times, it is the exact same data that a client just sent several times.\n    Pro:Simplified[conflict resolution](https://docs.influxdata.com/influxdb/v1.7/troubleshooting/frequently-asked-questions/#how-does-influxdb-handle-duplicate-points)increases write performance.\n    Con:Cannot store duplicate data; may overwrite data in rare circumstances.\n\n2.  Deletes are a rare occurrence. When they do occur it is almost always against large ranges of old data that are cold for writes.\n    Pro:Restricting access to deletes allows for increased query and write performance.\n    Con:Delete functionality is significantly restricted.\n\n3.  Updates to existing data are a rare occurrence and contentious updates never happen. Time series data is predominantly new data that is never updated.\n    Pro:Restricting access to updates allows for increased query and write performance.\n    Con:Update functionality is significantly restricted.\n\n4.  The vast majority of writes are for data with very recent timestamps and the data is added in time ascending order.\n    Pro:Adding data in time ascending order is significantly more performant.\n    Con:Writing points with random times or with time not in ascending order is significantly less performant.\n\n5.  Scale is critical. The database must be able to handle ahighvolume of reads and writes.\n    Pro:The database can handle ahighvolume of reads and writes.\n    Con:The InfluxDB development team was forced to make tradeoffs to increase performance.\n\n6.  Being able to write and query the data is more important than having a strongly consistent view.\n    Pro:Writing and querying the database can be done by multiple clients and at high loads.\n    Con:Query returns may not include the most recent points if database is under heavy load.\n\n7.  Many time[series](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#series)are ephemeral. There are often time series that appear only for a few hours and then go away, e.g. a new host that gets started and reports for a while and then gets shut down.\n    Pro:InfluxDB is good at managing discontinuous data.\n    Con:Schema-less design means that some database functions are not supported e.g. there are no cross table joins.\n\n8.  No one point is too important.\n    Pro:InfluxDB has very powerful tools to deal with aggregate data and large data sets.\n    Con:Points don't have IDs in the traditional sense, they are differentiated by timestamp and series.\n<https://docs.influxdata.com/influxdb/v1.7/concepts/insights_tradeoffs>\n# InfluxDB schema design and data layouts\n\nEvery InfluxDB use case is special and your[schema](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#schema)will reflect that uniqueness. There are, however, general guidelines to follow and pitfalls to avoid when designing your schema.\n\n1.  General Recommendations\n\n2.  Encouraged Schema Design\n\n3.  Discouraged Schema Design\n\n4.  Shard Group Duration Management\n<https://docs.influxdata.com/influxdb/v1.7/concepts/schema_and_data_layout>\n\n## Basic Concepts**\n\n1.  Measurement\n\nA measurement is loosely equivalent to the concept of a table in relational databases. Measurement is inside which a data is stored and a database can have multiple measurements. A measurement primarily consists of 3 types of columns Time, Tags and Fields\n\n2.  Time\n\nA time is nothing but a column tracking timestamp to perform time series operations in a better way. The default is the Influxdb time which is in nanoseconds, however, it can be replaced with event time.\n\n3.  Tags\n\nA tag is similar to an indexed column in a relational database.An important point to remember is that relational operations like WHERE, GROUP BY etc, can be performed on a column only if it is marked as a Tag\n\n4.  Fields\n\nFields are the columns on whichmathematical operations such as sum, mean, non-negative derivative etc can be performed.However, in recent versions string values can also be stored as a field.\n\n5.  Series\n\nA series is the most important concept of Influxdb.A series is a combination of tags, measurement, and retention policy (default of Influxdb).An Influxdb database performance is highly dependent on the number of unique series it contains,which in turn is the cardinality of tags x no. of measurement x retention policy\n**Glossary of Terms**\n\n1.  Replication factor\n\n2.  Retention policy\n\n3.  Schema\n\n4.  Series cardanility\n\n5.  Shard\n\n6.  Shard duration\n\n7.  Shard group\n\n8.  Subscription\n\n9.  tsm (Time Structured Merge tree)\n\n10. wal (Write Ahead Log)\n![Architecture Wrttâ€¢ WUS.cr Pod UI Slote Noes OÈ˜Ct ](media/InfluxDB-image1.png)[https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#field)\nTICK Stack (<https://www.influxdata.com/time-series-platform>)\n\na.  Telegraf\n\nb.  InfluxDB\n\nc.  Choronograf\n\nd.  Kapacitor\n![Chronograf Complete Interface for the InfluxData Platform System Stats Databases Networking Message Queues Apps result InfluxDB da shbcards access control Telegraf Agent for Collecting and Reporting Metrics and Events loc, glu ins Service Discovery Purpose Built Time Series Database Kapacitor Real-time streaming data processing engine. a eit Alerting Frameworks based Anomaly Detection Machine Learning user Defined Functions ](media/InfluxDB-image2.png)"},{"fields":{"slug":"/Databases/Time-Series-DB/Time-Series-Databases/","title":"Time Series Databases"},"frontmatter":{"draft":false},"rawBody":"# Time Series Databases\n\nCreated: 2018-09-08 17:06:57 +0500\n\nModified: 2020-04-04 01:20:20 +0500\n\n---\n-   Optimized for time-stamped or time series data\n-   built specifically for handling metrics and events or measurements that are time-stamped.\n-   optimized for measuring change over time.\n-   Time series are simply measurements or events that are tracked, monitored, downsampled, and aggregated over time.\n-   Example - server metrics, application performance monitoring, network data, sensor data, events, clicks, trades in a market, analytics data.\n-   The key difference with time series data from regular data is that you're always asking questions about it over time.\n-   Properties that make time series data very different than other data workloads are data lifecycle management, summarization, and large range scans of many records.\nWhat Distinguishes the Time Series Workload?\n\nTime Series Databases have key architectural design properties that make them very different from other databases. These include: time-stamp data storage and compression, data lifecycle management, data summarization, ability to handle large time series dependent scans of many records, and time series aware queries.\n![Time Series Database Graph](media/Time-Series-Databases-image1.png)\n*For example:*With a Time Series Database, it is common to request a summary of data over a large time period. This requires going over a range of data points to perform some computation like a percentile increase this month of a metric over the same period in the last six months, summarized by month. This kind of workload is very difficult to optimize for with a distributed key value store. TSDB's are optimized for exactly this use case giving millisecond level query times over months of data.*Another example:*With Time Series Databases, it's common to keep high precision data around for a short period of time. This data is aggregated and downsampled into longer term trend data. This means that for every data point that goes into the database, it will have to be deleted after its period of time is up. This kind of data lifecycle management is difficult for application developers to implement on top of regular databases. They must devise schemes for cheaply evicting large sets of data and constantly summarizing that data at scale. With a Time Series Database, this functionality is provided out of the box.\n**Time Series Databases**\n-   [InfluxDB](https://www.influxdata.com/)\n-   [Kdb+](https://kx.com/discover/)\n-   [RRDTool](https://oss.oetiker.ch/rrdtool/)\n-   [Graphite](https://github.com/graphite-project/graphite-web)\n-   [OpenTSDB](http://opentsdb.net/)\n-   [Prometheus](https://prometheus.io/)\n-   [Druid](http://druid.io/)\n-   [KairosDB](https://github.com/kairosdb/kairosdb)\n-   [eXtremeDB](http://www.mcobject.com/extremedbfamily.shtml)\n-   [Riak TS](http://basho.com/products/riak-ts/)\n-   [Axibase](https://axibase.com/products/axibase-time-series-database/)\n-   [TimescaleDB](https://www.timescale.com/)\n-   [FaunaDB](https://fauna.com/)\n-   [Hawkular Metrics](http://www.hawkular.org/)\n-   [Warp 10](http://www.warp10.io/)\n-   GridDB\n\nGridDB is a distributed Key-Value Store (KVS) database built by Toshiba. It has Key-Container data model and time-series functions which are designed specifically to target large data generated by IoT devices. It can be accessed by C and Java APIs, or TQL, a custom SQL-like query language.\n<https://dbdb.io/db/griddb>\n\n## Managed TimeSeries Databases**\n-   Amazon Timestream Database\n**References**\n\n<https://www.influxdata.com/time-series-database>\n\n<https://misfra.me/2016/04/09/tsdb-list>\n\n"},{"fields":{"slug":"/Databases/Time-Series-DB/TimeScaleDB/","title":"TimeScaleDB"},"frontmatter":{"draft":false},"rawBody":"# TimeScaleDB\n\nCreated: 2019-01-11 23:35:21 +0500\n\nModified: 2019-01-11 23:45:39 +0500\n\n---\n\nTimescaleDB is an open-source database designed to make SQL scalable for time-series data. It is engineered up from PostgreSQL, providing automatic partitioning across time and space (partitioning key), as well as full SQL support.\n**References**\n\n<https://www.timescale.com>\n\n<https://docs.timescale.com/v1.1/using-timescaledb>\n\n<https://github.com/timescale/timescaledb>\n"},{"fields":{"slug":"/Mathematics/Algebra/2.1-Functions/","title":"2.1 Functions"},"frontmatter":{"draft":false},"rawBody":"# 2.1 Functions\n\nCreated: 2018-05-13 18:10:11 +0500\n\nModified: 2018-05-17 23:11:22 +0500\n\n---\n\n1.  Combining functions\n\n2.  Composing functions\n\n3.  Modelling situation by combining and composing functions\n\n4.  Shifting functions\n\n5.  Stretching functions\n\n6.  Introduction to inverses of functions\n\n7.  Finding inverses of functions\n\n8.  Verifying that functions are inverses\n\n9.  Determining whether a function is invertible\r\n"},{"fields":{"slug":"/Mathematics/Algebra/2.2-Complex-Numbers/","title":"2.2 Complex Numbers"},"frontmatter":{"draft":false},"rawBody":"# 2.2 Complex Numbers\n\nCreated: 2018-05-23 20:53:10 +0500\n\nModified: 2018-12-09 20:43:49 +0500\n\n---\n\n1.  Principle Square root\n\n2.  sq. root(a.b) = sq. root(a) * sq. root(b), iff a and b both are not negative3.  **Complex Number => i^2^ = - 1**\n\n    a.  Real part\n\n    b.  Imaginary part\n\n    c.  Principle Square Root4.  Complex Plane\n\n    a.  The horizontal number line (what we know as thexxx-axis on a Cartesian plane) is thereal axis.\n\n    b.  The vertical number line (theyyy-axis on a Cartesian plane) is theimaginary axis.\n\n5.  Number Systems\n\n    a.  Counting number system ( greater than 0)\n\n    b.  Integer number system (positive and negative numbers)\n\n    c.  Rational number system (fractions)\n\n    d.  Real number system (decimals)\n\n    e.  Complex number system\n\n6.  Pure imaginary\n\n7.  Adding and subtracting complex numbers\n\n8.  Multiplying complex number\n\n9.  (a - bi)(a+bi) = a^2^+b^2^**Imaginary Numbers**\n\n[Imaginary Numbers are Real](https://www.youtube.com/playlist?list=PLiaHhY2iBX9g6KIvZ_703G3KJXapKkNaF)\n![â‘  ãƒ» ãƒ„ ã€ 3 ãˆ åˆˆ ãƒ° ãƒ» åœ§ å±± N ã‚³ ](media/2.2-Complex-Numbers-image1.jpg)\n1.  Introduction\n\n2.  History\n\n3.  Cardan's Problem\n\n4.  Bombelli's Solution\n\n5.  Numbers are Two Dimensional\n\n6.  The Complex Plane\n\n7.  Comple Multiplication\n\n8.  Math Wizardry\n\n9.  Closure\n\n10. Complex Functions\n\n11. Wandering in 4 Dimensions\n\n12. Riemann's Solution\n\n13. Riemann Surfaces\n\n"},{"fields":{"slug":"/Mathematics/Algebra/2.7.-Exponential-&-logarithms/","title":"2.7. Exponential & logarithms"},"frontmatter":{"draft":false},"rawBody":"# 2.7. Exponential & logarithms\n\nCreated: 2018-04-21 18:44:34 +0500\n\nModified: 2021-05-07 11:24:59 +0500\n\n---\n\n1.  Introduction to logarithms\n\n    a.  Intro to logarithms\n\n        i.  Common Logarithm\n\n        ii. Natural Logarithm\n\n    b.  Relationship between exponential and logarithms\n\n    c.  Relationship between exponential and logarithms: Graphs\n\n    d.  Relationship between exponential and logarithms: Tables\n\n2.  The constant *e* and the natural logarithm\n\n    a.  e and compound interest\n\n    b.  e as a limit\n\n    c.  Evaluating natural logarithm with calculator\n\n3.  Properties of logarithms\n\n    a.  The product rule = log~B~ A + log ~B~ C = log ~B~ (A.C)\n\n    b.  The quotient rule = log~B~ A - log ~B~ C = log ~B~ (A/C)\n\n    c.  The power rule = A . log~B~ C = log~B~ ( C^A^ )\n\n    d.  Change of base rule= log~B~ A = log~C~ A / log~C~ B\n\n4.  The change of base formula for logarithms\n\n5.  Solving exponential equations with logarithms\n\n6.  Solving exponential models\n\n7.  Graphs of exponential functions\n\n8.  Graphs of logarithmic functions\n\n9.  Logarithmic scale\n\n    a.  Richter Scale (is a logarithmic scale)\n\n    b.  Benford's Law\n# Exponentiation\n\n**Exponentiation**is a[mathematical](https://en.wikipedia.org/wiki/Mathematics)[operation](https://en.wikipedia.org/wiki/Operation_(mathematics)), written as*b^n^*, involving two numbers, the[*base*](https://en.wikipedia.org/wiki/Base_(exponentiation))*b*and the*exponent**n*. When*n*is a positive[integer](https://en.wikipedia.org/wiki/Integer), exponentiation corresponds to repeated[multiplication](https://en.wikipedia.org/wiki/Multiplication)of the base: that is,*b^n^*is the[product](https://en.wikipedia.org/wiki/Product_(mathematics))of multiplying*n*bases:\n\n![](media/2.7.-Exponential-&-logarithms-image1.png)\n![exponent or power 25 = 32 soluti ](media/2.7.-Exponential-&-logarithms-image2.png)\n\nThe number we multiply with itself is called the**base**.The number of times we multiply it with itself is called the**power**or**exponent**.\n**Properties of exponents**\n\n1.  Product of powers property\n\nx^a^â‹…x^b^ = x**^a+b^**\n\n2.  Power of a power property\n\n(x^a^)^b^ = x^a*b^\n\n3.  Power of a product property\n\n(x.y)^a^ = x^a^ * y^a^\n\n4.  Quotient of powers property\n\nx^a^/x^b^ = x**^a-b^** , xâ‰ 0\n\n5.  Power of a quotient power\n\n(x/y)^a^ = x^a^/y^a^ , yâ‰ 0\n\n6.  Identity Exponent\n\nx^0^ = 1, xâ‰ 0\n\n7.  Negative Exponent\n\nx **^-a^** = 1/x^a^ , xâ‰ 0\n\n8.  Square root of exponents\n\nâˆšx =x**^1/2^**\n**Exponentiation by Squaring**\n-   Square and Multiply algorithm\n-   Binary Exponentiation\n<https://www.youtube.com/watch?v=BfNlzdFa_a4&ab_channel=mCoding>\n\n## References**\n\n<https://medium.com/i-math/understanding-logarithms-and-roots-2fee92c3317f># Logarithms\n\nthelogarithmis the[inverse operation](https://en.wikipedia.org/wiki/Inverse_operation)to[exponentiation](https://en.wikipedia.org/wiki/Exponentiation), just as division is the inverse of multiplication. That means the logarithm of a number is the[exponent](https://en.wikipedia.org/wiki/Exponent)to which another fixed number, the[base](https://en.wikipedia.org/wiki/Base_(exponentiation)), must be raised to produce that number.\nWhat if we wanted to solve for the exponent in an exponential equation? In other words, we want to reverse the exponentiation. For example, what is the solution to this problem?\n\n![6â€¢=36 ](media/2.7.-Exponential-&-logarithms-image3.png)\n\nSince we've memorized the common powers and roots, we easily identify the solution as 2 since 6 to the power of 2 is 36.\nWriting a question mark in the equation isn't formal mathematics, instead we'll write the above expression using**logarithm notation,orlog**for short.\n\n![36 = 2 ](media/2.7.-Exponential-&-logarithms-image4.png)\n\nRead: \"the log, base six, of thirty-six is2.\"\n\nThe terminology:\n\n![solution (because 62 = 36) 2 36 = base ](media/2.7.-Exponential-&-logarithms-image5.png)\n\nAnother way to look at this is to ask,\n\n\"How many sixes need to be multiplied together to get36?\"\nAlogarithm **solves for the number of repeated multiplications**.Simple as that. Here are a few more examples.\n\n![log5 125 = log3 81 log2 32 3 since 5 since 3 since 2 = 125 = 81 = 32 ](media/2.7.-Exponential-&-logarithms-image6.png)\n**Special logarithms**\n\nWhile the base of a logarithm can have many different values, there are two bases that are used more often than others.\nSpecifically, most calculators have buttons for only these two types of logarithms. Let's check them out.\n**The common logarithm**\n\nThecommon logarithmis a logarithm whose base is10(\"base-10logarithm\").\nWhen writing these logarithms mathematically, we omit the base. It is understood to be10.\n\nlog10(x)=log(x)\n**The natural logarithm**\n\nThenatural logarithmis a logarithm whose base is the numbere(\"base-e logarithm\").\nInstead of writing the base ase, we indicate the logarithm withln.\n\nloge(x)=ln(x)\nThis table summarizes what we need to know about these two special logarithms:\n\n| **Name**          | **Base** | **Regular notation** | **Special notation** |\n|-------------------|----------|----------------------|----------------------|\n| Common logarithm  | 10       | log10(x)             | log(x)               |\n| Natural logarithm | e        | loge(x)              | ln(x)                |\n\nWhile the notation is different, the idea behind evaluating the logarithm is exactly the same!\n**Mathematical Constant -> Euler's Number -> e = 2.71828**\n\n**e is the epitome of universal growth**\nLike Ï€, *e* is irrational, it is not ratio of integers\nLike Ï€, *e* is transcendental, it is not root of any non-zero polynomial with rational coefficients.\nSequence - A001113 in OEIS (On-Line Encyclopedia of Integer Sequences, Sloane's)\nThe mathematical constant *e,* is the base of natural logarithm.\n**Steps for sketching a graph for any logarithm**\n\n1. Determine the correct shape of the graph\n\n2. Determine the position of its asymptote\n\n3. Find two points on the graph\nFor finding the shape, recall what graph looks like so it can be reduced from that.\n**Facts**\n-   Numbers greater than 0 and less than 1 have negative logarithms.\n\nTo avoid the need for separate tables to convert positive and negative logarithms back to their original numbers, a bar notation is used:\n\n![10ë…¸0(0.012) -2 å 0.07918 2.07918. ](media/2.7.-Exponential-&-logarithms-image7.png)\n![The bar over the characteristic indicates that it is negative whilst the mantissa remains positive. When reading a number in bar notation out loud, the symbol n is read as \"bar n\", so that 2.07918 is read as \"bar 2 point 07918...\". ](media/2.7.-Exponential-&-logarithms-image8.png)\n<https://en.wikipedia.org/wiki/Common_logarithm>"},{"fields":{"slug":"/Mathematics/Algebra/2.9.-Series/","title":"2.9. Series"},"frontmatter":{"draft":false},"rawBody":"# 2.9. Series\n\nCreated: 2018-04-24 00:42:58 +0500\n\nModified: 2018-06-04 10:41:44 +0500\n\n---\n\n1.  Arithmetic sequences (Arithmeric Progression)\n\n    a.  Only addition and subtraction from previous number\n\n    b.  a = base_term + difference (n-1)\n\n    c.  Explicit formula\n\n    d.  Recursive formula\n\n2.  Basic sigma notation\n\n3.  Finite arithmetic series\n\n4.  Geometric sequences\n\n5.  Finite geometric series\n\n6.  Finite geometric series applications**References**\n\n<https://www.khanacademy.org/math/calculus-home/series-calc>\n"},{"fields":{"slug":"/Mathematics/Algebra/Cheatsheet/","title":"Cheatsheet"},"frontmatter":{"draft":false},"rawBody":"# Cheatsheet\n\nCreated: 2018-05-28 22:13:46 +0500\n\nModified: 2018-05-28 22:14:15 +0500\n\n---\n\n![Algebra Cheat Sheet Basic Properties & Facts Arithmetic Operations ab + ac = a (b + c) -1 a b c a---b a bc ad + bc bd b a b c a b c d ab ac b ad --- bc bd ad bc 1 a an Properties of Inequalities If a < b thena+c < b+C an If a < b and c > 0 then ac If a < b and c < 0 then ac Properties of Absolute Val a ---a al 20 abl = lallbl ifa20 if a < 0 ---al = a b ab + ac =b+C, a a Exponent Properties anam 1 a a a a a b 1 a a b b a Ion a Properties of Radicals a + bl lal + lbl Triangle Ir Distance Formula If and points the distance between X2 --- Xl)2 + (Y2 Complex Numbers (a ---bi) = a 2 +192 ](media/Cheatsheet-image1.png)\n\n"},{"fields":{"slug":"/Mathematics/Algebra/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-04-01 23:51:28 +0500\n\nModified: 2022-12-11 19:29:37 +0500\n\n---\n\n**Algebra - 1**\n\n1.  Solving equations\n\n2.  Solving inequalities\n\n3.  Linear equations and graphs\n\n4.  Functions\n\n5.  Sequences\n\n6.  System of equations\n\n7.  Inequalities\n\n8.  Rational Exponents and radicals\n\n9.  Exponential growth and decay\n\n10. Polynomials\n\n11. Factorization\n\n12. Quadratics\n\n13. Irrational numbers\n**Algebra - 2**\n\n1.  Functions\n\n2.  Complex numbers\n\n3.  Polynomials\n\n4.  Radical relationships\n\n5.  Rational relationships\n\n6.  Exponential growth and decay\n\n7.  Exponential and logarithms\n\n8.  Trigonometry\n\n9.  Series\n\n10. Modeling\n\n11. Conic sections\n<https://www.freecodecamp.org/news/learn-algebra-to-improve-your-programming-skills>\n"},{"fields":{"slug":"/Mathematics/Algebra/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2018-07-02 00:39:29 +0500\n\nModified: 2018-07-02 00:40:09 +0500\n\n---\n\n**Slope**\n\n**Intercept**\n\nThex-intercept is the point where a line crosses thex-axis, and they-intercept is the point where a line crosses they-axis.\n\n![123 ](media/Others-image1.png)"},{"fields":{"slug":"/Mathematics/Algebra/Root/","title":"Root"},"frontmatter":{"draft":false},"rawBody":"# Root\n\nCreated: 2018-04-02 00:06:21 +0500\n\nModified: 2018-04-02 00:07:33 +0500\n\n---\n\nRoots get back the base number from the solution of an exponential equation.\n\nSquare Roots, Cube Roots andMore\n\nSuppose instead of finding the square of 9, which is 81, we wanted to find out what number multiplied with itself equals 81.\n\nIn other words,*what is the**square root**of 81?*\n\n![](media/Root-image1.png)\n\nThis equals 9 because nine squared is eighty-one.\n\n![â…  8 = ä¹™ 6 ](media/Root-image2.png)\n\n**We can take the square root of any non-negative number**, but only perfect square numbers yield whole number results. So familiarize yourself with those first. Here are a few to get you started:\n\n![= 2 =3 = 4 ã€ˆè©¹5 = 5 ã€ˆå°¸6 = 6 = 7 *ì£¼4 = 8 å¥©1 =9 êµ¬è€Œ= 10 ](media/Root-image3.png)\n\nNow for some terminology.\n\n![radical symbol solution or root! (since 92 = 81) radicand root index ](media/Root-image4.png)\n\nThe root index is*optional*on square roots. Square roots are often written:\n\n![64 ](media/Root-image5.png)\n\nThe index is only necessary to distinguish between higher indexed roots, such as cube roots, fourth roots, fifth roots, etc.\n\n**Cube roots**ask you to find the number that when multiplied with itself**three**times yields the radicand, like these:\n\n![1 since 1 2 since 2 3 since 33 4 since 4 5 since 5 8 27 64 125 ](media/Root-image6.png)\n\n**Fourth roots**ask you to find the number that when multiplied with itself**four**times yields the radicand.\n\n![= 1 since 1 1 2 since 16 {â‚¬1 = 3 since 34 = 81 ](media/Root-image7.png)\n\n**Fifth roots**ask you to find the number that when multiplied with itself**five**times yields the radicand.\n\n![Rfi2 1 since 1 2 since 25 = 32 ](media/Root-image8.png)\n\nAgain, you can take*any root of any non-negative number*(and in certain cases of negative numbers as well), but for many numbers you'll need a calculator since the answers are irrational.\n\n"},{"fields":{"slug":"/Mathematics/Algebra/Sets/","title":"Sets"},"frontmatter":{"draft":false},"rawBody":"# Sets\n\nCreated: 2018-01-27 11:28:49 +0500\n\nModified: 2020-01-03 02:18:56 +0500\n\n---\n\n**Symmetric Difference (** â–³ **) -** The symmetric difference of two sets is the collection of elements which are members of either set but not both - in other words, the union of the sets excluding their intersection. Forming the symmetric difference of two sets is simple, but forming the symmetric difference of three sets is a bit trickier.\n\nEx - Given two sets (for example set A = {1, 2, 3} and set B = {2, 3, 4}), the mathematical term \"symmetric difference\" of two sets is the set of elements which are in either of the two sets, but not in both (A â–³ B = C = {1, 4}).\n**Bijection, injection and surjection**\n\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics),injections,surjectionsandbijectionsare classes of[functions](https://en.wikipedia.org/wiki/Function_(mathematics))distinguished by the manner in which[arguments](https://en.wikipedia.org/wiki/Parameter)(input[expressions](https://en.wikipedia.org/wiki/Expression_(mathematics))from the[domain](https://en.wikipedia.org/wiki/Domain_(mathematics))) and[images](https://en.wikipedia.org/wiki/Image_(mathematics))(output expressions from the[codomain](https://en.wikipedia.org/wiki/Codomain)) are related ormapped toeach other.\nA function[maps](https://en.wikipedia.org/wiki/Map_(mathematics))elements from its domain to elements in its codomain. Given a function*f: X -> Y*-   The function is[injective](https://en.wikipedia.org/wiki/Injective_function), orone-to-one, if each element of the codomain is mapped to byat mostone element of the domain, or equivalently, if distinct elements of the domain map to distinct elements in the codomain. An injective function is also called aninjection.Notationally:\n\n![](media/Sets-image1.png)\n\nOr, equivalently (using[logical transposition](https://en.wikipedia.org/wiki/Transposition_(logic))),\n\n![](media/Sets-image2.png)\n-   The function is[surjective](https://en.wikipedia.org/wiki/Surjective_function), oronto, if each element of the codomain is mapped to byat leastone element of the domain. That is, the image and the codomain of the function are equal. A surjective function is asurjection.Notationally:\n\n![Vy e Y, e X such that y f(x). ](media/Sets-image3.png)\n-   The function is[bijective](https://en.wikipedia.org/wiki/Bijective_function)(one-to-one and ontoorone-to-one correspondence) if each element of the codomain is mapped to byexactlyone element of the domain. That is, the function isbothinjective and surjective. A bijective function is also called abijection.\nAn injective function need not be surjective (not all elements of the codomain may be associated with arguments), and a surjective function need not be injective (some images may be associated withmore than oneargument). The four possible combinations of injective and surjective features are illustrated in the adjacent diagrams.\n<https://en.wikipedia.org/wiki/Bijection,_injection_and_surjection>\n"},{"fields":{"slug":"/Mathematics/Aptitude/Chinese-Remainder-Theorem/","title":"Chinese Remainder Theorem"},"frontmatter":{"draft":false},"rawBody":"# Chinese Remainder Theorem\n\nCreated: 2018-07-31 01:11:22 +0500\n\nModified: 2018-07-31 01:12:20 +0500\n\n---\n\nThe**Chinese remainder theorem**is a theorem of[number theory](https://en.wikipedia.org/wiki/Number_theory), which states that if one knows the remainders of the[Euclidean division](https://en.wikipedia.org/wiki/Euclidean_division)of an[integer](https://en.wikipedia.org/wiki/Integer)*n*by several integers, then one can determine uniquely the remainder of the division of*n*by the product of these integers, under the condition that the[divisors](https://en.wikipedia.org/wiki/Divisor) are[pairwise coprime](https://en.wikipedia.org/wiki/Pairwise_coprime).\n\n"},{"fields":{"slug":"/Mathematics/Aptitude/Cube-Cutting/","title":"Cube Cutting"},"frontmatter":{"draft":false},"rawBody":"# Cube Cutting\n\nCreated: 2018-05-26 02:09:13 +0500\n\nModified: 2018-05-26 02:12:57 +0500\n\n---\n\nConcepts -\n\n1.  Cube has 6 faces, 8 vetices and 12 edges\nNumber of cubes of size y*y*y formed after cutting a large cube of size x*x*x\n\n= (x/y)^3^\nIf unit cubes than number of unit cubes formed after cutting a cube of size x = x^3^\n\n**Shortcut Formulae**\n-   For a cube of side n*n*n painted on all sides which is uniformly cut into smaller cubes of dimension 1*1*1,\n    -   Number of cubes with 0 side painted= (n-2) ^3\n    -   Number of cubes with 1 sides painted =6(n - 2) ^2\n    -   Number of cubes with 2 sides painted= 12(n-2)\n    -   Number of cubes with 3 sidess painted= 8(always)\n-   For a cuboid of dimension a*b*c painted on all sides which is cut into smaller cubes of dimension 1*1*1,\n    -   Number of cubes with 0 side painted= (a-2) (b-2) (c-2)\n    -   Number of cubes with 1 sides painted =2[(a-2) (b-2) + (b-2)(c-2) + (a-2)(c-2) ]\n    -   Number of cubes with 2 sides painted= 4(a+b+c -6)\n    -   Number of cubes with 3 sidess painted= 8\n**References**\n\n<https://www.hitbullseye.com/mba/reasoning/Painted-Cube-Problem-Formula.php>\n"},{"fields":{"slug":"/Mathematics/Aptitude/Distance-Speed-and-Time/","title":"Distance Speed and Time"},"frontmatter":{"draft":false},"rawBody":"# Distance Speed and Time\n\nCreated: 2018-02-17 23:38:26 +0500\n\nModified: 2018-09-26 23:25:48 +0500\n\n---\n\n# HOW TO CALCULATE TIME AND DISTANCE FROM ACCELERATION AND VELOCITY\n\nIn a physics equation, given a constant[acceleration](http://www.dummies.com/education/science/physics/how-to-calculate-acceleration/)and the change in velocity of an object, you can figure out both the[time involved and the distance traveled](http://www.dummies.com/education/math/basic-math/calculating-speed-time-and-distance/). For instance, imagine you're a drag racer. Your acceleration is 26.6 meters per second2, and your final speed is 146.3 meters per second. Now find the total distance traveled. Got you, huh? \"Not at all,\" you say, supremely confident. \"Just let me get my calculator.\"\n\nYou know the acceleration and the final speed, and you want to know the total distance required to get to that speed. This problem looks like a puzzler, but if you need the time, you can always solve for it. You know the final speed,vf,and the initial speed,vi(which is zero), and you know the acceleration,a.Becausevf-- vi= at,you know that\n\n![](media/Distance-Speed-and-Time-image1.png)\n\nNow you have the time. You still need the distance, and you can get it this way:\n\n![](media/Distance-Speed-and-Time-image2.png)\n\nThe second term drops out becausevi= 0, so all you have to do is plug in the numbers:\n\n![](media/Distance-Speed-and-Time-image3.png)\n\nIn other words, the total distance traveled is 402 meters, or a quarter mile. Must be a quarter-mile racetrack.\n\n# \n\n# HOW TO CALCULATE ACCELERATION\n\nIn physics terms, acceleration,a, is the amount by which your velocity changes in a given amount of time. Given the initial and final velocities,viandvf, and the initial and final times over which your speed changes,tiandtf, you can write the equation like this:\n![acceleration formula](media/Distance-Speed-and-Time-image4.png)\n\nIn terms of units, the equation looks like this:\n\n![acceleration formula by units](media/Distance-Speed-and-Time-image5.png)\n\nDistance per time squared? Don't let that throw you. You end up with time squared in the denominator because you divide velocity by time. In other words,accelerationis the rate at which your velocity changes, because rates have time in the denominator. For acceleration, you see units of meters per second2, centimeters per second2, miles per second2, feet per second2, or even kilometers per hour2.\n\nIt may be easier, for a given problem, to use units such as mph/s (miles per hour per second). This would be useful if the velocity in question had a magnitude of something like several miles per hour that changed typically over a number of seconds.\n\nSay you become a drag racer in order to analyze your acceleration down the dragway. After a test race, you know the distance you went --- 402 meters, or about 0.25 miles (the magnitude of your displacement) --- and you know the time it took --- 5.5 seconds. So what was your acceleration as you blasted down the track?\n\nWell, you can relate displacement, acceleration, and time as follows:\n\n![](media/Distance-Speed-and-Time-image2.png)\n\nand that's what you want --- you always work the algebra so that you end up relating all the quantities you know to the one quantity youdon'tknow. In this case, you have\n\n![](media/Distance-Speed-and-Time-image6.png)\n\n(Keep in mind that in this case, your initial velocity is 0 --- you're not allowed to take a running start at the drag race!) You can rearrange this equation with a little algebra to solve for acceleration; just divide both sides byt2and multiply by 2 to get\n\n![](media/Distance-Speed-and-Time-image7.png)\n\nGreat. Plugging in the numbers, you get the following:\n\n![](media/Distance-Speed-and-Time-image8.png)\n\nOkay, the acceleration is approximately 27 meters per second2. What's that in more understandable terms? The acceleration due to gravity,g,is 9.8 meters per second2, so this is about 2.7 g's --- you'd feel yourself pushed back into your seat with a force about 2.7 times your own weight.\n\n"},{"fields":{"slug":"/Mathematics/Calculus/Essence-of-Calculus---3Blue1Brown/","title":"Essence of Calculus - 3Blue1Brown"},"frontmatter":{"draft":false},"rawBody":"# Essence of Calculus - 3Blue1Brown\n\nCreated: 2018-05-28 16:14:32 +0500\n\nModified: 2021-03-27 01:05:55 +0500\n\n---\n\n1.  Introduction\n\n2.  The paradox of the derivative\n\n3.  Derivative formulas through geometry\n\n4.  Visualizing the chain rule and product rule\n\n5.  Derivatives of exponentials (What's so special about Euler's number e)\n\n6.  Implicit differentiation, what going on here?\n\n7.  Limits, L'Hopital's rule, and epsilon delta definitions\n\n8.  Integration and the fundamental theorem of calculus\n\n9.  What does area have to do with slope?\n\n10. Higher order derivatives\n\n11. Taylor series\n\n12. The other way to visualize derivatives (What they won't teach you in calculus)\n1.  **Introduction**\n-   Derivative formulas\n-   Product rule, chain rule\n-   Implicit differentiation\n-   Integral and derivatives are opposite\n-   Taylor series\n![2 ä¸Œ r ](media/Essence-of-Calculus---3Blue1Brown-image1.png)\n![20 15 10 Area 2rr ](media/Essence-of-Calculus---3Blue1Brown-image2.png)\n![Fundamental theorem of calculus Integral\" of 10 dc ](media/Essence-of-Calculus---3Blue1Brown-image3.png)\n2.  **The paradox of the derivative**\n\n![100 90 80 70 60 50 40 30 20 10 Distance traveled meters) } Change in distance Change in time Time (seconds) 9 10 ](media/Essence-of-Calculus---3Blue1Brown-image4.png)\n![100 90 80 70 60 50 40 30 20 10 Distance traveled meters) 0.01 dt dt dt Time (seconds) 9 10 ](media/Essence-of-Calculus---3Blue1Brown-image5.png)\n![100 ds 90 dt 80 70 60 50 40 30 20 10 Derivative dt 9 10 ](media/Essence-of-Calculus---3Blue1Brown-image6.png)\n![The paradox of the derivative I Chapter 2, Essence of calculus 00 16 Distance 14 12 10 0 12:03/ 18:37 s(t) i.dt? ds (2) dt (2 + dt)3 dt 23 + + 3(2) (dt)2 + (dt)3 dt Time (seconds) ](media/Essence-of-Calculus---3Blue1Brown-image7.png)\n![16 14 12 10 Distance s(t) ds (2) dt (2 + (103 dt (2)3 Contains dt Time (seconds) ](media/Essence-of-Calculus---3Blue1Brown-image8.png)\n![16 14 12 10 Distance Slope 3 12 ds (2) dt ds (2) dt (2)3 (2 + (103 dt Time (seconds ](media/Essence-of-Calculus---3Blue1Brown-image9.png)\n![16 14 12 10 Distance s(t) ds (t) dt (t) dt (t)3 dt 3(t)2 Time (seconds) ](media/Essence-of-Calculus---3Blue1Brown-image10.png)\n![If dt has a specific size. d(t3) t3 dt dt Messy d(t3) dt 3t2 Simple ](media/Essence-of-Calculus---3Blue1Brown-image11.png)\n3.  **Derivative formulas through geometry**\n    -   **The Power Rule**\n\n![dm dm d(m3) dm dm d(Å’5) Power rule\" d(Å’n) dm ](media/Essence-of-Calculus---3Blue1Brown-image12.png)\n![Ø³Ø³ la;) Ù„Ø³Ù‡ Ø¨ ](media/Essence-of-Calculus---3Blue1Brown-image13.png)\n![f@) dx { 3x2 dc ](media/Essence-of-Calculus---3Blue1Brown-image14.png)\n![f@) dx Area lost Area gained Negative d(l/x) dc ](media/Essence-of-Calculus---3Blue1Brown-image15.png)\n![Area dx a; New area ](media/Essence-of-Calculus---3Blue1Brown-image16.png)\n![sin (9) d(sin(9)) 2 Adj. Hyp. cos (0) 0 d(siÄ±Ä±(0)) 2 ](media/Essence-of-Calculus---3Blue1Brown-image17.png)\n4.  **Visualizing the chain rule and product rule**\n\n    1.  **Sum Rule**\n\n![dc dx Sum rule sm(x) cos(x) dx dx ](media/Essence-of-Calculus---3Blue1Brown-image18.png)\n![XP )gÙ - LI!S( Ø¨ (p) Ù¡- Ù†) (0 : Il IS XP 0- - geo = x Ù…Ù…Ø³Ø§Ù‡ Ù¡ 0ØŸ) Il!S ti!S(p -Ø³ ( p. era xp)x(soo t: S(p!11 Ø¨ IllS ](media/Essence-of-Calculus---3Blue1Brown-image19.png)\n![Ø­Ù…Ø³ÙƒØ§Ù‡ )Ù§GÙ - u!S( Ø¨ (p) Ù¡- Ù†) (0 : Il IS XP 0- - Ù¡ geo = x geo) Il!S 0ÙŠ)-/ Ø¯o( - ::: >:::::::----7 Ù†Ù†Ù†Ø«Ù†Ù†Ù†Ù†Ù†Ø«Ù†Ù† 6(0ÙŠ)) Ø¨ Ø¨ x(SOD) t. S(p!11 IllS XP ](media/Essence-of-Calculus---3Blue1Brown-image20.png)\n2.  **Product Rule**\n\n![sin(x = Area d(x2) -l- x2d(sin sin(Å’ + x cos(x da; ](media/Essence-of-Calculus---3Blue1Brown-image21.png)\n![0 6p@)q + â…¡ XP MP XP I XP 0 ](media/Essence-of-Calculus---3Blue1Brown-image22.png)\n![2) + x cos x d(Right) d(Left) Left + Right ](media/Essence-of-Calculus---3Blue1Brown-image23.png)\n3.  **Function composition / Chain Rule**\n\n![Function composition '2 sin 2) sin(x Derivative? ](media/Essence-of-Calculus---3Blue1Brown-image24.png)\n![1.5 d(x2) clx sin d(sin sin cos x2) 2a; (lx ](media/Essence-of-Calculus---3Blue1Brown-image25.png)\n![Chain rule\" 2) cos x2) 2x sin(x dc Inner dc Outer d(lnner dg (h(x)) dc d(Outer) ](media/Essence-of-Calculus---3Blue1Brown-image26.png)\n5.  **Derivatives of exponentials**\n\n![dt 0.00000001 0.6931472 0.00000001 dt dt dt ](media/Essence-of-Calculus---3Blue1Brown-image27.png)\nFor smaller and smaller choices of dt value 2^0.000001^ -1 / 0.000001 approaches a specific number. Unlike other functions all the values of dt is separate from the value of t itself\n\n![d(2t) dt d(8t) dt 2t (O .6931 8t(2.0794 . â€¢ â€¢ ) ](media/Essence-of-Calculus---3Blue1Brown-image28.png)\n![d(at) dt (Some constant) a IS there a base where that constant is 1? ](media/Essence-of-Calculus---3Blue1Brown-image29.png)\n![IP T0000000 Â· 0 Â· 0000000 Â· â…  â…  0000000 Â· 0 INP ](media/Essence-of-Calculus---3Blue1Brown-image30.png)\n![OdOIS ](media/Essence-of-Calculus---3Blue1Brown-image31.png)\n![od01S ](media/Essence-of-Calculus---3Blue1Brown-image32.png)\n![](media/Essence-of-Calculus---3Blue1Brown-image33.png)\n![e to the what equals 2?\" 2 In(2) ](media/Essence-of-Calculus---3Blue1Brown-image34.png)\n![d(2t) dt d(3t) dt d(7t) dt In(2) 2t 0.6931 . In(3) 3 (0.6931 (0.6931 . In(2) 2 7 111(7) (1.9459 . ](media/Essence-of-Calculus---3Blue1Brown-image35.png)\n![t 2 loge (2) (0.69315... t log T (2) (0.60551... ) t (0.18545... )t 42 ](media/Essence-of-Calculus---3Blue1Brown-image36.png)\nWe write some contant times t, as e to some constants time t, because rate of change of some thing (like population) is dependent on the size of that thing (i.e. the size of population). Other example, if we put a cup of water in a cool room, the rate at which water cools is proportional to difference in temperature of room and water, or otherwise **the rate at which that difference changes is proportional to itself.** If we invest our money the amount of interest depends on the amount of money invested.\n6.  **Implicit differentiation, what going on here?**\n\n![ãƒ­ ãƒ­ ãƒ­ ãƒ­ ä»¥ ä½ QE ç”° å£ BBAZ ãƒ­ ãƒ­ ãƒ­ ãƒ­ ç”° % ç”° 8 ç”° è® 8 ç”° % ( 3 ã€‚ 4 ) ä¸€ Slo e å ã€ƒ 2 2 ã‚§ clx å 2 ](media/Essence-of-Calculus---3Blue1Brown-image37.png)\n![Implicit differentiation ](media/Essence-of-Calculus---3Blue1Brown-image38.png)\n![After dt seconds... Think 0.01 5m 3m Related rates x(t) dx x(t) + 2y dt dx (3) +2 dt dt (-1) dt 4 ](media/Essence-of-Calculus---3Blue1Brown-image39.png)\n![0 â–  ãƒ» ãƒ­ ãƒ­ ãƒ­ ãƒ» â–  ãƒ» â–  ãƒ» ãƒ­ ãƒ­ ãƒ­ ãƒ» â–  ãƒ» ãƒ­ ãƒ» ãƒ» ãƒ­ ãƒ­ ãƒ­ ãƒ» â–  ãƒ» ãƒ­ XP 6 â…¡ ã® â–  ãƒ» ãƒ­ ãƒ­ ãƒ­ ãƒ» â–  ãƒ» ãƒ­ ãƒ» ãƒ» ãƒ­ ãƒ­ ãƒ­ ã¿ â–  â–  â–  ãƒ­ ãƒ­ ãƒ» â–  ãƒ» ãƒ­ æ—¥ å½± ãƒ» ãƒ­ ãƒ­ ãƒ­ ãƒ» ãƒ» ãƒ» ãƒ­ â–  ãƒ­ ãƒ­ ãƒ­ ãƒ» â–  ãƒ» ãƒ­ æ—¥ ãƒ­ ãƒ­ ãƒ­ ãƒ» ãƒ» â–  ãƒ» â–  â…£ ç€˜ ãƒ» ãƒ­ ãƒ­ ãƒ­ ãƒ» ãƒ» ãƒ» ãƒ­ ãƒŸ ãƒ­ ãƒ­ ãƒ­ ãƒ» ãƒ» ãƒ» ç”° ã‚’ ä¸€ å£ ãƒ­ ãƒ­ ãƒ» ãƒ­ \"p XP â…¡ ](media/Essence-of-Calculus---3Blue1Brown-image40.png)\n![y In(x) dy Slo e d(ln(x)) dc ](media/Essence-of-Calculus---3Blue1Brown-image41.png)\n![y In(x) dy SIO e clx d(ln(x)) dc dy ](media/Essence-of-Calculus---3Blue1Brown-image42.png)-   Multivariate Calculus\n7.  **Limits**\n    -   **Formal definition of a derivative**\n\n![Formal derivative definition (2) lim clx No 'infinitely small ](media/Essence-of-Calculus---3Blue1Brown-image43.png)\n**h = dx**-   **(Îµ, ğ›¿) definition of a limit (epsilon - delta)**\n\n![20 15 Undefined h 0? ](media/Essence-of-Calculus---3Blue1Brown-image44.png)\n![lim f ( ) h is not defined Can't get smaller ](media/Essence-of-Calculus---3Blue1Brown-image45.png)\n![](media/Essence-of-Calculus---3Blue1Brown-image46.png)-   **L'HÃ´pital's rule**\n\n![sin(T sin(T lim sin(T Ä±.ooooÄ± 1.5708 ... Ä±.ooooÄ± ](media/Essence-of-Calculus---3Blue1Brown-image47.png)\n![sin(T lim 1 sin T x sin T cos TT)ff dm ](media/Essence-of-Calculus---3Blue1Brown-image48.png)\n![sin sin(nx ](media/Essence-of-Calculus---3Blue1Brown-image49.png)\n![sin(n lim 2, Ida; dc 1 sin n x ](media/Essence-of-Calculus---3Blue1Brown-image50.png)\n![](media/Essence-of-Calculus---3Blue1Brown-image51.png)\n![](media/Essence-of-Calculus---3Blue1Brown-image52.png)\n![lim dc dg dc dx L HÃ¶pital's rule\" g(x) f@) sin@ lim Looks like 0/0 ](media/Essence-of-Calculus---3Blue1Brown-image53.png)\n8.  **Integration and the fundamental theorem of calculus**\n\n![Derivative 2t \"Antiderivative\" ](media/Essence-of-Calculus---3Blue1Brown-image54.png)\n![25 meters Velocity in second 20 15 10 5 1 t(8 t) 2 3 4 5 6 7 8 ](media/Essence-of-Calculus---3Blue1Brown-image55.png)\n![25 20 15 10 v(t) dt t(8 t) ](media/Essence-of-Calculus---3Blue1Brown-image56.png)\n![25 20 15 10 5 SCT) v(t) dt t(8 t) ](media/Essence-of-Calculus---3Blue1Brown-image57.png)\n![y b f (x) dx dc a Fundamental theorem of calculus b ](media/Essence-of-Calculus---3Blue1Brown-image58.png)\n![25 20 15 10 v(t) dt ds t(8 --- t) ](media/Essence-of-Calculus---3Blue1Brown-image59.png)\n![25 t(8 20 15 10 t) dt 4T2 (0)3 Cancels when T v(t) t(8 --- t) ](media/Essence-of-Calculus---3Blue1Brown-image60.png)\n9.  **What does area have to do with slope?**\n\n![What is the average value of f (x)? ](media/Essence-of-Calculus---3Blue1Brown-image61.png)\n![Average height? 37/2 sin(x) '27T ](media/Essence-of-Calculus---3Blue1Brown-image62.png)\n![](media/Essence-of-Calculus---3Blue1Brown-image63.png)\n![Hours of daylight 14 12 10 Summer 8 months 6 4 2 50 100 150 2.7 sin 2nx/365) + 12.4 Winter months Days since March 21 200 250 300 350 ](media/Essence-of-Calculus---3Blue1Brown-image64.png)\nFinding the average of a continuous thing.\n![Use an integral! 377/2 sin(x) '27T ](media/Essence-of-Calculus---3Blue1Brown-image65.png)\n![0 X)tIIS XP (x)111S -IL 0 ](media/Essence-of-Calculus---3Blue1Brown-image66.png)\n![n ä¸€ dc dr n/dc 2 ä¸¬ Slll X ](media/Essence-of-Calculus---3Blue1Brown-image67.png)\n![X)Ills ãƒ» å å ä¸€ å ä¸€ å ä¸€ å ä¸€ å å å å å å å å å å å å å å å å å å å å å ä¸€ å ä¸€ å ä¸€ å ä¸€ å ' ](media/Essence-of-Calculus---3Blue1Brown-image68.png)\n![Average height Area Width sin@) clx sin a; 27 ](media/Essence-of-Calculus---3Blue1Brown-image69.png)\n![--- sin(x) da; cos(x) sin(Å“ ](media/Essence-of-Calculus---3Blue1Brown-image70.png)\n![cos da; sili (x) sin 27r cos (T ](media/Essence-of-Calculus---3Blue1Brown-image71.png)\n![sin T O cos(T)) T (o)) cos --- cos sin O cos( Sili ](media/Essence-of-Calculus---3Blue1Brown-image72.png)\n![f (T) dm dm ](media/Essence-of-Calculus---3Blue1Brown-image73.png)\n10. **Higher order derivatives**\n\n![da: (l(df) (If2 dfl 0.01 0.0001 (Some (Ifi (1/2 ](media/Essence-of-Calculus---3Blue1Brown-image74.png)\n![df2 clfl (I(df) , (Some constant) (d:r)2 dfl (IL Interestingly, there is a notion in math called the \"exterior derivative\" which treats this \"d\" as having a more independent meaning, though it's less related to the intuitions I've introduced in this series. ](media/Essence-of-Calculus---3Blue1Brown-image75.png)\n![s(t) Displacement --- (t) Velocity dt ' ---(t) Acceleration c13s Jerk d 13 ](media/Essence-of-Calculus---3Blue1Brown-image76.png)\n11. **Taylor series**\n    -   **Powerful tool for approximating functions**\n    -   **Taking non-polynomial functions, and finding polynomials that approximate them near some input. Polynomials are much easier to deal with than other functions**\n\n![cos(o) d(cos) (o) da; (o) (13 ( cos (o) ÄX3 d4(cos (o) (1x4 sin (0) 0 cos(0) sin(()) cos(0) (1x4 24 24 1-2-3-4 24 â€¢ 24 ](media/Essence-of-Calculus---3Blue1Brown-image77.png)\n![CO + Cl T) 1 ---I--- C2 (a; + C (X T) 3 + C 4 (X Pa(x) Plugging in = is very nice. ](media/Essence-of-Calculus---3Blue1Brown-image78.png)\n![Â· å ---I å | 0 å I?1tL10uÃ„10d IOIÃ„?L, | å | 0 å I â…¡ @)d â…¡ å†– 0 ä¸€ 0 IllS ](media/Essence-of-Calculus---3Blue1Brown-image79.png)\n![f(a) dr (1x2 CIT3 CIT4 df (x a f(a) 1! dr CIX 2 2! 27F ](media/Essence-of-Calculus---3Blue1Brown-image80.png)\n![d/dx d/dx d/dx d/dx d/dx 2! ](media/Essence-of-Calculus---3Blue1Brown-image81.png)\n![area 2 (1x2 a f(a) da; area 1 CP f 2 da;2 a; Heigllt (a; Base d2f area (1x2 a) ](media/Essence-of-Calculus---3Blue1Brown-image82.png)\n![Taylor series cos(x) 1 2 2! 4 6 6! Infinite sum series Be careful Taylor polynomial ](media/Essence-of-Calculus---3Blue1Brown-image83.png)\n![Converges\" to 27 81 243 ](media/Essence-of-Calculus---3Blue1Brown-image84.png)\n![e +1 å 1 å å - å å å å å å å å Â· å Â· 2 Â· 7182818 ](media/Essence-of-Calculus---3Blue1Brown-image85.png)\n![111@) ---Å ('E Ä±) ](media/Essence-of-Calculus---3Blue1Brown-image86.png)\n![In(x) (x 1) Diverges\" Radius of convergence\" ](media/Essence-of-Calculus---3Blue1Brown-image87.png)-   Lagrange error bounds\n-   Convergence tests\n12. **What they won't teach you in calculus**\n    -   **Derivative is the slope of a graph**\n    -   **Integral is a certain area under that graph**\n    -   **Multivariable calculus**\n    -   **Complex analysis**\n    -   **Differential geometry**\n    -   **Polymorphic Function**\n    -   **Jacobian Determinant**\n\n![](media/Essence-of-Calculus---3Blue1Brown-image88.png)\n**References**\n\n[Essence of calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n![Derivative paradox 70 40 10 dt 3 4 5 ds dt 8 9 10 ](media/Essence-of-Calculus---3Blue1Brown-image89.jpg)"},{"fields":{"slug":"/Mathematics/Calculus/Functions/","title":"Functions"},"frontmatter":{"draft":false},"rawBody":"# Functions\n\nCreated: 2018-06-29 11:01:03 +0500\n\nModified: 2018-06-29 11:03:27 +0500\n\n---\n\n# Elementary Functions\n\n1.  **Polynomials**\n\ny = 3 x^2^ + 5x - 1\n\n2.  **Trigonometric functions**\n\ny = sin x; y = cos x; y = tan x\n\n3.  **Exponential functions**\n\ny = e^x^ , y = 10^x^\n\n4.  **Logarithmic functions**\n\ny = log x; y = log~2~ x\r\n"},{"fields":{"slug":"/Mathematics/Calculus/Gradient/","title":"Gradient"},"frontmatter":{"draft":false},"rawBody":"# Gradient\n\nCreated: 2019-03-06 21:21:26 +0500\n\nModified: 2019-03-06 21:25:08 +0500\n\n---\n\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics), the**gradient**is a multi-variable generalization of the[derivative](https://en.wikipedia.org/wiki/Derivative). While a derivative can be defined on functions of a single variable, for[functions of several variables](https://en.wikipedia.org/wiki/Function_of_several_variables), the gradient takes its place. The gradient is a[vector-valued function](https://en.wikipedia.org/wiki/Vector-valued_function), as opposed to a derivative, which is[scalar-valued](https://en.wikipedia.org/wiki/Scalar-valued_function).\nLike the derivative, the gradient represents the[slope](https://en.wikipedia.org/wiki/Slope)of the[tangent](https://en.wikipedia.org/wiki/Tangent)of the[graph of the function](https://en.wikipedia.org/wiki/Graph_of_a_function). More precisely, the gradient points in the direction of the greatest rate of increase of the function, and its[magnitude](https://en.wikipedia.org/wiki/Magnitude_(mathematics))is the slope of the graph in that direction. The components of the gradient in coordinates are the coefficients of the variables in the equation of the[tangent space](https://en.wikipedia.org/wiki/Tangent_space)to the graph. This characterizing property of the gradient allows it to be defined independently of a choice of coordinate system, as a[vector field](https://en.wikipedia.org/wiki/Vector_field)whose components in a coordinate system will transform when going from one coordinate system to another.\n![tttttt ](media/Gradient-image1.png)\n\nIn the above two images, the values of the function are represented in black and white, black representing higher values, and its corresponding gradient is represented by blue arrows.\n**See also**\n-   Computer Scient > Others > Computer Vision > Image Gradient\n**References**\n\n<https://en.wikipedia.org/wiki/Gradient>\n\n"},{"fields":{"slug":"/Mathematics/Calculus/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-05-17 19:59:55 +0500\n\nModified: 2021-07-11 03:32:57 +0500\n\n---\n\nThe word Calculus comes from Latin meaning \"small stone\",Because it is like understanding something by looking at small pieces.\nDifferential Calculuscuts something into small pieces to find how it changes.\n\nIntegral Calculusjoins (integrates) the small pieces together to find how much there is.\n**Limits**\n\nLimits are all about approaching. Sometimes you can't work something out directly, but you can see what it should be as you get closer and closer!\n**Derivatives (Differential Calculus)**\n\nThe Derivative is the \"rate of change\" or slope of a function at a point.\n\nIntuition - How steep is a function/graph at a point.\n**Integration (Integral Calculus)**\n\nIntegration can be used to find areas, volumes, central points and many useful things.\n\nIntuition - What is the area under the graph over a region\n**Differential Equations**\n\nIn our world things change, anddescribing how they changeoften ends up as a Differential Equation: an equation with afunctionand one or more of itsderivatives.\n**Derivative Rules**\n-   Product Rule\n-   Quotient Rule\n-   Chain Rule\n**Integral Rules**\n-   U-substitution\n-   Integration by parts\n-   Partial fraction decomposition\n**Higher Dimensions**\n-   Partial derivatives\n-   Double Integral\n**Scalar Field -** If a function has 3 inputs and 1 output\n\n**Vector Field -** If a function has 3 inputs and 1 vector output\n**References**\n\n<https://www.khanacademy.org/math/calculus-1>\n\nPrecalculus - <https://www.youtube.com/watch?v=eI4an8aSsgw>\n\n[What is Calculus? (Mathematics)](https://www.youtube.com/watch?v=w3GV9pumczQ)\n![What is Calculus? ](media/Intro-image1.jpg)\n<https://www.freecodecamp.org/news/learn-calculus-2-in-this-free-7-hour-course>\n-   Area Between Curves\n-   Volumes of Solids of Revolution\n-   Volumes Using Cross-Sections\n-   Arclength\n-   Work as an Integral\n-   Average Value of a Function\n-   Proof of the Mean Value Theorem for Integrals\n-   Integration by Parts\n-   Trig Identities\n-   Proof of the Angle Sum Formulas\n-   Integrals Involving Odd Powers of Sine and Cosine\n-   Integrals Involving Even Powers of Sine and Cosine\n-   Special Trig Integrals\n-   Integration Using Trig Substitution\n-   Integrals of Rational Functions\n-   Improper Integrals - Type 1\n-   Improper Integrals - Type 2\n-   The Comparison Theorem for Integrals\n-   Sequences - Definitions and Notation\n-   Series Definitions\n-   Sequences - More Definitions\n-   Monotonic and Bounded Sequences Extra\n-   L'Hospital's Rule\n-   L'Hospital's Rule on Other Indeterminate Forms\n-   Convergence of Sequences\n-   Geometric Series\n-   The Integral Test\n-   Comparison Test for Series\n-   The Limit Comparison Test\n-   Proof of the Limit Comparison Test\n-   Absolute Convergence\n-   The Ratio Test\n-   Proof of the Ratio Test\n-   Series Convergence Test Strategy\n-   Taylor Series Introduction\n-   Power Series\n-   Convergence of Power Series\n-   Power Series Interval of Convergence Example\n-   Proofs of Facts about Convergence of Power Series\n-   Power Series as Functions\n-   Representing Functions with Power Series\n-   Using Taylor Series to find Sums of Series\n-   Taylor Series Theory and Remainder\n-   Parametric Equations\n-   Slopes of Parametric Curves\n-   Area under a Parametric Curve\n-   Arclength of Parametric Curves\n-   Polar Coordinates\n\n"},{"fields":{"slug":"/Mathematics/Calculus/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-08-10 16:46:51 +0500\n\nModified: 2020-03-06 15:26:11 +0500\n\n---\n\n**Laplace Transform**\n\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics), theLaplace transformis an[integral transform](https://en.wikipedia.org/wiki/Integral_transform)named after its inventor[Pierre-Simon Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)([/lÉ™ËˆplÉ‘Ës/](https://en.wikipedia.org/wiki/Help:IPA/English)). It transforms a function of a real variablet(often time) to a function of a[complex variable](https://en.wikipedia.org/wiki/Complex_analysis)s([complex frequency](https://en.wikipedia.org/wiki/Complex_frequency)). The transform has many applications in science and engineering.\nThe Laplace transform is similar to the[Fourier transform](https://en.wikipedia.org/wiki/Fourier_transform). While the Fourier transform of a function is a[complex function](https://en.wikipedia.org/wiki/Complex_function)of arealvariable (frequency), the Laplace transform of a function is a complex function of acomplex variable. Laplace transforms are usually restricted to functions oftwithtâ‰¥ 0. A consequence of this restriction is that the Laplace transform of a function is a[holomorphic function](https://en.wikipedia.org/wiki/Holomorphic_function)of the variables. Unlike the Fourier transform, the Laplace transform of a[distribution](https://en.wikipedia.org/wiki/Distribution_(mathematics))is generally a[well-behaved](https://en.wikipedia.org/wiki/Well-behaved)function. Techniques of complex variables can also be used to directly study Laplace transforms. As a holomorphic function, the Laplace transform has a power series representation. This power series expresses a function as a linear superposition of[moments](https://en.wikipedia.org/wiki/Moment_(mathematics))of the function. This perspective has applications in[probability theory](https://en.wikipedia.org/wiki/Probability_theory).\nThe Laplace transform is invertible on a large class of functions. The inverse Laplace transform takes a function of a complex variables(often frequency) and yields a function of a real variablet(often time). Given a simple mathematical or functional description of an input or output to a[system](https://en.wikipedia.org/wiki/System), the Laplace transform provides an alternative functional description that often simplifies the process of analyzing the behavior of the system, or in synthesizing a new system based on a set of specifications.[[1]](https://en.wikipedia.org/wiki/Laplace_transform#cite_note-1)So, for example, Laplace transformation from the[time domain](https://en.wikipedia.org/wiki/Time_domain)to the[frequency domain](https://en.wikipedia.org/wiki/Frequency_domain)transforms differential equations into algebraic equations and[convolution](https://en.wikipedia.org/wiki/Convolution)into multiplication.\n<https://en.wikipedia.org/wiki/Laplace_transform>\n\n## Area under curve**"},{"fields":{"slug":"/Mathematics/Calculus/Product-Rule-for-Derivatives/","title":"Product Rule for Derivatives"},"frontmatter":{"draft":false},"rawBody":"# Product Rule for Derivatives\n\nCreated: 2018-12-05 00:20:24 +0500\n\nModified: 2018-12-05 00:33:06 +0500\n\n---\n\n![The Product Rule Suppose f (x) and g(x) are functions. There are 5 fundamental function operations: + --- For each operation there is a derivative rule: x O f (x) + g(x) 2. f (x) --- g(x) 4. Sum Rule Difference Rule Product Rule Quotient Rule Chain Rule' ](media/Product-Rule-for-Derivatives-image1.png)\n\nFifth is *function composition*\n![1. Sum Rule: 2. Difference Rule: 3. Product Rule: 4. Quotient Rule: 5. Chain Rule: f' (x) â€¢ g(x) + f (x) â€¢ g' (x) [ LIE.)] ' --- --- (x) [ g(x) 12 [f 0 g(x) ] ' = f' (g(x)) â€¢g' (x) ](media/Product-Rule-for-Derivatives-image2.png)\n3.  Product Rule\n\n![Let f (x) and g(x) be two functions. Question: What is the derivative of f (x) â€¢ g(x) ? It's common to leave out the x's: There are other ways to write the derivative: dxâ€¢g+fÃ† dx f' (x) can be written as dx â€¢ ](media/Product-Rule-for-Derivatives-image3.png)\n![Example Find the derivative of (x2 --- 2) (7 x3 + 5). There are two ways to do this: 1. Multiply the two polynomials, then take the derivative 2. Use the product rule l. Multiply, then differentiate: (x2 -2) - +5x2- 10 Derivative of (x2 --- 2) (7 x3 + 5) is 35 x4 --- 42x2 + 10 x. ](media/Product-Rule-for-Derivatives-image4.png)\n![2. Use the product rule: t t g(x) g (x) = 7 5 (x) = 21 = 14 X4+ 42 X2 =35x4-42x2+10x - 2) (21 x2) ](media/Product-Rule-for-Derivatives-image5.png)\n![Caution Common mistake: [f = f' â€¢g' Tip: Write down what you think is the formula, then test it! Test: Let f (x) = x2 and g(x) = x3 f (x) â€¢ g(x) = x2 â€¢ x3 = x5 [x5]' --- 5x4 Now test the formula: (2 x) â€¢ (3 x2) = 6 x3 * 5 x4 This formula is wrong! ](media/Product-Rule-for-Derivatives-image6.png)\n![Generalizations Find the derivative of 17 x â€¢ cos x â€¢ ex. This function is the product of three functions, not two: (17 x) (cos x) (ex). Trick: Use the product rule repeatedly. - + (f.g).h' ](media/Product-Rule-for-Derivatives-image7.png)\n![Example Find the derivative of 17 x â€¢ cos x â€¢ ex. This is the product of 3 functions which we know how to differentiate: (17 X) (COS X) (ex ) Let f (x) = 17 x, g(x) = cos x, and h(x) = ex. = (17) (COS X) (ex ) + (17 X) (---sin X) (ex ) + (17 X) (COS X) (ex) --- 17 cosxâ€¢ex--- 17 x â€¢sinxâ€¢eX+ 17 ](media/Product-Rule-for-Derivatives-image8.png)\n**References**\n\n[Product Rule for Derivatives (Calculus)](https://www.youtube.com/watch?v=8Qw2aPjqW9c)\n![The Product Rule ](media/Product-Rule-for-Derivatives-image9.jpg)\n"},{"fields":{"slug":"/Mathematics/Calculus/Quotient-Rule/","title":"Quotient Rule"},"frontmatter":{"draft":false},"rawBody":"# Quotient Rule\n\nCreated: 2018-12-05 00:30:46 +0500\n\nModified: 2018-12-05 00:51:08 +0500\n\n---\n\n![The Quotient Rule Let f (x) and g(x) be two functions. L.A \"Quotient\" Question: What is the derivative of ? g(x) Commutative Property ](media/Quotient-Rule-image1.png)\n![Example Use the quotient rule to find the derivative of y = Plug everything into the quotient rule: 3 (2 x-5) - (3 X+1).2 (2 x-5)2 (2 x-5)2 --- 6x-15-6x-2 (2 x-5)2 ---=LZ--- --- negative (2 x-5)2 positive 2x-5 --- g(x) ](media/Quotient-Rule-image2.png)\n![Common Mistake Wrong = Tip: Write down what you think the formula is, then test it! Test: Let f (x) = x3 and g(x) = x2 Compute the correct answer: Test the formula: ( L ) ---Q L 3x2 The formula is wrong! ](media/Quotient-Rule-image3.png)\n**References**\n\n[The Quotient Rule (Calculus)](https://www.youtube.com/watch?v=OPjN7Gvb4-8)\n![Quot(ept ' Rule ](media/Quotient-Rule-image4.jpg)\n\n"},{"fields":{"slug":"/Mathematics/Calculus/Tangent-Line-and-the-Derivative/","title":"Tangent Line and the Derivative"},"frontmatter":{"draft":false},"rawBody":"# Tangent Line and the Derivative\n\nCreated: 2018-12-04 23:51:45 +0500\n\nModified: 2018-12-05 00:14:13 +0500\n\n---\n\n1.  Tangent Line\n\nA line that touches the curve at only one point, and is perpendicular to the point it touches.\n\nDefinition - A tangent line to a point A is the limit of the secant lines as P approaches A.\n\nAlternative Definition - The tangent line through a point A is the line that passes through A and whose slope is the derivative at A.\n1.  Tangent line must touch the curve\n\n2.  Limit from left = Limit from right\n![x ](media/Tangent-Line-and-the-Derivative-image1.png)\n![sin 0 o undefined o ](media/Tangent-Line-and-the-Derivative-image2.png)\nTherefore this line doesn't have a tangent line at point (0,1), since tangent lines must touch the curve\n![å†– 0 'z-) ](media/Tangent-Line-and-the-Derivative-image3.png)\n![Left hand limit * Right hand limit (2 (1 0) ](media/Tangent-Line-and-the-Derivative-image4.png)\nSince the left hand limit line not equal to right hand limit line, we say limit does not exists, therefore there is no tangent line for the curve at point (-2, 0)\n![Tangent Line = Best approximation by a line ](media/Tangent-Line-and-the-Derivative-image5.png)\n2.  Secant Line\n\nA line that touches the curve at two points.\n\n![](media/Tangent-Line-and-the-Derivative-image6.png)\n**Derivative**\n\n![Derivative = Slope of Tangent Line ](media/Tangent-Line-and-the-Derivative-image7.png)\n![Ù„ - Ù„ ](media/Tangent-Line-and-the-Derivative-image8.png)\n![As P + A, secant + tangent, m --- -u derivative Ax ](media/Tangent-Line-and-the-Derivative-image9.png)\n![As P + A, secant + tangent, m = Ax dx ](media/Tangent-Line-and-the-Derivative-image10.png)\n![Secant Lines: Tangent Lines: m Ax dx Ax, dx, Ay are numbers dy are differentials ](media/Tangent-Line-and-the-Derivative-image11.png)\n![Finding equation of tangent line Point slope form: Y Â¯ Yl = m(x --- Xl) Point: A = (Xl, Yl) Slope: m = dx ](media/Tangent-Line-and-the-Derivative-image12.png)\n**References**\n\n[The Tangent Line and the Derivative (Calculus)](https://www.youtube.com/watch?v=O_cwTAfjgAQ)\n![Tangent Lines ](media/Tangent-Line-and-the-Derivative-image13.jpg)\n\n"},{"fields":{"slug":"/Mathematics/Combinatorics/Birthday-Paradox/","title":"Birthday Paradox"},"frontmatter":{"draft":false},"rawBody":"# Birthday Paradox\n\nCreated: 2017-11-14 11:46:58 +0500\n\nModified: 2018-05-15 10:48:49 +0500\n\n---\n\n**Problem - Exponents aren't intuitive**\n\nThe Birthday Paradox is one of the most surprising results in math. In a room of just 23 people there's a 50-50 chance of two people having the same birthday. In a room of 75 there's a 99.9% chance of two people matching.\n\nWhoa -- why does this happen? It turns out the \"paradox\" happens because of two common misunderstandings: we think with multiplication (not exponents), and we forget about everyone else in the room.\n\nInstead of just accepting the paradox as true (\"fine, it works\"), see it as a chance to dig into why math gets confusing. By fixing the root cause, we improve our overall math sense. (This philosophy was a turning point for me: being confused, while frustrating in the moment, is a chance to finally fix a leaky roof.)\n![displaystyle{left(frac{364}{365}right)^{253} = .4995}](media/Birthday-Paradox-image1.png)1.  Exponents aren't Intuitive\n\n2.  Humans are a tad bit selfish - They only compare themselves to others, not everyone to everyother person.\n\n"},{"fields":{"slug":"/Mathematics/Combinatorics/Conditional-Probability/","title":"Conditional Probability"},"frontmatter":{"draft":false},"rawBody":"# Conditional Probability\n\nCreated: 2017-09-15 09:48:42 +0500\n\nModified: 2017-09-15 09:50:19 +0500\n\n---\n\nConditional probability is calculating the probability of an event given that another event has already occured .\n\nThe formula for conditional probability P(A|B), read as P(A given B) is\n\n**P(A|B) = P (A and B) / P(B)**\nConsider the following example:\n\n**Example:**In a class, 40% of the students study math and science. 60% of the students study math. What is the probability of a student studying science given he/she is already studying math?\n\n**Solution**\n\nP(M and S) = 0.40\n\nP(M) = 0.60\n\nP(S|M) = P(M and S)/P(S) = 0.40/0.60 = 2/3 = 0.67\n"},{"fields":{"slug":"/Mathematics/Combinatorics/Inclusion-Exclusion-Principle/","title":"Inclusion-Exclusion Principle"},"frontmatter":{"draft":false},"rawBody":"# Inclusion-Exclusion Principle\n\nCreated: 2018-03-10 11:56:44 +0500\n\nModified: 2018-03-10 11:59:01 +0500\n\n---\n\nIn[combinatorics](https://en.wikipedia.org/wiki/Combinatorics)(combinatorial mathematics), the**inclusion--exclusion principle**is a counting technique which generalizes the familiar method of obtaining the number of elements in the[union](https://en.wikipedia.org/wiki/Union_(set_theory))of two finite[sets](https://en.wikipedia.org/wiki/Set_(mathematics)); symbolically expressed as\n\n![](media/Inclusion-Exclusion-Principle-image1.png)\n\nwhere*A*and*B*are two finite sets and |*S*| indicates the[cardinality](https://en.wikipedia.org/wiki/Cardinality)of a set*S*(which may be considered as the number of elements of the set, if the set is[finite](https://en.wikipedia.org/wiki/Finite_set)). The formula expresses the fact that the sum of the sizes of the two sets may be too large since some elements may be counted twice. The double-counted elements are those in the[intersection](https://en.wikipedia.org/wiki/Intersection_(set_theory))of the two sets and the count is corrected by subtracting the size of the intersection.\n\nThe principle is more clearly seen in the case of three sets, which for the sets*A*,*B*and*C*is given by\n\n![](media/Inclusion-Exclusion-Principle-image2.png)\n**Illustration using Venn Diagram:**\n\n![Ğ²Ğ¿Ñ Ñ ĞĞ¿Ğ’Ğ¿Ğ¡ ](media/Inclusion-Exclusion-Principle-image3.png)\n**References**\n-   <https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle>\n"},{"fields":{"slug":"/Mathematics/Combinatorics/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-02-22 23:43:56 +0500\n\nModified: 2021-06-06 16:49:43 +0500\n\n---\n\n**Combinatorics**is an area of[mathematics](https://en.wikipedia.org/wiki/Mathematics)primarily concerned with counting, both as a means and an end in obtaining results, and certain properties of[finite](https://en.wikipedia.org/wiki/Finite_set)[structures](https://en.wikipedia.org/wiki/Mathematical_structure). It is closely related to many other areas of mathematics and has many applications ranging from[logic](https://en.wikipedia.org/wiki/Logic)to[statistical physics](https://en.wikipedia.org/wiki/Statistical_physics), from[evolutionary biology](https://en.wikipedia.org/wiki/Evolutionary_biology)to[computer science](https://en.wikipedia.org/wiki/Computer_science), etc.\n**Twelve-fold way**\n\nIn[combinatorics](https://en.wikipedia.org/wiki/Combinatorics), thetwelvefold wayis a systematic classification of 12 related enumerative problems concerning two finite sets, which include the classical problems of [counting](https://en.wikipedia.org/wiki/Counting) [permutations](https://en.wikipedia.org/wiki/Permutations), [combinations](https://en.wikipedia.org/wiki/Combinations), [multisets](https://en.wikipedia.org/wiki/Multiset), and partitions either[of a set](https://en.wikipedia.org/wiki/Partition_of_a_set)or[of a number](https://en.wikipedia.org/wiki/Partition_(number_theory)). The idea of the classification is credited to[Gian-Carlo Rota](https://en.wikipedia.org/wiki/Gian-Carlo_Rota), and the name was suggested by[Joel Spencer](https://en.wikipedia.org/wiki/Joel_Spencer).\n<https://en.wikipedia.org/wiki/Twelvefold_way>\n"},{"fields":{"slug":"/Mathematics/Combinatorics/Permutation-and-Combination/","title":"Permutation and Combination"},"frontmatter":{"draft":false},"rawBody":"# Permutation and Combination\n\nCreated: 2018-04-10 19:30:16 +0500\n\nModified: 2020-04-26 01:55:05 +0500\n\n---\n\nPermutation - Order matters\n\nCombination - Order doesn't matter\npermutation sounds complicated, doesn't it? And it is. With permutations, every little detail matters. Alice, Bob and Charlie is different from Charlie, Bob and Alice (insert your friends' names here).\n\nCombinations, on the other hand, are pretty easy going. The details don't matter. Alice, Bob and Charlie is the same as Charlie, Bob and Alice.\n\nPermutations are for lists (order matters) and combinations are for groups (order doesn't matter).\n\nA joke: A \"combination lock\" should really be called a \"permutation lock\". The order you put the numbers in matters. (A true \"combination lock\" would accept both 10-17-23 and 23-17-10 as correct.)\n\nCombinations sound simpler than permutations, and they are. You have fewer combinations than permutations.\n**References**\n\n<https://www.freecodecamp.org/news/permutation-and-combination-the-difference-explained-with-formula-examples>\n<https://betterexplained.com/articles/easy-permutations-and-combinations/#!parentId=756>\n\n[Easy Combinations and Permutations | BetterExplained](https://www.youtube.com/watch?v=bAk_7p5gAWc)\n\n![COMBINATIONS vs PERMlJTATlONS E (ÎœÎ¬Î¹ ](media/Permutation-and-Combination-image1.jpg)\n\n"},{"fields":{"slug":"/Mathematics/Combinatorics/Pigeonhole-Principle/","title":"Pigeonhole Principle"},"frontmatter":{"draft":false},"rawBody":"# Pigeonhole Principle\n\nCreated: 2018-05-16 20:57:24 +0500\n\nModified: 2018-05-16 21:05:16 +0500\n\n---\n\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics), the**pigeonhole principle**states that if*n*items are put into*m*containers, with*n*>*m*, then at least one container must contain more than one item.\nExample -\n\nA bag contains 10 red marbles, 10 white marbles, and 10 blue marbles. What is the minimum no. of marbles you have to choose randomly from the bag to ensure that we get 4 marbles of same color?\n\n**Solution:**Apply pigeonhole principle.\n\nNo. of colors (pigeonholes) n = 3\n\nNo. of marbles (pigeons) K+1 = 4\n\nTherefore the minimum no. of marbles required = Kn+1\n\nBy simplifying we get Kn+1 = 10.\n\nVerification: ceil[Average] is [Kn+1/n] = 4\n\n[Kn+1/3] = 4\n\nKn+1 = 10\n\ni.e., 3 red + 3 white + 3 blue + 1(red or white or blue) = 10\n\n**Pigeonhole principle strong form --**\n\n**Theorem:**Let q1, q2, . . . , qnbe positive integers.\n\nIf q1+ q2+ . . . + qnâˆ’ n + 1 objects are put into n boxes, then either the 1st box contains at least q1objects, or the 2nd box contains at least q2objects, . . ., the nth box contains at least qnobjects.\n**Example -- 1:**In a computer science department, a student club can be formed with either 10 members from first year or 8 members from second year or 6 from third year or 4 from final year. What is the minimum no. of students we have to choose randomly from department to ensure that a student club is formed?\n\n**Solution:**we can directly apply from the above formula where,\n\nq1=10, q2=8, q3=6, q4=4 and n=4\n\nTherefore the minimum number of students required to ensure department club to be formed is\n\n10 + 8 + 6 + 4 -- 4 + 1 = 25**References**\n-   <https://en.wikipedia.org/wiki/Pigeonhole_principle>\n-   <https://www.geeksforgeeks.org/discrete-mathematics-the-pigeonhole-principle>\n"},{"fields":{"slug":"/Mathematics/Combinatorics/Probability/","title":"Probability"},"frontmatter":{"draft":false},"rawBody":"# Probability\n\nCreated: 2018-02-11 17:39:45 +0500\n\nModified: 2018-02-11 17:40:43 +0500\n\n---\n\n1.  Independent Events\n\n2.  Dependent Events (Conditional Probability)\nBayes Theorem"},{"fields":{"slug":"/Mathematics/General/Ackermann-Function/","title":"Ackermann Function"},"frontmatter":{"draft":false},"rawBody":"# Ackermann Function\n\nCreated: 2018-01-30 17:49:11 +0500\n\nModified: 2018-01-30 17:59:34 +0500\n\n---\n\nIn[computability theory](https://en.wikipedia.org/wiki/Computability_theory), the**Ackermann function**, named after[Wilhelm Ackermann](https://en.wikipedia.org/wiki/Wilhelm_Ackermann), is one of the simplest[^[1]^](https://en.wikipedia.org/wiki/Ackermann_function#cite_note-1)and earliest-discovered examples of a[total](https://en.wikipedia.org/wiki/Total_function)[computable function](https://en.wikipedia.org/wiki/Computable_function)that is not[primitive recursive](https://en.wikipedia.org/wiki/Primitive_recursive_function). All primitive recursive functions are total and computable, but the Ackermann function illustrates that not all total computable functions are primitive recursive.\nThis function has a value ackermann(n) < 5, for any value of*n*that can be written in this physical universe. So essentially it's a constant time operation\n"},{"fields":{"slug":"/Mathematics/General/Advanced-Topics/","title":"Advanced Topics"},"frontmatter":{"draft":false},"rawBody":"# Advanced Topics\n\nCreated: 2017-09-07 17:01:00 +0500\n\nModified: 2020-11-22 19:53:52 +0500\n\n---\n\nBayes' Theorem and Conditional Probability (<https://brilliant.org/wiki/bayes-theorem>)\nLinear Diophantine Equations (<https://brilliant.org/wiki/linear-diophantine-equations-one-equation/#linear-diophantine-equations>)\nMaximum Likelihood Estimation (MLE) (<https://brilliant.org/wiki/maximum-likelihood-estimation-mle>)\n\n<https://rpsychologist.com/d3/likelihood>\nMaximum A Posteriori estimation (MAP)\n\n<https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-and-maximum-a-posteriori-estimation-d7c318f9d22d>\n\n<https://towardsdatascience.com/mle-map-and-bayesian-inference-3407b2d6d4d9>\nMonty Hall Problem (<https://brilliant.org/wiki/monty-hall-problem>)\n\n<https://betterexplained.com/articles/understanding-the-monty-hall-problem>\nBayesian reasoning / Natural Frequencies\nConditional Probability\n\nJoint Probability (<https://brilliant.org/wiki/discrete-random-variables-joint-probability>)\n"},{"fields":{"slug":"/Mathematics/General/Conjecture/","title":"Conjecture"},"frontmatter":{"draft":false},"rawBody":"# Conjecture\n\nCreated: 2017-10-10 08:34:09 +0500\n\nModified: 2019-09-20 08:14:38 +0500\n\n---\n\nConjecture - Is a conclusion or proposition based on incomplete information, for which no proof has been found.\n\n1.  Collatz Conjecture\n\nCreate a series starting with N, if even then divide by 2, if odd then 3n+1. The series will always reach 1.\n-   Pick a number, a positive integer\n-   If even divide by 2\n-   If odd multiply by 3 and add one\n-   Repeat above two steps with new value\n2.  Riemann hypotesis\n"},{"fields":{"slug":"/Mathematics/General/Discrete-Mathematics/","title":"Discrete Mathematics"},"frontmatter":{"draft":false},"rawBody":"# Discrete Mathematics\n\nCreated: 2019-12-25 23:13:09 +0500\n\nModified: 2021-11-10 16:58:48 +0500\n\n---\n\nDiscrete mathematicsis the study of[mathematical structures](https://en.wikipedia.org/wiki/Mathematical_structures)that are fundamentally[discrete](https://en.wikipedia.org/wiki/Discrete_space)rather than[continuous](https://en.wikipedia.org/wiki/Continuous_function). In contrast to[real numbers](https://en.wikipedia.org/wiki/Real_number)that have the property of varying \"smoothly\", the objects studied in discrete mathematics -- such as[integers](https://en.wikipedia.org/wiki/Integer),[graphs](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)), and[statements](https://en.wikipedia.org/wiki/Statement_(logic))in[logic](https://en.wikipedia.org/wiki/Mathematical_logic)-- do not vary smoothly in this way, but have distinct, separated values.Discrete mathematics therefore excludes topics in \"continuous mathematics\" such as[calculus](https://en.wikipedia.org/wiki/Calculus)or[Euclidean geometry](https://en.wikipedia.org/wiki/Euclidean_geometry). Discrete objects can often be[enumerated](https://en.wikipedia.org/wiki/Enumeration)by integers. More formally, discrete mathematics has been characterized as the branch of mathematics dealing with[countable sets](https://en.wikipedia.org/wiki/Countable_set)(finite sets or sets with the same[cardinality](https://en.wikipedia.org/wiki/Cardinality)as the natural numbers). However, there is no exact definition of the term \"discrete mathematics.\"Indeed, discrete mathematics is described less by what is included than by what is excluded: continuously varying quantities and related notions.\nThe set of objects studied in discrete mathematics can be finite or infinite. The termfinite mathematicsis sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.\n<https://en.wikipedia.org/wiki/Discrete_mathematics>\n\n## Recurrence Relation**\n\nA recurrence relation is an equation that recursively defines a sequence where the next term is a function of the previous terms (ExpressingFnas some combination ofFiwithi<n).\nExampleâˆ’ Fibonacci series âˆ’Fn=Fnâˆ’1+Fnâˆ’2Fn=Fnâˆ’1+Fnâˆ’2, Tower of Hanoi âˆ’Fn=2Fnâˆ’1+1\n1.  **Linear Recurrence Relations**\n\nA linear recurrence equation of degree k or order k is a recurrence equation which is in the formatxn=A1xnâˆ’1+A2xnâˆ’1+A3xnâˆ’1+...Akxnâˆ’k (Anis a constant andAkâ‰ 0) on a sequence of numbers as a first-degree polynomial.\n2.  **Non-Homogeneous Recurrence Relation**\n\nA recurrence relation is called non-homogeneous if it is in the form\n\nFn=AFnâˆ’1+BFnâˆ’2+f(n)wheref(n)â‰ 0\n\nIts associated homogeneous recurrence relation isFn=AFn--1+BFnâˆ’2\n<https://www.tutorialspoint.com/discrete_mathematics/discrete_mathematics_recurrence_relation.htm>\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics), arecurrence relationis an[equation](https://en.wikipedia.org/wiki/Equation)that[recursively](https://en.wikipedia.org/wiki/Recursion)defines a[sequence](https://en.wikipedia.org/wiki/Sequence)or multidimensional array of values, once one or more initial terms are given; each further term of the sequence or array is defined as a[function](https://en.wikipedia.org/wiki/Function_(mathematics))of the preceding terms.\nThe term[difference equation](https://en.wikipedia.org/wiki/Recurrence_relation#Relationship_to_difference_equations_narrowly_defined)sometimes (and for the purposes of this article) refers to a specific type of recurrence relation. However, \"difference equation\" is frequently used to refer toanyrecurrence relation.**Fibonacci numbers**\n\nThe recurrence of order two satisfied by the[Fibonacci numbers](https://en.wikipedia.org/wiki/Fibonacci_number)is the archetype of a homogeneous[linear recurrence](https://en.wikipedia.org/wiki/Linear_recurrence)relation with constant coefficients. The Fibonacci sequence is defined using the recurrence\n\nFn = Fn-1 + Fn-2\n[https://en.wikipedia.org/wiki/Recurrence_relation](https://en.wikipedia.org/wiki/Recurrence_relation#Binomial_coefficients)\n**Common Recurrence Relations**\n\n| **Recurrence**         | **Algorithm**                      | **Big-Oh Solution** |\n|----------------------|----------------------------------|-----------------|\n| T(n) = T(n/2) + O(1)   | Binary Search                      | *O(log n)*          |\n| T(n) = T(n-1) + O(1)   | Sequential Search                  | *O(n)*              |\n| T(n) = 2 T(n/2) + O(1) | tree traversal                     | *O(n)*              |\n| T(n) = T(n-1) + O(n)   | Selection Sort (other n^2^sorts)  | *O(n^2^)*           |\n| T(n) = 2 T(n/2) + O(n) | Mergesort (average case Quicksort) | *O(n log n)*        |\n<https://users.cs.duke.edu/~ola/ap/recurrence.html>\n"},{"fields":{"slug":"/Mathematics/General/Fermat's-Last-Theorem/","title":"Fermat's Last Theorem"},"frontmatter":{"draft":false},"rawBody":"# Fermat's Last Theorem\n\nCreated: 2018-08-03 21:23:01 +0500\n\nModified: 2018-08-03 21:23:32 +0500\n\n---\n\nIn[number theory](https://en.wikipedia.org/wiki/Number_theory),**Fermat's Last Theorem**(sometimes called**Fermat's conjecture**, especially in older texts) states that no three[positive](https://en.wikipedia.org/wiki/Positive_number)[integers](https://en.wikipedia.org/wiki/Integer)*a*,*b*, and*c*satisfy the equation*a^n^*+*b^n^*=*c^n^*for any integer value of*n*greater than 2. The cases*n*= 1and*n*= 2have been known to have infinitely many solutions since antiquity.\n**References**\n\n<https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem>\n"},{"fields":{"slug":"/Mathematics/General/Fermat's-Little-Theorem/","title":"Fermat's Little Theorem"},"frontmatter":{"draft":false},"rawBody":"# Fermat's Little Theorem\n\nCreated: 2018-04-15 14:24:45 +0500\n\nModified: 2018-04-15 14:27:27 +0500\n\n---\n\nFermat's little theoremis a fundamental theorem in elementary number theory, which helps compute powers of integers[modulo](https://brilliant.org/wiki/modular-arithmetic/)[prime numbers](https://brilliant.org/wiki/prime-numbers/). It is a special case of[Euler's theorem](https://brilliant.org/wiki/eulers-theorem/), and is important in applications of elementary number theory, including[primality testing](https://brilliant.org/wiki/prime-testing/)and[public-key cryptography](https://brilliant.org/wiki/rsa-encryption/).\nLet p be a prime number, and a be any integer. Then a^p^ - a is always divisible by p.\n"},{"fields":{"slug":"/Mathematics/General/GCD/","title":"GCD"},"frontmatter":{"draft":false},"rawBody":"# GCD\n\nCreated: 2018-04-15 12:56:46 +0500\n\nModified: 2018-04-15 12:58:12 +0500\n\n---\n\n**Synonyms**\n\n1.  Greatest Common Divisor (GCD)\n\n2.  Greatest Common Factor (GCF)\n\n3.  Highest Common Factor (HCF)\n\n4.  Highest Common Divisor (HCD)\n\n5.  Greatest Common Measure (GCM)\n**GCD -**\n\nGreatest Common Divisor (GCD) of two integers A and B is thel**argest integer that divides both A and B.**\n**Euclidean Algorithm**\n\nthe**Euclidean algorithm**, or**Euclid's algorithm**, is an efficient method for computing thegreatest common divisor of two numbers.\ndef gcd(x, y):\n\nwhile (y):\n\nx, y = y, x%y\n\nreturn x\n**Applications-**\n\n1.  Reducing fractions to their simplest form\n\n2.  Performing division in modular arithmetic\n\n3.  Computations using this algorithm form part of the cryptographic protocols that are used to secure internet communications\n\n4.  Also used for breaking cryptosystems by factoring large composite numbers\n\n5.  Solve Diophantine equations, such as finding numbers that satisfy multiple congruences according to the Chinese remainder theorem\n\n6.  To construct continued fractions\n\n7.  To find accurate rational approximations to real numbers\n\n8.  Proving theorems such as Lagrange's four square theorem and uniqueness of prime factorizations\n"},{"fields":{"slug":"/Mathematics/General/Godel's-Incompleteness-Theorem/","title":"Godel's Incompleteness Theorem"},"frontmatter":{"draft":false},"rawBody":"# Godel's Incompleteness Theorem\n\nCreated: 2018-07-17 19:57:18 +0500\n\nModified: 2021-11-10 16:57:55 +0500\n\n---\n\nGÃ¶del's incompleteness theoremsare two[theorems](https://en.wikipedia.org/wiki/Theorem)of[mathematical logic](https://en.wikipedia.org/wiki/Mathematical_logic)that demonstrate the inherent limitations of every formal[axiomatic system](https://en.wikipedia.org/wiki/Axiomatic_system)containing basic[arithmetic](https://en.wikipedia.org/wiki/Arithmetic). These results, published by[Kurt GÃ¶del](https://en.wikipedia.org/wiki/Kurt_G%C3%B6del)in 1931, are important both in mathematical logic and in the[philosophy of mathematics](https://en.wikipedia.org/wiki/Philosophy_of_mathematics). The theorems are widely, but not universally, interpreted as showing that[Hilbert's program](https://en.wikipedia.org/wiki/Hilbert%27s_program)to find a complete and consistent set of[axioms](https://en.wikipedia.org/wiki/Axiom)for all[mathematics](https://en.wikipedia.org/wiki/Mathematics)is impossible.\nThe first incompleteness theorem states that no[consistent system](https://en.wikipedia.org/wiki/Consistency)of axioms whose theorems can be listed by an[effective procedure](https://en.wikipedia.org/wiki/Effective_procedure)(i.e., an[algorithm](https://en.wikipedia.org/wiki/Algorithm)) is capable of proving all truths about the arithmetic of the[natural numbers](https://en.wikipedia.org/wiki/Natural_number). For any such formal system, there will always be statements about the natural numbers that are true, but that are unprovable within the system. The second incompleteness theorem, an extension of the first, shows that the system cannot demonstrate its own consistency.\nEmploying a[diagonal argument](https://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument), GÃ¶del's incompleteness theorems were the first of several closely related theorems on the limitations of formal systems. They were followed by[Tarski's undefinability theorem](https://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem)on the formal undefinability of truth,[Church](https://en.wikipedia.org/wiki/Alonzo_Church)'s proof that Hilbert's[Entscheidungsproblem](https://en.wikipedia.org/wiki/Entscheidungsproblem)is unsolvable, and[Turing](https://en.wikipedia.org/wiki/Alan_Turing)'s theorem that there is no algorithm to solve the[halting problem](https://en.wikipedia.org/wiki/Halting_problem).\n**Entscheidungsproblem**\n\nTheEntscheidungsproblem(pronounced[[É›ntËˆÊƒaÉªÌ¯dÊŠÅ‹spÊoËŒbleËm]](https://en.wikipedia.org/wiki/Help:IPA/Standard_German),[German](https://en.wikipedia.org/wiki/German_language)for \"decision problem\") is a challenge posed by[David Hilbert](https://en.wikipedia.org/wiki/David_Hilbert)and[Wilhelm Ackermann](https://en.wikipedia.org/wiki/Wilhelm_Ackermann)in 1928.The problem asks for an[algorithm](https://en.wikipedia.org/wiki/Algorithm)that takes as input a statement of a[first-order logic](https://en.wikipedia.org/wiki/First-order_logic) (possibly with a finite number of[axioms](https://en.wikipedia.org/wiki/Axiom)beyond the usual axioms of first-order logic) and answers \"Yes\" or \"No\" according to whether the statement isuniversally valid, i.e., valid in every[structure](https://en.wikipedia.org/wiki/Structure_(mathematical_logic))satisfying the axioms. By[the completeness theorem of first-order logic](https://en.wikipedia.org/wiki/G%C3%B6del%27s_completeness_theorem), a statement is universally valid if and only if it can be deduced from the axioms, so the Entscheidungsproblem can also be viewed as asking for an algorithm to decide whether a given statement is provable from the axioms using the[rules of logic](https://en.wikipedia.org/wiki/Rules_of_logic).\nIn 1936,[Alonzo Church](https://en.wikipedia.org/wiki/Alonzo_Church)and[Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing)published independent papersshowing that a general solution to theEntscheidungsproblemis impossible, assuming that the intuitive notion of \"[effectively calculable](https://en.wikipedia.org/wiki/Effectively_calculable)\" is captured by the functions computable by a[Turing machine](https://en.wikipedia.org/wiki/Turing_machine)(or equivalently, by those expressible in the[lambda calculus](https://en.wikipedia.org/wiki/Lambda_calculus)). This assumption is now known as the[Church--Turing thesis](https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis).\n<https://en.wikipedia.org/wiki/Entscheidungsproblem>\n\n## References**\n\n<https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems>\n\n[GÃ¶del's Incompleteness Theorem - Numberphile](https://www.youtube.com/watch?v=O4ndIDcDSGc)\n"},{"fields":{"slug":"/Mathematics/General/Golden-Ratio---phi/","title":"Golden Ratio - phi"},"frontmatter":{"draft":false},"rawBody":"# Golden Ratio - phi\n\nCreated: 2018-06-01 00:24:37 +0500\n\nModified: 2018-06-01 00:30:49 +0500\n\n---\n\nTwo quantities are in the**golden ratio**if their[ratio](https://en.wikipedia.org/wiki/Ratio)is the same as the ratio of their[sum](https://en.wikipedia.org/wiki/Summation)to the larger of the two quantities. Expressed algebraically, for quantities*a*and*b*with*a*>*b*>0,\n\n![a def ](media/Golden-Ratio---phi-image1.png)\n\nwhere the Greek letter[phi](https://en.wikipedia.org/wiki/Phi_(letter))(Ï•orÏ†) represents the golden ratio. It is an[irrational number](https://en.wikipedia.org/wiki/Irrational_number)with a value of:\n![1.6180339887. 2 ](media/Golden-Ratio---phi-image2.png)\nThe Geometric Relationship -\n\n![a+b a+bis toaasa is tob ](media/Golden-Ratio---phi-image3.png)\nThe golden ratio is also called the**golden mean**or**golden section**(Latin:*sectio aurea*).Other names include**extreme and mean ratio**,**medial section**,**divine proportion**,**divine section** (Latin:*sectio divina*),**golden proportion**,**golden cut**,and**golden number**.\n"},{"fields":{"slug":"/Mathematics/General/Greek-Letters---Latin/","title":"Greek Letters / Latin"},"frontmatter":{"draft":false},"rawBody":"# Greek Letters / Latin\n\nCreated: 2018-05-24 11:31:28 +0500\n\nModified: 2021-10-03 20:19:20 +0500\n\n---\n\n| **Name**                                                                                                                    | **Symbol** | **Name**                                                                                                                    | **Symbol** | **Name**                                                                                                                  | **Symbol** | **Name**                                                                                                                    | **Symbol** | **Name**                                                                                                                    | **Symbol** |\n|--------|-------|---------|-------|--------|-------|--------|-------|-------|-------|\n| [Alpha](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%91%CE%B1_(alpha))     | Î‘ Î±        | [Digamma](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CF%9C%CF%9D_(digamma)) | Ïœ Ï        | [Kappa](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%9A%CE%BA_(kappa))   | Îš Îº Ï°      | [Omicron](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%9F%CE%BF_(omicron)) | ÎŸ Î¿        | [Upsilon](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%A5%CF%85_(upsilon)) | Î¥ Ï…        |\n| [Beta](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%92%CE%B2_(beta))       | Î’ Î²        | [Zeta](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%96%CE%B6_(zeta))       | Î– Î¶        | [Lambda](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%9B%CE%BB_(lambda)) | Î› Î»        | [Pi](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%A0%CF%80_(pi))           | Î  Ï€ Ï–      | [Phi](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%A6%CF%86_(phi))         | Î¦ Ï• Ï†      |\n| [Gamma](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%93%CE%B3_(gamma))     | Î“ Î³        | [Eta](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%97%CE%B7_(eta))         | Î— Î·        | [Mu](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%9C%CE%BC_(mu))         | Îœ Î¼        | [Rho](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%A1%CF%81_(rho))         | Î¡ Ï Ï±      | [Chi](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%A7%CF%87_(chi))         | Î§ Ï‡        |\n| [Delta](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%94%CE%B4_(delta))     | Î” Î´        | [Theta](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%98%CE%B8_(theta))     | Î˜ Î¸ Ï‘      | [Nu](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%9D%CE%BD_(nu))         | Î Î½        | [Sigma](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%A3%CF%83_(sigma))     | Î£ Ïƒ Ï‚      | [Psi](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%A8%CF%88_(psi))         | Î¨ Ïˆ        |\n| [Epsilon](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%95%CE%B5_(epsilon)) | Î• Ïµ Îµ      | [Iota](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%99%CE%B9_(iota))       | Î™ Î¹        | [Xi](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%9E%CE%BE_(xi)) / Ksi   | Î Î¾        | [Tau](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%A4%CF%84_(tau))         | Î¤ Ï„        | [Omega](https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering#%CE%A9%CF%89_(omega))     | Î© Ï‰        |\n![Ğ’ Ğ Ğ reek ĞĞ›Ñ€Ñ›Ğ°ĞªĞ beta Ñ‚Ñ‚Ğ° delta epsiIon zeta eta theta iota Ğš ĞšĞ°Ñ€ lambda Ñ‚Ğ¸ N Ğ¢ Ğ¿Ğ¸ ksi omicron sigma tau upsi!pvy---' Ñ… chi si Ğ¿ Ñ‚ Ğ° ](media/Greek-Letters---Latin-image1.png)\n**Others**\n\nUnion - âˆª\n\nIntersection - **âˆ©**\n\nSubset - âŠ†\n\nProper subset - âŠ‚\n\nNot subset - âŠ„\n\nSuperset - âŠ‡\n\nProper superset - âŠƒ\n\nNot superset - âŠ…\n\nSymmetric difference - âˆ† / âŠ–\n\nElement of - âˆˆ\n\nNot element of - âˆ‰\n\nUniversal quantifier - âˆ€\n\nPlus-minus sigh - Â±\n**Hebrew Letter**\n\naleph - ×\n**References**\n\n<https://en.wikipedia.org/wiki/Greek_letters_used_in_mathematics,_science,_and_engineering>\n\n"},{"fields":{"slug":"/Mathematics/General/Handshaking-Lemma/","title":"Handshaking Lemma"},"frontmatter":{"draft":false},"rawBody":"# Handshaking Lemma\n\nCreated: 2018-04-04 15:25:56 +0500\n\nModified: 2018-04-04 15:26:50 +0500\n\n---\n\nIn any group of people the number of people who have shaken hands with an odd number of other people from the group is even.\r\n"},{"fields":{"slug":"/Mathematics/General/Logic/","title":"Logic"},"frontmatter":{"draft":false},"rawBody":"# Logic\n\nCreated: 2018-09-27 08:57:59 +0500\n\nModified: 2018-12-04 23:37:57 +0500\n\n---\n\n# Conceptions of logic\n-   Logic as a way to evaluate reasoning\n-   Logic as a way of looking at language in general\n**Concpets of Logic**\n-   Informal logic\n-   Formal logic\n-   Symbolic logic\n-   Mathematical logic\n**Types of Logic**\n-   Syllogistic logic\n-   Propositional logic\n-   Predicate logic\n-   Modal logic\n-   Informal reasoning and dialectic\n-   Mathematical logic\n-   Philosophical logic\n-   Computational logic\n-   Non-classical logic\nSymbolic Logic\n-   Constants - symbols with fixed meaning\n-   Variables - symbols with no fixed meaning\n    -   for variables: x, y, z\n    -   for statements: p and q\n# Statements\n-   have a subject and predicate\n-   composed of atleast one symbol\nSubject\n\nA subject is a term that a predicate says something about.\n-   part of sentence frame\n-   topic of a sentence\n-   main argument of verb\nPredicate (denoted by capital alphabets)\n\nA predicate is a term that says something about a subject.\n-   part of sentence frame\n-   comments on subject\n-   verb and all non-subject arguments\n![x sees y Jill sees the movie. Sjm ](media/Logic-image1.png)\n\nPredicate comes first\n![L L h English is a language. Jill saw the movie. The house is large. The house is red. predicate constants ](media/Logic-image2.png)\n![L â€¢m L h English is a language. Jill saw the movie. The house is large. The house is red. individual constants ](media/Logic-image3.png)\n**Valency**\n\nThis is the number of terms that predicate says something about.\n\nLx,Rx - 1 slots (x), Valency - 1\n\nSxy - 2 slots (x, y), Valency - 2\n\nGxyz - 3 slots (x, y, z), Valency - 3\n![1. Socrates is a gadfly. 2. George went on vacation. 3. The fuzzy dog took the bone from my hand. 4. Well, somebody said something to someone! 5. Those women are such good people. ](media/Logic-image4.png)\n![1. socrates isGadfly 2. george Vacationed -> 3. dog Took bone from-hand Vg Tdbh Sxyz 4. x said y to-z 5. women areGood -> ](media/Logic-image5.png)\n# Quantifiers & Bound Variables\n-   quantity: how many?\n-   examples from language include singular (1) & plural (1 or more)\n-   three logical quantifiers\n    -   all\n    -   some (at least 1)\n    -   none\n![\"All\" as quantifier - variable x x is a person - variable x with quantifier x (x)Px for any x, x is a person = for any x, x saw y for any x, for any y, x saw y (x)Sxy = (x)(y)Sxy ](media/Logic-image6.png)\n![Universal quantifier - the symbol V (all) for all x, x is a person = (Vx)Px Of thern are people.\" for all x, Jill saw x \"Jill saw all of them.\" --- (Vx)Sjx ](media/Logic-image7.png)\n![Existential quantifier - the symbol 3 (at least one) for some x, x is a person = (ax)Px for some x, Jill saw x \"Jill saw something\" for all x, Jill saw x \"Jill saw everything\" (Vx)Sjx ](media/Logic-image8.png)\n![Negative - the symbol statements or (not) John saw the movie. My house is red. John didn't see the movie. My house isn't red. ](media/Logic-image9.png)\n![Negative - the symbol Sjm -Sjm predicates or -t (not) John saw the movie John didn't see the movie. My house isn't red -Sxy, -Sxy, etc ](media/Logic-image10.png)\n![At least one thing is a turtle. ( ax)Tx ax)Tx No one thing is a turtle. Everything is a turtle. (V x)Tx e V x)Tx Not every thing is a turtle. ](media/Logic-image11.png)\n![\"some fish\" \"all fish\" \"no fish\" \"not all fish\" Vx Fx 3x)Fx -Vx)Fx ](media/Logic-image12.png)\n![Scope of quantifiers - variables bound by a quantifier is free universal x is bound quantifier on x quantifiers scope of those quantifiers ](media/Logic-image13.png)\n![IJhy bind variables? - variable as blank space Sjx \"John saw --- free variables may fail to represent ordinary language (ax)Sjx ' John saw something\" - so use quantifiers on x, y, z in your sentences! Free variables belong in formulas. ](media/Logic-image14.png)\n![1 . All those things fell down. 2. Jim said nothing. 3. All of them are human. 4. Jim didn't watch TV. 5. Something moved something else. ](media/Logic-image15.png)\n![1. all x fell 2. Jim said no x 3. all x are human 4. Jim not watch TV 5. some x moved some y ( Vx)Fx ax)Sjx (Vx)Hx ---Wjt ( ay)Mxy ](media/Logic-image16.png)# Logical Operators/Connectives\n-   **Logical conjuction**\n-   join symbols with \"and\" (. & Î›)\n-   conjuction with statements p.q\n-   conjunction with variables x.y\n![Logical conjunction - conjunction with predicates Bob is a good person. Bob is human AND Bob is good some good person. for some x, x is human AND x is good (ax)Hx â€¢ Gx ](media/Logic-image17.png)-   **Logical disjunction**\n![Logical disjunction Everything is vanilla or chocolate. V Cx) - V is inclusive \"or\" vanilla OR chocolate = vanilla, choc, vanilla & choc o is exclusive \"or\" vanilla XOR chocolate = either vanilla or chocolate ](media/Logic-image18.png)\n![(Vx)(Hx â€¢ Sx) Everyone can swim. for all x, x is human and x swims V cx) Something is dirty or cluttered. for some x, x is dirty or x is cluttered Something is either not dirty or not cluttered. ](media/Logic-image19.png)\n![Negating junctions not chocolate or vanilla not (chocolate and vanilla) not chocolate and vanilla not (chocolate or vanilla) ](media/Logic-image20.png)\n![Translation 1. every good dog 2. Bill saw the movie or went running. 3. not friends and enemies 4. either something ugly or something beautiful ](media/Logic-image21.png)\n**Logical Equivalence ( = )**\n**Conditional**\n\n![Conditional - il..then... symbol (---+ or D) --- used even in basic sentences \"English is a language.\" for al/ x, if x is English then x is a language (vx)Ex Lx ](media/Logic-image22.png)\n![Everybody swims. for all x, if x is human then x swims (V x) D SX ](media/Logic-image23.png)\n**Biconditional**\n\n![Biconditional ...if and only if... symbol (H or E) It's water if and only if it's refreshing. If it's water, it's refreshing. If it's refreshing, it's water. (vx) VVX Rx ](media/Logic-image24.png)\n![It's edible if and only if it's good food. x is edible x is good & x is food (vx) Ex (Gx â€¢ Fx) ](media/Logic-image25.png)\n![Drawing a conclusion --- therefore symbol ( â€¢ â€¢ â€¢ ) Everybody knows something. (Vx)(3y) Hx D Kxy John is somebody. Therefore John knows something. ' Kjy ](media/Logic-image26.png)\n![Other logics - logics built for a variety of uses - for example, modal logic it is necessary that p it is possible that p must versus can it isn't necessary that p it isn't possible that p ](media/Logic-image27.png)\n![If this sign means (BX â€¢ Ty) oxy just be careful. (BX â€¢ Ty) D oxy serious y reconsider your outing. (BX â€¢ Ty) D oxy turn around NOW! TRAILS ](media/Logic-image28.png)\n![Translation 1. Everyone goes somewhere on vacation. 2. Nobody goes everywhere on vacation. 3. It's only exciting if it's dangerous! 4. Jill ate everything that's not chocolate and not vanilla. 5. Steve taught everyone, but not everyone listened to him. ](media/Logic-image29.png)\n![Hints 1. all x, some y, x Human & y Place then Vacations x at y 2. not some x, all y, x Human & y Place then Vacations x y 3. all x, x Exciting if-only-if x Dangerous 4. all x, ---(Choc x or Vanilla x) then jill Ate x 5. all x, x human then steve Taught x and x not Listened to steve ](media/Logic-image30.png)\n![Answers 1. (Ğ½Ñ… â€¢ Ğ Ñƒ) Ñ vxy 2. (Ğ½Ñ… â€¢ Ğ Ñƒ) Ñ vxy 4. (Ğ£Ñ…) -(Ğ¡Ñ… V Vx) Ñ ĞÑ˜Ñ… 5. (Ğ£Ñ…) Ğ½Ñ… D (Tsx â€¢ -Lxs) ](media/Logic-image31.png)\n# The language of sets & probability\n\n![Brief tips for logical translation 1. break the sentence into parts 2. break the translation into stages a. into logical language b. then into symbolic logic c. watch for chances to use variables, quantifiers & operators ](media/Logic-image32.png)\n![Logical set - group of x that belong together - those things that belong together are members of that set SET ice cream MEMBERS eggs cream sugar eggs e ice cream ingredients ](media/Logic-image33.png)-   Intersection\n-   Union\n![Logical set - group of x that belong together mutton e ice cream ingredients ice cream ingreds n dairy products ice cream ingreds U dairy products ](media/Logic-image34.png)\n![Subsets - sets within sets ice cream ingreds C foods - proper subset if elements in A differ from elements in B ice cream ingreds c foods ](media/Logic-image35.png)\n![](media/Logic-image36.png)\n**Probability**\n-   probability of A or B, P(A U B)\n-   probability of A and B, P(A **âˆ©** B)\n-   probability of A given B, P(A | B)\n-   probability of B given A, P(B | A)\n**Other Logics**\n-   Inverse\n-   Converse\n-   Contrapositive\n\n![inverse B converse contrapositive ](media/Logic-image37.png)\n**Fallacy**\n\nA logical fallacy allows an argument to persuade even though the conclusion does not follow the premises.\n-   Ad Baculum - *I'll hit you unless* you accept that A is C. Therefore A is C\n\nThis only supports the conclusion *I'll hit you*, Not that A is C\n-   Ad Hominem - *John's an idiot* and he says that A is not C. Therefore A is C\n\nThis only supports the conclusion *One idiot doesn't agree.* Not that A is C.\n-   Ad Verecundiam - *Experts agree* that A is C. Therefore A is C\n\nThis only supports the conclusion *Experts agree.* Not that A is C.\n-   Ad Misericordiam - *It will be so tragic* if A isn't C. Therefore A is C\n\nThis only supports the conclusion *It's tragic.* Not that A is C.\n**Logic**\n-   Arguments (premises, conclusion)\n-   Validity\n-   Truth values\n-   Soundness\n-   Basic Syllogism\n-   Deduction (deductive reasoning)\n-   Induction (inductive reasoning)\n-   Cogent argument\n-   coherence\n-   correspondence\n-   foundationalism\n-   pragmatic\n-   consensus\n-   deflationary\n**References**\n\n[Logic & Language - a short introduction to logic](https://www.youtube.com/playlist?list=PL48654681292CF456)\n![11ì´ ,01100001ì´1ì´0]100ì´1ì´ì´011ì´10( 01ì´ì´]ì´å¼“ã€ ì´10ì´100ì´ ,2 11000101100110 001ì´ 11 10ì´ì´1 1011101ì´010] 1 ê¸°é­¯01]11100ê¸°ã€ 0ì´ì´0] |10ì´C; .,lì´00\"! |ì´0ã€•]0L10]ì´00,,- ì´01 10ì´000ì´1 111010111ì´111^ INTRO rroLOGIC 100] 0ì´ 0 ê¸° 00 It)lOì´ ! 0ì´00 : lã€•010010 subJects & predicates iC)ì´010ê¸° 0010010 0ì´ 1ì´ 1001 40ì´ 1 0ì´000- : ](media/Logic-image38.jpg)"},{"fields":{"slug":"/Mathematics/General/Modulus---Modulo-10^9+7-(1000000007)/","title":"Modulus / Modulo 10^9+7 (1000000007)"},"frontmatter":{"draft":false},"rawBody":"# Modulus / Modulo 10^9+7 (1000000007)\n\nCreated: 2018-03-18 11:04:28 +0500\n\nModified: 2018-04-01 23:51:20 +0500\n\n---\n\n**References**\n\n<https://www.geeksforgeeks.org/modulo-1097-1000000007>\n"},{"fields":{"slug":"/Mathematics/General/Numbers/","title":"Numbers"},"frontmatter":{"draft":false},"rawBody":"# Numbers\n\nCreated: 2018-04-03 00:32:25 +0500\n\nModified: 2021-03-27 11:31:57 +0500\n\n---\n\n![NUMBERS NATURAL INTEGERS COH?LEX SYMBOL EXAMPLES It... CLOSED UNDER NATURAL NUMBERS N INTEGERS RATIONAL NUMBERS -5/2 0.08... IRRATIONAL NUMBERS TRANSCENDENTAL NUMBERS, REAL NUMBERS COMPLEX NUMBERS ](media/Numbers-image1.png)\n**Types of numbers**\n\n1.  Natural numbers (N) - 1,2,3, ...\n\nThe counting numbers\n2.  Whole numbers - 0,1,2,3, ...\n\nThe counting numbers including 0\n3.  Integer numbers (Z) - ... ,-3,-2,-1,0,1,2,3, ...\n\nPositive and negative counting numbers as well as 0\n\n1.  Even and odd integers\n\n2.  Prime number\n\n3.  Composite number\n\n4.  Polygonal numbers\n4.  Rational numbers (Q) -\n\nThe numbers that can be represented as ratio of an integer to a non-zero integer.\n\nAll integers are rational but the converse is not true.\n5.  Real numbers (R) -\n\nNumbers that have decimal representations that have a finite or infinite sequence of digits to the right of the decimal point. They can be positive, negative, or zero. All rational numbers are real, but the converse is not true.\n6.  Irrational numbers (I) -\n\nReal numbers that are not rational\n7.  Imaginary numbers -\n\nNumbers that equal the product of a real number and the square root of --1. The number 0 is both real and imaginary.\n8.  Complex numbers (C) -\n\nIncludes real numbers, imaginary numbers, and sums and differences of real and imaginary numbers.\n## Algebraic Numbers\n\n1.  Algebraic numbers\n\nAny number that is the[root](https://en.wikipedia.org/wiki/Root_of_a_function)of a non-zero[polynomial](https://en.wikipedia.org/wiki/Polynomial)with rational coefficients.\n\n2.  Transcendental number\n\nAny real or complex number that is not algebraic. Examples include[*e*](https://en.wikipedia.org/wiki/E_(mathematical_constant))and[*Ï€*](https://en.wikipedia.org/wiki/Pi).\n\n3.  Trignometric number\n\nAny number that is the sine or cosine of a rational multiple of pi.\n\n4.  Quadratic surd\n\nAn algebraic number that is the root of a[quadratic equation](https://en.wikipedia.org/wiki/Quadratic_equation). Such a number can be expressed as the sum of a rational number and the[square root](https://en.wikipedia.org/wiki/Square_root)of a rational.\n\n5.  Constructible number\n\nA number representing a length that can be constructed using a[compass and straightedge](https://en.wikipedia.org/wiki/Compass_and_straightedge_constructions). These are a[subset](https://en.wikipedia.org/wiki/Subset)of the algebraic numbers, and include the quadratic surds.\n\n6.  Algebraic integer\n\nAn algebraic number that is the root of a[monic polynomial](https://en.wikipedia.org/wiki/Monic_polynomial)with integer coefficients.\n9.  Hypercomplex numbers -\n\ninclude various number-system extensions:[quaternions](https://en.wikipedia.org/wiki/Quaternion)(H),[octonions](https://en.wikipedia.org/wiki/Octonion)(O),[sedenions](https://en.wikipedia.org/wiki/Sedenion)(S),[tessarines](https://en.wikipedia.org/wiki/Tessarine),[coquaternions](https://en.wikipedia.org/wiki/Coquaternion), and[biquaternions](https://en.wikipedia.org/wiki/Biquaternion).\n10. p-adic numbers -\n\nVarious number systems constructed using limits of rational numbers, according to notions of \"limit\" different from the one used to construct the real numbers.\n**Number Representations**\n\n1.  Decimal\n\n2.  Binary\n\n3.  Roman numerals\n\n4.  Fractions\n\n5.  Scientific notation\n\n6.  Knuth's up-arrow notation and Conway chained arrow notation\n**Non Standard Numbers**\n\n1.  Transfinite numbers\n\nNumbers that are greater than any natural number.\n\n2.  Ordinal number\n\nFinite and infinite numbers used to describe the [order type] of[well-ordered sets](https://en.wikipedia.org/wiki/Well-ordered_set).\n\n3.  Cardinal numbers\n\nFinite and infinite numbers used to describe the[cardinalities](https://en.wikipedia.org/wiki/Cardinality)of[sets](https://en.wikipedia.org/wiki/Set_(mathematics)).\n\n4.  Infinitesimals\n\nNilpotent numbers. These are smaller than any positive real number, but are nonetheless greater than zero. These were used in the initial development of[calculus](https://en.wikipedia.org/wiki/Calculus), and are used in[synthetic differential geometry](https://en.wikipedia.org/wiki/Synthetic_differential_geometry).\n\n5.  Hyperreal numbers\n\nThe numbers used in[non-standard analysis](https://en.wikipedia.org/wiki/Non-standard_analysis). These include infinite and infinitesimal numbers which possess certain properties of the real numbers.\n\n6.  Surreal numbers\n\nA number system that includes the hyperreal numbers as well as the ordinals. The surreal numbers are the largest possible[ordered field](https://en.wikipedia.org/wiki/Ordered_field).\n**Important Constants / Numbers**\n\nA**mathematical constant**is a key[number](https://en.wikipedia.org/wiki/Number)whose value is fixed by an unambiguous definition, often referred to by a symbol (e.g., an[alphabet letter](https://en.wikipedia.org/wiki/Letter_(alphabet))), or by mathematicians' names to facilitate using it across multiple[mathematical problems](https://en.wikipedia.org/wiki/Mathematical_problem).^[[1]](https://en.wikipedia.org/wiki/Mathematical_constant#cite_note-1)[[2]](https://en.wikipedia.org/wiki/Mathematical_constant#cite_note-2)^Constants arise in many areas of[mathematics](https://en.wikipedia.org/wiki/Mathematics), with constants such as[*e*](https://en.wikipedia.org/wiki/E_(mathematical_constant))and[*Ï€*](https://en.wikipedia.org/wiki/Pi)occurring in such diverse contexts as[geometry](https://en.wikipedia.org/wiki/Geometry),[number theory](https://en.wikipedia.org/wiki/Number_theory), and[calculus](https://en.wikipedia.org/wiki/Calculus).\nEuler's Number - 2.71828182845\n\nPi - 3.14159 265 359\n\nsqrt(2) - 1.4142135\n<https://en.wikipedia.org/wiki/List_of_mathematical_constants>\n\n## Aleph Numbers (×)**\n\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics), and in particular[set theory](https://en.wikipedia.org/wiki/Set_theory), thealeph numbersare a sequence of numbers used to represent the[cardinality](https://en.wikipedia.org/wiki/Cardinality)(or size) of[infinite sets](https://en.wikipedia.org/wiki/Infinite_set)that can be[well-ordered](https://en.wikipedia.org/wiki/Well-ordered). They are named after the symbol used to denote them, the[Hebrew](https://en.wikipedia.org/wiki/Hebrew_alphabet)letter[aleph](https://en.wikipedia.org/wiki/Aleph)(×) (though in older mathematics books the letter aleph is often printed upside down by accident,partly because a[monotype](https://en.wikipedia.org/wiki/Monotype)matrix for aleph was mistakenly constructed the wrong way up).\n<https://en.wikipedia.org/wiki/Aleph_number>\n\n## Euler's Number (e - 2.71828182845)**\n\ne is the natural language of growth\n\ne is the epitome of the universal growth\n<https://www.youtube.com/watch?v=AuA2EAgAegE&ab_channel=Numberphile>\n\n<https://www.youtube.com/watch?v=yTfHn9Aj7UM&ab_channel=BetterExplained>\n\n## Euler's Identity**\n\nMost beautiful equation - e^iÏ€^+ 1 = 0\nThe five contants are:\n-   The[number 0](https://www.livescience.com/27853-who-invented-zero.html).\n-   The number 1.\n-   The[numberÏ€](https://www.livescience.com/29197-what-is-pi.html), an irrational number (with unending digits) that is the ratio of the circumference of a circle to its diameter. It is approximately 3.14159...\n-   The numbere, also an irrational number. It is the base of[natural logarithms](https://www.livescience.com/50940-logarithms.html)that arises naturally through study of compound interest and[calculus](https://www.livescience.com/50777-calculus.html). The numberepervades math, appearing seemingly from nowhere in a vast number of important equations. It is approximately 2.71828....\n-   The[numberi](https://www.livescience.com/42748-imaginary-numbers.html), defined as the square root of negative one: âˆš(-1). The most fundamental of the imaginary numbers, so called because, in reality, no number can be multiplied by itself to produce a negative number (and, therefore, negative numbers have no real square roots). But in math, there are many situations where one is forced to take the square root of a negative. The letteriis therefore used as a sort of stand-in to mark places where this was done.\n"},{"fields":{"slug":"/Mathematics/General/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2018-10-25 20:44:26 +0500\n\nModified: 2021-11-10 16:58:16 +0500\n\n---\n\n**Arity**\n\nIn[logic](https://en.wikipedia.org/wiki/Logic),[mathematics](https://en.wikipedia.org/wiki/Mathematics), and[computer science](https://en.wikipedia.org/wiki/Computer_science), thearity of a[function](https://en.wikipedia.org/wiki/Function_(mathematics))or[operation](https://en.wikipedia.org/wiki/Operation_(mathematics))is the number of [arguments](https://en.wikipedia.org/wiki/Argument_of_a_function) or[operands](https://en.wikipedia.org/wiki/Operand)that the function takes. In mathematics, arity may also be namedrank,but this word can have many other meanings in mathematics. In logic and philosophy, it is also calledadicityanddegree.In[linguistics](https://en.wikipedia.org/wiki/Linguistics), it is usually named[valency](https://en.wikipedia.org/wiki/Valency_(linguistics)).\n<https://en.wikipedia.org/wiki/Arity>\n\n## Category Theory**\n\n**Category theory**[^[1]^](https://en.wikipedia.org/wiki/Category_theory#cite_note-1)formalizes[mathematical structure](https://en.wikipedia.org/wiki/Mathematical_structure)and its concepts in terms of a labeled[directed graph](https://en.wikipedia.org/wiki/Directed_graph)called a[*category*](https://en.wikipedia.org/wiki/Category_(mathematics)), whose nodes are called*objects*, and whose labelled directed edges are called*arrows*(or[morphisms](https://en.wikipedia.org/wiki/Morphism)). A[category](https://en.wikipedia.org/wiki/Category_(mathematics))has two basic properties: the ability to[compose](https://en.wikipedia.org/wiki/Function_composition)the arrows[associatively](https://en.wikipedia.org/wiki/Associativity)and the existence of an[identity](https://en.wikipedia.org/wiki/Identity_function)arrow for each object. The language of category theory has been used to formalize concepts of other high-level[abstractions](https://en.wikipedia.org/wiki/Abstractions)such as[sets](https://en.wikipedia.org/wiki/Set_theory),[rings](https://en.wikipedia.org/wiki/Ring_theory), and[groups](https://en.wikipedia.org/wiki/Group_theory).\nCategory theory has practical applications in[programming language theory](https://en.wikipedia.org/wiki/Programming_language_theory), for example the usage of[monads in functional programming](https://en.wikipedia.org/wiki/Monad_(functional_programming)). It may also be used as an axiomatic foundation for mathematics, as an alternative to[set theory](https://en.wikipedia.org/wiki/Set_theory)and other proposed foundations.\n<https://en.wikipedia.org/wiki/Category_theory>\n\n## Orthogonality**\n\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics),orthogonalityis the generalization of the notion of[perpendicularity](https://en.wikipedia.org/wiki/Perpendicularity)to the[linear algebra](https://en.wikipedia.org/wiki/Linear_algebra)of[bilinear forms](https://en.wikipedia.org/wiki/Bilinear_form). Two elementsuandvof a[vector space](https://en.wikipedia.org/wiki/Vector_space)with bilinear formBareorthogonalwhenB(u,v) = 0. Depending on the bilinear form, the vector space may contain nonzero self-orthogonal vectors. In the case of[function spaces](https://en.wikipedia.org/wiki/Function_space), families of[orthogonal functions](https://en.wikipedia.org/wiki/Orthogonal_functions)are used to form a[basis](https://en.wikipedia.org/wiki/Basis_(linear_algebra)).\nBy extension, orthogonality is also used to refer to the separation of specific features of a system. The term also has specialized meanings in other fields including art and chemistry.\n<https://en.wikipedia.org/wiki/Orthogonality>\n\n## Tools**\n\n**Lean Theorem Prover**\n\nLean 4 programming language and theorem prover\n\nLean is a functional programming language that makes it easy to write correct and maintainable code. You can also use Lean as an interactive theorem prover. Lean programming primarily involves defining types and functions. This allows your focus to remain on the problem domain and manipulating its data, rather than the details of programming.\n\n<https://github.com/leanprover/lean4>\n\n<https://leanprover.github.io/about>\n"},{"fields":{"slug":"/Mathematics/General/Outline/","title":"Outline"},"frontmatter":{"draft":false},"rawBody":"# Outline\n\nCreated: 2018-11-14 00:36:17 +0500\n\nModified: 2022-12-11 19:31:00 +0500\n\n---\n\n[**Pure mathematics**](https://en.wikipedia.org/wiki/Pure_mathematics)\n-   [Mathematical logic](https://en.wikipedia.org/wiki/Mathematical_logic)and[Foundations of mathematics](https://en.wikipedia.org/wiki/Foundations_of_mathematics)\n\nFoundation which underlies mathematic logic and the rest of mathematics. It tries to formalize valid reasoning. It attempts to define what constitutes a proof.-   [Intuitionistic logic](https://en.wikipedia.org/wiki/Intuitionistic_logic)\n-   [Modal logic](https://en.wikipedia.org/wiki/Modal_logic)\n-   [Model theory](https://en.wikipedia.org/wiki/Model_theory)\n-   [Proof theory](https://en.wikipedia.org/wiki/Proof_theory)\n-   [Recursion theory](https://en.wikipedia.org/wiki/Recursion_theory)\n-   [Set theory](https://en.wikipedia.org/wiki/Set_theory)-   [Algebra](https://en.wikipedia.org/wiki/Algebra)([outline](https://en.wikipedia.org/wiki/Outline_of_algebra))\n\nStudy of algebraic structures, which are sets and operations defined on these sets satisfying certain axioms.-   [Associative algebra](https://en.wikipedia.org/wiki/Associative_algebra)\n-   [Category theory](https://en.wikipedia.org/wiki/Category_theory)\n    -   [Topos theory](https://en.wikipedia.org/wiki/Topos)\n-   [Differential algebra](https://en.wikipedia.org/wiki/Differential_algebra)\n-   [Field theory](https://en.wikipedia.org/wiki/Field_theory_(mathematics))\n-   [Group theory](https://en.wikipedia.org/wiki/Group_theory)\n    -   [Group representation](https://en.wikipedia.org/wiki/Group_representation)\n-   [Homological algebra](https://en.wikipedia.org/wiki/Homological_algebra)\n-   [K-theory](https://en.wikipedia.org/wiki/K-theory)\n-   [Lattice theory](https://en.wikipedia.org/wiki/Lattice_theory)([Order theory](https://en.wikipedia.org/wiki/Order_theory))\n-   [Lie algebra](https://en.wikipedia.org/wiki/Lie_algebra)\n-   [Linear algebra](https://en.wikipedia.org/wiki/Linear_algebra)([Vector space](https://en.wikipedia.org/wiki/Vector_space))\n-   [Multilinear algebra](https://en.wikipedia.org/wiki/Multilinear_algebra)\n-   [Non-associative algebra](https://en.wikipedia.org/wiki/Non-associative_algebra)\n-   [Representation theory](https://en.wikipedia.org/wiki/Representation_theory)\n-   [Ring theory](https://en.wikipedia.org/wiki/Ring_theory)\n    -   [Commutative algebra](https://en.wikipedia.org/wiki/Commutative_algebra)\n    -   [Noncommutative algebra](https://en.wikipedia.org/wiki/Noncommutative_algebra)\n-   [Universal algebra](https://en.wikipedia.org/wiki/Universal_algebra)-   [Analysis](https://en.wikipedia.org/wiki/Mathematical_analysis)\n\nCalculus and Analysis - Studies the computation of limits, derivatives, and integrals of functions of real numbers, and in particular studies instantaneous rates of change.-   [Complex analysis](https://en.wikipedia.org/wiki/Complex_analysis)\n-   [Functional analysis](https://en.wikipedia.org/wiki/Functional_analysis)\n    -   [Operator theory](https://en.wikipedia.org/wiki/Operator_theory)\n-   [Harmonic analysis](https://en.wikipedia.org/wiki/Harmonic_analysis)\n    -   [Fourier analysis](https://en.wikipedia.org/wiki/Fourier_analysis)\n-   [Non-standard analysis](https://en.wikipedia.org/wiki/Non-standard_analysis)\n-   [Ordinary differential equations](https://en.wikipedia.org/wiki/Ordinary_differential_equations)\n-   [p-adic analysis](https://en.wikipedia.org/wiki/P-adic_analysis)\n-   [Partial differential equations](https://en.wikipedia.org/wiki/Partial_differential_equations)\n-   [Real analysis](https://en.wikipedia.org/wiki/Real_analysis)\n    -   [Calculus](https://en.wikipedia.org/wiki/Calculus)([outline](https://en.wikipedia.org/wiki/Outline_of_calculus))-   [Probability theory](https://en.wikipedia.org/wiki/Probability_theory)\n\n[Probability theory](https://en.wikipedia.org/wiki/Probability_theory)is the formalization and study of the mathematics of uncertain events or knowledge. The related field of[mathematical statistics](https://en.wikipedia.org/wiki/Mathematical_statistics)develops[statistical theory](https://en.wikipedia.org/wiki/Statistical_theory)with mathematics.[Statistics](https://en.wikipedia.org/wiki/Statistics), the science concerned with collecting and analyzing data,-   [Ergodic theory](https://en.wikipedia.org/wiki/Ergodic_theory)\n-   [Measure theory](https://en.wikipedia.org/wiki/Measure_theory)\n    -   [Integral geometry](https://en.wikipedia.org/wiki/Integral_geometry)\n-   [Stochastic process](https://en.wikipedia.org/wiki/Stochastic_process)-   [Geometry](https://en.wikipedia.org/wiki/Geometry)([outline](https://en.wikipedia.org/wiki/Outline_of_geometry)) and[Topology](https://en.wikipedia.org/wiki/Topology)\n\nStudy of spatial figures like circles and cubes.\n\nTopology developed from geometry; it looks at those properties that do not change even when the figures are deformed by stretching and bending, like dimension-   [Affine geometry](https://en.wikipedia.org/wiki/Affine_geometry)\n-   [Algebraic geometry](https://en.wikipedia.org/wiki/Algebraic_geometry)\n-   [Algebraic topology](https://en.wikipedia.org/wiki/Algebraic_topology)\n-   [Convex geometry](https://en.wikipedia.org/wiki/Convex_geometry)\n-   [Differential topology](https://en.wikipedia.org/wiki/Differential_topology)\n-   [Discrete geometry](https://en.wikipedia.org/wiki/Discrete_geometry)\n-   [Finite geometry](https://en.wikipedia.org/wiki/Finite_geometry)\n-   [Galois geometry](https://en.wikipedia.org/wiki/Galois_geometry)\n-   [General topology](https://en.wikipedia.org/wiki/General_topology)\n-   [Geometric topology](https://en.wikipedia.org/wiki/Geometric_topology)\n-   [Integral geometry](https://en.wikipedia.org/wiki/Integral_geometry)\n-   [Noncommutative geometry](https://en.wikipedia.org/wiki/Noncommutative_geometry)\n-   [Non-Euclidean geometry](https://en.wikipedia.org/wiki/Non-Euclidean_geometry)\n-   [Projective geometry](https://en.wikipedia.org/wiki/Projective_geometry)-   [Number theory](https://en.wikipedia.org/wiki/Number_theory)\n\nStudies natural, whole and prime numbers.-   [Algebraic number theory](https://en.wikipedia.org/wiki/Algebraic_number_theory)\n-   [Analytic number theory](https://en.wikipedia.org/wiki/Analytic_number_theory)\n-   [Arithmetic combinatorics](https://en.wikipedia.org/wiki/Arithmetic_combinatorics)\n-   [Geometric number theory](https://en.wikipedia.org/wiki/Geometric_number_theory)\n[Applied mathematics](https://en.wikipedia.org/wiki/Applied_mathematics)\n-   [Approximation theory](https://en.wikipedia.org/wiki/Approximation_theory)\n-   [Combinatorics](https://en.wikipedia.org/wiki/Combinatorics)([outline](https://en.wikipedia.org/wiki/Outline_of_combinatorics))\n\nStudy of discrete (and usually finite) objects\n\na.  Enumerative combinatorics\n\nCounting the objects satisfying certain criteria\n\nb.  Combinatorial designs and matroid theory\n\nDeciding when the criteria can be met, and constructing and analyzing objects meeting the criteria\n\nc.  External combinatorics and combinatorial optimization\n\nFinding \"largest\", \"smallest\", or \"optimal\" objects\n\nd.  Algebraic combinatorics\n\nFinding algebraic structures these objects may have-   [Coding theory](https://en.wikipedia.org/wiki/Coding_theory)-   [Cryptography](https://en.wikipedia.org/wiki/Cryptography)\n-   [Dynamical systems](https://en.wikipedia.org/wiki/Dynamical_systems)\n\nA differential equation is an equation involving an unknown function and its derivatives-   [Chaos theory](https://en.wikipedia.org/wiki/Chaos_theory)\n-   [Fractal geometry](https://en.wikipedia.org/wiki/Fractal_geometry)-   [Game theory](https://en.wikipedia.org/wiki/Game_theory)\n\n[Game theory](https://en.wikipedia.org/wiki/Game_theory)is a branch of[mathematics](https://en.wikipedia.org/wiki/Mathematics)that uses[models](https://en.wikipedia.org/wiki/Model_(abstract))to study interactions with formalized incentive structures (\"games\").-   [Graph theory](https://en.wikipedia.org/wiki/Graph_theory)\n-   [Information theory](https://en.wikipedia.org/wiki/Information_theory)\n-   [Mathematical physics](https://en.wikipedia.org/wiki/Mathematical_physics)\n    -   [Quantum field theory](https://en.wikipedia.org/wiki/Quantum_field_theory)\n    -   [Quantum gravity](https://en.wikipedia.org/wiki/Quantum_gravity)\n        -   [String theory](https://en.wikipedia.org/wiki/String_theory)\n    -   [Quantum mechanics](https://en.wikipedia.org/wiki/Quantum_mechanics)\n    -   [Statistical mechanics](https://en.wikipedia.org/wiki/Statistical_mechanics)\n-   [Numerical analysis](https://en.wikipedia.org/wiki/Numerical_analysis)\n-   [Operations research](https://en.wikipedia.org/wiki/Operations_research)\n\n[Operations research](https://en.wikipedia.org/wiki/Operations_research)is the study and use of mathematical models, statistics and algorithms to aid in decision-making, typically with the goal of improving or optimizing performance of real-world systems.-   [Assignment problem](https://en.wikipedia.org/wiki/Assignment_problem)\n-   [Decision analysis](https://en.wikipedia.org/wiki/Decision_analysis)\n-   [Dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming)\n-   [Inventory theory](https://en.wikipedia.org/wiki/Inventory_theory)\n-   [Linear programming](https://en.wikipedia.org/wiki/Linear_programming)\n-   [Mathematical optimization](https://en.wikipedia.org/wiki/Mathematical_optimization)\n-   [Optimal maintenance](https://en.wikipedia.org/wiki/Optimal_maintenance)\n-   [Real options analysis](https://en.wikipedia.org/wiki/Real_options_analysis)\n-   [Scheduling](https://en.wikipedia.org/wiki/Job_shop_scheduling)\n-   [Stochastic processes](https://en.wikipedia.org/wiki/Stochastic_processes)\n-   [Systems analysis](https://en.wikipedia.org/wiki/Systems_analysis)-   [Statistics](https://en.wikipedia.org/wiki/Statistics)([outline](https://en.wikipedia.org/wiki/Outline_of_statistics))\n    -   [Actuarial science](https://en.wikipedia.org/wiki/Actuarial_science)\n    -   [Demography](https://en.wikipedia.org/wiki/Demography)\n    -   [Econometrics](https://en.wikipedia.org/wiki/Econometrics)\n    -   [Mathematical statistics](https://en.wikipedia.org/wiki/Mathematical_statistics)\n    -   [Data visualization](https://en.wikipedia.org/wiki/Data_visualization)\n-   [Theory of computation](https://en.wikipedia.org/wiki/Theory_of_computation)\n\nStudy of algorithms and data structures-   [Computational complexity theory](https://en.wikipedia.org/wiki/Computational_complexity_theory)-   Information theory and signal processing\n\nQuantification of information (fundamental limits on compressing and reliably communicating data)\n\nProcessing signals (filtering, storage and reconstruction, separation of information from noise, compression, and feature extraction)\n![AACHINE LEARNW&I FUNDAMENTAL RULES MATHEmATlCAL LOGIC SET THEORY C0NSlSfEA)T SET AXIHS ? ODEL INCOHPLETEÂ»JESS THEOREAS CATEGORY THEORY gos CARDWAL ALEPH COMPLEXITY 3 3 THEORY 1464 L THEogy PI EXPONWTIAL REAL tounÃŸERS INT GERS aâ€¢bi+cj+dk COAPLEB RATIONAL NVRBERS CRYPTOGRAPHY PR0BABlLtry COMPUTER SCIENCE TURING Ã¦wake : science() sew. Fabse .reeÂ«ir--- bruin ( ) PARTITION i | 01 Z. NATURAL CORB)NATORICS NUMBERS X + THEoRY TREE GROUP THEORY GUPH THEORY PERAutâ€¢Atm agoup PUE RULE P(AIB) : PCB) PERSIA c. 820 c.l+30 NATHENATICAL ALGEBRA N0TAT10N STATISTICS LINEAR ALGEBRA NATRICES ALGEBRA NUMBER SYSTE S MEASURE THE0RY 3 1 OF 30 I THEORYI GAME THEORY EGUATIOB) c.ag FIRST ZERO VECToRS STRUCTURES *4EG.mvE NVRBERS CAT AATHEAATICAL ECONOMICS mÃ¶610S s-rmp TOPOLOGY CE0hETRY (GENUS L) FRACTAL comPLEX ADJALYSIS GEOAETRY BUTTERFLY MATHEÃ„ATICS Sino 1 PYTHAÃ¥0RAS TRIGONOMETRY DYNAMICAL SYSTEAS Â¯ 8 CBC FIRST EauanoÂ» aooov CHINA Zoo BCS 600-300 O Z7 CHANGES MATHEMATICAL APPLIED MATHEMATICS NUAERICAL ANALYSIS 3 DIFFERENTIAL FLUID FLOW 2 INTEGRAL PH SIC THEORETICAL PHYSIC MATHEMATICAL CHEMI$RY I BY SYSTCÃ„ CONTROL THE0R7 BIOAATHERATIC VECTOR CALCULUS ECOSYSTEMS AREA DIFFERENTIAL EQUATIONS ](media/Outline-image1.png)\n![MATH ÎÎŸÎ¤Î‘Î¤Î™ÎŸÎ. CHEAT SHEET ARlTHMETlC ADDlTlON Î·Î±Î¹Î¿Ï…Ï‚ FORMS 0F Î·Ï…Î¹.Î¤Î™Î Î™Î™Î‘Î¤Î™OÎ alb Î½Î‘2Î¹Î¿Ï…Ï‚ 0F DlVlSlON ÎµÎ±Ï…Î‘Î™Î™Î¤Î¥ 3=Î™+Î£ 3 \" b Ï‚Î¯Î·(Î¿.Î¿Î¹) PROPORTlONAL Î¤ÎŸ Î‘Î¡Î¡Î‘ÎŸÎ§Î™ÎœÎ‘Î¤Î•Î™Î¥ Ï„Î¿ COMPARlS0N A14E8RA A8SOLVTE Î•ÎÎŸ PROBABll-lTY Î‘ÎÎŸ STATlSTllS PROU81UTY 0F EVENT Î‘ Î‘Î•ÎŸÎ™Î‘Î Î¡(Î‘Î›8) ENTE2SElTlON Î¡20Î’. 0F Î‘ Î‘ÎÎŸ 8 Î¡(Î‘Î™.Î™Î’) Ï…ÎÎ™OÎ PROB. 0F Î¡(Î‘Î™Î’) CONDlTlONAL ÎœÎŸÎ’. 0F Î‘ lVEN 8 FONt.TlO\" ANDA2w POPVlATlON MEAN DEVlAT10N AL4EBRA 5Îœ0Î»Î™=Î™ Î™' Î±Î¹ ---Î± ÎÎ Î”Î¤Î™Î• Î¡ÎŸÎ™ÎÎ¤ Î· Î± pown Î· Î± uss Î¤Î—Î‘Î b aSb a LESS Î¤Î—Î‘Î Î¤ÎŸ b aÂ«b a SAAlLER Ï„Î¹.â€¢Î™Î‘Î b Î± Î¤Î—Î‘Î OR EQVALTO b aÂ»b Î¤ÎšÎ‘Î b DKREES START SE21ES Î¡Î™Î™Î™$ 02 ÎœÏ‰Ï…Ï‚ Î± SQWRED 2=231818... 42 = 576 ÎœÎ™ ÎÎ™Îš a 3 BRAlKETs RESOLVE PARENTHESES FlRST Î•Î™Î) Î¤Î—Î• A uMlrs. 02 Î¤Î—Î• 4 Î½ÎµÎ¹Ï„Î¿2Ï‚ ÏÎ¿Ï‰ VEGTOR aoss PR0DVCT DOT PRODV'T NORM Î¿Ï‚ V MATRllES 4 S ÎœÎ‘Î¤Î‘Î™Î§ HADAMARD KRONECKER Î—Î•2ÎœÎ™Î¤Î™Î‘Î ÎÎ‘Î¤2Î™Î§ TANSPOSEO Î¿Ï MATRlX TRANSPOSE Î‘ Î™Î‘Î™ Î¹Î¹ Î‘Î™Î™ T\"VERSE DETERMWAMT NORM COAPlEX Ï€ = 3.14159... Î‘Î™Î¡Î—Î‘Î²Î•Î¤ Î‘Îº Î“Î³ Î”Î´ Î•Îµ Î–Î¶ DElTA ÎµÏÏ‚Î¹Î¹.Î¿Ï‰ Î–Î•Î¤Î‘ ÎœÎ¼ RHO Î¤Î‘Î™' VPSllON Î¡Î—Î™ Î³Î¿Ï…Ï„Ï…Î’Î• / 00ÎœÎ‘Î™Î 0F Î™ Hov Ï„Î¿ READ ÎœÎ‘Î¤Î— 3600=2mra4 F12ST 0ERlVATlVE DlFFE2ENTlATlON RESPECTTO X 02 DERlVATlVE ÎŸÎ¡ Î‘ FlwcrloN. F12ST Î‘ÎÎŸ 0ERlVATlVE F12ST Î‘ÎÎŸ stcouo OERlVATlVE 2ESPECTTO Î¤Î™Î‘Î• ÏÎ‘2Î¤Î™Î‘Î™ Î¿ÎµÏÎ¹Î½Î±Ï„Î¹Î½Îµ RESPECTT0 X Î–=3+2Î¯ WHERE Î™ REAl Î¡Î‘2Î¤ 0F Î•ÎÎ–'=3-2Î¯ DOMlN16 Î™Î™Î™ÎœÎ‘\" @Î™OÎ™8 ](media/Outline-image2.png)\n**References**\n\n[A few of the best math explainers from this summer](https://www.youtube.com/watch?v=F3Qixy-r_rQ)"},{"fields":{"slug":"/Mathematics/General/Pie/","title":"Pie"},"frontmatter":{"draft":false},"rawBody":"# Pie\n\nCreated: 2018-05-28 20:21:23 +0500\n\nModified: 2018-05-28 20:23:56 +0500\n\n---\n\nPie is the ratio of circumference of a circle to its diameter\n[What is Pi?](https://www.youtube.com/watch?v=DLcjed7qy4I)\n![150k+ views ](media/Pie-image1.jpg)\n[Area of a circle, how to get the formula.](https://www.youtube.com/watch?v=YokKp3pwVFc)\n![](media/Pie-image2.jpg)"},{"fields":{"slug":"/Mathematics/General/Properties/","title":"Properties"},"frontmatter":{"draft":false},"rawBody":"# Properties\n\nCreated: 2018-05-23 23:33:59 +0500\n\nModified: 2018-05-23 23:40:00 +0500\n\n---\n\n1.  [The Commutative Property of Addition](http://www.coolmath.com/prealgebra/06-properties/01-properties-commutative-addition-01) ( x + y = y + x )\n\n2.  [The Commutative Property of Multiplication](http://www.coolmath.com/prealgebra/06-properties/02-properties-commutative-multiplication-01) ( x * y = y * x )\n\n3.  [The Associative Property of Addition](http://www.coolmath.com/prealgebra/06-properties/03-properties-associative-addition-01) ( x + (y + z) = (x + y) + z )\n\n4.  [The Associative Property of Multiplication](http://www.coolmath.com/prealgebra/06-properties/04-properties-associative-multiplication-01) ( x * (y * z) = (x * y) * z )\n\n5.  [The Distributive Property](http://www.coolmath.com/prealgebra/06-properties/05-properties-distributive-01) ( x (y + z) = xy + yz )\n\n6.  [The Additive Identity Property](http://www.coolmath.com/prealgebra/06-properties/06-properties-additive-identity-01) ( x + 0 = x)\n\n7.  [The Additive Inverse Property](http://www.coolmath.com/prealgebra/06-properties/07-properties-additive-inverse-01) ( -x is additive inverse of x, as **x + (-x) = 0** )\n\n8.  [The Multiplicative Identity Property](http://www.coolmath.com/prealgebra/06-properties/08-properties-multiplicative-identity-01) ( x * 1 = x )\n\n9.  [The Multiplicative Inverse Property](http://www.coolmath.com/prealgebra/06-properties/09-properties-multiplicative-inverse-01) ( 1/x is the multiplicative inverse of x, as **x * 1/x = 1** )\r\n"},{"fields":{"slug":"/Mathematics/Geometry/Analytic-Geometry/","title":"Analytic Geometry"},"frontmatter":{"draft":false},"rawBody":"# Analytic Geometry\n\nCreated: 2018-05-24 16:05:10 +0500\n\nModified: 2018-05-24 16:05:15 +0500\n\n---\n\n[Distance and midpoints:Analytic geometry](https://www.khanacademy.org/math/geometry/hs-geo-analytic-geometry#hs-geo-distance-and-midpoints)\n\n[Dividing line segments:Analytic geometry](https://www.khanacademy.org/math/geometry/hs-geo-analytic-geometry#hs-geo-dividing-segments)\n\n[Problem solving with distance on the coordinate plane:Analytic geometry](https://www.khanacademy.org/math/geometry/hs-geo-analytic-geometry#hs-geo-dist-problems)\n\n[Parallel & perpendicular lines on the coordinate plane:Analytic geometry](https://www.khanacademy.org/math/geometry/hs-geo-analytic-geometry#hs-geo-parallel-perpendicular-lines)\n\n[Equations of parallel & perpendicular lines](https://www.khanacademy.org/math/geometry/hs-geo-analytic-geometry#hs-geo-parallel-perpendicular-eq)\r\n"},{"fields":{"slug":"/Mathematics/Geometry/Circles/","title":"Circles"},"frontmatter":{"draft":false},"rawBody":"# Circles\n\nCreated: 2018-05-24 16:05:21 +0500\n\nModified: 2018-05-24 16:05:25 +0500\n\n---\n\n[Circle basics:Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-circle-basics)\n\n[Arc measure:Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-arc-measures)\n\n[Arc length (degrees):Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-arc-length-deg)\n\n[Introduction to radians:Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-radians-intro)\n\n[Arc length (radians):Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-arc-length-rad)\n\n[Sectors:Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-sectors)\n\n[Inscribed angles:Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-inscribed-angles)\n\n[Inscribed shapes problem solving:Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-inscribed-shapes)\n\n[Properties of tangents:Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-tangents)\n\n[Standard equation of a circle:Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-circle-standard-equation)\n\n[Expanded equation of a circle:Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-circle-expanded-equation)\n\n[Constructing regular polygons inscribed in circles:Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-inscribed-polygons)\n\n[Constructing circumcircles & incircles:Circles](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-circum-in-circles)\n\n[Constructing a line tangent to a circle](https://www.khanacademy.org/math/geometry/hs-geo-circles#hs-geo-constructing-tangents)\r\n"},{"fields":{"slug":"/Mathematics/Geometry/Congruence/","title":"Congruence"},"frontmatter":{"draft":false},"rawBody":"# Congruence\n\nCreated: 2018-05-24 16:04:16 +0500\n\nModified: 2018-05-24 16:04:23 +0500\n\n---\n\n[Transformations & congruence:Congruence](https://www.khanacademy.org/math/geometry/hs-geo-congruence#hs-geo-trans-and-congruence)\n\n[Triangle congruence:Congruence](https://www.khanacademy.org/math/geometry/hs-geo-congruence#hs-geo-triangle-congruence)\n\n[Theorems concerning triangle properties:Congruence](https://www.khanacademy.org/math/geometry/hs-geo-congruence#hs-geo-congruence-theorems)\n\n[Working with triangles:Congruence](https://www.khanacademy.org/math/geometry/hs-geo-congruence#hs-geo-working-with-triangles)\n\n[Theorems concerning quadrilateral properties:Congruence](https://www.khanacademy.org/math/geometry/hs-geo-congruence#hs-geo-quadrilaterals-theorems)\n\n[Proofs of general theorems that use triangle congruence:Congruence](https://www.khanacademy.org/math/geometry/hs-geo-congruence#hs-geo-triangle-theorems)\n\n[Constructing bisectors of lines & angles](https://www.khanacademy.org/math/geometry/hs-geo-congruence#hs-geo-bisectors)\r\n"},{"fields":{"slug":"/Mathematics/Geometry/Geometry-Foundations/","title":"Geometry Foundations"},"frontmatter":{"draft":false},"rawBody":"# Geometry Foundations\n\nCreated: 2018-05-24 16:03:18 +0500\n\nModified: 2021-05-08 16:15:38 +0500\n\n---\n\n[Intro to Euclidean geometry](https://www.khanacademy.org/math/geometry/hs-geo-foundations#hs-geo-intro-euclid)\n\n[Angles](https://www.khanacademy.org/math/geometry/hs-geo-foundations#hs-geo-angles)\n\n[Polygons](https://www.khanacademy.org/math/geometry/hs-geo-foundations#hs-geo-polygons)\n\n[Area](https://www.khanacademy.org/math/geometry/hs-geo-foundations#hs-geo-area)\n**Euclidean Geometry**\n\n**Euclid's Postulates**\n\n1.  A straight line segment can be drawn joining any two points\n\n2.  Any straight line segment can be extended indefinitely in a straight line\n\n3.  Given any straight line segment, a circle can be drawn having the segment as radius and one endpoint as center\n\n4.  All right angles are congruent\n\n5.  If two lines are drawn which intersect a third in such a way that the sum of the inner angles on one side is less than two right angles, then the two lines inevitably must intersect each other on that side if extended far enough\nSimplied Version\n\n1.  A straight line is the shortest possible line between two points\n\n2.  Parallel ines are a constant distance from each other\n\n3.  The angles in triangles always add up to 180\n[How do non-euclidean games work? | Bitwise](https://www.youtube.com/watch?v=lFEIUcXCEvI)\n**Euclidean Space**\n\nEuclidean space, In geometry, a two- or three-dimensional[space](https://www.britannica.com/science/space-physics-and-metaphysics)in which the axioms and postulates of[Euclidean geometry](https://www.britannica.com/science/Euclidean-geometry)apply; also, a space in any finite number of dimensions, in which points are designated by[coordinates](https://www.britannica.com/science/coordinate-system)(one for each dimension) and the distance between two points is given by a[distance formula](https://www.britannica.com/science/distance-formula). The only[conception](https://www.merriam-webster.com/dictionary/conception)of physical space for over 2,000 years, it remains the most compelling and useful way of modeling the world as it is experienced. Though non-Euclidean spaces, such as those that emerge from[elliptic geometry](https://www.britannica.com/science/Riemannian-geometry)and[hyperbolic geometry](https://www.britannica.com/science/hyperbolic-geometry), have led scientists to a better understanding of the universe and of[mathematics](https://www.britannica.com/science/mathematics)itself, Euclidean space remains the point of departure for their study.\n**Euclidean Distance**\n\n**Euclidean Distance**\n\nIt is the ordinary straight line distance between two points in Euclidean Space\n*/***\n** Returns the Euclidean distance between this point and that point.*\n**/*\npublic double **distanceTo**(Point2D that) {\ndouble dx = this.x - that.x;\ndouble dy = this.y - that.y;\nreturn Math.**sqrt**(dx*dx + dy*dy);\n}\n**Hyperbolic functions**\n\n<https://betterexplained.com/articles/hyperbolic-functions>\n"},{"fields":{"slug":"/Mathematics/Geometry/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2018-05-26 01:46:02 +0500\n\nModified: 2018-05-26 01:46:41 +0500\n\n---\n\n1.  Law of haversines / Haversine formula\n**References**\n\n<https://en.wikipedia.org/wiki/Haversine_formula>\n"},{"fields":{"slug":"/Mathematics/Geometry/Right-Triangles-and-Geometry/","title":"Right Triangles and Geometry"},"frontmatter":{"draft":false},"rawBody":"# Right Triangles and Geometry\n\nCreated: 2018-05-24 16:04:43 +0500\n\nModified: 2018-05-26 00:40:01 +0500\n\n---\n\n1.  [Pythagorean theorem](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-pyth-theorem)\n\n2.  [Pythagorean theorem proofs](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-pythagorean-proofs)\n\n3.  [Special right triangles](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-special-right-triangles)\n    -   30-60-90 triangle rule\n\n![A 30-60-90-degree right triangle.](media/Right-Triangles-and-Geometry-image1.jpg)\n4.  [Introduction to the trigonometric ratios](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-trig-ratios-intro)\n\n![ã€‡ ` ã‚‚ ã€ ã® \" 5 ã® â‘  ã„ 5 ã® ä¸€ ã® 0 ã® ä¸€ & S ã‚­ 95 ã€ ã™ 5 â‘  ã„ Ad ãƒ¬ ](media/Right-Triangles-and-Geometry-image2.png)\nMnemonic - SOH CAH TOA\n\nSine - Opposite / Hypotenuse\n\nCosine - Adjacent / Hypotenuse\n\nTangent - Opposite / Adjacent\n![o sin(A) --- cos(A) tan (A) adjacent opposite hypotenuse adjacent hypotenuse opposite adjacent ](media/Right-Triangles-and-Geometry-image3.png)\n5.  [Solving for a side in a right triangle using the trigonometric ratios:](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-solve-for-a-side)\n\n6.  [Solving for an angle in a right triangle using the trigonometric ratios](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-solve-for-an-angle)\n\n7.  [Modeling with right triangles](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-modeling-with-right-triangles)\n\n8.  [Trigonometric ratios & similarity](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-trig-ratios-similarity)\n\n9.  [Sine & cosine of complementary angles](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-complementary-angles)\n\n10. [Law of sines](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-law-of-sines)\n\n11. [Law of cosines](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-law-of-cosines)\n\nC^2^ = A^2^ + B^2^ - 2.A.B.cos **Î˜**\n\n12. [Solving general triangles](https://www.khanacademy.org/math/geometry/hs-geo-trig#hs-geo-solving-general-triangles)\n"},{"fields":{"slug":"/Mathematics/Geometry/Similarity/","title":"Similarity"},"frontmatter":{"draft":false},"rawBody":"# Similarity\n\nCreated: 2018-05-24 16:04:26 +0500\n\nModified: 2018-05-24 16:04:36 +0500\n\n---\n\n[Definitions of similarity:Similarity](https://www.khanacademy.org/math/geometry/hs-geo-similarity#hs-geo-similarity-definitions)\n\n[Introduction to triangle similarity:Similarity](https://www.khanacademy.org/math/geometry/hs-geo-similarity#hs-geo-triangle-similarity-intro)\n\n[Solving similar triangles:Similarity](https://www.khanacademy.org/math/geometry/hs-geo-similarity#hs-geo-solving-similar-triangles)\n\n[Angle bisector theorem:Similarity](https://www.khanacademy.org/math/geometry/hs-geo-similarity#hs-geo-angle-bisector-theorem)\n\n[Solving problems with similar & congruent triangles:Similarity](https://www.khanacademy.org/math/geometry/hs-geo-similarity#hs-geo-similar-and-congruent-triangles)\n\n[Solving modeling problems with similar & co](https://www.khanacademy.org/math/geometry/hs-geo-similarity#hs-geo-similar-and-congruent-triangles-modeling)\r\n"},{"fields":{"slug":"/Mathematics/Geometry/Solid-Geometry/","title":"Solid Geometry"},"frontmatter":{"draft":false},"rawBody":"# Solid Geometry\n\nCreated: 2018-05-24 16:04:59 +0500\n\nModified: 2018-05-24 16:05:03 +0500\n\n---\n\n[Solid geometry intro:Solid geometry](https://www.khanacademy.org/math/geometry/hs-geo-solids#hs-geo-solids-intro)\n\n[Density:Solid geometry](https://www.khanacademy.org/math/geometry/hs-geo-solids#hs-geo-density)\n\n[2D vs. 3D objects](https://www.khanacademy.org/math/geometry/hs-geo-solids#hs-geo-2d-vs-3d)\r\n"},{"fields":{"slug":"/Mathematics/Geometry/Transformations/","title":"Transformations"},"frontmatter":{"draft":false},"rawBody":"# Transformations\n\nCreated: 2018-05-24 16:04:04 +0500\n\nModified: 2018-05-24 16:04:09 +0500\n\n---\n\n[Introduction to rigid transformations:Transformations](https://www.khanacademy.org/math/geometry/hs-geo-transformations#hs-geo-transformations-intro)\n\n[Translations:Transformations](https://www.khanacademy.org/math/geometry/hs-geo-transformations#hs-geo-translations)\n\n[Rotations:Transformations](https://www.khanacademy.org/math/geometry/hs-geo-transformations#hs-geo-rotations)\n\n[Reflections:Transformations](https://www.khanacademy.org/math/geometry/hs-geo-transformations#hs-geo-reflections)\n\n[Rigid transformations overview:Transformations](https://www.khanacademy.org/math/geometry/hs-geo-transformations#hs-geo-rigid-transformations-overview)\n\n[Dilations:Transformations](https://www.khanacademy.org/math/geometry/hs-geo-transformations#hs-geo-dilations)\n\n[Properties & definitions of transformations:Transformations](https://www.khanacademy.org/math/geometry/hs-geo-transformations#hs-geo-transformations-definitions)\n\n[Symmetry](https://www.khanacademy.org/math/geometry/hs-geo-transformations#hs-geo-symmetry)\r\n"},{"fields":{"slug":"/Mathematics/Linear-Algebra/3Blue1Brown/","title":"3Blue1Brown"},"frontmatter":{"draft":false},"rawBody":"# 3Blue1Brown\n\nCreated: 2018-05-26 12:43:19 +0500\n\nModified: 2018-12-30 18:49:26 +0500\n\n---\n\n1.  Vectors\n\n2.  Linear combinations, span and basis vectors\n\n3.  Linear transformations and matrices\n\n4.  Matrix multiplication as composition\n\n5.  Three dimensional linear transformation\n\n6.  The determinant\n\n7.  Inverse matrices, column space, rank and null space\n\n8.  Non square matrices\n\n9.  Dot product and duality\n\n10. Cross product\n\n11. Cross product in the light of linear transformations\n\n12. Change of basis\n\n13. Eigenvectors and eigenvalues\n\n14. Abstract vector spaces\n1.  **Vectors**\n\n    a.  Vector addition (adding two vectors)\n\n    b.  Scalar multiplication (increasing the length of a vector by a given number)\n\n2.  **Linear combinations, span and basis vectors**\n![and J are t e \"basis vectors\" of the xy coordinate system ](media/3Blue1Brown-image1.png)\n\ncalled as i hat and j hat\n![of v and is the The span\" set of all their linear combinations. Let a and b vary verâ€allâ€realâ€numbers ](media/3Blue1Brown-image2.png)\n**Linearly dependent vectors -** If one vector can be removed without changing the span of the vectors. i.e. one of the vectors can be expressed as a linear combination of the other vector.\n\n![\"Linearly dependent\" u For some values of a and b ](media/3Blue1Brown-image3.png)\n![\"Linearly independent\" u all values of a and b For ](media/3Blue1Brown-image4.png)\n**Basis -** The **basis** of a vector space is a set of linearly independent vectors that span the full space.\n3.  **Linear Transformations and Matrices**\n\nFor transformations to be linear, there are two property\n-   All lines must remain line and not get curved\n-   Origin must remain fixed in place\n\nGrid lines remain parallel and evenly spaced\n4.  **Matrix Multiplication as composition**\n\nIf you apply a rotation transformation followed by a shear transformation, the combined effect is a composition of both rotation and shear transformation\n\n![Shear Rotation Composition ](media/3Blue1Brown-image5.png)\nIn matrix multiplication, order matters\n\n![ITV ã€Œ ](media/3Blue1Brown-image6.png)\nBut Matrix multiplication is associative i.e. (AB)C = A(BC)\n5.  **Three dimensional linear transformations**\n\n6.  **The determinant**\n\n![](media/3Blue1Brown-image7.png)\n-   If one area changes by some factor than every area changes by that factor.\n-   The determinant of a transformation is the change in area of a grid after applying linear transformation\n-   The factor by which a linear transformation changes any area is called the determinant of that transformation.\n-   Negative determinant tells that orientation of space is inverted.\n-   In 3 dimensions, determinant tells us how much volume gets scaled.\n\nVolume of the parallelepiped\n\n![det l) ad bc ](media/3Blue1Brown-image8.png)\n![det a det det ](media/3Blue1Brown-image9.png)\n![det(M M ) det (Ml) det ( ](media/3Blue1Brown-image10.png)\n7.  **Inverse matrices, column space, rank and null space**\n    -   **Gaussian Elimination**\n    -   **Row echelon form**\n    -   **Linear system of equations**\n\n![2x+5y+3z 4x+0y+8z 1x+3y+0z ](media/3Blue1Brown-image11.png)-   Inverse Transformation\n\nA^-1^ A = [[1 0] [0 1]] Identity Transformation (changes nothing)\n\nUntil det(A) not equal to 0, A^-1^ exists-   Rank - Number of dimensions in the output\n    -   Rank 1 - One dimensional line\n    -   Rank 2 - Two dimensional plane\n    -   Rank 3 - Three dimensional 3d space\n-   Set of all possible outputs of the matrix is called the \"column space\" of the matrix A\n-   Full rank - When this rank is as high as it can be meaning it equals the number of column, we call the matrix full rank\n-   Null Space Kernel - All the vectors that squishes into origin when transformation takes place\n\nIn Ax = v, when v happens to be the zero vector, null space gives all of the possible solution to the linear equation\n8.  **Non Square Matrices as transformations between dimensions**\n    -   **A 2x3 matrix i.e. there are 2 rows and 3 columns represent that the transformation starts in 3 dimension and the landing space is represented in 2 dimension**\n\n![3 basis vectors 3 1 1 5 4 9 2 coordinates for each landing spots ](media/3Blue1Brown-image12.png)\n9.  **Dot products and duality**\n\nMatching coordinates, multiplying pair and adding them together.\n\n![Dot product ](media/3Blue1Brown-image13.png)\nIf we projected vector w onto the line that passes through the origin and the tip of vector v, multiplying the length of this projection, by the length of v, we have the dot product.\n![Length of projected of ](media/3Blue1Brown-image14.png)\n![Length of rojected w) Length of v) Should be negative ](media/3Blue1Brown-image15.png)-   If vectors are pointing in the same direction, then the dot product is positive\n-   If vectors are perpendicular to each other, there dot product is 0\n-   If vectors are pointing in the opposing direction, then there dot product is negative\n-   Order doesn't matter, V.W = W.V\n-   Dot product is a very useful geometric tool for understanding projections, and for testing whether or not vectors tend to point in the same direction\n10. **Cross Products**\n\n![Area of parallelogram ](media/3Blue1Brown-image16.png)\nIf v is to the right of w, than cross product is positive\n\n![Area of parallelogram ositive ](media/3Blue1Brown-image17.png)\nIf v is to the left of w, than cross product is negative\n\n![Area of parallelogra egative ](media/3Blue1Brown-image18.png)\n![](media/3Blue1Brown-image19.png)\n![V det ](media/3Blue1Brown-image20.png)-   Cross product of two perpendicular vectors are greater than the vectors that are pointing in very similar direction\n-   If we scale a vector by a factor of a, than area of parallelogram also scales up by a factor of a\n\n![](media/3Blue1Brown-image21.png)\nCross product of two vectors is a vector that is the area of the parallelogram and is perpendicular to the parallelogram and in the direction of the thumb of the right hand rule.\n![vector With length 2.5 Perpendicular to the parallelogram ](media/3Blue1Brown-image22.png)\n![ãƒ ãƒ ](media/3Blue1Brown-image23.png)\n![x 102 Î™Î™)3 101 â€¢ 122 ](media/3Blue1Brown-image24.png)\n![02 x 03 --- det 03 V3W1---VIW3 + ](media/3Blue1Brown-image25.png)\n11. **Cross Product in the light of linear transformations**\n![Dual vector Transform t.....-l,.,., Some linear transformation to the number line 2 1 2 1 ](media/3Blue1Brown-image26.png)\n![Pl P2 Y = det Y 02 rujl 02 03 03 â€¢ 02 Pl ](media/3Blue1Brown-image27.png)\n![](media/3Blue1Brown-image28.png)\n![What vector p has the property that PI Y = det ](media/3Blue1Brown-image29.png)\n12. **Change of basis**\n    -   **Coordinate system**\n    -   **Alternate coordinate system**\n    -   **How do you translate between coordinate systems**\n    -   **Matrix multiplication by \"Inverse change of basis matrix\"**\n    -   **How to translate a matrix**\n        -   **Start with any vectors written in Jennifer's language**\n        -   **Translate it to our language using change of basis matrix**\n        -   **Apply transformation matrix in our language, this tells us where that vector lands but still in our language**\n        -   **Apply the inverse change of basis matrix to get the transformed vector but in Jennifer's language**\n![Same vector language in our Transformation matrix language m our ](media/3Blue1Brown-image30.png)\n![How to translate a matrix Transformed vector her language in 2 1 1 1 1 O 1 1 O 2 1 1 1 1 2 I livers e change of basis matrix ](media/3Blue1Brown-image31.png)\n![An expression like suggests a mathematical sort of empathy ](media/3Blue1Brown-image32.png)\n13. **Eigenvectors and eigenvalues**\n\nEigenvectors - the vectors that doesn't change its span after a transformation. The factor by which they are changed (either stretched or squished) during the transformation is eigenvalue.\n![4 igenvectors with ei envalue 3 ](media/3Blue1Brown-image33.png)\n![Transformation Eigenvalue matrix Eigenvector ](media/3Blue1Brown-image34.png)\n![Matrix-vector multiplication Scalar multiplication ](media/3Blue1Brown-image35.png)-   Negative eigenvalue corresponds to flipping during transformation\n-   There could be no eigenvectors in a transformation\n\nEx - Rotation by 90 degrees\n\n![](media/3Blue1Brown-image36.png)\nSince there are no real numbers, therefore there are no eigenvectors for this transformation-   A transformation can have a single eigenvalue with a lot of eigenvector\n\nEx - Scaling by some scalar eg. 2, there is only one eigenvalue of 2, but all the vectors have that value.-   Diagonal matrix - all values other than diagonal is 0\n\nAll the basis vectors are eigenvectors, with the diagonal entries of this matrix being there eigenvalues-   Eigenbasis\n\nWe can change our coordinate system so that our eigenvectors are our basis vectors\n\n![Eigen QSis ](media/3Blue1Brown-image37.png)\n14. **Abstract Vector Spaces**\n\n![Formal definition of linearity dditivity:L(V + L V) + Scaling:L( L(V) ](media/3Blue1Brown-image38.png)-   Additivity - If you add two vectors and then perform a transformation, the result is same vector for the transformed addition of vectors and the previous addition vector.\n-   Scaling - Is if you scale a vector v by some number, then apply the transformation, you get the same ultimate vector as if you scale the transformed version of v by that same amount.\n-   Linear transformations preserve addition and scalar multiplication\n![Rules for vectors addition and scaling 3. There is a vector O such that O + v 4. For every vector v there is a vector (bv) 5. v for all v v so that V + ( ---V) Axioms ](media/3Blue1Brown-image39.png)\n\n**References**\n\n[Essence of linear algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n![Vectors vs. ](media/3Blue1Brown-image40.jpg)\n\n"},{"fields":{"slug":"/Mathematics/Linear-Algebra/Alternate-Coordinate-systems-(bases)/","title":"Alternate Coordinate systems (bases)"},"frontmatter":{"draft":false},"rawBody":"# Alternate Coordinate systems (bases)\n\nCreated: 2018-05-24 02:27:07 +0500\n\nModified: 2018-05-24 15:19:46 +0500\n\n---\n\n1.  [Orthogonal complements](https://www.khanacademy.org/math/linear-algebra/alternate-bases#othogonal-complements)\n\n2.  [Orthogonal projections](https://www.khanacademy.org/math/linear-algebra/alternate-bases#orthogonal-projections)\n\n3.  [Change of basis](https://www.khanacademy.org/math/linear-algebra/alternate-bases#change-of-basis)\n\n4.  [Orthonormal bases and the Gram-Schmidt process](https://www.khanacademy.org/math/linear-algebra/alternate-bases#orthonormal-basis)\n\n5.  [Eigen-everything](https://www.khanacademy.org/math/linear-algebra/alternate-bases#eigen-everything)\n**References**\n\n<https://www.khanacademy.org/math/linear-algebra/alternate-bases>\n"},{"fields":{"slug":"/Mathematics/Linear-Algebra/Cheetsheet/","title":"Cheetsheet"},"frontmatter":{"draft":false},"rawBody":"# Cheetsheet\n\nCreated: 2018-05-26 12:33:00 +0500\n\nModified: 2020-11-22 20:34:01 +0500\n\n---\n\n<https://www.freecodecamp.org/news/linear-algebra-full-course>\n![Linear algebra explained in four page: Excerpt from the No BULLSHIT GUIDE TO LINEAR ALGEBRA by Ivan Savo Abstract---This document Will review the fundamental ideas Of linear algebra. We will learn about matrices, matrix operations, linear transformations and discuss both the theoretical and computational aspects Of linear algebra. The tools of linear algebra open the gateway to the study of more advanced mathematics. A lot Of knowledge buzz awaits you if you choose to follow the path Of understanding, instead Of trying to memorize a bunch Of formulas. I. INTRODUCTION Linear algebra is the math of vectors and matrices. Let n be a positive integer and let R denote the set of real numbers, then is the set of all n-tuples of real numbers. A vector e R n is an n-tuple of real numbers. The notation ' 'G S' is read \"element of S.\" For example, consider a vector that has three components: F = e (R, R, R) Ã¦R3. A matrix A e is a rectangular array of real numbers with m rows and n columns. For example, a 3 x 2 matrix looks like this: B. Matrix operations We denote by A the matrix as a wh The mathematical operations defined fo addition (denoted + ) subtraction (the inverse of addition matrix product. The product of ma is another matrix C â‚¬ IR m xe an avg 1 C = AB al'2 b12 a22 b21 b22 a32 given all bl (131 bl all 012 (121 022 032 IR IR R3x2. The purpose of this document is to introduce you to the mathematical operations that we can perform on vectors and matrices and to give you a feel of the power of linear algebra. Many problems in science, business, and technology can be described in terms of vectors and matrices so it is important that you understand how to work with these. Prerequisites The only prerequisite for this tutorial is a basic understanding of high school math conceptsl like numbers, variables, equations, and the fundamental arithmetic operations on real numbers: addition (denoted -F), subtraction (denoted ---), multiplication (denoted implicitly), and division (fractions). You should also be familiar with functions that take real numbers as inputs and give real numbers as outputs, f : R ---4 R. Recall that, by definition, the inverse function f 1 undoes the effect of f. If you are given f (x) and you want to find you can use the inverse function as follows: (f(x)) x. For example, the function f (x) = In(x) has the inverse ex, and the inverse of g(x) = is = x2. Il. DEFINITIONS A. Vector operations We now define the math operations for vectors. The operations we can (VI, v2, v3) are: addition, perform on vectors = 111, t12, ug) and V = subtraction, scaling, norm (length), dot product, and cross product: d -F F = 011 u'2 + + v.3) matrix inverse (denoted A matrix transpose (denoted T): 02 03 '31 (32 '33 matrix trace: Tr(A) Etn=l aii determinant (denoted det(A) or IA Note that the matrix product is not a co C. Matrix-vector product The matrix-vector product is an imp matrix product. The product of a 3 x vector results in a 3 x I vector j giu all an all (121 am all, (a31 , There are tw02 fundamentally different matrix-vector product. In the column pic matrix A by the vector produces a lin ](media/Cheetsheet-image1.png)\n\n![Instead of writing = T A (i) for the linear transformation TA applied to the vector i, we simply write j AE. Applying the linear transformation TA to the vector .F corresponds to the product of the matrix A and the column vector E. We say TA is represented by the matrix A. You can think of linear transformations as \"vector functions\" and describe their properties in analogy with the regular functions you are familiar with: function f : R -9 R input x G R output go f function inverse zeros of f range of f linear transformation TA : R n ---+ R m input G output T A (i) = AE e matrix inverse A N (A) E- null space of A C(A) column space of A range of T A Note that the combined effect of applying the transformation TA followed by TB on the input vector is equivalent to the matrix product 13 AE. E. Fundamental vector spaces A vector space consists of a set of vectors and all linear combinations of these vectors. For example the vector space S = span{F1, F2} consists of all vectors of the form = aF1 + '3132, where a and '3 are real numbers. We now define three fundamental vector spaces associated with a matrix A. The column space of a matrix A is the set of vectors that can be produced as linear combinations of the columns of the matrix A: C(A) -E {j e IR m I for some IR n } . The column space is the range of the linear transformation TA (the set of possible outputs). You can convince yourself of this fact by reviewing the definition of the matrix-vector product in the column picture (C). The vector AE contains times the I st column of A, times the 2nd column of A, etc. Varying over all possible inputs F, we obtain all possible linear combinations of the columns of A, hence the name \"column space.\" The null space MA) of a matrix A â‚¬ IR m x n consists of all the vectors that the matrix A sends to the zero vector. The vectors in the null space are orthogonal to all the rows of the matrix. We can see this from the row picture (R): the output vectors is (j if and only if the input vector is orthogonal to all the rows of A. The row space of a matrix A, denoted R(A), is the set of linear combinations of the rows of A. The row space R(A) is the orthogonal complement of the null space N This means that for all vectors e R(A) and all vectors e M (A), we have â€¢ ti.; = O. Together, the null space and the row space form the domain of the transformation T A, IR n = M (A) R(Ã„), where stands for orthogonal direct sum. E Matrix inverse Ill. COMPUTATIONAL I Okay, I hear what you are saying \"Dud see some calculations.\" In this section w algorithms of linear algebra called Gaus A. Solving systems of equations Suppose we're asked to solve the foll 3X1 + 9.1'2 Without a knowledge of linear algebra, tion, or subtraction to find the values of Gauss---Jordan elimination is a system of equations based the following tow o/ a) Adding a multiple of one row to al {3) Swapping two rows 7) Multiplying a row by a constant These row operations allow us to simpli changing their solution. To illustrate the Gauss---Jordan elimina sequence of row operations required to s described above. We start by constructin The first column in the augmented matri: the variable Xl, the second column con and the third column contains the const The Gauss-Jordan elimination procedl the first phase, we proceed left-to-right one in the leftmost column (called a pil that row from all rows below it to get ze the second phase, we start with the righ all the numbers above it in the same co I) 2) 3) The first step is to use the pivot Il variable in the second row. We the first row from the second row, Next, we create a pivot in the sec We now start the backward phase from the first row. We do this by row from the first row RI e--- RI â€¢ The matrix is now in reduced ](media/Cheetsheet-image2.png)\n\n![IV. COMPUTING THE INVERSE OF A MATRIX In this section we'll look at several different approaches for computing the inverse of a matrix. The matrix inverse is unique so no matter which method we use to find the inverse, we'll always obtain the same answer. A. Using row operations One approach for computing the inverse is to use the Gauss---Jordan elimination procedure. Start by creating an array containing the entries of the matrix A on the left side and the identity matrix on the right side: 121 0 39() 1 Now we perform the Gauss-Jordan elimination procedure on this array. l) The first row operation is to subtract three times the first row from the second row: 112 e--- 112 --- 3111. We obtain: â€¢ Ã¥R2 2) The second row operation is divide the second row by 3. 1 1 ---1 3) The third row operation is RI e--- RI --- 2112 2 3 1 ---1 3 The array is now in reduced row echelon form (RREF). The inverse matrix appears on the right side of the array. Observe that the sequence of row operations we used to solve the specific system of equations in b in the previous section are the same as the row operations we used in this section to find the inverse matrix. Indeed, in both cases the combined effect of the three row operations is to \"undo\" the effects of A. The right side of the 2 x 4 array is simply a convenient way to record this sequence of operations and thus obtain A I B. Using elementary matrices Every row operation we perform on a matrix is equivalent to a left- multiplication by an elementary matrix. There are three types of elementary matrices in correspondence with the three types of row operations: C. Using a computer The last (and most practical) approach is to use a computer algebra system lik A = Matrix( [[1,2), [3,911 >>> A. inv() [3, -2/3) 1/31 [-1, You can use sympy to \"check\" your V. OTHER We'll now discuss a number of other A. Basis Intuitively, a basis is any set of vect01 system for a vector space. You are certail for the xy-plane that is made up of tw the y-axis. A vector can be described respect to these axes, or equivalently and j (O, I) are unit vectors that respectively. However, other coordinate Definition (Basis). A basis for a n-dim of n linearly independent vectors that Any set of two linearly independent vec for R 2. We can write any vector F e R basis vectors F = l,'lÃ©l + V2Ã©2. Note the same vector corresponds to ing on the basis used: = (vr, vy) in F = (VI, v2) in the basis Be {Ã©l, Ã©2 in mind the basis with respect to whict necessary specify the basis as a subscril Converting a coordinate vector from performed as a multiplication by a chan 'F 'F : RI RI + mR2 1 o o 1 m o m 1 1 o o 1 Let's revisit the row operations we used to find in the above section representing each row operation as an elementary matrix multiplication. l) The first row operation R2 --- 3R1 corresponds to a multipli- cation by the elementary matrix El : 1 01 [1 21 [1 21 Note the change of basis matrix is actua vector F remains unchanged---it is simpl coordinate system. The change of basis is accomplished using the inverse B. Matrix representations of linear tran Bases play an important role in the r' tions T: R n ---4 IR\"'. To fully describe tl linear transformation T, it is sufficient vectors of the standard basis for the inpu T: R 2 ---+ 122, the matrix representation ](media/Cheetsheet-image3.png)\n\n![C. Dimension and bases for vector spaces The dimension of a vector space is defined as the number of vectors in a bÃ¦sis for that vector space. Consider the following vector space S span{(l, O, O), (O, 1, O), (1, 1, O)}. Seeing that the space is described by three vectors, we might think that S is 3-dimensional. This is not the case, however, since the three vectors are not linearly independent so they don't form a basis for S. Two vectors are sufficient to describe any vector in S; we can write S = span{(1,O, O), (O, 1, O)}, and we see these two vectors are linearly independent so they form a basis and dim(S) = 2. There is a general procedure for finding a basis for a vector space. Suppose you are given a description of a vector space in terms of m vectors V = span{FI, â€¢ , and you are asked to find a basis for V and the dimension of V. To find a basis for V, you must find a set of linearly independent vectors that span V. We can use the Gauss---Jordan elimination procedure to accomplish this task Write the vectors as the rows of a matrix M. The vector space V corresponds to the row space of the matrix M. Next, use row operations to find the reduced row echelon form (RREF) of the matrix M. Since row operations do not change the row space of the matrix, the row space of reduced row echelon form of the matrix M is the same as the row space of the original set of vectors. The nonzero rows in the RREF of the matrix form a basis for vector space V and the numbers of nonzero rows is the dimension of V. D. Row space, columns space, and rank of a matrix Recall the fundamental vector spaces for matrices that we defined in Section Il-E: the column space C(A), the null space N (A), and the row space R(A). A standard linear algebra exam question is to give you a certain matrix A and ask you to find the dimension and a basis for each of its fundamental spaces. In the previous section we described a procedure based on Gauss---Jordan elimination which can be used \"distill\" a set of linearly independent vectors which form a basis for the row space R(A). We will now illustrate this procedure with an example, and also show how to use the RREF of the matrix A to find bases for C(A) and N (A). Consider the following matrix and its reduced row echelon form: 1 â‚¬3 3.3 267 c, 3 9 9 10 1 30 0 The reduced row echelon form of the matrix A contains three pivots. The locations of the pivots will play an important role in the following steps. The vectors {(1, 3, O, O), (O, O, 1, O), (O, O, O, 1)} form a basis for R(A). To find a basis for the column space C(A) of the matrix A we need to find which of the columns of A are linearly independent. We can do this by identifying the columns which contain the leading ones in rref(A). The corresponding columns in the original matrix form a basis for the column space of A. Looking at rref(A) we see the first, third, and fourth columns of the matrix are linearly independent so the vectors {(1, 2, 3) T, (3, 7, 9) T, (3, 6, 10) T} form a basis for C(A). E. Invertible matrix theorem There is an important distinction betw those that are not as formalized by the Theorem. For an n x n matrix A, theft l) A is invertible 2) The RREF of A is the n x n iden 3) The rank of the matrix is n 4) The row space of A is R n 5) The column space of A is IR n 6) A doesn't have a null space (only 7) The determinant of A is nonzero For a given matrix A, the above statem An invertible matrix A corresponds tc maps the n-dimensional input vector spa vector space R n such that there exists a can faithfully undo the effects of T A. On the other hand, an n X n matrix input vector space R n to a subspace CO space. Once TB sends a vector e that can undo this operation. E Determinants The determinant of a matrix, denoted combine the entries of a matrix that serv or not. The determinant formulas for 2 all al'2 = --- 012021, (121 a22 all (113 022 a23 021 (122 a23 = all 032 (133 am 0.32 a33 If the IAI = O then A is not invertible. G. Eigenvalues and eigenvectors The set of eigenvectors of a matrix i: which the action of the matrix is desc a matrix is multiplied by one of its eigenvector multiplied by a constant AÃ© an eigenvalue of A. To find the eigenvalues of a matrix we VA, insert the identity I, and This equation will have a solution whene , denoted {Al, , Of r X n IA --- All. The polynomial --- eigenvalue are the vectors in the null Certain matrices can be written entir and their eigenvalues. Consider the mal the matrix A on the diagonal, and ](media/Cheetsheet-image4.png)\n"},{"fields":{"slug":"/Mathematics/Linear-Algebra/Matrix-Transformations/","title":"Matrix Transformations"},"frontmatter":{"draft":false},"rawBody":"# Matrix Transformations\n\nCreated: 2018-05-24 02:26:55 +0500\n\nModified: 2018-05-24 15:19:36 +0500\n\n---\n\n1.  [Functions and linear transformations](https://www.khanacademy.org/math/linear-algebra/matrix-transformations#linear-transformations)\n\n2.  [Linear transformation examples](https://www.khanacademy.org/math/linear-algebra/matrix-transformations#lin-trans-examples)\n\n3.  [Transformations and matrix multiplication](https://www.khanacademy.org/math/linear-algebra/matrix-transformations#composition-of-transformations)\n\n4.  [Inverse functions and transformations](https://www.khanacademy.org/math/linear-algebra/matrix-transformations#inverse-transformations)\n\n5.  [Finding inverses and determinants](https://www.khanacademy.org/math/linear-algebra/matrix-transformations#inverse-of-matrices)\n\n6.  [More determinant depth](https://www.khanacademy.org/math/linear-algebra/matrix-transformations#determinant-depth)\n\n7.  [Transpose of a matrix](https://www.khanacademy.org/math/linear-algebra/matrix-transformations#matrix-transpose)\n**References**\n\n<https://www.khanacademy.org/math/linear-algebra/matrix-transformations>\n"},{"fields":{"slug":"/Mathematics/Linear-Algebra/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2021-04-03 10:47:06 +0500\n\nModified: 2021-06-13 19:58:53 +0500\n\n---\n\n**Concepts**\n-   Matrix factorization\n-   Singular value decomposition\n-   Moore-Penrose Pseudoinverse\n-   Hadamard product\n-   Entropy\n-   Kullback-Leibler Divergence\n-   Gradient Descent\n<https://www.datacamp.com/community/tutorials/demystifying-mathematics-concepts-deep-learning>\n\n## Norm**\n\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics), anormis a[function](https://en.wikipedia.org/wiki/Function_(mathematics))from a real or complex[vector space](https://en.wikipedia.org/wiki/Vector_space)to the nonnegative real numbers that behaves in certain ways like the distance from the[origin](https://en.wikipedia.org/wiki/Origin_(mathematics)): it[commutes](https://en.wikipedia.org/wiki/Equivariant_map)with scaling, obeys a form of the[triangle inequality](https://en.wikipedia.org/wiki/Triangle_inequality), and is zero only at the origin. In particular, the[Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)of a vector from the origin is a norm, called the[Euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm), or[2-norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#p-norm), which may also be defined as the square root of the[inner product](https://en.wikipedia.org/wiki/Inner_product)of a vector with itself.\nApseudonormorseminormsatisfies the first two properties of a norm, but may be zero for other vectors than the origin.A vector space with a specified norm is called a[normed vector space](https://en.wikipedia.org/wiki/Normed_vector_space). In a similar manner, a vector space with a seminorm is called aseminormed vector space.\n<https://en.wikipedia.org/wiki/Norm_(mathematics)>\n\n## Vector Norm**\n\nCalculating the size or length of a vector is often required either directly or as part of a broader vector or vector-matrix operation.\nThe length of the vector is referred to as the vector norm or the vector's magnitude.\nThe length of a vector is a nonnegative number that describes the extent of the vector in space, and is sometimes referred to as the vector's magnitude or the norm.-   The L1 norm that is calculated as the sum of the absolute values of the vector.\n-   The L2 norm that is calculated as the square root of the sum of the squared vector values.\n-   The max norm that is calculated as the maximum vector values.\n<https://machinelearningmastery.com/vector-norms-machine-learning>\n"},{"fields":{"slug":"/Mathematics/Linear-Algebra/Vectors-and-Spaces/","title":"Vectors and Spaces"},"frontmatter":{"draft":false},"rawBody":"# Vectors and Spaces\n\nCreated: 2018-05-24 02:25:25 +0500\n\nModified: 2018-05-26 12:51:30 +0500\n\n---\n\n1.  [Vectors](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces#vectors)\n    -   Vector = Magnitude + Direction\n\nEx - 5mph is a scaler quantity, because it doesn't tell the direction in which object is moving.\n\nEx - 5mph East is a vector quantity (and we will not call this as speed, we will call this as velocity, therefore velocity is a vector quantity)\n\n![I cooramate spaces I vectors ana spaces I Linear Algeora I Knan loc a -2 uecl 2 recÃ„ - v wweD c-pycÃ¦c-.g Ik3- o X-C..A-..Q coo ](media/Vectors-and-Spaces-image1.png)-   Real Coordinate Space - Can be any dimensions, all possible real-valued ordered 2-tuple for a 2 dimensional real coordinate space.\n-   Adding Vectors algebraically and graphically\n-   Multiplying vector by a scaler ( change its magnitude, but scale it only on the same dimension, colinear )\n-   Unit Vectors\n\n![7 b + ) j ](media/Vectors-and-Spaces-image2.png)\n-   Parametric representations of the line\n2.  [Linear combinations and spans](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces#linear-combinations)\n\nspan(v1 + v2 + v3 + ... + vn) = {c1v1 + c2v2 + c3v3 + ... + cnvn | ci belongs to set of Real numbers and 1 <= i <= n}\n3.  [Linear dependence and independence](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces#linear-independence)\n\n    a.  Linear dependent - Means one vector in the set can be represented as other vectors in the set.\n\n    b.  Linear Independent - If we cannot scale using any scalar one vector to the other vector in the set, than both are linearly independent of each other\n\nC1 and C2 must be equal to 0 for R2\n\n4.  [Subspaces and the basis for a subspace](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces#subspace-basis)\n    -   Must contain 0 vector\n    -   **Closure under scalar Multiplication -** In order to be in a subspace, any vector multiplied by a real scalar must also be in the subspace\n    -   **Closure under Addition -** If we add two vectors belonging to the same subspace, than the addition of both the vectors must also belong to the same subspace.\n    -   Span of n vectors is a valid subspace of Rn\n    -   **S is a basis of V**, if something is a basis for a set, that means that, if you take the span of those vectors, you can get to any of the vectors in that subspace and that those vectors are linearly independent.\n        -   Span (s) = R2\n        -   Must be Linearly Independent\n        -   Standard Basis = T = {[1 0] , [0 1]}\n\n5.  [Vector dot and cross products](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces#dot-cross-products)\n    -   Dot product\n    -   Vector dot product is commutative, V.W = W.V\n    -   Vector dot product is distributive, (V + W).X = (V.X + W.X)\n    -   Vector dot product is associative ( (c.V).W = c.(V.W) )\n    -   Length of vector X = ||X||\n    -   ||X||^2^ = X.X\n    -   Cauchy -Schwarz inequality\n        -   |X.Y|<= ||X||.||Y||\n        -   |X.Y| = ||X||.||Y|| only when X and Y are colinear i.e. X = c.Y\n    -   Vector Triangle Inequality\n    -   Angles between Vectors\n        -   (A.B) = ||A||*||B||*cos **Î˜**\n        -   If A and B are perpendicular than there dot product is 0, since cos **90 = 0**\n        -   If A.B = 0 (dot product of vector A and dot product of vector B is equal to 0) and A and B are non zero vectors than A and B are perpendicular to each other.\n        -   But if only A.B = 0 satisfies, than A and B are orthogonal\n        -   All perpendicular vectors are Orthogonal\n        -   Zero vector is orthogonal to everything else, even to itself\n\n6.  [Matrices for solving systems by elimination](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces#matrices-elimination)\n\n7.  [Null space and column space](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces#null-column-space)\n**References**\n\n<https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces>"},{"fields":{"slug":"/Mathematics/Precalculus/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-05-15 11:45:07 +0500\n\nModified: 2021-06-06 16:54:11 +0500\n\n---\n\n1.  Trigonometry\n\n2.  Conic sections\n\n3.  Vectors\n\n    a.  Adding vectors in magnitude and direction form\n\n4.  Matrices\n\n5.  Complex numbers\n\n6.  Probability and combinatorics\n\n    a.  Basic probability\n\n    b.  Venn diagrams and the addition rule\n\n    c.  Compound probability of independent events using diagrams\n\n    d.  Compound probability of independent events using multiplication rule\n\n    e.  Dependent events\n\n    f.  Permutations\n\n    g.  Combinations\n    h.  **Probability using combinatorics**\n\n        i.  Birthday Paradox\n\n7.  Series\n\n    a.  Arithmetic sequences\n\n    b.  Basic sigma notation\n\n    c.  Finite arithmetic series\n\n    d.  Geometric sequences\n\n    e.  Finite geometric series\n\n    f.  Finite geometric series applications\n"},{"fields":{"slug":"/Mathematics/Probability/365-DS---Probability/","title":"365 DS - Probability"},"frontmatter":{"draft":false},"rawBody":"# 365 DS - Probability\n\nCreated: 2020-04-14 03:28:00 +0500\n\nModified: 2021-10-24 18:09:32 +0500\n\n---\n\n**The Basics of Probability**\n\nWhat is the probability formula?\n\nComputing Expected Values\n\nThe Probability Frequency Distribution\n\nComplements\n**Combinatorics**\n\nFundamentals of Combinatorics\n\nComputing Permutations\n\nSolving Factorials\n\nComputing Variations with Repetition\n\nComputing Variations without Repetition\n\nComputing Combinations\n\nSymmetry of Combinations\n\nCombinations with Separate Sample Spaces\n\nWinning the Lottery\n\nA Summary of Combinatorics\n\nCombinatorics: Practical Example\n**Bayesian Inference**\n\nSets and Events\n\nThe Different Ways Events Can Interact\n\nThe Intersection of Two Sets\n\nThe Union of Two Sets\n\nMutually Exclusive Sets\n\nDependent and Independent Events\n\nConditional Probability\n\nLaw of Total Probability\n\nAdditive Law\n\nMultiplication Rule\n\nBayes Rule\n\nBayesian: Practical Example\n**Probability Distributions**\n\nAn overview of distributions\n\nTypes of Distributions\n\nDiscrete Distributions\n\nDiscrete Uniform Distributions\n\nBernoulli Distributions\n\nBinomial Distributions\n\nPoisson Distributions\n\nContinuous Distributions\n\nNormal Distributions\n\nStandardizing Normal Distributions\n\nStudents' T Distributions\n\nChi Squared Distributions\n\nExponential Distributions\n\nLogistic Distributions\n\nProbability Distributions: A Practical Example\n<https://365datascience.teachable.com/courses/enrolled/506102>\n"},{"fields":{"slug":"/Mathematics/Probability/Binomial-Random-Variables/","title":"Binomial Random Variables"},"frontmatter":{"draft":false},"rawBody":"# Binomial Random Variables\n\nCreated: 2018-05-14 19:49:19 +0500\n\nModified: 2021-10-20 20:45:05 +0500\n\n---\n\n1.  Binomial Random Variables\n\nThe conditions of a binomial random variable are -\n-   Made up of independent trials\n-   Each trial can be classified as either success or failure\n-   Fixed number of trials\n-   Probability of success on each trial is constant\nEx - X = Number of heads after 10 flips of a coin with P(H) = 0.6 and P(T) = 0.4\n\nNot binomial variable - Number of kings after taking 2 cards from standard deck without replacement (It doesn't meet independent trial, the second trial depends on 1st trial)\nSRS - Simple Random Sample, is a sample taken so that each member in a set of n members has an equal chance of being in the sample.\n2.  Distributions\n\n    a.  Normal distribution, (Continuous distribution - bell curve)\n\n    b.  Binomial distributions (Normal distributions with discrete steps)\n\n    c.  10% rule of assuming independence between trials\n\n    d.  Binomial Distribution\n\n![10 1--1 01 FI ](media/Binomial-Random-Variables-image1.png)\n![(d-D ã¾ ( ä¹ ãƒ» 0 ã© 0 ä¸€ 7 ãƒ» 0 ãƒ ã€‚ ã€‚ ã— 0 è±¸ ãƒ» 0 ã‚³ ã€‚ 0 ã¨ ãƒ¼ --- ä¸¿ 0 ã¤ 5 CIO ã€ ](media/Binomial-Random-Variables-image2.png)\ne.  Binompdf (Binomial Probability Distribution Function) and Binomcdf (Binomial Cumulative Distribution Function) functions\n\nf.  Bimodal distribution\n\ng.  Uniform distribution\n\nh.  Bernoulli distribution\n**Others**\n-   **Galton Board** - Every time creates normal distribution, pegs are arranged in pattern of **Quincunx (pattern of 5 pegs, where 4 are on sides of square and 1 in middle, like dice - 5)**\n-   Follow Binomial Distribution (**Central limit theorem**, says that under large number of trials like 3000 balls a binomial distribution approximates a normal distribution)\n-   **Pascal's Triangle**\n\n![Image result for pascal's triangle](media/Binomial-Random-Variables-image3.png)\n-   **Creates Fibonacci Series**\n\n![Image result for pascal's triangle with diagonals](media/Binomial-Random-Variables-image4.gif)-   **Rows gives sequence of coefficients in binomial powers**\n\n**(a + b)^2^ = a^2^ + 2.a.b + b^2^ [FOIL - First, Outer, Inner, Last]**\n\n(a+b)^4^ = a^4^+ **4**a^3^b + **6**a^2^b^2^+ **4**ab^3^+ b^4^\n\nThese have same coefficients as 4th row of pascal's triangle\n[The Galton Board](https://www.youtube.com/watch?v=UCmPmkHqHXk)\n**References**\n\n<https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/binomial-random-variables/v/binomial-variables>\n\n"},{"fields":{"slug":"/Mathematics/Probability/Central-Limit-Theorem/","title":"Central Limit Theorem"},"frontmatter":{"draft":false},"rawBody":"# Central Limit Theorem\n\nCreated: 2018-07-02 00:57:48 +0500\n\nModified: 2021-10-02 15:13:14 +0500\n\n---\n\nIn[probability theory](https://en.wikipedia.org/wiki/Probability_theory), thecentral limit theorem(CLT) establishes that, in some situations, when [independent random variables](https://en.wikipedia.org/wiki/Statistical_independence) are added, their properly normalized sum tends toward a[normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)(informally a \"bell curve\") even if the original variables themselves are not normally distributed. The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.\nThe distribution of sample means for an independent random variable, will get closer and closer to a normal distribution as the size of the sample gets bigger and bigger, even if the original population distribution isn't normal itself\n**What is the Central Limit Theorem and why is it important?**\n\nSuppose that we are interested in estimating the average height among all people. Collecting data for every person in the world is impossible. While we can't obtain a height measurement from everyone in the population, we can still sample some people. The question now becomes, what can we say about the average height of the entire population given a single sample. The Central Limit Theorem addresses this question exactly.\nFormally, it states that if we sample from a population using a sufficiently large sample size, the mean of the samples (also known as thesample population) will be normally distributed (assuming true random sampling). What's especially important is that this will be trueregardlessof the distribution of the original population.\n<https://spin.atomicobject.com/2015/02/12/central-limit-theorem-intro>\n\n<https://en.wikipedia.org/wiki/Central_limit_theorem>\n\n<https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/central-limit-theorem>\n\n## What is the fundamental difference between the Law of large numbers & the Central limit theorom?**\n\nThe Law of Large Numbers basically tells us that if we take a sample (n) observations of our random variable & avg the observation (mean)-- it will approach the expected value E(x) of the random variable.\nThe Central Limit Theorem, tells us that if we take the mean of the samples (n) and plot the frequencies of their mean, we get a normal distribution! And as the sample size (n) increases --> approaches infinity, we find a normal distribution.\n**Sampling distribution of the sample mean**\n\n![](media/Central-Limit-Theorem-image1.jpeg)\n**Positive skew and negative skew in a probability distribution**\n\n![](media/Central-Limit-Theorem-image2.jpeg)\n**Positive Kurtosis and Negative Kurtosis in a probability distribution**\n\nPositive Kurtosis - Fatter tails and pointy middle\n\nNegative Kurtosis\n\n![](media/Central-Limit-Theorem-image3.jpeg)\n**Chebyshev's inquality**\n\nIn a normal distribution, for instance, roughly two thirds of the values have to fall within one standard deviation of the mean and 95% within two standard deviations. Chebyshev's inequality is more general, stating that a minmum of just 75% of values must lie within two standard deviations of the mean and 88.89% within three standard deviations for a broad range of different probability distributions\n"},{"fields":{"slug":"/Mathematics/Probability/Cheatsheet/","title":"Cheatsheet"},"frontmatter":{"draft":false},"rawBody":"# Cheatsheet\n\nCreated: 2019-03-13 20:31:39 +0500\n\nModified: 2021-10-20 20:44:42 +0500\n\n---\n\nMVN - MultiVariate Normal Distribution\n\n![Probability Cheatsheet v2.O by wjt1, ÑƒĞµĞ¼, ĞĞ¾Ğ¸, Hwug_ pmiv.bjjjty ÑÑ Ğ°! bttp: tbub. 2013 Counting Multiplication Rule ĞºĞ°Ñƒ Ğ° with thc â€2 _ the wole Sampling Ğ¢Ğ°Ğ«Ğµ Ğ¾ 000 .pIe Mat with Naive Definition of probability Not Ğ¼Ğ°\". If Ğ°Ğ¸ Ğ¿Ñ‹Ñƒ, Ğ Thinking Conditionally Indep endence Ğ Ğ’ (â€hieh if P(AlB) Ğ (Ğ) P(BIA) = Ğ (Ğ’) Ğ¡Ğ¾Ñˆ1Ğ¢Ğ³_Ñ‚] Ğ Ğ’ Ğ¡ Ğ’Ğ˜Ğ¡) Ğ°--- imply imply Unions, 1ntersections, and Complements vim w'th (Ğ ĞĞµĞ¸Ğ²\" J0int, Marginal, and Conditional Joint Ğ Ñ‚'â€ÑŒÑˆĞµÑƒ Ğ (Ğ, Ğ’) Ğ Ğ’. Ğ©Ğ) - pmtmbj1jty Ğ, ÑĞ¾,Ğ¸.Ñ‚Ğ¾Ñ‹ Ğ (Ğ'Ğ’) = Ğ©Ğ, ÑĞ¾,Ğ¸.Ñ‚Ğ¾Ñ‹ P(AlB) ĞĞ¿Ñƒ ProbabiIity 0f Ğ°Ğ¿ [ntersection Ğ¾Ğ³ Union via Ğ²ĞµÑ) via Ğ¸ Ğ’) â€ Ğ©Ğ) Ğ (Ğ’) --- Ğ’) Ğ (ĞĞ¸ Ğ’Ğ˜Ğ¡) = Ğ©Ğ) + Ğ (Ğ’) + Ğ (Ğ¡) Simpson's Paradox Ğ¾Ğ¾Ğ°Ğ¾Ğ¾Ğ¾Ğ¾Ğ¾Ğ¾Ñ Ğ¾ÑĞ¾Ğ¾Ğ¾ÑĞ¾Ğ¾Ğ¾Ğ¾ Ğ¾ÑĞ¾Ğ¾Ğ¾Ğ¾Ğ¾Ğ¾Ğ¾Ğ¾ Ğ²,Ğ¡) Ğ²Ğµ,Ğ¡) Ğ½Ğµ, Law of Tota1 Probability (LOTP) disjoint \"d is .pIe Ğ³Ğ» Ğ (Ğ›Ğ³-, Ğ’2) + + Ñ‚ FT_w LOTP w1th in Nent Ğ¡! ĞĞĞ¡) Ğ (Ñ‰Ğ², + - + Ñ‰ĞĞ²â€, Ğ Ğ¡ ĞĞ¡) + ĞŸ Bâ€IC) 1.ĞĞ¢Ğ  wite, Ğ’ Ğ²Ğ³ Bayes Rule Ğ’Ğ°Ñƒ---' Ru1eâ€ w1th \"tra Ğ¡!) P(AlB) --- Ğ (Ğ©Ğ, P(BlC) Odds of Ğ’Ğ°Ğ·. ' Ru'e P(AlB) P(BlAe) Ñ‰ Ğ»Ğµ) mIds Ğ mâ€¢imâ€¢ Random Variables and their Distributions PMF, CDF, and rndependence (PMF) Ğ° Ñ€Ñ…(Ñ…) Ñ…) Ğ ,Ğ¼Ñ€ ](media/Cheatsheet-image1.png)\n\n![(CDP) is to Fx(x) PC x CDP is i---ing, with Fx(r) Fx(r) 1 two if kÃ„ing of gim X Y if of y pc X Y y) PIX y) Expected Value and Indicators Expected Value and Linearity (a.k.a. is a weighted of p---ible of if . all of the X of X is max + by + c) aE(x) + distrmtion If X Y distributm, El X) E(Y) value is like E(xlA) EXP(x XIA) Indicator Random Variables Variable is a that tako on or O, It is always of if m, the is 1; it O. Me about of Write 1 if A O if A not Bem(p) p = P(A). Bridge of A is the of A; E(/A) PIA), Variance and Standard Deviation Vu(X) Continuous RVs, LOTUS, U0U Continuous Random Variables (CRVs) a CRV is Take CDP PDF later). --- --- Fx(b) ---Fx(a) X 02) , this P(asx is (PDF)? PDF f is the of CDP F. A PDF is to 1. By of to get PDF to CDF integrate To cm.' o PDF interval, a in do 1 of a CRV? to diurete PMF. CRVs integnte the PDF. LOTUS ofa of r.v. of X this of (LOTUS) find the of a of y(X), in by in PMF/PDF by g(z) still with PMFJPDF X: a or a A of a a if X is nmber of in how 9,' X) 2X is of bike in that hi X) = (x) = of of that you of in You to the PMF/PDF of to its All ym PMF/PDF of X, Universality of Uniform (UoU) plug CRV into its CDP, get plug a r, v. into CDF, r,v. with that CDF, let's X CDP By if w, plug X into this get a F(X) â€¢ 1 Unif(o, 1) if U 1) then CDF F. is that X , it buk by its CDF. Moments and MGFs MO ments Z (X --- be of X Eth of X is E(XE) kth of X E(ZE), of the of distributm. Sk--- Skew (X) m. Kurt(X) --- ](media/Cheatsheet-image2.png)\n\n![Moment Generating Functions MCF For X , Mx(t) ECe'X) i. (MGF) of X, if it an t in O. t well u v, It's a lets with function of Why is it the Eth of g.ting O, is kth of X E(Xk) This by of Mx(t) --- MGF of If Y ax + b, ) = ebtMx(at) If it the MGF This that my two X Y, they are distritmtÂ«l the (thwir PMFs/PDFs if only if MGFs Summing RVs by Multiplying MGFs. ) = E(etx = Mx(t) . MX+Y(t) = MGF of of is of MGFs of Joint PDFs and CDFs Joint Distributions joint CDF of X Y is In X Y joint PMF PIX In tiwy a Joint PDF joint PMF/PDF to Conditional Distributions ylX Bayes' u) rx I V (zly)fy (y) Marginal Distributions To the of PMF/PDF. sum/ integrate the PMF joint PMF PDF Joint PDF y y)dy Independence Of Random Variables a joint fylx(ylx) Hybrid Bayes' rule P(A'X = x Jx(zlA) fx (z) X Y if if of follrmng â€¢ Joint CDP is of CDFs â€¢ Joint PMF/PDF PMFs/PDFs distritmtion of Y given X is the distribution of Y Write X Y that X Y Multivariate LOTUS LOTUS in is to ID LOTUS. Covariance and \"fransformations Covariance and Correlation is of two cm-(X, Y) --- --- E(Y))) --- E(XY) --- Note t X) --- E(X2) --- --- var(X) a of that is C.w(X, Y) ---ily (e.g., X MO. 1) Y X 2). X Y Cov(X, Y) E(XY) of a by + Y) + vu(Y) + Y) If X Y thwy O, X .L. Y Var(X Y) Var(X) VM(Y) xâ€xa,.. by , + X, nvar(x,) Cov(X, Y) Cov(Y, X) Cw(W + x, Y + Z) cov(w, Y) + Z) + Y) FIN with a Transfo ns X with PDF in function X _ we this Y = = g(x)_ If g strictly iming strictly PDF of Y is of is Variable say joint PDF of t.' V but in the (X, Y) by (X, Y) 9(U, V). matrix. If in this matrix exist of O, OCX, u) to take to the In a 2 2 Convo I ut ions Integra' If mt to PDF of sum of CRV, X Y, ym do Example X, Y t, By a PDF to 1, this out to being NO, 2) PDF, ](media/Cheatsheet-image3.png)\n\n![Poisson Process W, a of 1. a time t 2. of in disjoint time For the of [O. (5, 12), 113, 23) with pois(5A), a of in an rate per hour. of of nth time 0) N, of in 10. IA's find of T, > t, the have to t to get is event N, = O, which so PCT, > = = By ---ins, Expo(A), --- i.i.d. Order Statistics IÃ„t's say n thorn to the ith in list the ith statistic, XO). so XO, in list is in the list. Note e.g., 42 us the that 42 > .12. Taking i.i.d. X i, X\", , CDP F(x) PDF f(c), CDP PDF of Xo, statistics of i.i.d. -.Bet.0,n-j+1). Conditional Expectation . with symttle that t minuto, waiting by the is, Ecrrr > t) = 10. FAY) = E, a Vmâ€¢iable We E(YIX ) , the of Y the variable X, This a function of It is in such if X _IL Y. To E(YIX), E(YIX ---x) plug in X If E(YIX + E(YIX) --- + SX. Y of in 10 trials with p of X the number of trials. E(YIX) = X + X Y X2. E(Ylx X Y E(XIY y) z o if Y = y X = with (by so E(YIX) O. properties of Expectation E(YIX) = ED\") if x (taking = ED') 1m, a.k.a. of Tota1 (a.k.a. Imw of Total Adm s written in a that to LOTP_ is A, Ae , this Eve's of Variance) vu(Y) + MVN, LLN, CLT Law Of Large Numbers (LLN) X.. X2,Xs i.i.d. with is of xo with l. example, in of a with p of X, of Rip LLN of to p (with probability 1). Central Limit Theorern (CLT) Approximation using Limit to the of a Xi + that is a of n i.i.dâ€ Xi i.i.d. with CLT Asymptotic Distributions using CLT Wc to to n CLT that X, + â€¢ +xn distribution of the to 1) n m: In CDP of side to N.] CDP , In of CLT Markov Chains D efinition A is a in a state which will is finite. say (l, 2, ...M). X, which of walk visiting at t, is the walk at in XO. X,.XZ..... By a satisfy which if to will the state then entire --- jlxo --- io,x, --- il, State Properties i) --- PI xo --- jlxn ---i) on We find E(YIA), X Note E(YIA) is â€¢ of die roll. gim it is is Y of in 10 trials with p of A be that trials an E(YIA) of ---g the 7 trials Bin(7, p). A state is If at state, will to that state at in You you like, you you at prokmbility will to go you stau is If state of k, CCD the JÃ„ible of it take to is at CCD of p---ible it to buk is 1. ](media/Cheatsheet-image4.png)\n\n![Transition Matrix , M). matrix Q is M x M qii is i to in --- --- jlxâ€ --- i) To find probability chain i to in (i, j) of Q\" If Xo to PMF F, = j), PMF xo is Chain Properties A is if get to If a chain finite state is an of its A its Me is if of its In chain. all have A chain with to Z if all with = with walk Stationary Distribution say , ss,) be PMF we will the if = As a if X, . the distribution. For the is is the of chain at i. To find the matrix (Q' --- = O. distribution is of Q to 1. Implies If a PMF chain with matrix Q, = stati.y, Random Walk on an Undirected Network If of of which by i by the to is to it by Â«lge. this a walk of this chain is to the (this is of of is to it). statiÃ„y distributim-, of walk on the to (3, 2), it's ( â€¢d, -I, Hi)' Continuous Distributions Uniform Distribution U is Unif(a. b). We of Uniform of draw from within is to of the of statistic, his the they're likely to Uniform of The is only distribution of hitting in is to of that von, of In is Normal Distribution X is distributed ..VW we Limit of Limit which sample of i.i.d. will a of initial shift a r v. (by a or (by multiplying by a ---tant), it to r, v. X wc it to MO, 1) by l. Its CDF is by Exponential Distribution X is we Stmâ€¢y sitting on right of wishing in night sky wish right You 1-5 a is â€¢due\" to come long. You is the you'w waiting the is Expo(4) = 4 reeter, sinee at a rate of 1 1/4 until 1/4 The is only distribution. for X any s t, PIX > , + tlX > s) pc X > t) Equivale ntly, Ã¦ple, a with is new\" (it Given that ptGuet a the time that it win Min of If have X, Expo(Ai), , Xk) ho distributif_m Y, + Y, the Y, Gamma Distribution Let X is we Stmâ€¢y sit for waiting a go total time for nth slmting is Yc_m at 3 of The for is with 2 only be of miting it's to is Beta Distribution of In the to The â€-ior its distribution data. The is the data. is of Binomial p Bine_mial data is xlp p Bcta(a, b) otmNing X r, get pl(x = --- r) of onler statisties_ If X A), Y A), with X Y ](media/Cheatsheet-image5.png)\n\n![This km X2 (Chi-Square) Distribution x we story A is of Norm.] r.v.s. P Rep x + z: i.i.d. z, 1) X Gum-na(n/2, Discrete Distributions Distributions for four sampling schemes # trials (n) until success NBin Bernoulli Distribution distribution is the of Bimial distributm, only trial (n 1), X is we story A is with p , X is of 1 O Let X be o' fair X 1 ---X is of Tails. Binomial Distribution IA X is distributÂ«] Story X of \"su---\" will in n trials, where trial a with the probability p of we X of multiple Int X Bin(n.p) X, where all of If Lin getting in, of he is Bin(10. * â€¢ n --- X Bin(n, --- p) Pois(A) p is Geometric Distribution X is Geom(p). V\" kÃ„ folkwing: X is of that will Our p, If to the of will First Success Distribution to it in the This I of If X FS(p) --- Negative Binomial Distribution X is We X is of that will ucuraey faint a wild in hits, of faints with is Hypergeometric Distribution X is b. n). We kÃ„ X is of will in a draw of n The draw of to simple (all of n likely). Ã¦pw say that b w f---t, X is of in of in a 5 You u, white b bluk balls. You will X white You u. white bans b bluk witlmt replmentâ€ of white in is b, n); the of n), A forot h\" N elk, n of of m. elk in Poisson Distribution X is pois(A), We following; (high of of unit or of that in of is X. A of 2 nu-,nth, is a low ways, it to the of in mc_mth Pois(2). Then of that in two at POG(4). 2. 3. X Y with X Y. XI(X + Y = Chie'megg 11 Z Pois(A) 'mpt\" euh item with p, the of Z, poi*' XP), of zz Pois(M1 --- Z, za_ Multivariate Distributions Multinomial Distribution say that St.-wy Wc have which fall into of k with = 100 in mto of with pmbability, of in of tiw is distributÂ«l Mult.(100, F), = Note Xi + + â€¢ â€¢ â€¢ + 100, ttwy Joint PMF PMF, X, define to If lump multiple a it is still Multinomial, For Xi x j + p,) to in i j. if k G lump 1-2 3-5, (X, X, X8,X6) (p, + p2,p3 on x j still a Multivariate Uniform Distribution 20 on probability is to in the dâ€asity, of Multivariate Normal (MVN) Distribution . , Xk) if h. A X (X,.xa, combination is i.e., t, X, + t. X , is , q. The of MultivMiate = matrix (i. j) is â€¢ Any is MVN. If within MVN joint PDF of a (X, Y) with S(O, 1) ](media/Cheatsheet-image6.png)\n\n![Distribution Properties Important CDFs Norm' 4' Convolutions of Random Variables A of is simply For let X Y of i.i.d. X + Y with of i.i.d. Expo(A) r.vs. X Y NBin(r1 + Special Cases of Distributions 2. Unif(o, 1) S. NBin(1, p) Inequalit ies 3. Chebyshev POX --- a) E(X) Var(X) Formulas Geometric Series Exponential Function Gamma and Beta Integrals You integnls by to bet. rct) Aim, + 1) --- ar(a), r(n) --- if Euler's Approximation for Harmonic Sums Stirling's Approximation for Factorials Miscellaneous Definitions X CDP F, X F(m) > 0.5 Pix m) 0.5_ Fur X = 1/2Z In the ath of X is the is 1/2. log to to log (ix., e). Example Problems Chiu Calculating Probability A which ---gst its n indegmlently. You pick a page. What is pmbability it no (I --- mlMbi1ity typo page, a this is e â€¢ I Linearity and Indicators (I) In of n what is the of (math day)? wymt is of X the of be the jth day being E(l,) --- P(no day j) --- (36.1/365)\" By E(X) = (1 --- (364/365)\") Y Linearity and First Success Thi* is At get What is the of a N bc of to get NZ to draw on, By story of First - type, --- l) 'n get Similarly. N, --- 2)/n), N, --- + By This 0.577) by Orderings of i.i.d. random variables 2 Lyfts at the If the it rid8 to is that .11 the Lyfts will first? the of of the arrivals likely, a!2! the L prokmbility Lyfts ---Z--- 1 / 10 (O birt y that ith of Ã„e b i r t h day. is E(Y) 0/365 Linearity and Indicators (2) This is kw as the n at a puty, with hat. At of with who with right Euh hat h\" a of to the rwght li---ity, of go is = 1 to of 5 slots Lyfts to of likely. of all 3 of Lyfts Expectation of Negative Hypergeometric What the of you draw pick a this to be that will first Notc that j is all 4 of the in this is 1/5 by X the of Minimum and Maximum Of RVs is CDP of of n Unif(O.1) Notc that r,v.s Xâ€X2,. , Xo) P(XI we will this to End â€ i.i.d. P(Ui pattern-matching with el Taylor series X pois(A), find E Answer: By LOTUS, ](media/Cheatsheet-image7.png)\n\n![Adam's Law and Eve's Law Rubik's But pmtty at it, Gils. on will attempt N Rubik's SuppÃ„ time. h\" p of the T of Rubik's Cubes he during the of T, Note TIA' so by E(T) --- --- E(Np)--- Similarly, by 1Ã„w, VM(T) + --- p)) + VM(Np) MGF - Finding Moments Find E(X3) X Expo(x) MGF of X. MGF MO) = To get third take of MGF ud at = O: But to MGF via Ml t) like it a g.tric of is nth of X, hue = for all integen n. Markov chains (I) X. a with the chain with to that --- and --- -B) By of wc To chain is with to i, we sq i i,j. This if Markov chains (2) Willim play a gm of of (which tile) to of Is this Is it The Markov is it get from to The to the CCD is What of this this is a on is to The the is 3, the 4, the for To this divide by its The of is 6(3) + + 7(6) = statiÃ„â€¢y of Ewing a is 3/84 1/28. on is 4/84 1/21, in is 6/84 1/14. What of time will tile in this in By is it will take to to tile? this is to the to win to to Problem-Solving Strategies Yuqi Getting by (\"Let A be that 1 pick the X of thinking! what it to be in of to = 31,4)\"). Think type of bc (a A A PMF? A PDF?) it in of. simple To make try a making up of the mve calculating of if naive of probability applÃ„ Is of to for to on, apply Rule or the of Total Pmtâ€.bility. Finding distribution of a the full ito-n). of what it take on? this rule ht. Is there a for of that fits at v-uiablc a of with a Y = g(X)? 5. 6. 9, 10. 11. 12. If it a table of If it's of with a try LOTUS, If it's a of ---thing, try bmaking it up If Â«.mething Adam's law. Cm-aside, LOTUS. If of it up into of r If it's sum, of If on ---thing Eve's E(X2). Do E(X) var(X)? var(x) E(X2)--- try of If wu're trying to find the of Sy---try. If X. i.i.d., ---ider \"---try. of all of i.i.d. X, , Determining Think simple to if Do a If if you in of a known PDF (like Beta), PDFs integrate to 1? Before Ã„1ng simple ext. the for Biohazards Hâ€mg 5. of \"What is that n birth math?\" , it to the being 12 1---, that the list of birth is just likely April, is six Joint In apply,ng PI Alb) it is not ---t to say \"P(B) 1 B PCB) prim, probability of B, Don't P(AIB) with PIA, B). ---e wit'mt In probability that match trials Bimial in the forget to do mity be O l. > O. make PMFs to l. PDFs integnte to 1. X2, F(X), r.v.s. PIX* X IX O), E(X), VM(X), X Fix) 2 -1 It not to F (X) is a It ](media/Cheatsheet-image8.png)\n![](media/Cheatsheet-image9.jpg)\n![Distributions in R Command help(distributions) dbinom (k , n, p) pbinom ( x , n , p) qbinom (a , n , p) rbinom (r p) dgeom(k , p) dhyper (k , v , b , n) dnbinom (k , r , p) dpois (k , r) dbeta (x, a, b) dchisq(x ,n) dexp (x , b) dgamma (x , a, r) dLnorm (X S) dnorm (x, m, s) dt (x dunÃ¥f (x, , b) What it does shows documentation on distributions PMF PC X = k) for X CDP r) for X ath quantile for X Bin(n, p) vector of r i.i.d. Bin(n, p) r.v.s PMF = k) for X Geom(p) PMF = k) for X HGeom(w, b, n) PMF = k) for X PMF = k) for X Pois(r) PDF f(x) for X b) PDF f(x) for X PDF f(Ã¦) for X Expo(b) PDF f(zr) for X Gamma(a, r) PDF for X PDF f(Ã¦) for X .-v PDF for X PDF TO) for X The table above gives R commands for working with various named distri butions. Commands analogous to pbinom, qbinom, and rbinom work for the other distributions in the table. For example, pnorm, qnorm, and rnorm can be used to get the CDP, quantiles, and random generation for the Normal. For the Multinomial, dmultinom can be used for calculating the joint P MF and rmultinom can be used for generating random vectors. For the Multivariate Normal, after installing and loading the mvtnorm package dmvnorm can be used for calculating the joint PDF and rmvnorm can be used for generating random vectors. ](media/Cheatsheet-image10.jpg)\n**Table fo Distributions**\n\n![Distribution Bernoulli Bern (p) Binomial Bin(n, p) Geometric Geom (p) Negative Binomial NBin(r, p) Hypergeometric HGeom(tv, b, n) Poisson Uniform Unif(a, b) Normal Exponential Gamma Gamma(a, A) Beta Beta(a, b) Log-Normal CM (g, 02) Chi-Square Student-t PMF/PDF and Support --- (k)pkqn-k --- k) = qkp )prqn ke {0, 1,2,...,n} x) 1 e (---00, 00) f (c) = X eÂ¯Ã…x 'X â‚¬ (O, 00) r(a) cr â‚¬ (O, 00) (log /(202) â‚¬ (O, 00) :r â‚¬ (O, 00) (---00, 00) Expected Value np q/p rq/p 2 1 1.1 = Variance npq q/p2 rq/p2 12 2 02 _ 1) 2n n if n 2 MGF q -F pet (q + pet)n I---qe qet < I messy Met---I) e messy doesn't exist (1 --- 2t)-n/2, t < 1/2 doesn't exist ](media/Cheatsheet-image11.jpg)"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/","title":"Intro - Syllabus"},"frontmatter":{"draft":false},"rawBody":"# Intro - Syllabus\n\nCreated: 2018-06-01 21:27:26 +0500\n\nModified: 2021-10-24 18:08:12 +0500\n\n---\n\n# Introduction to Probability - The Science of Uncertainty - MITx - 6.041x\n\nAn introduction to probabilistic models, including random processes and the basic elements of statistical inference.\n**Topics**\n-   The basic structure and elements of probabilistic models\n-   Random variables, their distributions, means, and variances\n-   Probabilistic calculations\n-   Inference methods (Bayesian Inference)\n-   Laws of large numbers and their applications\n-   Random processes (Poisson processes and Markov chains)\n-   multiple discrete or continuous random variables, expectations, and conditional distributions\n**Course Objectives**\n-   Probabilistic concepts, and language\n-   Common models\n-   Mathematical tools\n-   Intuition\n-   Acquire working knowledge of the subject\n**Syllabus**\n\n**Unit 1: Probability models and axioms**\n\nL1: Probability models and axioms\n-   **Sample Space**\n    -   **Sample Space**\n    -   **Probability Law**\n-   **Probability Axioms**\n    -   **Nonnegativity**\n    -   **Normalization**\n    -   **(Finite) Additivity**\n-   Properties of Probability\n-   Discrete example\n-   Continuous example\n-   Countable additivity\nMathematical Background: Set, Sequences, Limits and Series, (un)countable sets\n-   Sets\n-   **De Morgan's Law**\n-   Sequences and their limits\n-   When does a sequence converge\n-   Infinite series\n-   Geometric series\n-   Order of summation of series with multiple indices\n-   Countable and uncountable sets\n-   Set of real numbers is uncountable - **Cantor's diagonalization argument**\n**Solved problems**\n-   The probability of difference of two sets\n-   Genuises and chocolates\n-   Uniform probabilities on a square\n-   Bonferroni's inequality\n**Unit 2: Conditioning and independence**\n\nL2: Conditioning and Bayes' rule\n-   **Conditional Probability**\n    -   **Multiplication Rule**\n    -   **Total Probability Theorem**\n    -   **Bayes' Rule**\n-   A die roll example\n-   Conditional probabilities obey the same axioms\n-   A radar example: models based on conditional probabilities and three basic tools\n-   The multiplication rule\n-   Total probability theorem\n-   Bayes' rule\nL3: Independence\n-   Coin tossing example\n-   Independence of two events\n-   Independence of event complements\n-   Conditional independence\n-   Independence vs conditional independence\n-   Independence of a collection of events\n-   Independence vs pairwise independence\n-   Reliability\n-   The King's sibling\n**Solved problems**\n-   Conditional probability example\n-   A chess tournament puzzle\n-   A coin tossing puzzle\n-   The Monty Hall Problem\n-   A random walker\n-   Communication over a noisy channel\n-   Network reliability\n**Unit 3: Counting**\n\nL4: Counting\n-   The counting principle\n-   Die roll example\n-   Combinations\n-   Binomial Probabilities\n-   A coin tossing examples\n-   Partitions\n    -   Multinomial Coefficient\n    -   Binomial Coefficient\n-   Each person gets an ace\n**Solved Problems**\n-   The birthday paradox\n-   Rooks on a chessboard\n-   Hypergeometric probabilities\n-   Multinomial probabilities\n**Unit 4: Discrete random variables**\n\nL5: Probability mass functions and expectations\n-   Definition of random variables\n-   Probablity mass functions\n-   Bernoulli and indicator random variables\n-   Uniform random variables\n-   Binomial random variables\n-   Geometric random variables\n-   Expectation\n-   Elementary properties of expectation\n-   The expected value rule\n-   Linearity of expectations\nL6: Variance; Conditioning on an event; Multiple r.v.'s\n-   Variance\n-   Variance of the Bernoulli and the uniform\n-   Conditional PMFs and expectations given an event\n-   Total expectation theorem\n-   Geometric PMF, memorylessness, and expection\n-   Joint PMFs and the expected value rule\n-   Linearity of expectations and the mean of the binomial\nL7: Conditioning on a random variable; Independence of r.v.'s\n-   Conditional PMFs\n-   Conditional expectation and the total expectation theorem\n-   Independece of random variables\n-   Independence and expectations\n-   Independence, variances, and the binomial variance\n-   The hat problem\n**Unit 5: Continuous random variables**\n\nL8: Probability density functions\n-   Probability Density Functions (PDFs)\n-   Uniform and piecewise constant PDFs\n-   Means and variances\n-   Mean and variance of the uniform\n-   Exponential random variables\n-   Cumulative distribution functions\n-   Normal random variables\n-   Calculation of normal probabilities\nL9: Conditioning on an event; Multiple r.v.'s\n-   [Conditioning a continuous random variable on an event](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L09_1)\n-   [Conditioning example](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L09_2)\n-   [Memorylessness of the exponential PDF](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L09_3)\n-   [Total probability and expectation theorems](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L09_4)\n-   [Mixed random variables](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L09_5)\n-   [Joint PDFs](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L09_6)\n-   [From the joint to the marginal](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L09_7)\n-   [Continuous analogs of various properties](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L09_8)\n-   [Joint CDFs](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L09_9)\nL10: Conditioning on a random variable; Independence; Bayes' rule\n-   [Conditional PDFs](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L10_1)\n-   [Comments on conditional PDFs](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L10_2)\n-   [Total probability and total expectation theorems](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L10_3)\n-   [Independence](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L10_4)\n-   [Stick-breaking example](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L10_5)\n-   [Independent normals](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L10_6)\n-   [Bayes rule variations](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L10_7)\n-   [Mixed Bayes rule](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L10_8)\n-   [Detection of a binary signal](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L10_9)\n-   [Inference of the bias of a coin](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L10_10)\n**Unit 6: Further topics on random variables**\n\nL11: Derived distributions\n-   [The PMF of a function of a discrete r.v.](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L11_1)\n-   [A linear function of a continuous r.v.](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L11_2)\n-   [A linear function of a normal r.v.](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L11_3)\n-   [The PDF of a general function](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L11_4)\n-   [The monotonic case](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L11_5)\n-   [The intuition for the monotonic case](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L11_6)\n-   [A nonmonotonic example](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L11_7)\n-   [A function of multiple r.v.'s](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L11_8)\nL12: Sums of r.v.'s; Covariance and correlation\n-   [The sum of independent discrete random variables](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L12_1)\n-   [The sum of independent continuous r.v.'s](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L12_2)\n-   [The sum of independent normal r.v.'s](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L12_3)\n-   [Covariance](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L12_4)\n-   [Covariance properties](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L12_5)\n-   [The variance of the sum of r.v.'s](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L12_6)\n-   [The correlation coefficient](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L12_7)\n-   [Derivation of key properties of the correlation coefficient](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L12_8)\n-   [Interpreting the correlation coefficient](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L12_9)\n-   [Correlations matter](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L12_10)\nL13: Conditional expectation and variance revisited; Sum of a random number of r.v.'s\n-   [Conditional expectation as a r.v.](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L13_1)\n-   [The law of iterated expectations](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L13_2)\n-   [Stick-breaking revisited](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L13_3)\n-   [Forecast revisions](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L13_4)\n-   [The conditional variance](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L13_5)\n-   [Derivation of the law of total variance](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L13_6)\n-   [A simple example](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L13_7)\n-   [Section means and variances](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L13_8)\n-   [Mean of the sum of a random number of random variables](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L13_9)\n-   [Variance of the sum of a random number of random variables](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L13_10)\n**Unit 7: Bayesian inference**\n\nL14: Introduction to Bayesian inference\n-   [Overview of some application domains](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L14-1)\n-   [Types of inference problems](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L14-2)\n-   [The Bayesian inference framework](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L14-3)\n-   [Discrete parameter, discrete observation](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L14-4)\n-   [Discrete parameter, continuous observation](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L14-5)\n-   [Continuous parameter, continuous observation](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L14-6)\n-   [Inferring the unknown bias of a coin and the Beta distribution](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L14-7)\n-   [Inferring the unknown bias of a coin - point estimates](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L14-8)\nL15: Linear models with normal noise\n-   [Recognizing normal PDFs](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@15-1)\n-   [Normal unknown and additive noise](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@15-2)\n-   [The case of multiple observations](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@15-3)\n-   [The mean squared error](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@15-4)\n-   [Multiple parameters; trajectory estimation](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@15-5)\n-   [Linear normal models](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@15-6)\n-   [Trajectory estimation illustration](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@15-7)\nL16: Least mean squares (LMS) estimation\n-   [LMS estimation without any observations](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@16-1)\n-   [LMS estimation; single unknown and observation](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@16-2)\n-   [LMS performance evaluation](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@16-3)\n-   [Example: the LMS estimate](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@16-4)\n-   [Example: LMS performance evaluation](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@16-5)\n-   [The multidimensional case](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@16-6)\n-   [Properties of the LMS estimation error](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@16-7)\nL17: Linear least mean squares (LLMS) estimation\n-   [LLMS formulation](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@L17-1)\n-   [LLMS solution](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@17-2)\n-   [Remarks and the error variance](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@17-3)\n-   [LLMS example](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@17-4)\n-   [Coin bias example](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@17-5)\n-   [LLMS with multiple observations](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@17-6)\n-   [Example with multiple observations](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@17-7)\n-   [Choices in representing the observations](https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/jump_to/block-v1:MITx+6.041x_4+1T2017+type@vertical+block@17-8)\n**Unit 8: Limit theorems and classical statistics**\n\nL18: Inequalities, convergence, and the Weak Law of Large Numbers\nL19: The Central Limit Theorem (CLT)\nL20: An introduction to classical statistics\n**Unit 9: Bernoulli and Poisson processes**\n\nL21: The Bernoulli process\nL22: The Poisson process\nL23: More on the Poisson process\n**Unit 10: Markov chains**\n\nL24: Finite-state Markov chains\nL25: Steady-state behavior of Markov chains\nL26: Absorption probabilities and expected time to absorption\nCourse - <https://www.edx.org/course/introduction-probability-science-mitx-6-041x-2>\n\nSyllabus - <https://courses.edx.org/courses/course-v1:MITx+6.041x_4+1T2017/course>"},{"fields":{"slug":"/Mathematics/Probability/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-08-06 16:05:35 +0500\n\nModified: 2021-10-24 18:17:43 +0500\n\n---\n\n**Two types of probability**\n\n1.  Empirical probability\n\nSomething we observer in actual data\n\n2.  Theoratical probability\n\nMore of an ideal or truth out there that we can't directly see-   Binomial distributions\n\n![P ( 48 / , 2X åœ = 0 ã€ 95 ) = 50 ( çŸ¿ 8 ( 1 ä¸€ 0 Â· å‘ 2 48 P(data | success rate) ä¸€ P (success rate | data) 30 % 20 % 10 % 0 Binomial Distribution 5 10 15 20 25 30 35 40 45 = 0 6110 50 ](media/Intro-image1.png)-   Bayesian updating\n-   Probability density functions\n-   Beta distribution\n**Laplace's rule of succession**\n\nTo any count add 1 positive and 1 negative number and calculate the new probability.\n\nEx - Amazon reviews\n\n![1 1 1 2 9L7 % 10 0 of 10 4 5 9 2 9 2 % 48 out of 50 187 92 , 6 % 186 out of 200 202 ](media/Intro-image2.png)\nEx - 2 defects found in a test of 100 cars. What is the probability of getting a defect? (2% ?)\nIn[probability theory](https://en.wikipedia.org/wiki/Probability_theory), therule of successionis a formula introduced in the 18th century by[Pierre-Simon Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)in the course of treating the[sunrise problem](https://en.wikipedia.org/wiki/Sunrise_problem).\nThe formula is still used, particularly to estimate underlying probabilities when there are few observations, or for events that have not been observed to occur at all in (finite) sample data. Assigning events a[zero probability](https://en.wikipedia.org/wiki/Zero_probability)contravenes[Cromwell's rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule); such contravention can never be strictly justified in physical situations, albeit sometimes must be assumed in practice.\n<https://en.wikipedia.org/wiki/Rule_of_succession>\n\n## Probability part 1 - Rules and Patterns**\n\na.  Pareidolia - A product of our brains that causes us to see the pattern of a face in a non-face objects\n\nb.  Empirical probability\n\nc.  Theoratical probability\n\nd.  Conditional probability\n**Probability part 2 - Updating your beliefs with Bayes**\n-   Naive bayes filters\n\n![](media/Intro-image3.png)\n![](media/Intro-image4.png)-   Bayesian Statistics\n-   Simulation\n-   Law of large numbers (applies to almost any distribution as long as the distribution doesn't have an infinite variance)\n**Geometric Distributions and The Birthday Paradox**\n\na.  Geometric probability formula\n\nb.  Geometric distribution\n\nc.  Cumulative geometric distribution\n[Binomial distributions | Probabilities of probabilities, part 1](https://www.youtube.com/watch?v=8idr1WZ1A7Q)\n\n[Why \"probability of 0\" does not mean \"impossible\" | Probabilities of probabilities, part 2](https://www.youtube.com/watch?v=ZA4JkHKZM50)\n**Probability Density Function**\n\nIn[probability theory](https://en.wikipedia.org/wiki/Probability_theory), aprobability density function(PDF), ordensityof a[continuous random variable](https://en.wikipedia.org/wiki/Continuous_random_variable), is a[function](https://en.wikipedia.org/wiki/Function_(mathematics)), whose value at any given sample (or point) in the[sample space](https://en.wikipedia.org/wiki/Sample_space)(the set of possible values taken by the random variable) can be interpreted as providing arelative likelihoodthat the value of the random variable would equal that sample.\n<https://en.wikipedia.org/wiki/Probability_density_function>\n\n## Two views of probability**\n-   Frequentist view\n\nIn the frequentist view, probability measures the frequency with which an event will occur, if it is repeated many times\n-   In theory, you want to measure the frequency of an outcome if an experiment is repeated an infinite number of times\n-   While it is objective, it requires that an event be repeatable, and is thus narrow-   Subjectivist (Bayesian) view\n\nIn the subjectivist view, probability is the value that a rational agent would assign to the likelihood of an event happening\n-   You can assign probabilities to events, even if they are not repeatable\n-   Unlike the frequentist view, rational individuals can disagree on the probability of an event occuring\n**Cumulative and Conditional Probabilities**\n-   When you have sequential events, the cumulative probability measures the likelihood of a specific series of outcomes. To compute cumulative probabilities, each event in the sequence has to be independent of the others in that sequence\n    -   Thus you can estimate the probability that an investor will beat the market ten years in a row, even if he or she is picking stocks randomly\n-   A conditional probability measures the likelihood of an event or outcome, based upon the occurrence of a prior event or outcome\n    -   For instance, you can estimate the probability that the market will be up tomorrow, given that it was up today\n**Probability Rules**\n-   It is **bounded**: For any event X, 0 <= P(X) <= 1\n-   It **covers the universe of outcomes**: The sum of the probabilities of all possible outcomes is one\n-   The **Complement Rule** states that P(not X) = 1 - P(X)\n-   The **General Addition Rule** states that for any two events, P(X or Y) = P(X) + P(Y) - P(X and Y) is the probability of both X and Y occuring at the same time. If they are mutually exclusive, the rule simplifies to P(X or Y) = P(X) + P(Y)\n-   The **Multiplication Rule** states that for two independent events to occur together, P(X and Y) = P(X)*P(Y). If they are not independent, P(X and Y) = P(X) * P(Y). If they are not independent, P(X and Y) = P(X) * P(Y/X), where P(Y/X) is the conditional probability of Y happening, given that X has happened\n\n"},{"fields":{"slug":"/Mathematics/Probability/Monte-Carlo-Simulation/","title":"Monte Carlo Simulation"},"frontmatter":{"draft":false},"rawBody":"# Monte Carlo Simulation\n\nCreated: 2018-06-22 20:17:46 +0500\n\nModified: 2021-10-20 20:46:20 +0500\n\n---\n\nMonte Carlo simulation(also known as the Monte Carlo Method) lets you see all the possible outcomes of your decisions and assess the impact of risk, allowing for better decision making under uncertainty.\nMonte Carlo simulation performs risk analysis by building models of possible results by substituting a range of values---a probability distribution---for any factor that has inherent uncertainty. It then calculates results over and over, each time using a different set of random values from the probability functions. Depending upon the number of uncertainties and the ranges specified for them, a Monte Carlo simulation could involve thousands or tens of thousands of recalculations before it is complete. Monte Carlo simulation produces distributions of possible outcome values.\nBy using probability distributions, variables can have different probabilities of different outcomes occurring. Probability distributions are a much more realistic way of describing uncertainty in variables of a risk analysis.\nCommon probability distributions include:\n-   **Normal (Bell curve)**\n    The user simply defines the mean or expected value and a standard deviation to describe the variation about the mean. Values in the middle near the mean are most likely to occur. It is symmetric and describes many natural phenomena such as people's heights. Examples of variables described by normal distributions include inflation rates and energy prices.\n-   **Lognormal**\n    Values are positively skewed, not symmetric like a normal distribution. It is used to represent values that don't go below zero but have unlimited positive potential. Examples of variables described by lognormal distributions include real estate property values, stock prices, and oil reserves.\n-   **Uniform**\n    All values have an equal chance of occurring, and the user simply defines the minimum and maximum. Examples of variables that could be uniformly distributed include manufacturing costs or future sales revenues for a new product.\n-   **Triangular**\n    The user defines the minimum, most likely, and maximum values. Values around the most likely are more likely to occur. Variables that could be described by a triangular distribution include past sales history per unit of time and inventory levels.\n-   **PERT**\n    The user defines the minimum, most likely, and maximum values, just like the triangular distribution. Values around the most likely are more likely to occur. However values between the most likely and extremes are more likely to occur than the triangular; that is, the extremes are not as emphasized. An example of the use of a PERT distribution is to describe the duration of a task in a project management model.\n-   **Discrete**\n    The user defines specific values that may occur and the likelihood of each. An example might be the results of a lawsuit: 20% chance of positive verdict, 30% change of negative verdict, 40% chance of settlement, and 10% chance of mistrial.\nDuring a Monte Carlo simulation, values are sampled at random from the input probability distributions. Each set of samples is called an iteration, and the resulting outcome from that sample is recorded. Monte Carlo simulation does this hundreds or thousands of times, and the result is a probability distribution of possible outcomes. In this way, Monte Carlo simulation provides a much more comprehensive view of what may happen. It tells you not only what could happen, but how likely it is to happen.\nMonte Carlo simulation provides a number of advantages over deterministic, or \"single-point estimate\" analysis:\n-   **Probabilistic Results:** Results show not only what could happen, but how likely each outcome is.\n-   **Graphical Results:** Because of the data a Monte Carlo simulation generates, it's easy to create graphs of different outcomes and their chances of occurrence. This is important for communicating findings to other stakeholders.\n-   **Sensitivity Analysis:** With just a few cases, deterministic analysis makes it difficult to see which variables impact the outcome the most. In Monte Carlo simulation, it's easy to see which inputs had the biggest effect on bottom-line results.\n-   **Scenario Analysis:** In deterministic models, it's very difficult to model different combinations of values for different inputs to see the effects of truly different scenarios. Using Monte Carlo simulation, analysts can see exactly which inputs had which values together when certain outcomes occurred. This is invaluable for pursuing further analysis.\n-   **Correlation of Inputs:** In Monte Carlo simulation, it's possible to model interdependent relationships between input variables. It's important for accuracy to represent how, in reality, when some factors goes up, others go up or down accordingly.\nAn enhancement to Monte Carlo simulation is the use of Latin Hypercube sampling, which samples more accurately from the entire range of distribution functions.\n<http://www.palisade.com/risk/monte_carlo_simulation.asp>\n\n<https://en.wikipedia.org/wiki/Monte_Carlo_method>\n"},{"fields":{"slug":"/Mathematics/Probability/Normal-Distributions/","title":"Normal Distributions"},"frontmatter":{"draft":false},"rawBody":"# Normal Distributions\n\nCreated: 2018-07-02 00:56:44 +0500\n\nModified: 2021-10-20 20:45:37 +0500\n\n---\n\nIn[probability theory](https://en.wikipedia.org/wiki/Probability_theory), the**normal(orGaussianorGaussorLaplace--Gauss)**distributionis a very common[continuous probability distribution](https://en.wikipedia.org/wiki/Continuous_probability_distribution). Normal distributions are important in[statistics](https://en.wikipedia.org/wiki/Statistics)and are often used in the[natural](https://en.wikipedia.org/wiki/Natural_science)and[social sciences](https://en.wikipedia.org/wiki/Social_science)to represent real-valued[random variables](https://en.wikipedia.org/wiki/Random_variable)whose distributions are not known. A[random variable](https://en.wikipedia.org/wiki/Random_variable)with a Gaussian distribution is said to benormally distributedand is called anormal deviate.\nThe normal distribution is useful because of the[central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem). In its most general form, under some conditions (which include finite[variance](https://en.wikipedia.org/wiki/Variance)), it states that averages of samples of observations of[random variables](https://en.wikipedia.org/wiki/Random_variables)independently drawn from independent distributions[converge in distribution](https://en.wikipedia.org/wiki/Convergence_in_distribution)to the normal, that is, become normally distributed when the number of observations is sufficiently large. Physical quantities that are expected to be the sum of many independent processes (such as[measurement errors](https://en.wikipedia.org/wiki/Measurement_error)) often have distributions that are nearly normal.Moreover, many results and methods (such as[propagation of uncertainty](https://en.wikipedia.org/wiki/Propagation_of_uncertainty)and[least squares](https://en.wikipedia.org/wiki/Least_squares) parameter fitting) can be derived analytically in explicit form when the relevant variables are normally distributed.\n![ä¸€ suea A!l!qeq0Jd * 2 Â· 1 % 13 Â· 6 % * Â· 1 % ; * 34 Â· 1 % 13 Â· 6 % 2 +10 +20 + 30 P b * 68 Â· 2 % 99.7 % ](media/Normal-Distributions-image1.jpeg)-   99% of the data should fall in 3 standard deviations from the mean\n**Properties of normal distributions**\n-   **Dispersion:** The standard deviation in a normal distribution measures the spread around the mean, but with links to probabilities of a number occuring in the sample falling within or out of that spread\n-   **Skewness:** A normal distribution is symmetric and has no skewness\n-   **Kurtosis:** A variable that is normally distributed can take on values from minus infinity to plus infinity, but the likelihood of extreme values is contrained. The kurtosis for a normal distribution is three, which becomes the standard against which other distributions are measured\n\n![](media/Normal-Distributions-image2.jpeg)-   The t distribution\n\n![Z distribution (standard normal) t-distribution (n close to 30) t-distribution (n smaller than 30) ](media/Normal-Distributions-image3.jpg)-   The Symmetric Triangular Distribution\n\n![](media/Normal-Distributions-image4.jpg)\n-   A Uniform Distribution\n\n![mean: g = E X --- ](media/Normal-Distributions-image5.jpg)\n-   Negative Skew: Minimum Extreme Value\n\n![Weibull Distribution ](media/Normal-Distributions-image6.jpeg)\n-   Positive Skew: Log Normal Distribution\n\n![2 ë¶€ 0.02 0.14 0.26 0.38 0.50 0.62 0.74 0.86 1.10 1.22 1.34 1.46 1.70 1.82 1.94 2.06 2.18 2.30 2.42 2.54 2.66 2.78 ](media/Normal-Distributions-image7.jpeg)\n-   Thin tails and Fat tails\n\n![o Losses \"Normal\" Distribution: Low probability of events producing outsized market gains or losses \"Fat Tail\" Distribution: Greater probability of events producing outsized gains or losses Gains ](media/Normal-Distributions-image8.jpeg)\n**Measured with kurtosis**\n-   Kurtosis is a measure of the combined weights of the tails, relative to the rest of the distribution\n-   Most often, kurtosis is measured against the **normal distribution.** Pearson's kurtosis is the excess kurtosis over three\n    -   If the Pearson kurtosis is close to 0, then a normal distribution is often assumed. These are called mesokurtic distributions\n    -   If the Pearson kurtosis is less than 0, then the distribution has thin tails and is called a platykurtic distribution. (Uniform distribution is a good example)\n    -   If the Pearson kurtosis is greater than 0, then the distribution has fat tails and is called a leptokurtic distribution\n![ard symme e \"l s Birorul m nthe Only Mirimum è¡€ â–³ äºº è¾¶ ](media/Normal-Distributions-image9.jpeg)\n**Properties of Normal Distribution are as follows**\n\n1.  Unimodal-onemode\n\n2.  Symmetrical -left and right halves are mirror images\n\n3.  Bell-shaped-maximumheight(mode)atthemean\n\n4.  Mean,Mode,andMedianarealllocatedinthecenter\n\n5.  Asymptotic\n<https://en.wikipedia.org/wiki/Normal_distribution>\n"},{"fields":{"slug":"/Mathematics/Probability/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2018-06-16 20:31:57 +0500\n\nModified: 2021-10-24 21:00:08 +0500\n\n---\n\n**Quantile**\n\nEach of any set of values of a variate which divide a frequency distribution into equal groups, each containing the same fraction of the total population.\nIn statistics and probability**quantiles**are cut points dividing the range of a[probability distribution](https://en.wikipedia.org/wiki/Probability_distribution)into contiguous intervals with equal probabilities, or dividing the observations in a sample in the same way. There is one less quantile than the number of groups created. Thus[quartiles](https://en.wikipedia.org/wiki/Quartiles)are the three cut points that will divide a dataset into four equal-sized groups. Common quantiles have special names: for instance quartile, decile (creating 10 groups: see below for more). The groups created are termed halves, thirds, quarters, etc., though sometimes the terms for the quantile are used for the groups created, rather than for the cut points.\n**Decile**\n-   each of ten equal groups into which a population can be divided according to the distribution of values of a particular variable.\n-   each of the nine values of the random variable which divide a population into ten deciles.\n**Law of Large Numbers (LLN)**\n\nIn[probability theory](https://en.wikipedia.org/wiki/Probability_theory), the**law of large numbers**(**LLN**) is a[theorem](https://en.wikipedia.org/wiki/Theorem)that describes the result of performing the same experiment a large number of times. According to the law, the[average](https://en.wikipedia.org/wiki/Average)of the results obtained from a large number of trials should be close to the[expected value](https://en.wikipedia.org/wiki/Expected_value), and will tend to become closer as more trials are performed, if the observations are independent and identically distributed\nFew exceptions to the law of large numbers\n-   The first is if the sample observatoins are not independent or identically distributed\n-   The other is for distributions with fat tails (higher chance of extreme outcomes)\n**The Weak Law of Large Numbers**\n\nEven when observations are not independent or identically distributed, the sample average will approach the population average, albeit slower and only if the variance is finite\n**Kullback-Leibler Divergence (KL Divergence)**\n\nIn[mathematical statistics](https://en.wikipedia.org/wiki/Mathematical_statistics), the**Kullback--Leibler divergence**(also called**relative entropy**) is a measure of how one[probability distribution](https://en.wikipedia.org/wiki/Probability_distribution)diverges from a second, expected probability distribution.Applications include characterizing the[**relative**(Shannon)**entropy**](https://en.wikipedia.org/wiki/Entropy_(information_theory))in information systems,**randomness**in continuous[time-series](https://en.wikipedia.org/wiki/Time_series), and**information gain**when comparing statistical models of[inference](https://en.wikipedia.org/wiki/Inference). In contrast to[variation of information](https://en.wikipedia.org/wiki/Variation_of_information), it is a distribution-wise*asymmetric*measure and thus does not qualify as a statistical*metric*of spread. In the simple case, a Kullback--Leibler divergence of 0 indicates that we can expect similar, if not the same, behavior of two different distributions, while a Kullback--Leibler divergence of 1 indicates that the two distributions behave in such a different manner that the expectation given the first distribution approaches zero. In simplified terms, it is a measure of surprise, with diverse applications such as applied statistics,[fluid mechanics](https://en.wikipedia.org/wiki/Fluid_mechanics),[neuroscience](https://en.wikipedia.org/wiki/Neuroscience)and[machine learning](https://en.wikipedia.org/wiki/Machine_learning).\n**Continuous probability distribution**\n\nA**continuous probability distribution**is a probability distribution that has a cumulative distribution function that is continuous. Most often they are generated by having a[probability density function](https://en.wikipedia.org/wiki/Probability_density_function). Mathematicians call distributions with probability density functions**absolutely continuous**, since their[cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function)is[absolutely continuous](https://en.wikipedia.org/wiki/Absolute_continuity)with respect to the[Lebesgue measure](https://en.wikipedia.org/wiki/Lebesgue_measure)*Î»*. If the distribution of*X*is continuous, then*X*is called a**continuous random variable**. There are many examples of continuous probability distributions:[normal](https://en.wikipedia.org/wiki/Normal_distribution),[uniform](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)),[chi-squared](https://en.wikipedia.org/wiki/Chi-squared_distribution), and[others](https://en.wikipedia.org/wiki/List_of_probability_distributions#Continuous_distributions).\n**Entropy**\n\n\"The entropy of a random variable is a function which attempts to characterize the unpredictability of a random variable.\" ([Entropy and Mutual Information](https://people.cs.umass.edu/~elm/Teaching/Docs/mutInf.pdf))\" It is used for the construction of an automatic decision tree at each step of tree building; feature selection is done such using entropy criteria. Model selection is based on the principle of the maximum entropy, which states from the conflicting models, the one with the highest entropy is the best.\n\"If a random variable X takes on values in a set Ï‡={x1, x2,..., xn}, and is defined by a probability distribution P(X), then we will write the entropy of the random variable as,\" ([Entropy and Mutual Information](https://people.cs.umass.edu/~elm/Teaching/Docs/mutInf.pdf))\n\n![](media/Others-image1.png)\n\n\"If the log in the above equation is taken to be to the base 2, then the entropy is expressed in bits. If the log is taken to be the natural log, then the entropy is expressed in nats. More commonly, entropy is expressed in bits.\"\n**Softmax Function**\n\n**Softmax turns arbitrary real values into probabilities**\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics), thesoftmax function,also known as**softargmax**or**normalized exponential function**,is a function that takes as input a vector ofKreal numbers, and normalizes it into a[probability distribution](https://en.wikipedia.org/wiki/Probability_distribution)consisting of K probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the[interval](https://en.wikipedia.org/wiki/Interval_(mathematics))(0, 1), and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. Softmax is often used in[neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network), to map the non-normalized output of a network to a probability distribution over predicted output classes.\n![The standard (unit) softmax function a : eZi for i --- 1, K and z is defined by the formula e IRK ](media/Others-image2.png)\n\n<https://en.wikipedia.org/wiki/Softmax_function>\n\n## Probabilistic Tools**\n-   **Probit/Logit:** A probit/logit measures the likelihood or probability of an event happening, based upon observable variables\n    -   Example: Measuring the probability that a company will be acquired, given variables that you believe are correlated\n-   **Decision Trees:** When you have a series of discrete events that are sequential, a decision tree allows you to compute the likelihood of events happening, condidtional on events leading up to it\n    -   Example: The process by which an FDA drug works its way through the drug approval pipeline, from research to commercial development\n-   **Monte Carlo Simulations:** When you have multiple inputs that determine an output variable, a simulation allows you specify probabilistic distributions for each input variable and get a distribution for the output variable\n    -   Example: Valuing a company, given its cash flows, growth and risk characteristics\n![1. Probit & Logit Models: Techniques for estimating probabilities A probit model is a variant on the standard regression approach, with the key difference being that the dependent variable can take on only one of two values (binary) and a secondary one being a transformation of the linear model to ensure that the predicted value from the model is a number between 0 and 1 (limits for a probability estimate). n Example: You can estimate the probability of a hostile acquisition of a company based upon its size, past stock price performance and insider holdings. A logit model is a close relative to a probit, with the difference revolving around the function that they use to transform the standard linear model, with the logit model using a logistic distribution and the probit a normal distribution. ](media/Others-image3.jpeg)\n![A Logit Model: Predicting Bankruptcy In investing and valuation, one of the concerns that you face is that the business that you are investing in or valuing may go out of business. Predicting the probability of bankruptcy becomes a part of the challenge. There are logit models that have been used to predict bankruptcy, where You start with all firms at the start of the period n The dependent variable becomes the stand-in for whether a firm survives or goes bankrupt during the study period n The independent variables reflect what you believe are key drivers of bankruptcy (earnings level and volatility, debt level and payments due, market access to capital). n You build a logit model that will yield as output an equation that resembles a regression, but will yield a probability of bankruptcy. ](media/Others-image4.jpeg)\n![A Probit Model: Hostile Acquisitions While there are no easy pathways to making money, it seems clear that investors in companies that are targeted in acquisitions (especially hostile ones) earn high returns, but only if they invest before the event. There are probit models for predicting companies that will be targeted, and they involve: You start with all firms that publicly traded at the start of a period n The dependent variable becomes the stand-in for whether a firm is targeted in a hostile acquisition n The independent variables reflect what you believe are key drivers of hostile acquisitions, including poor stock price performance, lagging accounting returns and managers with little or no shareholdings. n You build a probit model that will yield as output an equation that resembles a regression, but will yield a probability of a hostile acquisition. ](media/Others-image5.jpeg)\n\n"},{"fields":{"slug":"/Mathematics/Probability/Probability-Distribution/","title":"Probability Distribution"},"frontmatter":{"draft":false},"rawBody":"# Probability Distribution\n\nCreated: 2018-08-09 00:19:01 +0500\n\nModified: 2021-10-20 20:45:15 +0500\n\n---\n\nIn[probability theory](https://en.wikipedia.org/wiki/Probability_theory)and[statistics](https://en.wikipedia.org/wiki/Statistics), a**probability distribution**is a mathematical function that provides the probabilities of occurrence of different possible outcomes in an[experiment](https://en.wikipedia.org/wiki/Experiment_(probability_theory)). In more technical terms, the probability distribution is a description of a[random](https://en.wikipedia.org/wiki/Randomness)phenomenon in terms of the[probabilities](https://en.wikipedia.org/wiki/Probability)of[events](https://en.wikipedia.org/wiki/Event_(probability_theory)). For instance, if the[random variable](https://en.wikipedia.org/wiki/Random_variable)*X*is used to denote the outcome of a coin toss (\"the experiment\"), then the probability distribution of*X*would take the value 0.5 for*X*= heads, and 0.5 for*X*= tails(assuming the coin is fair). Examples of random phenomena can include the results of an[experiment](https://en.wikipedia.org/wiki/Experiment_(probability_theory))or[survey](https://en.wikipedia.org/wiki/Survey_methodology).\n1.  Discrete Probability Distribution\n\nA**discrete probability distribution**(applicable to the scenarios where the set of possible outcomes is[discrete](https://en.wikipedia.org/wiki/Discrete_probability_distribution), such as a coin toss or a roll of dice) can be encoded by a discrete list of the probabilities of the outcomes, known as a[probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function).\n\n2.  Continuous Probability Distribution\n\nA**continuous probability distribution**(applicable to the scenarios where the set of possible outcomes can take on values in a continuous range (e.g. real numbers), such as the temperature on a given day) is typically described by[probability density functions](https://en.wikipedia.org/wiki/Probability_density_function)(with the probability of any individual outcome actually being 0).\nA probability distribution whose sample space is the set of real numbers is called[univariate](https://en.wikipedia.org/wiki/Univariate_distribution), while a distribution whose sample space is a[vector space](https://en.wikipedia.org/wiki/Vector_space)is called[multivariate](https://en.wikipedia.org/wiki/Multivariate_distribution). A univariate distribution gives the probabilities of a single[random variable](https://en.wikipedia.org/wiki/Random_variable)taking on various alternative values; a multivariate distribution (a[joint probability distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution)) gives the probabilities of a[random vector](https://en.wikipedia.org/wiki/Random_vector)-- a list of two or more random variables -- taking on various combinations of values. Important and commonly encountered univariate probability distributions include the[binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution), the[hypergeometric distribution](https://en.wikipedia.org/wiki/Hypergeometric_distribution), and the[normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). The[multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)is a commonly encountered multivariate distribution.\n**Types of distribution**\n\n1.  Uniform distribution\n\n2.  Normal distribution / Gaussian distribution\n\n3.  Gamma distribution\n\n4.  Exponential distribution\n\n5.  Poisson distribution\n\n6.  Binomial distribution\n\n7.  Bernoulli distribution\n<https://en.wikipedia.org/wiki/Probability_distribution>\n\n<https://www.datacamp.com/community/tutorials/probability-distributions-python>\n"},{"fields":{"slug":"/Mathematics/Probability/Random-Variables/","title":"Random Variables"},"frontmatter":{"draft":false},"rawBody":"# Random Variables\n\nCreated: 2018-08-09 00:14:23 +0500\n\nModified: 2021-10-03 19:53:34 +0500\n\n---\n\nIn[probability and statistics](https://en.wikipedia.org/wiki/Probability_and_statistics), a**random variable**,**random quantity**,**aleatory variable**, or**stochastic variable**is a variable whose possible values are [outcomes](https://en.wikipedia.org/wiki/Outcome_(probability))of a[random](https://en.wikipedia.org/wiki/Randomness)phenomenon.As a function, a random variable is required to be[measurable](https://en.wikipedia.org/wiki/Measurable_function), which rules out certain[pathological](https://en.wikipedia.org/wiki/Pathological_(mathematics))cases where the quantity which the random variable returns is infinitely sensitive to small changes in the outcome.\nA random variable is defined as a[function](https://en.wikipedia.org/wiki/Function_(mathematics))that maps the outcomes of unpredictable processes to numerical quantities (labels), typically[real numbers](https://en.wikipedia.org/wiki/Real_numbers). In this sense, it is a procedure for assigning a numerical quantity to each physical outcome. Contrary to its name, this procedure itself is neither random nor variable. Rather, the underlying process providing the input to this procedure yields random (possibly non-numerical) output that the procedure maps to a real-numbered value.\nA random variable has a[probability distribution](https://en.wikipedia.org/wiki/Probability_distribution), which specifies the probability that its value falls in any given interval.\n**Random variables**\n\n1.  Discrete random variables\n\n2.  Continuous random variables\n\n3.  Transforming random variables\n\n4.  Combining random variables5.  **Binomial random variables**\n\nBinomial mean and standard deviation formulas\n\n6.  Geometric random variables\n**Types of random variables**\n\n1.  **Discrete random variable**\n    -   Taking any of a specified finite or countable list of values, endowed with a[probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function)characteristic of the random variable's probability distribution.\n    -   Can be infinite but countable meaning integers\n    -   Example\n        -   Toss a coin\n        -   Year that a student was born (infinite but countable)\n        -   Number of ants born tomorrow in the universe\n        -   Winning time in men's 100 meter dash in 2016 olympics rounded up to nearest hundreds\n\n2.  **Continuous random variable**\n    -   Taking any numerical value in an interval or collection of intervals, via a[probability density function](https://en.wikipedia.org/wiki/Probability_density_function)that is characteristic of the random variable's probability distribution.\n    -   Should be real valued since more and more can be added in between two values\n    -   Example\n        -   Exact mass of a random animal selected a district zoo\n        -   Exact winning time in men's 100 meter dash in 2016 olympics\n3.  Mixture of discrete and continuous random variable\nTwo random variables with the same probability distribution can still differ in terms of their associations with, or[independence](https://en.wikipedia.org/wiki/Independence_(probability_theory))from, other random variables. The realizations of a random variable, that is, the results of randomly choosing values according to the variable's probability distribution function, are called[random variates](https://en.wikipedia.org/wiki/Random_variate).\n<https://en.wikipedia.org/wiki/Random_variable>\n"},{"fields":{"slug":"/Mathematics/Statistics/Bivariate-Analysis/","title":"Bivariate Analysis"},"frontmatter":{"draft":false},"rawBody":"# Bivariate Analysis\n\nCreated: 2018-07-31 23:49:59 +0500\n\nModified: 2021-10-06 21:55:20 +0500\n\n---\n\n**Bivariate analysis**is one of the simplest forms of[quantitative (statistical) analysis](https://en.wikipedia.org/wiki/Statistics).It involves the analysis of two[variables](https://en.wikipedia.org/wiki/Dependent_and_independent_variables)(often denoted as*X*,*Y*), for the purpose of determining the empirical relationship between them.\nBivariate analysis can be helpful in testing simple[hypotheses](https://en.wikipedia.org/wiki/Hypotheses)of[association](https://en.wikipedia.org/wiki/Association_(statistics)). Bivariate analysis can help determine to what extent it becomes easier to know and predict a value for one variable (possibly a[dependent variable](https://en.wikipedia.org/wiki/Dependent_variable)) if we know the value of the other variable (possibly the[independent variable](https://en.wikipedia.org/wiki/Independent_variable)) (see also[correlation](https://en.wikipedia.org/wiki/Correlation)and[simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression)).\nBivariate analysis can be contrasted with[univariate analysis](https://en.wikipedia.org/wiki/Univariate_analysis)in which only one variable is analysed.Like univariate analysis, bivariate analysis can be[descriptive](https://en.wikipedia.org/wiki/Descriptive_statistics)or[inferential](https://en.wikipedia.org/wiki/Inferential_statistics). It is the analysis of the relationship between the two variables.Bivariate analysis is a simple (two variable) special case of[multivariate analysis](https://en.wikipedia.org/wiki/Multivariate_analysis)(where multiple relations between multiple variables are examined simultaneously).\n\n![Old FaRhful Eruptions `ì¼ë— ,,ê°€ ](media/Bivariate-Analysis-image1.png)\n\nWaiting time between eruptions and the duration of the eruption for the[Old Faithful Geyser](https://en.wikipedia.org/wiki/Old_Faithful_Geyser)in[Yellowstone National Park](https://en.wikipedia.org/wiki/Yellowstone_National_Park),[Wyoming](https://en.wikipedia.org/wiki/Wyoming), USA. This[scatterplot](https://en.wikipedia.org/wiki/Scatterplot)suggests there are generally two \"types\" of eruptions: short-wait-short-duration, and long-wait-long-duration.\n**Graphical Methods**\n\n[Graphs](https://en.wikipedia.org/wiki/Statistical_graphics)that are appropriate for bivariate analysis depend on the type of variable. For two continuous variables, a[scatterplot](https://en.wikipedia.org/wiki/Scatterplot)is a common graph. When one variable is categorical and the other continuous, a[box plot](https://en.wikipedia.org/wiki/Box_plot)is common and when both are categorical a[mosaic plot](https://en.wikipedia.org/wiki/Mosaic_plot)is common. These graphs are part of[descriptive statistics](https://en.wikipedia.org/wiki/Descriptive_statistics).\n**Questions**\n\nSuppose we have two random variables, X and Y, which are bivariate normal. The correlation between them is -0.2. Let A = cX + Y and B = X + cY. For what values of c are A and B independent?\n<https://en.wikipedia.org/wiki/Bivariate_analysis>\n\n"},{"fields":{"slug":"/Mathematics/Statistics/Confidence-Intervals/","title":"Confidence Intervals"},"frontmatter":{"draft":false},"rawBody":"# Confidence Intervals\n\nCreated: 2021-09-15 23:22:16 +0500\n\nModified: 2021-09-15 23:22:31 +0500\n\n---\n\nDefinition of confidence intervals\n\nPopulation variance known, z-score\n\nConfidence Interval Clarifications\n\nStudent's T Distribution\n\nPopulation variance unknown, t-score\n\nMargin of error\n\nConfidence intervals. Two means. Dependent samples\n\nConfidence intervals. Two means. Independent samples (Part1)\n\nConfidence intervals. Two means. Independent samples (Part2)\n\nConfidence intervals. Two means. Independent samples (Part 3)\r\n"},{"fields":{"slug":"/Mathematics/Statistics/Correlation-and-Covariance/","title":"Correlation and Covariance"},"frontmatter":{"draft":false},"rawBody":"# Correlation and Covariance\n\nCreated: 2018-07-31 23:53:24 +0500\n\nModified: 2021-10-19 20:39:43 +0500\n\n---\n\n**Data Relationships**\n\nIn statistics, we can also examine how two or more variables are related to each other\n-   In some cases, we restrict ourselves to *chronicling whether that co-movement* is in the same direction, opposite directions and that there is no co-movement at all\n-   In others, we try to find *whether there is causation,* where one variable's movement is the cause of the other variable's movement\n-   Finally, if there is a link, we can use statistics *to try to predict a variable,* based upon observed values of another variable\n\nAs an example, consider the linkage between stock prices and interest rates\n-   We can measure whether interest rates and stock prices move together, in opposite directions and are unrelated\n-   We can also examine which direction the causation runs (do higher stock prices cause higher interest rates or vice versa)\n-   And if there is causation or a link, we can see if we use changes in interest rates can be use to predict changes in stock prices or vice versa\n**Exploring data linkages/relationships**\n-   When you have two or more data series, you can check for linkages between the data, i.e., whether the data move together (positive co-movement), move inversely (negative co-movement) or are unrelated (no co-movement)\n-   If there is a linkage, you can explore further to see if\n    -   **Time lags and leads:** Changes in one data variable lead changes in the other\n    -   **Correlation vs Causation:** Changes in one data variable are causing changes in the other\n    -   **Prediction:** You can predict one variable, using the other variable\n**Correlation**\n\nCorrelationis a bivariate analysis that measures the strength of association between two variables and the direction of the relationship.\nCorrelation coefficient measures how two variables move together.\nUnlike covariance, correlation could be thought of as a standardized measure. It is easy for us to interpret the result\nIn terms of the strength of relationship, the value of the **correlation coefficient** varies between +1 and -1. A value of Â± 1 indicates a perfect degree of association between the two variables. As the correlation coefficient value goes towards 0, the relationship between the two variables will be weaker. The direction of the relationship is indicated by the sign of the coefficient; a + sign indicates a positive relationship and a -- sign indicates a negative relationship.\nUsually, in statistics, we measure four types of correlations:\n-   [Pearson correlation](http://www.statisticssolutions.com/academic-solutions/membership-resources/member-profile/conducting-analyses-results/videos/pearson-correlation/)\n-   Kendall rank correlation\n-   Spearman correlation\n-   Point-Biserial correlation\n**General Correlation**\n\n1.  1, Positive Correlation - Both variables change in the same direction\n\n2.  0, Neutral Correlation - No relationship in the change of the variables, independent\n\n3.  -1, Negative Correlation - Variables change in opposite directions\n![Sxy Sample correlation formula: r = sxsy oxy Population correlation formula: p = In Excel, correlation is calculated by: =CORRELO ](media/Correlation-and-Covariance-image1.jpg)\nThe performance of some algorithms can deteriorate if two or more variables are tightly related, called **multicollinearity**. An example is linear regression, where one of the offending correlated variables should be removed in order to improve the skill of the model. We can quantify the relationship between samples of two variables using a statistical method called Pearson's correlation coefficient, named for the developer of the method, Karl Pearson.\n**Pearson r Correlation**\n\nPearsonrcorrelation is the most widely used correlation statistic to measure the degree of the relationship between linearly related variables. For example, in the stock market, if we want to measure how two stocks are related to each other, Pearsonrcorrelation is used to measure the degree of relationship between the two. The point-biserial correlation is conducted with the Pearson correlation formula except that one of the variables is dichotomous. The following formula is used to calculate the Pearsonrcorrelation:\n![](media/Correlation-and-Covariance-image2.jpg)\nr= Pearson r correlation coefficient\n\nN = number of observations\n\nâˆ‘xy = sum of the products of paired scores\n\nâˆ‘x = sum of x scores\n\nâˆ‘y = sum of y scores\n\nâˆ‘x2= sum of squared x scores\n\nâˆ‘y2= sum of squared y scores\n**Types of research questions a Pearson correlation can examine**\n-   Is there a statistically significant relationship between age, as measured in years, and height, measured in inches?\n-   Is there a relationship between temperature, measured in degrees Fahrenheit, and ice cream sales, measured by income?\n-   Is there a relationship between job satisfaction, as measured by the JSS, and income, measured in dollars?\n**Assumptions**\n\nFor the Pearsonrcorrelation, both variables should be normally distributed (normally distributed variables have a bell-shaped curve). Other assumptions include linearity and homoscedasticity. Linearity assumes a straight line relationship between each of the two variables and homoscedasticity assumes that data is equally distributed about the regression line.\n**Key Terms**\n\n**Effect size:**Cohen's standard may be used to evaluate the correlation coefficient to determine the strength of the relationship, or the effect size. Correlation coefficients between .10 and .29 represent a small association, coefficients between .30 and .49 represent a medium association, and coefficients of .50 and above represent a large association or relationship.\n**Continuous data:**Data that is interval or ratio level. This type of data possesses the properties of magnitude and equal intervals between adjacent units. Equal intervals between adjacent units means that there are equal amounts of the variable being measured between adjacent units on the scale. An example would be age. An increase in age from 21 to 22 would be the same as an increase in age from 60 to 61.\n**Kendall rank correlation**\n\nKendall rank correlation is a non-parametric test that measures the strength of dependence between two variables. If we consider two samples, a and b, where each sample size isn, we know that the total number of pairings with a b isn(n-1)/2. The following formula is used to calculate the value of Kendall rank correlation:\n\n![---n(n --- 1) ](media/Correlation-and-Covariance-image3.jpg)\n\nNc= number of concordant\n\nNd= Number of discordant\n**Key Terms**\n\n**Concordant:**Ordered in the same way.\n\n**Discordant:**Ordered differently.\n**Spearman rank correlation**\n\nSpearman rank correlation is a non-parametric test that is used to measure the degree of association between two variables. The Spearman rank correlation test does not carry any assumptions about the distribution of the data and is the appropriate correlation analysis when the variables are measured on a scale that is at least ordinal.\n\nThe following formula is used to calculate the Spearman rank correlation:\n![](media/Correlation-and-Covariance-image4.jpg)\nÏ= Spearman rank correlation\n\ndi= the difference between the ranks of corresponding variables\n\nn= number of observations\n**Types of research questions a Spearman Correlation can examine**\n-   Is there a statistically significant relationship between participants' level of education (high school, bachelor's, or graduate degree) and their starting salary?\n-   Is there a statistically significant relationship between horse's finishing position a race and horse's age?\n**Assumptions**\n\nThe assumptions of the Spearman correlation are that data must be at least ordinal and the scores on one variable must be monotonically related to the other variable.\n**Key Terms**\n\n**Effect size**\n\nCohen's standard may be used to evaluate the correlation coefficient to determine the strength of the relationship, or the effect size. Correlation coefficients between .10 and .29 represent a small association, coefficients between .30 and .49 represent a medium association, and coefficients of .50 and above represent a large association or relationship.\n**Ordinal data**\n\nIn an ordinal scale, the levels of a variable are ordered such that one level can be considered higher/lower than another. However, the magnitude of the difference between levels is not necessarily known. An example would be rank ordering levels of education. A graduate degree is higher than a bachelor's degree, and a bachelor's degree is higher than a high school diploma. However, we cannot quantify how much higher a graduate degree is compared to a bachelor's degree. We also cannot say that the difference in education between a graduate degree and a bachelor's degree is the same as the difference between a bachelor's degree and a high school diploma.\n<http://www.statisticssolutions.com/correlation-pearson-kendall-spearman>\n\n<https://machinelearningmastery.com/how-to-calculate-nonparametric-rank-correlation-in-python>\n\n<https://towardsdatascience.com/clearly-explained-pearson-v-s-spearman-correlation-coefficient-ada2f473b8>\n\n<https://www.freecodecamp.org/news/what-is-a-correlation-coefficient-r-value-in-statistics-explains>\n\n## Covariance**\n\nCovariance is a measure of the joint variability of two variables\nCovariance can take on values from -inf to +inf. This is the problem as it is very hard to put such numbers into perspective\n\n![Sample covariance formula: s n---l Population covariance formula: In Excel, the covariance is calculated by: Sample covariance: -COVARIANCE-SO Population covariance. â€¢ -COVARIANCE-PO ](media/Correlation-and-Covariance-image5.jpg)\nLike the correlation, a positive covariance indicates that two variables move together, a zero covariance that there is no relationship between the two variables, and a negative covariance an indication that they move in opposite directions\n**Correlation doesn't equal Causation**\n\na.  Scatterplot\n\nb.  Bivariate data\n\nc.  Regression line\n\nd.  Correlation\n\n    a.  Positive correlation\n\n    b.  Negative correlation\n\ne.  We use standard deviations to scale our correlations so that it always stays between -1 and 1. This is our correlation coefficient, r.\n\nf.  Squared correlation coefficient - r^2^ is always between 0 and 1, and tells us - in decimal form - how much of the variance in one variable is predicted by the other.\n\ng.  Correlation doesn't equal causation\n\nh.  Spurious correlations: If two variables are correlated, we are often tempted to create elaborate explanations for why. In many cases, that correlation can be suprious, with the two variables that are correlated both driven by a third and often unseen variable\n\ni.  Past vs Future: The data that we use to estimate correlation come from the past, and past correlation is not always a predictor of future correlation\n**A Best Fit Line**\n\n![The Intercept: In the scatter plot, the intercept is where the best-fit line crosses the Y axis. In simple terms, it is the value that the Y variable has when the X variable is zero. The-Rsguared: This measures the how well the line fits the data. If the line is a perfect fit (every point is on it), the R squared will be one (as will the correlation). The Best Fit Line: In an ordinary-least-squares (OLS) regression, the best fit line is the one that minimizes the squared distances from the line. 14.0 12 10 8 6 The-Slope: The slope of the best fit line is also a measure of how changes in the X variable show up as changes in the Y variable. When there is no (positive, negative) relationship between the variables, the slope will be zero (positive, negative). ](media/Correlation-and-Covariance-image6.jpeg)\n"},{"fields":{"slug":"/Mathematics/Statistics/Crash-Course-Statistics/","title":"Crash Course Statistics"},"frontmatter":{"draft":false},"rawBody":"# Crash Course Statistics\n\nCreated: 2018-08-04 14:23:55 +0500\n\nModified: 2022-05-03 19:42:27 +0500\n\n---\n\n[Statistics](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr)\n1.  **Z-scores and percentiles**\n\n    a.  Z-scores - Amount of standard deviation above or below the mean\n\n    b.  Z-scores in general allow us to compare things that are not on the same scale, as long as they're normally distributed.\n\n    c.  Percentiles - Tell you what percentage of the population has a score or value that's lower than yours.\n2.  **Confidence intervals**\n\n    a.  Confidence intervals - An estimated range of values that seem reasonable based on what we've observed. It's center is still the sample mean, but we've got some room on either side for our uncertainity.\n\n    b.  T-distribution - A continuous probability distribution that's unimodal; it's useful to represent sampling distributions\n\n    c.  Margin of Error - Just like a confidence interval, reflects the uncertainity that surrounds sample estimates of parameters like the mean or a proportion\n3.  **How p-value help us test hypothesis**\n\n    a.  Statistical inference\n\n    b.  Null Hypothesis Significance Testing (NHST)\n\nA form of the Reduction Ad Absurdum argument which tries to discredit an idea by assuming the idea is true, and then showing that if you make that assumption, something contradictory happens.\n\nEx- we use this to prove that there is no largest positive integer\n\n3.  **p-values (how rare your data is)**\n\nIf your p-value were 0.10 you could say that your sample is in the top 10% most extreme samples we'd expect to see based on the distribution of sample means.\n<https://www.freecodecamp.org/news/what-is-statistical-significance-p-value-defined-and-how-to-calculate-it>\n4.  **p-value problems**\n\n    a.  Alternative Distribution\n\n    b.  Alternative Hypothesis\n\n    c.  Failing to reject the null hypothesis doen't mean that there isn't an effect or relationship, it just means we didn't get enought evidence to say there definitely is one.\n5.  **Playing with power - p-value part 3**\n\n    a.  Type 1 Error -\n        -   rejecting the null, even if it's true. It can therefore only happen if the null is true\n        -   Essentially false positives: we think we've detected an effect, but there isn't one\n\n    b.  Type 2 Error -\n        -   False negatives: There was an effect, but we didn't see it.\n6.  **You know i am all about that Bayes**\n    -   **Bayesian statistics is about updating beliefs and can be used to test hypotheses.**\n    -   **Bayes' Factor - Represents the amount of information that we've learned about our hypotheses from the data**\n    -   **Bayesian hypothesis testing provides a structured way to quantify a logical process that we do everyday day, incorporating new events into the way we see the world**\n7.  **Bayes in science and everyday life**\n\n![Ù… Ù„Ø§) Ù¡Ù§Ù¡ POSTERIOR Ù„Ø¥N)0 Ù¡ PRIOR LIKELIHOOD P X)H) (( Ù… X) Ù‡Ù… NORMALIZATION ](media/Crash-Course-Statistics-image1.png)\n-   We will take our prior belief and update it with the likelihood of our evidence\n8.  **Test Statistics**\n    -   **Allow us to quantify how close things are to our expectations or theories**\n    -   **Sampling Distribution - The distribution of all possible group means for a certain sample size**\n    -   **Z-statistics around 1 or -1 tell us that the sample mean is the typical distance we'd expect a typical sample mean to be from the mean of the null hypothesis**\n    -   **We can use Z-tests to do hypothesis tests about means, differences between means, proportions, or even differences between proportions**\n    -   **Critical value - A value of our test statistic that marks the limits of our extreme values.**\n    -   **But sometimes, a Z-Test won't apply. And when that happens, we can use the T-Distribution and corresponding T-statistic to conduct a hypothesis test**\n    -   **Reject H~0~ if the p is too low**\n9.  **T-Tests: A matched pair made in heaven**\n\nT-Statistics tells us how many standard errors away from the mean our observed difference is.\n\n![OBSERVED mm - WHAT WE EXPECT IF THE WU IS TEST STATISTIC: AVERAGE VARIATION ](media/Crash-Course-Statistics-image2.png)\n10. **Degrees of Freedom and Effect Sizes**\n    -   **Degrees of freedom is the number of independent pieces of information we have.**\n\n![DISTRIBUTION COMPARISON Z-DISTRIBUTION (STANDARD NORMAL) T-DISTRIBUTION (N CLOSE TO 30) T-DISTRIBUTION (N SMALLER THAN 30) ](media/Crash-Course-Statistics-image3.png)-   Effect Size - Tells us how big the effect we observed was, compared to random variation.\n27. **Chi-Squared Tests**\n\nA**chi-squared test**, also written as***Ï‡*^2^test**, is any[statistical hypothesis test](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing)where the[sampling distribution](https://en.wikipedia.org/wiki/Sampling_distribution)of the test statistic is a[chi-squared distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution)when the[null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis)is true. Without other qualification, 'chi-squared test' often is used as short for[*Pearson's*chi-squared test](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test). The chi-squared test is used to determine whether there is a significant difference between the expected frequencies and the observed frequencies in one or more categories.\nTests of Independence - Look to see whether being a member of one category is independent of the other\n\nTest of Homogeneity - Looking at whether it's likely that different samples come from the same population\n\n![7VAdX3-Å’ sgo sgo 7VAdX3- ](media/Crash-Course-Statistics-image4.png)\n28. **P-Hacking**\n    -   **P-Hacking - Manipulating data or analyses to artificially get significant P-values**\n    -   **P-Hacking - When analyses are being chosen based on what makes the P-value significant, not what's the best analysis plan**\n    -   **Bonferroni Correction**\n30. **The Replication Crisis**\n    -   **Replication**\n    -   **Reproducible analysis**\n31. **Regression**\n    -   **General Linear Model (GLM)**\n        -   **Data = Model + Error**\n        -   **Model (Y = MX + B)**\n        -   **Error (A deviation from our model)**\n        -   **Inferences**\n        -   **Types of GLM**\n            -   **Regression model (Continuous variable)**\n                -   **One reason we are concerned with outliers in regression is that values that are really far away from the rest of our data can have undue influence on the regression line**\n                -   **The sum of all squared distances of each point to the line**\n                -   **Residuals**\n                -   **F-Test - Helps us quantify how well we think our data fit a distribution, like the null distribution**\n\n![DIISWIS-Y ](media/Crash-Course-Statistics-image5.png)\n-   Sum of Squares for Regression (SSR)\n-   Sum of Squares for Error (SSE)\n\n![TOTAL SUMS SQUARES 0 000 o SUMS SQUARES REGRESSION o o O 0 0 80 8 ssr= SSR + SSE DATA = MODEL + ERROR SUMS SQUARES FOR ERROR OBO SSR F-STATISTICâ€¢. SSE ](media/Crash-Course-Statistics-image6.png)\n-   Degrees of Freedom - represents the amount of independent information we have\n32. **ANOVA (Analysis of Variance)**\n\nAllows us to compare three or more groups for statistical significance\nOmnibus Test - A test that contains many items or groups (Ex - F-Test)\n![ANOVA TABLE MODEL RESIDUAL 3.0288 MS MODEL MS ERROR 0.000829 240 SUMS OF SQUARES 832.3 5995.8 (Xi-P)2 MEAN SQUARE 75.667 24.983 ](media/Crash-Course-Statistics-image7.png)\n33. **ANOVA Part 2: Dealing with Intersectional Groups**\n    -   **Factorial ANOVA - A factorial ANOVA does almost exactly what a regular ANOVA does: it takes the overall variation - or sums of squares - and portions it out into different categories.**\n    -   **Example - Does car color and manufacture effect the overall price.**\n    -   **Sums of squares total**\n\n![SUMS OF SQUARES TOTAL MEAN OVERALL PRICE ](media/Crash-Course-Statistics-image8.png)\n![PRICE = BASELINE+ x MANUFACTURE + x COLOR + ERROR DATA = MODEL + ERROR ](media/Crash-Course-Statistics-image9.png)\n![F-STATISTIC VARIABLE VARIABLE ERROR ERROR VARIABLE l',ts ERROR ](media/Crash-Course-Statistics-image10.png)\n\nSS - Sum of Squares\n\nMS - Mean squares\n-   ANOVA Table\n\n![ANOVA TABLE VARIABLE COLOR MANUFACTURER ERROR 4 3 92 49396875 1330110663 MS 12349219 391952170 14457725 0.8542 27.1102 0.4947 1.186 x 10-12 ](media/Crash-Course-Statistics-image11.png)\nBut just like with our T-Tests, we know that a significant F-Test only means that the result is statistically significant. It doesn't always mean it's practically significant to you.\n![CALCULATING ETA SQUARED 272 = EFFECT TOTAL ERROR 2555364047 ](media/Crash-Course-Statistics-image12.png)-   ETA squared tells you the proportion of total variation tha's accounted for by your specific variable\n-   Interaction\n-   Two-way ANOVA\n\n![iou[ 311-108 NV) 183dX3 N33Md38 NOIDVH3lNl 311108 1i3dX3xX3d=S9Nl1Vi+d ) 311108 X3Ù‡3Ù†! 31LL08+XlNId( Â« ](media/Crash-Course-Statistics-image13.png)\n![SUMS OF SQUARES BETWEEN GROUPS oo (DO O EXPERTISE --- o EXPERT - FANCY BOTTLE o EXPERT - PLAIN BOTTLE o NOVICE - FANCY BOTTLE o NOVICE - PLAIN BOTTLE BOTTLE:EXPERT = BETWEEN --- BOTTLE ](media/Crash-Course-Statistics-image14.png)\nSSG - Sum of Squares between Groups\n\nSSG tell us how much variation can be explained by coming from one of the four possible combination of olive oil expertise and bottle type\n\n![INTERACTION PLOT PLAIN FANCY ](media/Crash-Course-Statistics-image15.png)\n![INTERACTION PLOT PLAIN â€¢ EXPERT â€¢ NOVICE FANCY ](media/Crash-Course-Statistics-image16.png)\n![ANOVA Î¤Î‘Î’Î™Î• VARlABlE EXPERTISE Î’ÎŸÎ¤Î¤Î™Î• Î¤Î¥Î¡Î• EXPERTISE : Î’ÎŸÎ¤Î¤Î™Î• Î¤Î¥Î¡Î• ERROR 1.338 2.1622 7.9999 0.251743 0.145567 0.005979 76 477.6 774.2 2864.3 27211.4 477.6 774.2 2864.3 358.04 ](media/Crash-Course-Statistics-image17.png)-   Main Effects\n\n![INTERACTION PLOT â€¢ GENE Y â€¢ NO GENE Y MED NO MED ](media/Crash-Course-Statistics-image18.png)\n34. **Fitting models is like Tetris**\n    -   **ANCOVA - Analysis of Covariance**\n    -   **Repeated measures ANOVA**\n    -   **ANOVA - Allow us to analyze the effect of variables with two or more groups on continuos variables**\n    -   **Regressions - Allow us to analyze two continuous variables**\n    -   **General Linear Model - Explain the data we observe by building a model to predict that data, and then keeping track of how close the prediction is.**\n    -   **We can combine ANOVA and regression to give us more flexible ANCOVA**\n    -   **Example - we want to calculate the amount of anaesthesia needed for red heads and non red heads. We have two categorical variables (red and non-red) and a continuous variable i.e. weight. Because weight plays a important role in the amount of anaesthesia used.**\n    -   **Covariates - Continuous variables that are used to explain our outcome variable.**\n    -   **Repeated measures ANOVA - Asks whether there's a significant difference between two or more groups or conditions**\n35. **War**\n    -   **Bayesian search theory**\n    -   **For finding the number of tanks based on number of observations**\n        -   **Max = M + (M/N) + 1**\n        -   **M is max serial number observed**\n        -   **N is number of observations**\n41. **When predictions fail**\n-   Logistic Curve\n\n![carrying capacity; exponential versus logistic population growth](media/Crash-Course-Statistics-image19.jpg)\n\ncarrying capacity; exponential versus logistic population growth\nIn an ideal environment (one that has no limiting factors) populations grow at an exponential rate. The growth curve of these populations is smooth and becomes increasingly steep over time (left). However, for all populations, exponential growth is curtailed by factors such as limitations in food, competition for other resources, or disease. As competition increases and resources become increasingly scarce, populations reach the carrying capacity (K) of their environment, causing their growth rate to slow nearly to zero. This produces an S-shaped curve of population growth known as the logistic curve (right).-   Inflection Point\n\nA point on a curve at which the curve changes from being concave (concave downward) to convex (concave upward), or vice versa.\n\n"},{"fields":{"slug":"/Mathematics/Statistics/Descriptive-Statistics/","title":"Descriptive Statistics"},"frontmatter":{"draft":false},"rawBody":"# Descriptive Statistics\n\nCreated: 2021-09-15 23:20:16 +0500\n\nModified: 2021-10-04 22:38:47 +0500\n\n---\n\nLevels of measurement\n\nCategorical variables. Visualization techniques\n\nNumerical variables. Frequency distribution table\n\nThe histogram\n\nCross table and scatter plot\n\nMean, median, mode\n\nSkewness\n\nVariance\n\nStandard deviation and coefficient of variation\n**Descriptive Statistics (Summarizing Data)**\n-   Itrefer to methods for summarizing raw observations into information that we can understand and share.\n-   Usually include things like where the middle of the data is - what statisticians call measure of central tendency - and measures of how spread out the data are\n\n![EXPLORING DATA DESCRlPTlVE STATlSTlCS ÎŸÎ‘Î¤Î‘ ÎœÎ¿Î½ STANDARD ÎŸÎ•'.Î™Î™Î‘Î¤Î™ÎŸÎ RANGE (ÎœÎ™Î, ÎœÎ‘Î§) lNTERQl.JARTlLE RANGE DATA ÎœÎŸÎ™- MEAN MEOlAN MOOE CATEGORlCAL ÎŸÎ‘Î¤Î‘ FREQlJENCY PERCENTAGE (ROWS, ÎŸÎ‘ TOTAL) Î½Î¹Ï‚Ï…Î‘Î™Î™Î–Î‘Î¤Î™ÎŸÎ CONTlNUOUS DATA HlSTOG2AM BOX & WHlSKERS Î¡Î™ÎŸÎ¤ ÎŸÎŸÎ¤ Î¡Î™ÎŸÎ¤ (CM b. against categorical data) SCATTER Î¡Î™ÎŸÎ¤ (two CATEGORlCAL DATA BAR CHART CLlJSTERED BAR CHART (2 categorlcal varlables) BAR CHART WlTH ERRORS ](media/Descriptive-Statistics-image1.png)\n-   Descriptive statistics are mostly used to summarize values and may not be sufficient to make conclusions about the entire population or to infer or predict data patterns\n**Graphs and tables that represent categorical variables**\n\n![Frequency distribution tables Frequency 100 Audi BMW Mercedes Total 124 98 113 Bar charts Sales BMW Mercedes Bar charts are very common. Each bar represents a category. On the y-axis we have the absolute frequency. Pie charts Merced BMW 29% Pie charts are used when we want to see the share of an item as a part of the total. Market share is almost always represented with a pie chart. E Frequency distribution tables show the category and its corresponding frequency. absolute Pareto diagrams Sales 100% 50 Audi Mercedes BMW The Pareto diagram is a special type of bar chart where the categories are shown in descending order of frequency, and a separate curve shows the cumulative frequency. ](media/Descriptive-Statistics-image2.jpg)\n\n**Excel formulas**\n\n![Frequency distribution tables Frequency Audi BMW Mercedes Total 124 98 113 Bar charts Sales 100 50 BMW Bar charts are Me rcedes also called Pie charts Merced Aud i 37% BMW 29% Pie charts are created in the following way: Choose your data, Insert -> Charts -> Pie chart Pareto diagrams 80% 100 60% 0 Audi Mercedes BMW Next slide. In Excel, we can either hard code the frequencies or count them with a count function. This will come up later on. Total formula: -SUMO Il clustered column charts in Excel. Choose your data, Insert -> Charts -> Clustered column or Bar chart. l] ](media/Descriptive-Statistics-image3.jpg)\n**Pareto diagrams in Excel**\n\n![1] Creating Pareto diagrams in Excel: Sales 90% 80% 50% 30% 20% 10% 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. Order the data in your frequency distribution table in descending order. Create a bar chart. Add a column in your frequency distribution table that measures the cumulative frequency. Select the plot area of the chart in Excel and Right click Choose Select series. Click Add Series name doesn't matter. You can put 'Line' For Series values choose the cells that refer to the cumulative frequency. Click OK. You should see two side-by-side bars Select the plot area of the chart and Right click. Choose Change Chart Type. Select Combo. Choose the type of representation from the dropdown list. Your initial categories should be 'Clustered Column'. Change the second series, that you called 'Line', to 'Line'. Done. ](media/Descriptive-Statistics-image4.jpg)\n\n**Numerical variables. Frequency distribution table and histogram**\n\n![](media/Descriptive-Statistics-image5.jpg)\n![Histogram (41, 611 Histograms are the one of the most common ways to represent numerical data. Each bar has width equal to the width of the interval. The bars are touching as there is continuation between intervals: where one ends -> the other begins. (21, 411 11, 211 (41, 611 (61, 811 (81, 1011 Histogram relative frequency 2. 3. Creating a histogram in Excel: Choose your data Insert -> Charts -> Histogram To change the number of bins (intervals): Select the x-axis 2. Click Chart Tools - > Format -> Axis options 3. You can select the bin width (interval width), number of bins, etc. (21, 411 (61, 811 (81, 1011 ](media/Descriptive-Statistics-image6.jpg)\n**Graphs and tables for relationships between variables. Cross tables**\n\n![Type of investment  Investor Real E swe Of Investor Investor A Investor B Investor C Tota 29 21 210 91 Investor A \"westor 8 Investor C Total Cross tables (or contingency tables) are used to represent categorical variables. One set of categories is labeling the rows and another is labeling the columns. We then fill in the table with the applicable data. It is a good idea to calculate the totals. Sometimes, these tables are constructed with the relative frequencies as shown in the table below. 010 010 017 037 004 0 003 0 016 042 023 t A common way to represent the data from a cross table is by using a side-by-side bar chart. Creating a side-by-side chart in Excel: 1. Choose your data 2. Insert -> Charts -> Clustered Column Selecting more than one series ( groups of data ) will prompt Excel to create a side-by-side bar (column) chart. automatically Side-by-side bar chart Investor A â€¢ Stocks In vestor B â€¢ Bonds â€¢ Real Estate ](media/Descriptive-Statistics-image7.jpg)\n\n**Scatter plots**\n\n![100 21.5 200 40 300 60 400 80 500 120 600 140 700 160 800 180 When we want to represent two numerical variables on the same graph, we usually use a scatter plot. Scatter plots are useful especially later on, when we talk about regression analysis, as they help us detect patterns (linearity, homoscedasticity). Scatter plots usually represent lots and lots of data. Typically, we are not interested in single observations, but rather in the structure of the dataset. Creating a scatter plot in Excel: 1. Choose the two datasets you want to plot. 2. Insert Charts -Y Scatter A scatter plot that looks in the following way (down) represents data that doesn't have a pattern. Completely vertical 'forms' show no association. Conversely, the plot above shows a linear pattern, meaning that the observations move together. 100 ](media/Descriptive-Statistics-image8.jpg)\n**Levels of measurement**\n\n![Levels of measurement Qualitative Nominal Ordinal Quantitative Interval Ratio There are two qualitative levels: nominal and ordinal. The nominal level represents categories that cannot be put in any order, while ordinal represents categories that can be ordered. Examples: Nominal: four seasons (winter, spring, summer, autumn) Ordinal: rating your meal (disgusting, unappetizing, neutral, tasty, and delicious) There are two quantitative levels: interval and ratio. They both represent \"numbers\", however, ratios have a true zero, while intervals don't. Examples: Interval: degrees Celsius and Fahrenheit Ratio: degrees Kelvin, length ](media/Descriptive-Statistics-image9.jpg)\n**When faced with sample data, data descriptives try to summarize the data with metrics. Those metrics can include**\n-   **Measures of location (Measures of centrality):** Measures of location try to identify the number or numbers around which the data is centered or is most likely to take\n-   **Measures of dispersion:** Measures of dispersion measure how much divergence there is on a data item, across a sample\n-   **Measures of skewness (Measures of symmetry):** Measures of skewness look at whether how symmetric or asymmetric the data is around the central value\n-   **Measures of extremes:** Measure the likelihood of extreme values (the fatness of distributional tails)\n**Mean, Median, and Mode: Measure of Central Tendency**\n\n![Mean The mean is the most widely spread measure of central tendency. It is the simple average of the dataset. Note: easily affected by outliers The formula to calculate the mean is: or XI X2 X3 XN---I XN Il In Excel, the mean is calculated by: -AVERAGE() Median The median is the midpoint of the ordered dataset. It is not as popular as the mean, but is often used in academia and data science. That is since it is not affected by outliers. In an ordered dataset, the median is the number at position n+l 2 If this position is not a whole number, it, the median is the simple average of the two numbers at positions closest to the calculated value. r In Excel, the median is calculated by: Mode The mode is the value that occurs most often. A dataset can have O modes, 1 mode or multiple modes. The mode is calculated simply by finding the value with the highest frequency. Il In Excel, the mode is calculated by: =MODE.SNGLO -> returns one mode =MODE.MULT() -> returns an array with the modes. It is used when we have more than 1 mode. ](media/Descriptive-Statistics-image10.jpg)\na.  Normal distribution\n\nb.  Mode - The value that appears most in our dataset\n\nc.  Bimodal Data is an example of Multimodal data which has many values that are similarly common. Usually multimodal data results from two or more underlying groups all being measured together\n\nd.  The fact that the median and mean are the same tells us that the distibution is symmetric: that there's equal amount of data on either side of the median, and equal amounts on either side of the mean\n\ne.  If the mean and median are different that the distribution is skewed\n**Measures of Spread**\n\na.  Range takes the largest number in our dataset and subtracts the smallest number in the set to give us the distance between the two extremes. The larger the distance, the more spread out our data is.\n\nb.  Interquartile range - Doesn't consider extreme values, the IQR looks at the spread of the middle 50% of your data\n\n![MEDIAN MEDIAN 4 5166 MEDIAN ](media/Descriptive-Statistics-image11.jpg)\n**Measures of Dispersion**\n-   **Ranges and variants:** With the range, you look at the difference between the highers and lowest values for a variable, across a sample. In variants, you can look at the difference between the first and third quartile of the data (interquartile range) or between the first and the ninth decile of the data\n-   **Standard deviation/Variance:** With the standard deviation, you estimate the difference between each value of a variable and its mean and arrive at a measure of dispersion\n-   **Coefficient of variation:** With the coefficient of variation, you divide the standard deviation of a data series by its mean, to provide a measure of comparision with data series of different levels\n**Variance and standard deviation**\n\n![Point 1 Point 2 Point 3 Mean Point 4 Point 5 Point 6 Variance and standard deviation measure the dispersion of a set of data points around its mean value. There are different formulas for population and sample variance & standard deviation. This is due to the fact that the sample formulas are the unbiased estimators of the population formulas. More on the mathematics behind it. Sample variance formula: Population variance formula: Sample standard deviation formula: Population standard deviation formula: s 2 n---l Calculating variance in Excel: Sample variance: = VAR.SO Population variance: =VAR.P() Sample standard deviation : = STDEV.SO Population standard deviation: =STDEV.PO ](media/Descriptive-Statistics-image12.jpg)\n**Standard Deviation/Variance & Coefficient of Variation**\n-   The standard deviation is a measure of dispersion which uses all of the observations, computes the difference between each one and the mean, and summing up the squared differences:\n-   Variance\n\n![Variance = ](media/Descriptive-Statistics-image13.jpg)\n\nWhere Î¼ is the average across the n observations, and X~j~ is the value that of the j^th^ observation; The sum of the squared differences is divided by n, if your data comprises the entire population, or n-1, if it is a sample\n-   The standard deviation is the square root of the variance\n-   When there is more divergence from the mean, the standard deviation will be higher, but it will be in the same units as the base data. Thus, if the base data is in dollars, the standard deviation will be in dollars, and if it is in percent, it will be in percent\n-   Since standard deviations cannot be compared across two samples with different units or levels, you can compute a standidized version of the measure\n    -   **Coefficient of Variation = Std deviation in value / average value**\n**Standard Deviation and Standard Error**\n-   The standard deviation measures the amount of variability, or dispersion from the individual data values to the mean\n-   The standard error of the mean measures how far the sample mean (average) of the data is likely to be from the true population mean. It is computed as follows\n\n![Standard Deviation Standard Error Number of observations in sample ](media/Descriptive-Statistics-image14.jpg)\n-   As sample size increases, there will be no discernible effect on the former, but the latter will always decrease\n-   When you are extrapolating from sample findings to the population, the standard errors become useful because they can be used to provide ranges for estimates. Thus, if your averate is Î¼, and your standard error is SE, drawing on the central limit theorem, you can estimate the population mean:\n    -   With 67% confidence: Î¼ Â± SE\n    -   With 95% confidence: Î¼ Â± 2*SE\n**Measures of Asymmetry**\n-   When the data is symmetric, the deviations from the mean fall equally or roughly equally on either side of the mean\n-   When the data is asymmetric, deviations on one side of the mean are much more pronounced than deviations on the other side. This deviation is measured with skewness\n    -   If the deviations are more pronounced/extreme for the observations that have values higher than the average, the distribution is positively skewed\n    -   If the deviations are more pronounced/extreme for the observations that have values lower than the average, the distribution is negatively skewed\n-   When data is asymmetric, the average will be skewed in the same direction as the asymmetry, and in some cases the skew can be large enough to make it unrepresentative of the sample\n**Skewness**\n\n![Median Mean Mode Calculating skewness in Excel: Skewness is a measure of asymmetry that indicates whether the observations in a dataset are concentrated on one side. Right (positive) skewness looks like the one in the graph. It means that the outliers are to the right (long tail to the right). Left (negative) skewness means that the outliers are to the left. Usually, you will use software to calculate skewness. 1 Formula to calculate skewness: 1 (x --- .02 ](media/Descriptive-Statistics-image15.jpg)\n**Measures of extreme values**\n-   You can measure of how much, and how frequently, data takes extreme values, relative to its central value. That measure is called kurtosis\n-   While variance and kurtosis are both affected by the presence (or absence) of extreme values, they measure different phenomenon\n    -   You can have high variance and low kurtosis, low variance and high kurtosis or high variance and high kurtosis\n    -   Distributions that have more frequent occurrences of extreme values are referred to as having fat tails or **leptokurtic.** Distribution that have less frequent occurrences of extreme values are referred to as **platykurtic**\n**Randomness**\n\na.  Expectation\n\nb.  Variance (2nd moment)\n\nc.  Skeweness (3rd moment)\n\nd.  Kurtosis (4th moment)\n\ne.  The mean of the sum is the sum of the means\n\nf.  The variance of the sum of the two independent variables is the sum of their variances.\n"},{"fields":{"slug":"/Mathematics/Statistics/Discriminant-Analysis/","title":"Discriminant Analysis"},"frontmatter":{"draft":false},"rawBody":"# Discriminant Analysis\n\nCreated: 2018-09-04 23:11:39 +0500\n\nModified: 2021-10-04 23:25:45 +0500\n\n---\n\nDuring a study, there are often questions that strike the researcher that must be answered. These questions include questions like 'are the groups different?', 'on what variables, are the groups most different?', 'can one predict which group a person belongs to using such variables?' etc. In answering such questions, discriminant analysis is quite helpful.\nDiscriminant analysis is a technique that is used by the researcher to analyze the research data when the criterion or the dependent variable is categorical and the predictor or the independent variable is interval in nature. The term categorical variable means that the dependent variable is divided into a number of categories. For example, three brands of computers, Computer A, Computer B and Computer C can be the categorical dependent variable.\nThe objective of discriminant analysis is to develop discriminant functions that are nothing but the linear combination of independent variables that will discriminate between the categories of the dependent variable in a perfect manner. It enables the researcher to examine whether significant differences exist among the groups, in terms of the predictor variables. It also evaluates the accuracy of the classification.\n**Discriminant analysis is described by the number of categories that is possessed by the dependent variable.**\nAs in statistics, everything is assumed up until infinity, so in this case, when the dependent variable has two categories, then the type used is two-group discriminant analysis. If the dependent variable has three or more than three categories, then the type used is multiple discriminant analysis. The major distinction to the types of discriminant analysis is that for a two group, it is possible to derive only one discriminant function. On the other hand, in the case of multiple discriminant analysis, more than one discriminant function can be computed.\nThere are many examples that can explain when discriminant analysis fits. It can be used to know whether heavy, medium and light users of soft drinks are different in terms of their consumption of frozen foods. In the field of psychology, it can be used to differentiate between the price sensitive and non price sensitive buyers of groceries in terms of their psychological attributes or characteristics. In the field of business, it can be used to understand the characteristics or the attributes of a customer possessing store loyalty and a customer who does not have store loyalty.\nFor a researcher, it is important to understand the relationship of discriminant analysis with Regression and Analysis of Variance (ANOVA) which has many similarities and differences. Often we can find similarities and differences with the people we come across. Similarly, there are some similarities and differences with discriminant analysis along with two other procedures. The similarity is that the number of dependent variables is one in discriminant analysis and in the other two procedures, the number of independent variables are multiple in discriminant analysis. The difference is categorical or binary in discriminant analysis, but metric in the other two procedures. The nature of the independent variables is categorical in Analysis of Variance (ANOVA), but metric in regression and discriminant analysis.\nThe steps involved in conducting discriminant analysis are as follows:\n\nâ€¢ The problem is formulated before conducting.\n\nâ€¢ The discriminant function coefficients are estimated.\n\nâ€¢ The next step is the determination of the significance of these discriminant functions.\n\nâ€¢ One must interpret the results obtained.\n\nâ€¢ The last and the most important step is to assess the validity.\n<http://www.statisticssolutions.com/discriminant-analysis>\n"},{"fields":{"slug":"/Mathematics/Statistics/Estimation-Statistics/","title":"Estimation Statistics"},"frontmatter":{"draft":false},"rawBody":"# Estimation Statistics\n\nCreated: 2019-07-18 19:45:27 +0500\n\nModified: 2021-10-02 13:38:42 +0500\n\n---\n\nEstimation statistics may be used as an alternative to statistical hypothesis tests. Statistical hypothesis tests can be used to indicate whether the difference between two samples is due to random chance, but cannot comment on the size of the difference. A group of methods referred to as new statistics are seeing increased use instead of or in addition to p-values in order to quantify the magnitude of effects and the amount of uncertainty for estimated values. This group of statistical methods is referred to as estimation statistics. Estimation statistics is a term to describe three main classes of methods. The three main classes of methods include:\n\n1.  Effect Size. Methods for quantifying the size of an effect given a treatment or intervention.\n\n2.  Interval Estimation. Methods for quantifying the amount of uncertainty in a value.\n\n3.  Meta-Analysis. Methods for quantifying the findings across multiple similar studies.\nOf the three, perhaps the most useful methods in applied machine learning are interval estimation. There are three main types of intervals. They are:\n\n1.  Tolerance Interval: The bounds or coverage of a proportion of a distribution with a specific level of confidence.\n\n2.  Confidence Interval: The bounds on the estimate of a population parameter.\n\n3.  Prediction Interval: The bounds on a single observation.\nA simple way to calculate a confidence interval for a classification algorithm is to calculate the binomial proportion confidence interval, which can provide an interval around a model's estimated accuracy or error. This can be implemented in Python using the confint() Statsmodels function. The function takes the count of successes (or failures), the total number of trials, and the significance level as arguments and returns the lower and upper bound of the confidence interval. The example below demonstrates this function in a hypothetical case where a model made 88 correct predictions out of a dataset with 100 instances and we are interested in the 95% confidence interval (provided to the function as a significance of 0.05).\n## calculate the confidence interval\n```\nfrom statsmodels.stats.proportion import proportion_confint\n```\n\n## calculate the interval\n```\nlower, upper = proportion_confint(88, 100, 0.05) print('lower=%.3f, upper=%.3f' % (lower, upper))\n```"},{"fields":{"slug":"/Mathematics/Statistics/Glossary/","title":"Glossary"},"frontmatter":{"draft":false},"rawBody":"# Glossary\n\nCreated: 2020-04-15 00:37:54 +0500\n\nModified: 2021-09-21 22:16:17 +0500\n\n---\n\n| **Word**                        | **Definition**                                                                                                                                                                                                                                                 |\n|---------------|---------------------------------------------------------|\n| Population                      | The collections of all items of interest to our study; denoted N.                                                                                                                                                                                              |\n| Sample                          | A subset of the population; denoted n.                                                                                                                                                                                                                         |\n| Parameter                       | Is a value that refers to a population. It is the opposite of statistic.                                                                                                                                                                                       |\n| Statistic                       | Is a value that refers to a sample. It is the opposite of a parameter.                                                                                                                                                                                         |\n| Random sample                   | A random sample is collected when each member of the sample is chosen from the population strictly by chance.                                                                                                                                                  |\n| Representative sample           | A representative sample is a subset of the population that accurately reflects the members of the entire population.                                                                                                                                           |\n| Variable                        | A variable is a set of characteristics of a person, object, thing, idea, etc. Variables can vary from case to case. For example, 'height' is a variable that describes a characteristic of a person. It varies from person to person.                        |\n| Frequency distribution table    | A table that represents the frequency of each variable.                                                                                                                                                                                                        |\n| Frequency                       | Measures the occurrence of a variable.                                                                                                                                                                                                                         |\n| Absolute frequency              | Measures the NUMBER of occurrences of a variable.                                                                                                                                                                                                              |\n| Relative frequency              | Measures the RELATIVE NUMBER of occurrences of a variable. Usually, expressed in percentages.                                                                                                                                                                  |\n| Cumulative frequency            | The sum of relative frequencies so far. The cumulative frequency of all members is 100% or 1.                                                                                                                                                                  |\n| Pareto diagram                  | A special type of bar chart, where frequencies are shown in descending order. There is an additional line on the chart, showing the cumulative frequency.                                                                                                      |\n| Histogram                       | A type of bar chart that represents numerical data. It is divided into intervals (or bins) that are not overlapping and span from the first observation to the last. The intervals (bins) are adjacent - where one stops, the other starts.                    |\n| Bins (histogram)                | The intervals that are represented in a histogram.                                                                                                                                                                                                             |\n| Cross table / Contigency table  | A table which represents categorical data. On one axis we have the categories, and on the other - their frequencies. It can be built with absolute or relative frequencies.                                                                                    |\n| Scatter plot                    | A plot that represents numerical data. Graphically, each observation looks like a point on the scatter plot.                                                                                                                                                   |\n| Measures of central tendency    | Measures that describe the data through the so called 'averages'. The most common are the mean, median and mode. There is also geometric mean, harmonic mean, weighted-average mean, etc.                                                                    |\n| Mean                            | The simple average of the dataset. Denoted Î¼.                                                                                                                                                                                                                  |\n| Median                          | The middle number in an ordered dataset.                                                                                                                                                                                                                       |\n| Mode                            | The value that occurs most often. A dataset can have 0, 1 or multiple modes.                                                                                                                                                                                   |\n| Measures of asymmetry           | Measures that describe the data through the level of symmetry that is observed. The most common are skewness and kurtosis.                                                                                                                                     |\n| Skewness                        | A measure that describes the symmetry of the dataset around its mean.                                                                                                                                                                                          |\n| Sample formula                  | A formula, that is calculated on a sample. The value obtained is a statistic.                                                                                                                                                                                  |\n| Population formula              | A formula, that is calculated on a population. The value obtained is a parameter.                                                                                                                                                                              |\n| Measures of variability         | Measures that describe the data through the level of dispersion (variability). The most common ones are variance and standard deviation.                                                                                                                       |\n| Variance                        | Measures the dispersion of the dataset around its mean. It is measured in units squared. Denoted Ïƒ2 for a population and s2 for a sample.                                                                                                                      |\n| Standard deviation              | Measures the dispersion of the dataset around its mean. It is measured in original units. It is equal to the square root of the variance. Denoted Ïƒ for a population and s for a sample.                                                                       |\n| Coefficient of variation        | Measures the dispersion of the dataset around its mean. It is also called 'relative standard deviation'. It is useful for comparing different datasets in terms of variability.                                                                              |\n| Univariate measure              | A measure which refers to a single variable.                                                                                                                                                                                                                   |\n| Multivariate measure            | A measure which refers to multiple variables.                                                                                                                                                                                                                  |\n| Covariance                      | A measure of relationship between two variables. Usually, because of its scale of measurement, covariance is not directly interpretable. Denoted Ïƒxy for a population and sxy for a sample.                                                                    |\n| Linear correlation coefficient  | A measure of relationship between two variables. Very useful for direct interpretation as it takes on values from [-1,1]. Denoted Ïxy for a population and rxy for a sample.                                                                                 |\n| Correlation                     | A measure of the relationship between two variables. There are several ways to compute it, the most common being the linear correlation coefficient.                                                                                                           |\n| Distribution                    | A distribution is a function that shows the possible values for a variable and the probability of their occurrence.                                                                                                                                            |\n| Bell curve                      | A common name for the normal distribution.                                                                                                                                                                                                                     |\n| Gaussian distribution           | The original name of the normal distribution. Named after the famous mathematician Gauss, who was the first to explore it through his work on the Gaussian function.                                                                                           |\n| To control for the mean/std/etc | holding this particular value constant, we change the other variables and observe the effect.                                                                                                                                                                  |\n| Standard normal distribution    | A normal distribution with a mean of 0, and a standard deviation of 1                                                                                                                                                                                          |\n| z-statistic                     | The statistic associated with the normal distribution                                                                                                                                                                                                          |\n| Standardized variable           | In statistics, we usually standardize a variable using the z-score formula. This is done by first subtracting the mean and then dividing by the standard deviation                                                                                             |\n| Central limit theorem           | No matter the distribution of the underlying dataset, the sampling distribution of the means of the dataset approximate a normal distribution.                                                                                                                 |\n| Sampling distribution           | the distribution of a sample.                                                                                                                                                                                                                                  |\n| Standard error                  | the standard error is the standard deviation of the sampling distribution. It takes into account the size of the sample.                                                                                                                                       |\n| Estimator                       | A function or a rule, according to which we make estimations.                                                                                                                                                                                                  |\n| Estimate                        | A particular value that was estimated through an estimator.                                                                                                                                                                                                    |\n| Bias                            | An unbiased estimator has an expected value the population parameter. A biased one has an expected value different from the population parameter. The bias is the deviation from the true value.                                                               |\n| Efficiency (in estimators)      | in the context of estimators, the efficiency loosely refers to 'lack of variability'. The most efficient estimator is the one with the least variability. It is a comparative measure, e.g. one estimator is more efficient than another.                    |\n| Point estimator                 | A function or a rule, according to which we make estimations that will result in a single number.                                                                                                                                                              |\n| Point estimate                  | A single number that was derived from a certain point estimator.                                                                                                                                                                                               |\n| Interval estimator              | A function or a rule, according to which we make estimations that will result in an interval. In this course, we will only consider confidence intervals. Another instance that we don't discuss are also credible intervals (Bayesian statistics).           |\n| Interval estimate               | A particular result that was obtained from an interval estimator. It is an interval.                                                                                                                                                                           |\n| Confidence interval             | A confidence interval is the range within which you expect the population parameter to be. You have a certain probability of it being correct, equal to the significance level.                                                                                |\n| Reliability factor              | A value from a z-table, t-table, etc. that is associated with our test.                                                                                                                                                                                        |\n| Level of confidence             | Shows in what % of the cases we expect the population parameter to fall into the confidence interval we obtained. Denoted 1 - Î±. Example: 95% confidence level means that in 95% of the cases, the population parameter will fall into the specified interval. |\n| Critical value                  | A value coming from a table for a specific statistic (z, t, F, etc.) associated with the probability Î± that the researcher has chosen.                                                                                                                         |\n| z-table                         | A table associated with the Z-statistic, where given a probability (Î±), we can see the value of the standardized variable, following the standard normal distribution.                                                                                         |\n| t-statistic                     | A statistic that is generally associated with the Student's T distribution, in the same way the z-statistic is associated with the normal distribution.                                                                                                       |\n| A rule of thumb                 | A principle, which is approximately true, but is widely used in practice due to its simplicity.                                                                                                                                                                |\n| t-table                         | A table associated with the t-statistic, where given a probability (Î±), and certain degrees of freedom, we can check the reliability factor.                                                                                                                   |\n| Degrees of freedom              | The number of variables in the final calculation that are free to vary.                                                                                                                                                                                        |\n| Margin of error                 | Half the width of a confidence interval. It drives the width of the interval.                                                                                                                                                                                  |\n| Hypothesis                      | Loosely, a hypothesis is 'an idea that can be tested'                                                                                                                                                                                                        |\n| Hypothesis test                 | A test that is conducted in order to verify if a hypothesis is true or false.                                                                                                                                                                                  |\n| Null hypothesis                 | The null hypothesis is the one to be tested. Whenever we are conducting a test, we are trying to reject the null hypothesis.                                                                                                                                   |\n| Alternative hypothesis          | The alternative hypothesis is the opposite of the null. It is usually the opinion of the researcher, as he is trying to reject the null hypothesis and thus accept the alternative one.                                                                        |\n| To accept a hypothesis          | The statistical evidence shows, that the hypothesis is likely to be true.                                                                                                                                                                                      |\n| To reject a hypothesis          | The statistical evidence shows, that the hypothesis is likely to be false.                                                                                                                                                                                     |\n| One-tailed (one-sided) test     | Tests which are determining if a value is lower (lower or equal) or higher (higher or equal) to a certain value are one-sided. This is because they can be rejected only on one side.                                                                          |\n| Two-tailed (two-sided) test     | Tests which are determining if a value is equal (or different) to a certain value are two-sided. This is because they can be rejected on two sides - if the parameter is too big or too small.                                                                 |\n| Significance level              | The probability of rejecting the null hypothesis, if it is true. Denoted Î±. You choose the significance level. All else equal, the lower the level, the better the test.                                                                                       |\n| Rejection region                | The part of the distribution, for which we would reject the null hypothesis.                                                                                                                                                                                   |\n| Type I error (false positive)   | This error consists of rejecting a null hypothesis that is true. The probability of committing it is Î±, the significance level.                                                                                                                                |\n| Type II error (false negative)  | This error consists of accepting a null hypothesis that is false. The probability of committing it is Î².                                                                                                                                                       |\n| Power of the test               | Probability of rejecting a null hypothesis that is false (the researcher's goal). Denoted by 1- Î².                                                                                                                                                            |\n| z-score                         | The standardized variable associated with the dataset we are testing. It is observed in the table with an Î± equal to the level of significance of the test.                                                                                                    |\n| Î¼0                              | The hypothesized population mean.                                                                                                                                                                                                                              |\n| p-value                         | The smallest level of significance at which we can still reject the null hypothesis, given the observed sample statistic.                                                                                                                                      |\n<https://global.oup.com/uk/orc/xedition/brymanbrm4exe/student/mcqs/ch12>\n"},{"fields":{"slug":"/Mathematics/Statistics/Hypothesis-Testing/","title":"Hypothesis Testing"},"frontmatter":{"draft":false},"rawBody":"# Hypothesis Testing\n\nCreated: 2021-09-15 23:22:40 +0500\n\nModified: 2021-09-15 23:57:09 +0500\n\n---\n\nNull vs Alternative\n\nRejection region and significance level\n\nType I error vs type II error\n\nTest for the mean. Population variance known\n\np-value\n\nTest for the mean. Population variance unknown\n\nTest for the mean. Dependent samples\n\nTest for the mean. Independent samples (Part1)\n\nTest for the mean. Independent samples (Part2)\n**Scientific Method**\n\n![The 'scientific method' is a procedure that has characterized natural science since the 17th century. It consists in systematic observation, measurement, experiment, and the formulation, testing and modification of hypotheses. Since then we've evolved to the point where most people and especially professionals realize that pure observation can be deceiving. Therefore, business decisions are increasingly driven by data. That's also the purpose of data science. While we don't 'name' the scientific method in the videos, that's the underlying idea. There are several steps you would follow to reach a data-driven decision (pictured). STEPS Formula-I-e a Find righâ€¢i- exea*e Make a decision ](media/Hypothesis-Testing-image1.jpg)\n**Hypotheses**\n-   A hypothesis is \"an idea that can be tested\"\n-   It is a supposition or proposed explanation made on the basis of limited evidence as a starting point for further investigation\n\n![Null hypothesis (Ho) The null hypothesis is the hypothesis to be tested. It is the status-quo. Everything which was believed until now that we are contesting with our test. The concept of the null is similar to: innocent until proven guilty We assume innocence until we have enough evidence to prove that a suspect is guilty. Alternative hypothesis (HI or HA) The alternative hypothesis is the change or innovation that is contesting the status-quo. Usually the alternative is our own opinion. The idea is the following: If the null is the status-quo (i.e., what is generally believed), then the act of performing a test, shows we have doubts about the truthfulness of the null. More often than not the researcher's opinion is contained in the alternative hypothesis. ](media/Hypothesis-Testing-image2.jpg)\n**Examples of hypotheses**\n\n![](media/Hypothesis-Testing-image3.jpg)\n**Decisions you can take**\n\n![When testing, there are two decisions that can be made: to accept the null hypothesis QC to reject the null hypothesis. To accept the null means that there isn't enough data to support the change or the innovation brought by the alternative. To reject the null means that there is enough statistical evidence that the status-quo is not representative of the truth. rejechon region nccÃ‰pr reje& Different ways of reporting the result: Accept rejechon region rejec+ At x% significance, we accept the null hypothesis At x% significance, A is not significantly different from B At x% significance, there is not enough statistical evidence that... At x% significance, we cannot reject the null hypothesis Given a two-tailed test: Graphically, the tails of the distribution show when we reject the null hypothesis ('rejection region'). Everything which remains in the middle is the 'acceptance region'. The rationale is: if the observed statistic is too far away from 0 (depending on the significance level), we reject the null. Otherwise, we accept it. Reject At x% significance, we reject the null hypothesis At x% significance, A is significantly different from B At x% significance, there is enough statistical evidence... At x% significance, we cannot say that *restate the null* ](media/Hypothesis-Testing-image4.jpg)\n**Level of significance and types of tests**\n-   Level of significance (Î±) - The probability of rejecting a null hypothesis that is true; the probability of making this error.\n\n![Common significance levels Two-sided (two-tailed) test Used when the null contains an equality or an inequality sign 0.10 0.05 0.01 a 0.0s rejechon region a 12 0.02s One-sided (one-tailed) test Used when the null doesn't contain equality or inequality sign rejechon region Tejechon region Ct12 = 0.02S ](media/Hypothesis-Testing-image5.jpg)\n**Statistical errors (Type I and Type II Error)**\n\n![In general, there are two types of errors we can make while testing: Type I error (False positive) and Type Il Error (False negative). Statisticians summarize the errors in the following table: : SÃ¥-a+vs quo (sYaÃ¥vs quo) gejecY The is -Hue Type I eyyor (false pos*iue) is false Type 11 en-ov- (False nega-hue) Here's the table with the example from the lesson: : She doesn'+ like you The She doesn'+ like you She likes you Type Il eryov (False negahue) (s}-aÃ¥vs quo) She doesn'â€¢}â€¢ like (you inui+e RejecF her (Inui-Ye hen Type I evyov (fâ€¢alse pos-hue) The probability of committing Type I error (False positive) is equal to the significance level (a). The probability of committing Type Il error (False negative) is equal to the beta (ÃŸ) and is called 'power of the test'. ](media/Hypothesis-Testing-image6.jpg)\n**P-value**\n-   The p-value is the smallest level of significance at which we can still reject the null hypothesis, given the observed sample statistic\n\n![](media/Hypothesis-Testing-image7.jpg)\n**Formulae for Hypothesis Testing**\n\n![# populations One One Two Two Two Population variance known unknown Known unknown, assumed equal Samples Statistic dependent independent independent z t t z t Variance 2 2 S difference 2 (nx --- I)sk + (ny --- I)s} Formula for test statistic S d/ vrfi nx ny 2 2 Decision rule There are several ways to phrase the decision rule and they all have the same meaning. Reject the null if: 1) Itest statisticl > lcritical valuel 2) The absolute value of the test statistic is bigger than the absolute critical value 3) p-value < some significance level most often 0.05 Usually, you will be using the p-value to make a decision. 2 ](media/Hypothesis-Testing-image8.jpg)"},{"fields":{"slug":"/Mathematics/Statistics/Inferential-Statistics/","title":"Inferential Statistics"},"frontmatter":{"draft":false},"rawBody":"# Inferential Statistics\n\nCreated: 2021-09-15 23:20:49 +0500\n\nModified: 2021-09-15 23:42:48 +0500\n\n---\n\nIntroduction\n\nWhat is a distribution\n\nThe Normal Distribution\n\nThe Standard Normal Distribution\n\nCentral limit theorem\n\nStandard error\n\nEstimators and estimates\n**Inferential Statistics (Drawing Conclusion)**\n\nItis a fancy name for methods that aid in quantifying properties of the domain or population from a smaller set of obtained observations called a sample\n**Distributions**\n\n![Definition In statistics, when we talk about distributions we usually mean probability distributions. Definition (informal): A distribution is a function that shows the possible values for a variable and how often they occur. Definition (Wikipedia): In probability theory and statistics, a probability distribution is a mathematical function that, stated in simple terms, can be thought of as providing the probabilities of occurrence of different possible outcomes in an experiment. Examples: Normal distribution, Student's T distribution, Poisson distribution, Uniform distribution, Binomial distribution Graphical representation It is a common mistake to believe that the distribution is the graph. In fact the distribution is the 'rule' that determines how values are positioned in relation to each other. Very often, we use a graph to visualize the data. Since different distributions have a particular graphical representation, statisticians like to plot them. Examples: Uniform distribution Normal distribution Binomial distribution Student's T distribution ](media/Inferential-Statistics-image1.jpg)\n**The Normal Distribution**\n\n![The Normal distribution is also known as Gaussian distribution or the Bell curve. It is one of the most common distributions due to the following reasons: It approximates a wide variety of random variables Distributions of sample means with large enough samples sizes could be approximated to normal All computable statistics are elegant Heavily used in regression analysis Good track record NÃ¦(g, 02) N stands for normal; stands for a distribution; u is the mean; 02 is the variance. Examples: Biology. Most biological measures are normally distributed, such as: height; length of arms, legs, nails; blood pressure; thickness of tree barks, etc. IQ tests Stock market information ](media/Inferential-Statistics-image2.jpg)\n![Origin Controlling for the standard deviation Keeping the standard deviation constant, the graph of a normal distribution with: â€¢ a smaller mean would look in the same way, but be situated to the left (in gray) â€¢ a larger mean would look in the same way, but be situated to the right (in red) c = 140 c = 140 1.1 = 743 = 470 14b - = 960 ](media/Inferential-Statistics-image3.jpg)\n![Controlling for the mean 70 = 140 a = 210 Origin = 743 = 743 = 743 Keeping the mean constant, a normal distribution with: a smaller standard deviation would be situated in the same spot, but have a higher peak and thinner tails (in red) a larger standard deviation would be situated in the same spot, but have a lower peak and fatter tails (in gray) ](media/Inferential-Statistics-image4.jpg)\n**The Standard Normal Distribution**\n\n![The Standard Normal distribution of the Normal distribution. It has standard deviation of 1. Every Normal distribution can be the standardization formula: is a particular case a mean of 0 and a 'standardized' using Why standardize? Standardization allows us to: compare different normally distributed datasets detect normality detect outliers create confidence intervals test hypotheses perform regression analysis A variable following the Standard Normal distribution is denoted with the letter z. Rationale of the formula for standardization: We want to transform a random variable from 02) to Subtracting the mean from all observations would cause a transformation from 02) to 02), moving the graph to the origin. Subsequently, dividing all observations by the standard deviation would cause a transformation from 02) to ), standardizing the peak and the tails of the graph. ](media/Inferential-Statistics-image5.jpg)\n**The Central Limit Theorem**\n\n![The Central Limit Theorem (CLT) is one of the greatest statistical insights. It states that no matter the underlying distribution of the dataset, the sampling distribution of the means would approximate a normal distribution. Moreover, the mean of the sampling distribution would be equal to the mean of the original distribution and the variance would be n times smaller, where n is the size of the samples. The CLT applies whenever we have a sum or an average of many variables (e.g. sum of rolled numbers when rolling dice). The theorem No matter the distribution The distribution of y-1, *72, x3, G, Why is it useful? The CIT allows us to assume normality for many different variables. That is very useful for confidence intervals, hypothesis testing, and regression analysis. In fact, the Normal distribution is so predominantly observed around us due to the fact that following the CLT, many variables converge to Normal. Click here for a CLT simulator. Where can we see it? Since many concepts and events are a sum or an average of different effects, CLT applies and we observe normality all the time. For example, in regression analysis, the dependent variable is explained through the sum of error terms. would tend to N The more samples, the Normal (k The bigger the samples, to Normal (n closer to the closer ](media/Inferential-Statistics-image6.jpg)\n**Estimators and Estimates**\n\n![Estimators Broadly, an estimator is a mathematical function that approximates a population parameter depending only on sample information. Examples of estimators and the corresponding parameters: Estimates An estimate is the output that you get from the estimator (when you apply the formula). There are two types of estimates: point estimates and confidence interval Term Mean Variance Correlation Estimator Parameter 2 s 02 p Estimators have two important properties: Bias The expected value of an unbiased estimator is the population parameter. The bias in this case is 0. If the expected value of an estimator is (parameter + b), then the bias is b. Efficiency The rnost effcient estimator is the one with the smallest vanance. estimates. Point estimates A single value. Examples: 5 â€¢ 122.67 0.32 Confidence intervals An interval. Examples: (1,5) (12 , 33) ( 221.78, 745.66) (-0.71,0.11) Confidence intervals are much more precise than point estimates. That is why they are preferred when making inferences. ](media/Inferential-Statistics-image7.jpg)\n**Confidence Intervals and the Margin of Error**\n\n![Interval start Point estimate Interval end Definition: A confidence interval is an interval within which we are confident (with a certain percentage of confidence) the population parameter will fall. We build the confidence interval around the point estimate. (I-a) is the level of confidence. We are confident that the population parameter will fall in the specified interval. Common alphas are: 0.01, 0.05, 0.1. General formula: - ME.* + ME] , where ME is the margin of error. standard deviation ME = reliabilityfactor* sample size Term (I-a) T Effect on width of Cl s tu,a/2 * ](media/Inferential-Statistics-image8.jpg)\n\n**Student's T Distribution**\n\n![The Student's T distribution is used predominantly for creating confidence intervals and testing hypotheses with normally distributed populations when the sample sizes are small. It is particularly useful when we don't have enough information or it is too costly to obtain it. All else equal, the Student's T distribution has fatter tails than the Normal distribution and a lower peak. This is to reflect the higher level of uncertainty, caused by the small sample size. A random variable following the t-distribution is denoted tu,a , where v are the degrees of freedom. We can obtain the student's T distribution for a variable with a Normally distributed population using the formula: Student's I distrhuticn s/v\"7 ](media/Inferential-Statistics-image9.jpg)\n\n**Formulas for Confidence Intervals**\n\n![# populations One One Two Two Two Two Population variance known unknown Known unknown, assumed equal unknown, assumed different Samples dependent independent independent independent Statistic z z Variance S Sdifference s Formula Â± za/2 --- s n---1,a/2 Â± tn-1,a/2 Â¯ 02 02 (f --- Y) Â± za/2 nx ny (i --- Y) Â± tnx+ny-2,a/2 2 (X --- Y) Â± tua/2 nx ny ](media/Inferential-Statistics-image10.jpg)\n\n"},{"fields":{"slug":"/Mathematics/Statistics/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-05-14 19:32:20 +0500\n\nModified: 2022-09-23 12:54:52 +0500\n\n---\n\nStatistics is a subfield of mathematics. It refers to a collection of methods for working with data and using data to answer questions.\nEyeball statistics\n**Statistics drives practice, policy and laws**\n-   In almost every aspect of our lives, practice and policy is determined by statistics\n    -   In fiscal policy, governments decide taxation and spending, based upon statistical assessments of their effects on the economy\n    -   In health care, questions of what drugs should be approved and what treatment patients should get is based upon statistics (often in medical research)\n    -   In our personal lives, our choices of where to work, live and how to save/invest are at least loosely driven by statistics\n-   Making good policy and personal decisions requires an understanding of statistics and data. If we misread the statistics or the statistics are unreliable, policy will as well\n**Lying with Statistics**\n-   **Agendra-driven data:** As access to data has increased, the misuse of that data has also gone up, especially when people have agendas and want to further them. These people mislead, without technically lying, as the selectively pick and choose which data they use, and how they present that data\n-   **Social media as magnifier:** Bad data can take on a life of its won, especially with social media operating as a weapon to widen its reach\n    -   Gresham's Law\n        -   A monetary principle stating that \"bad money drives out good\". It is primarily used for consideration and application in currency markets. Gresham's law was originally based on the composition of minted coins and the value of the precious metals used in them.\n-   **Caveat emptor:** As people weaponize data and use selective and slanted statistics, based upon that data, we need to be able to protect ourselves from misinformation\n    -   Understanding statistics allows us to\n        -   Look for red flags that can be used to detect data manipulation\n        -   Asking the right questions to separate fact from fiction\n**Mathematical Thinking**\n\nMathematical thinking is about seeing the world in a different way. Which means sometimes seeing beyond our intuition or gut feeling\n**Population**\n-   Collection of all items of interest of our study\n-   Denoted by **N**\n-   Numbers obtained are called parameters\n**Sample**\n-   A subset of the population\n-   Denoted by **n**\n-   Numbers obtained are called statistics\n**Time Series vs Cross Section**\n\nThe data that you are trying to study can be a phenomenon that you observe over time (time series data) or across different subjects at a point in time (cross sectional data)\n-   Time series example: If stock returns over time is your population, stock returns from 1960-2021 is a sample\n-   Cross sectional example: If all publicly traded companies is your population, looking at only US companies or companies with market caps that exceed $10 million is a sample\n**Regression toward the mean**\n\nIn[statistics](https://en.wikipedia.org/wiki/Statistics),regression toward the mean(also calledregression to the mean,reversion to the mean, andreversion to mediocrity) is the phenomenon that arises if a[sample point](https://en.wikipedia.org/wiki/Sample_point)of a[random variable](https://en.wikipedia.org/wiki/Random_variable) is [extreme](https://en.wikipedia.org/wiki/Extreme_value_theory) (nearly an[outlier](https://en.wikipedia.org/wiki/Outlier)), a future point is likely to be closer to the[mean](https://en.wikipedia.org/wiki/Mean)or[average](https://en.wikipedia.org/wiki/Average).To avoid making incorrect[inferences](https://en.wikipedia.org/wiki/Statistical_inference), regression toward the mean must be considered when designing scientific experiments and interpreting data.\n<https://en.wikipedia.org/wiki/Regression_toward_the_mean>\n\n## Controlled Experiments**\n\na.  Randomness\n\nb.  Allocation bias and Selection bias\n\nc.  Randomized block design (where number of items in each group can be forced to be equal)\n\nd.  Control groups\n\ne.  Placebo effects - Placebo meaning - I shall please\n\nf.  Single blind study\n\ng.  Double blind study\n\nh.  Matched-pair experiments\n\ni.  Repeated measures design\n**Henrietta Lacks, the Tuskegee Experiment, & Ethical Data Collection**\n\na.  Informed Consent\n\nb.  Nuremberg code\n\nc.  Beneficence\n**Outliers**\n\n<https://www.freecodecamp.org/news/what-is-an-outlier-definition-and-how-to-find-outliers-in-statistics>\n\n## Courses**\n\n<https://www.youtube.com/watch?v=VPZD_aij8H0>\n\n<https://www.youtube.com/watch?v=xxpc-HPKN28>\n\n<https://www.youtube.com/watch?v=Vfo5le26IhY>\n\n[Statistics 101](https://www.youtube.com/playlist?list=PLUkh9m2BorqmXcRzWFbzcjMd7fYErVexF)\n\n<http://people.stern.nyu.edu/adamodar/New_Home_Page/webcaststatistics.htm>\n\n<https://365datascience.teachable.com/courses/enrolled/233979>\n\n<https://www.khanacademy.org/math/ap-statistics>\n\n## Outline**\n-   [Astrostatistics](https://en.wikipedia.org/wiki/Astrostatistics)\n-   [Biostatistics](https://en.wikipedia.org/wiki/Biostatistics)\n**References**\n\nAbraham Wald and the Missing Bullet Holes\n\n<https://medium.com/@penguinpress/an-excerpt-from-how-not-to-be-wrong-by-jordan-ellenberg-664e708cfc3d>\n\n<https://datascienceprep.com/blog/stat-guide-for-data-science-interviews>\n"},{"fields":{"slug":"/Mathematics/Statistics/Nonparametric-Statistics/","title":"Nonparametric Statistics"},"frontmatter":{"draft":false},"rawBody":"# Nonparametric Statistics\n\nCreated: 2020-03-25 01:06:36 +0500\n\nModified: 2021-06-28 13:08:01 +0500\n\n---\n\nNonparametric statisticsis the branch of[statistics](https://en.wikipedia.org/wiki/Statistics)that is not based solely on[parametrized](https://en.wikipedia.org/wiki/Statistical_parameter)families of[probability distributions](https://en.wikipedia.org/wiki/Probability_distribution)(common examples of parameters are the mean and variance). Nonparametric statistics is based on either being distribution-free or having a specified distribution but with the distribution's parameters unspecified. Nonparametric statistics includes both[descriptive statistics](https://en.wikipedia.org/wiki/Descriptive_statistics)and[statistical inference](https://en.wikipedia.org/wiki/Statistical_inference).\n**Non-parametric models**\n\nNon-parametric modelsdiffer from[parametric](https://en.wikipedia.org/wiki/Parametric_statistics)models in that the model structure is not specifieda prioribut is instead determined from data. The termnon-parametricis not meant to imply that such models completely lack parameters but that the number and nature of the parameters are flexible and not fixed in advance.\n-   A[histogram](https://en.wikipedia.org/wiki/Histogram)is a simple nonparametric estimate of a probability distribution.\n-   **[Kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation)(KDE)** provides better estimates of the density than histograms.\n\nIn[statistics](https://en.wikipedia.org/wiki/Statistics),kernel density estimation(KDE) is a[non-parametric](https://en.wikipedia.org/wiki/Non-parametric_statistics)way to [estimate](https://en.wikipedia.org/wiki/Density_estimation) the [probability density function](https://en.wikipedia.org/wiki/Probability_density_function)of a[random variable](https://en.wikipedia.org/wiki/Random_variable). Kernel density estimation is a fundamental data smoothing problem where inferences about the[population](https://en.wikipedia.org/wiki/Statistical_population)are made, based on a finite data[sample](https://en.wikipedia.org/wiki/Statistical_sample). In some fields such as[signal processing](https://en.wikipedia.org/wiki/Signal_processing)and[econometrics](https://en.wikipedia.org/wiki/Econometrics)it is also termed the**Parzen--Rosenblatt window**method, after[Emanuel Parzen](https://en.wikipedia.org/wiki/Emanuel_Parzen)and[Murray Rosenblatt](https://en.wikipedia.org/wiki/Murray_Rosenblatt), who are usually credited with independently creating it in its current form.One of the famous applications of kernel density estimation is in estimating the class-conditional marginal densities of data when using a[naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier),which can improve its prediction accuracy.-   [Nonparametric regression](https://en.wikipedia.org/wiki/Nonparametric_regression)and[semiparametric regression](https://en.wikipedia.org/wiki/Semiparametric_regression)methods have been developed based on[kernels](https://en.wikipedia.org/wiki/Kernel_(statistics)),[splines](https://en.wikipedia.org/wiki/Spline_(mathematics)), and[wavelets](https://en.wikipedia.org/wiki/Wavelet).\n-   [Data envelopment analysis](https://en.wikipedia.org/wiki/Data_envelopment_analysis)provides efficiency coefficients similar to those obtained by[multivariate analysis](https://en.wikipedia.org/wiki/Multivariate_analysis)without any distributional assumption.\n-   [KNNs](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)classify the unseen instance based on the K points in the training set which are nearest to it.\n-   A[support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine)(with a Gaussian kernel) is a nonparametric large-margin classifier.\n-   [Method of moments (statistics)](https://en.wikipedia.org/wiki/Method_of_moments_(statistics))with polynomial probability distributions.\n**Methods**\n\nNon-parametric(ordistribution-free)inferential statistical methodsare mathematical procedures for statistical hypothesis testing which, unlike[parametric statistics](https://en.wikipedia.org/wiki/Parametric_statistics), make no assumptions about the[probability distributions](https://en.wikipedia.org/wiki/Probability_distribution)of the variables being assessed. The most frequently used tests include\n-   [Analysis of similarities](https://en.wikipedia.org/wiki/Analysis_of_similarities)\n-   [Anderson--Darling test](https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test): tests whether a sample is drawn from a given distribution\n-   [Statistical bootstrap methods](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)): estimates the accuracy/sampling distribution of a statistic\n-   [Cochran's Q](https://en.wikipedia.org/wiki/Cochran%27s_Q_test): tests whetherktreatments in randomized block designs with 0/1 outcomes have identical effects\n-   [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa): measures inter-rater agreement for categorical items\n-   [Friedman two-way analysis of variance](https://en.wikipedia.org/wiki/Friedman_test)by ranks: tests whetherktreatments in randomized block designs have identical effects\n-   [Kaplan--Meier](https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator): estimates the survival function from lifetime data, modeling censoring\n-   [Kendall's tau](https://en.wikipedia.org/wiki/Kendall_tau_rank_correlation_coefficient): measures statistical dependence between two variables\n-   [Kendall's W](https://en.wikipedia.org/wiki/Kendall%27s_W): a measure between 0 and 1 of inter-rater agreement\n-   [Kolmogorov--Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test): tests whether a sample is drawn from a given distribution, or whether two samples are drawn from the same distribution\n-   [Kruskal--Wallis one-way analysis of variance](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance)by ranks: tests whether >2 independent samples are drawn from the same distribution\n-   [Kuiper's test](https://en.wikipedia.org/wiki/Kuiper%27s_test): tests whether a sample is drawn from a given distribution, sensitive to cyclic variations such as day of the week\n-   [Logrank test](https://en.wikipedia.org/wiki/Logrank_test): compares survival distributions of two right-skewed, censored samples\n-   [Mann--Whitney U](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U)or Wilcoxon rank sum test: tests whether two samples are drawn from the same distribution, as compared to a given alternative hypothesis.\n-   [McNemar's test](https://en.wikipedia.org/wiki/McNemar%27s_test): tests whether, in 2 Ã— 2 contingency tables with a dichotomous trait and matched pairs of subjects, row and column marginal frequencies are equal\n-   [Median test](https://en.wikipedia.org/wiki/Median_test): tests whether two samples are drawn from distributions with equal medians\n-   [Pitman's permutation test](https://en.wikipedia.org/wiki/Pitman_permutation_test): a statistical significance test that yields exactpvalues by examining all possible rearrangements of labels\n-   [Rank products](https://en.wikipedia.org/wiki/Rank_product): detects differentially expressed genes in replicated microarray experiments\n-   [Siegel--Tukey test](https://en.wikipedia.org/wiki/Siegel%E2%80%93Tukey_test): tests for differences in scale between two groups\n-   [Sign test](https://en.wikipedia.org/wiki/Sign_test): tests whether matched pair samples are drawn from distributions with equal medians\n-   [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient): measures statistical dependence between two variables using a monotonic function\n-   [Squared ranks test](https://en.wikipedia.org/wiki/Squared_ranks_test): tests equality of variances in two or more samples\n-   [Tukey--Duckworth test](https://en.wikipedia.org/wiki/Tukey%E2%80%93Duckworth_test): tests equality of two distributions by using ranks\n-   [Wald--Wolfowitz runs test](https://en.wikipedia.org/wiki/Wald%E2%80%93Wolfowitz_runs_test): tests whether the elements of a sequence are mutually independent/random\n-   [Wilcoxon signed-rank test](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test): tests whether matched pair samples are drawn from populations with different mean ranks\n<https://en.wikipedia.org/wiki/Nonparametric_statistics>\n"},{"fields":{"slug":"/Mathematics/Statistics/Other-Statistics/","title":"Other Statistics"},"frontmatter":{"draft":false},"rawBody":"# Other Statistics\n\nCreated: 2018-07-02 00:59:00 +0500\n\nModified: 2021-10-15 11:52:59 +0500\n\n---\n\n**Gaussian Noise**\n\n**Gaussian noise**is[statistical noise](https://en.wikipedia.org/wiki/Statistical_noise)having a[probability density function](https://en.wikipedia.org/wiki/Probability_density_function)(PDF) equal to that of the[normal distribution](https://en.wikipedia.org/wiki/Normal_distribution), which is also known as the[Gaussian distribution](https://en.wikipedia.org/wiki/Gaussian_distribution).^[[1]](https://en.wikipedia.org/wiki/Gaussian_noise#cite_note-Barbu-1)[[2]](https://en.wikipedia.org/wiki/Gaussian_noise#cite_note-Handbook-2)^In other words, the values that the noise can take on are Gaussian-distributed.\n![The probability density function p of a Gaussian random variable z is given by: 1 PG (z) --- a 27T where z represents the grey level, the mean value and the standard deviation. ](media/Other-Statistics-image1.png)\nA special case is*white Gaussian noise*, in which the values at any pair of times are[identically distributed](https://en.wikipedia.org/wiki/Iid)and[statistically independent](https://en.wikipedia.org/wiki/Statistically_independent)(and hence[uncorrelated](https://en.wikipedia.org/wiki/Uncorrelated)). In[communication channel](https://en.wikipedia.org/wiki/Channel_(communications))testing and modelling, Gaussian noise is used as additive[white noise](https://en.wikipedia.org/wiki/White_noise)to generate[additive white Gaussian noise](https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise).\n<https://en.wikipedia.org/wiki/Gaussian_noise>\n\n## Monotonic Function**\n\nA monotonic function is a function which is either entirely[nonincreasing](http://mathworld.wolfram.com/NonincreasingFunction.html)or[nondecreasing](http://mathworld.wolfram.com/NondecreasingFunction.html). A function is monotonic if its first[derivative](http://mathworld.wolfram.com/Derivative.html)(which need not be continuous) does not change sign.\n\n![](media/Other-Statistics-image2.png)\n\nFigure 1. A monotonically increasing function.\n\n![](media/Other-Statistics-image3.png)\n\nFigure 2. A monotonically decreasing function\n\n![](media/Other-Statistics-image4.png)\n\nFigure 3. A function that is not monotonic\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics), a**monotonic function** (or**monotone function**) is a[function](https://en.wikipedia.org/wiki/Function_(mathematics))between[ordered sets](https://en.wikipedia.org/wiki/List_of_order_structures_in_mathematics)that preserves or reverses the given[order](https://en.wikipedia.org/wiki/Order_relation).\n<https://en.wikipedia.org/wiki/Monotonic_function>\n\n## Null Hypothesis**\n-   (in a statistical test) the hypothesis that there is no significant difference between specified populations, any observed difference being due to sampling or experimental error.\n-   The idea that there's no effect\n**Alternative hypothesis**\n-   is one that states that sample observations are influenced by some non-random cause. From an A/B test perspective, the alternative hypothesis states that thereisa difference between the control and variant group.\n**F-distribution**\n\nIn[probability theory](https://en.wikipedia.org/wiki/Probability_theory)and[statistics](https://en.wikipedia.org/wiki/Statistics), the***F*-distribution**, also known as**Snedecor's*F*distribution**or the**Fisher--Snedecor distribution**(after[Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher)and[George W. Snedecor](https://en.wikipedia.org/wiki/George_W._Snedecor)) is a[continuous probability distribution](https://en.wikipedia.org/wiki/Continuous_probability_distribution)that arises frequently as the[null distribution](https://en.wikipedia.org/wiki/Null_distribution)of a[test statistic](https://en.wikipedia.org/wiki/Test_statistic), most notably in the[analysis of variance](https://en.wikipedia.org/wiki/Analysis_of_variance)(ANOVA), e.g.,[*F*-test](https://en.wikipedia.org/wiki/F-test).\n**F-test**\n\nAn***F*-test**is any[statistical test](https://en.wikipedia.org/wiki/Statistical_test)in which the[test statistic](https://en.wikipedia.org/wiki/Test_statistic)has an[*F*-distribution](https://en.wikipedia.org/wiki/F-distribution)under the[null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis). It is most often used when[comparing statistical models](https://en.wikipedia.org/wiki/Model_selection)that have been fitted to a[data](https://en.wikipedia.org/wiki/Data)set, in order to identify the model that best fits the[population](https://en.wikipedia.org/wiki/Population_(statistics))from which the data were sampled. Exact \"*F*-tests\" mainly arise when the models have been fitted to the data using[least squares](https://en.wikipedia.org/wiki/Least_squares).\n**F1-score**\n\nIn[statistical](https://en.wikipedia.org/wiki/Statistics)analysis of[binary classification](https://en.wikipedia.org/wiki/Binary_classification), the**F~1~score**(also**F-score**or**F-measure**) is a measure of a test's accuracy. It considers both the[precision](https://en.wikipedia.org/wiki/Precision_(information_retrieval))*p*and the[recall](https://en.wikipedia.org/wiki/Recall_(information_retrieval))*r*of the test to compute the score:*p*is the number of correct positive results divided by the number of all positive results returned by the classifier, and*r*is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F~1~score is the[harmonic average](https://en.wikipedia.org/wiki/Harmonic_mean)of the[precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall), where an F~1~score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n**Moving Average (MA)**\n\nIn[statistics](https://en.wikipedia.org/wiki/Statistics), a**moving average**(**rolling average**or**running average**) is a calculation to analyze data points by creating series of[averages](https://en.wikipedia.org/wiki/Average)of different subsets of the full data set. It is also called a**moving mean**(**MM**)or**rolling mean**and is a type of[finite impulse response](https://en.wikipedia.org/wiki/Finite_impulse_response) filter.\nGiven a series of numbers and a fixed subset size, the first element of the moving average is obtained by taking the average of the initial fixed subset of the number series. Then the subset is modified by \"shifting forward\"; that is, excluding the first number of the series and including the next value in the subset.\nA moving average is commonly used with[time series](https://en.wikipedia.org/wiki/Time_series)data to smooth out short-term fluctuations and highlight longer-term trends or cycles. The threshold between short-term and long-term depends on the application, and the parameters of the moving average will be set accordingly. For example, it is often used in[technical analysis](https://en.wikipedia.org/wiki/Technical_analysis)of financial data, like stock[prices](https://en.wikipedia.org/wiki/Price),[returns](https://en.wikipedia.org/wiki/Return_(finance))or trading volumes. It is also used in[economics](https://en.wikipedia.org/wiki/Economics)to examine gross domestic product, employment or other macroeconomic time series. Mathematically, a moving average is a type of[convolution](https://en.wikipedia.org/wiki/Convolution)and so it can be viewed as an example of a[low-pass filter](https://en.wikipedia.org/wiki/Low-pass_filter)used in[signal processing](https://en.wikipedia.org/wiki/Signal_processing). When used with non-time series data, a moving average filters higher frequency components without any specific connection to time, although typically some kind of ordering is implied. Viewed simplistically it can be regarded as smoothing the data.-   Simple Moving Average\n-   Cumulative Moving Average\n-   Weighted Moving Average\n-   Exponential Moving Average\n-   Centered Moving Average\n<https://en.wikipedia.org/wiki/Moving_average>\n\n## Parameter Space**\n\nIn[statistics](https://en.wikipedia.org/wiki/Statistics), a**parameter space**is the[space](https://en.wikipedia.org/wiki/Space_(mathematics))of all possible combinations of values for all the different[parameters](https://en.wikipedia.org/wiki/Parameter)contained in a particular[mathematical model](https://en.wikipedia.org/wiki/Mathematical_model). The ranges of values of the parameters may form the axes of a[plot](https://en.wikipedia.org/wiki/Plot_(graphics)), and particular outcomes of the model may be plotted against these axes to illustrate how different regions of the parameter space produce different types of behaviour in the model.\n<https://en.wikipedia.org/wiki/Parameter_space>\n\n## Ziph's Law**\n\nZipf's law([/zÉªf/](https://en.wikipedia.org/wiki/Help:IPA/English)) is an[empirical law](https://en.wikipedia.org/wiki/Empirical_law)formulated using[mathematical statistics](https://en.wikipedia.org/wiki/Mathematical_statistics)that refers to the fact that many types of data studied in the[physical](https://en.wikipedia.org/wiki/Physical_science)and[social](https://en.wikipedia.org/wiki/Social_science)sciences can be approximated with a Zipfian distribution, one of a family of related discrete[power law](https://en.wikipedia.org/wiki/Power_law)[probability distributions](https://en.wikipedia.org/wiki/Probability_distribution).Zipf distributionis related to the[zeta distribution](https://en.wikipedia.org/wiki/Zeta_distribution), but is not identical.\nFor example, Zipf's law states that given some[corpus](https://en.wikipedia.org/wiki/Text_corpus)of[natural language](https://en.wikipedia.org/wiki/Natural_language)utterances, the frequency of any word is[inversely proportional](https://en.wikipedia.org/wiki/Inversely_proportional)to its rank in the[frequency table](https://en.wikipedia.org/wiki/Frequency_table). Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the[rank-frequency distribution](https://en.wikipedia.org/wiki/Rank-frequency_distribution)is an inverse relation. For example, in the[Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus)of American English text, the word[the](https://en.wikipedia.org/wiki/English_articles#Definite_article)is the most frequently occurring word, and by itself accounts for nearly 7% of all word occurrences (69,971 out of slightly over 1 million). True to Zipf's Law, the second-place wordofaccounts for slightly over 3.5% of words (36,411 occurrences), followed byand(28,852). Only 135 vocabulary items are needed to account for half the[Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus).\n<https://en.wikipedia.org/wiki/Zipf%27s_law>\n\n## Power Law Distribution**\n\nIn[statistics](https://en.wikipedia.org/wiki/Statistics), a**power law**is a functional relationship between two quantities, where a relative change in one quantity results in a proportional relative change in the other quantity, independent of the initial size of those quantities: one quantity varies as a[power](https://en.wikipedia.org/wiki/Exponentiation)of another. For instance, considering the area of a square in terms of the length of its side, if the length is doubled, the area is multiplied by a factor of four.\n\n![](media/Other-Statistics-image5.png)\n\nAn example power-law graph, being used to demonstrate ranking of popularity. To the right is the[long tail](https://en.wikipedia.org/wiki/Long_tail), and to the left are the few that dominate (also known as the[80--20 rule](https://en.wikipedia.org/wiki/Pareto_principle)).\n<https://en.wikipedia.org/wiki/Power_law>\n\n## Pivot Table**\n\nA**pivot table**is a[table](https://en.wikipedia.org/wiki/Table_(information))of statistics that summarizes the data of more extensive table (such as from a[database](https://en.wikipedia.org/wiki/Database),[spreadsheet](https://en.wikipedia.org/wiki/Spreadsheet), or[business intelligence program](https://en.wikipedia.org/wiki/Business_intelligence_software)). This summary might include sums, averages, or other statistics, which the pivot table groups together in a meaningful way.\nPivot tables are a technique in[data processing](https://en.wikipedia.org/wiki/Data_processing). They enable a person to arrange and rearrange (or \"pivot\") statistics in order to draw attention to useful information.\n<https://en.wikipedia.org/wiki/Pivot_table>\n\n## Empirical Cumulative Distribution Function**\n\nThis plot draws a line showing us what percent of our data falls below a certain value on the x-axis. Here the x-axis displays the value of the data and the y-axis the percent of our data that falls below a specific value on the x-axis.\nOn the chart below, take the point 6 on the x-axis and use your finger to trace a straight path up until you hit the line---the value on the y-axis at the point you hit the line is about 75%. So 75% of the data in the example below are lower than 6.\n\n![pasted image 0 (8)](media/Other-Statistics-image6.png)\n\n**Gini Coefficient**\n\nIn[economics](https://en.wikipedia.org/wiki/Economics), theGini coefficient, sometimes called theGini indexorGini ratio, is a[measure of statistical dispersion](https://en.wikipedia.org/wiki/Statistical_dispersion#Measures_of_statistical_dispersion)intended to represent the[income](https://en.wikipedia.org/wiki/Income_distribution)or[wealth distribution](https://en.wikipedia.org/wiki/Wealth_distribution)of a nation's residents, and is the most commonly used measurement of[inequality](https://en.wikipedia.org/wiki/Economic_inequality). It was developed by the Italian [statistician](https://en.wikipedia.org/wiki/Statistics) and [sociologist](https://en.wikipedia.org/wiki/Sociology) [Corrado Gini](https://en.wikipedia.org/wiki/Corrado_Gini) and published in his 1912 paperVariability and Mutability.\nThe Gini coefficient measures the inequality among values of a[frequency distribution](https://en.wikipedia.org/wiki/Frequency_distribution)(for example, levels of[income](https://en.wikipedia.org/wiki/Income)). A Gini coefficient of zero expresses perfect equality, where all values are the same (for example, where everyone has the same income). A Gini coefficient of one (or 100%) expresses maximal inequality among values (e.g., for a large number of people, where only one person has all the income or consumption, and all others have none, the Gini coefficient will be very nearly one).For larger groups, values close to one are very unlikely in practice. Given the normalization of both the cumulative population and the cumulative share of income used to calculate the Gini coefficient, the measure is not overly sensitive to the specifics of the income distribution, but rather only on how incomes vary relative to the other members of a population. The exception to this is in the[redistribution of income](https://en.wikipedia.org/wiki/Redistribution_of_income)resulting in a minimum income for all people. When the population is sorted, if their income distribution were to approximate a well-known function, then some representative values could be calculated.\nThe Gini coefficient was proposed by Gini as a measure of[inequality](https://en.wikipedia.org/wiki/Social_inequality)of[income](https://en.wikipedia.org/wiki/Income_inequality_metrics)or[wealth](https://en.wikipedia.org/wiki/Wealth_concentration).For[OECD countries](https://en.wikipedia.org/wiki/Organisation_for_Economic_Co-operation_and_Development), in the late 20th century, considering the effect of taxes and[transfer payments](https://en.wikipedia.org/wiki/Transfer_payments), the income Gini coefficient ranged between 0.24 and 0.49, with Slovenia being the lowest and Mexico the highest.African countries had the highest pre-tax Gini coefficients in 2008--2009, with South Africa the world's highest, variously estimated to be 0.63 to 0.7,although this figure drops to 0.52 after social assistance is taken into account, and drops again to 0.47 after taxation.The global income Gini coefficient in 2005 has been estimated to be between 0.61 and 0.68 by various sources.\nThere are some issues in interpreting a Gini coefficient. The same value may result from many different distribution curves. The demographic structure should be taken into account. Countries with an aging population, or with a baby boom, experience an increasing pre-tax Gini coefficient even if real income distribution for working adults remains constant. Scholars have devised over a dozen variants of the Gini coefficient.\n<https://en.wikipedia.org/wiki/Gini_coefficient>\n\n<https://www.investopedia.com/terms/g/gini-index.asp>\n\n## Lorenz Curve**\n\nThe Lorenz curve is a graphical representation of[income inequality](https://www.investopedia.com/terms/i/income-inequality.asp)or wealth inequality developed by American economistMax Lorenz in 1905. The graph plotspercentiles of the population on the horizontal axis according to income or wealth. It plots cumulativeincome or wealth on the vertical axis, so that an x-value of 45 and a y-value of 14.2 would mean that the bottom 45% of the population controls 14.2% of the total income or wealth.\n\n![00 ãƒ­ ãƒ­ ãƒ­ 000000 ãƒ­ 00 ãƒ­ 000000 ãƒ­ ãƒ­ 0 00 ãƒ­ ãƒ­ 00 ãƒ­ 000 ãƒ­ ãƒ­ 00 ãƒ­ ãƒ­ 00 ãƒ­ ãƒ­ 00 ã‚ 0 ãƒ¼ 0 å› ãƒ­ ãƒ­ ãƒ­ å› ãƒ­ ãƒ­ 00 ãƒ­ ãƒ­ 00 ãƒ­ 000 ãƒ­ ãƒ­ 0 0 ãƒ­ ãƒ¼ 0 ãƒ­ ãƒ­ ãƒ­ ãƒ­ 00000 ãƒ­ 000 ãƒ­ ãƒ­ 00 ãƒ­ ãƒ­ ã‚ 0 ãƒ­ ãƒ­ ! 3WO)Nl ã¡ V ã¾ å²© ãƒŸ ã ) ç¼¶ d 00 ãƒ­ ãƒ­ 00 ãƒ­ ãƒ­ 00 ãƒ­ ãƒ­ 00 ãƒ­ ãƒ­ 00 ã‚ 000 ãƒ­ 000 ãƒ­ ãƒ­ 0 ãƒ­ ãƒ­ 00 ãƒ­ 000 ãƒ­ ãƒ­ 0 ã‚ 0000 ãƒ­ ãƒª 00 ãƒ­ ãƒ­ ãƒ­ 00 ãƒ­ 00 ãƒ­ ãƒ­ 0 å£ ã‚ 0 ãƒ­ ãƒ­ ãƒ­ 0 ãƒ¼ åŠ‡ 00 ãƒ­ ãƒ­ 0 ãƒ­ ãƒ­ ãƒ­ 0 ãƒ­ ãƒ­ â€² ã€ 0 0 å› ãƒ­ ãƒ­ ãƒ­ 00 ãƒ­ 0 å› 00 å› ãƒ­ ãƒ­ 0 0 ãƒ­ 00 ãƒ­ ãƒ­ 00 ãƒ­ 0 ãƒ­ ã€ 0 ãƒ­ 0 0 ãƒ­ ãƒ­ 0 ãƒ­ ãƒ­ 0 ãƒ­ ãƒ­ ãƒ­ 00 ãƒ­ 0 ãƒ­ æ¾„ ã‚ 000 ãƒ­ ç›´ æœŸ 0 ãƒ­ â€² ãƒ­ ãƒ­ ãƒ­ ãƒ­ 0 ãƒ­ ãƒ­ ãƒ­ 0 ãƒ­ ãƒ­ 0 0000 ãƒ­ å¯ å»¶ 00 00 ãƒ­ ãƒ­ 00 ãƒ­ ãƒ­ 0000 00 ãƒ­ ãƒ­ 0 ãƒ­ 0 0 0 å®¿ 0 ãƒ­ ãƒ­ ãƒ­ ãƒ­ ãƒ­ ãƒ­ ãƒ­ ãƒ­ ãƒ­ ã‚ 0 ãƒ­ ãƒ­ ãƒ­ ãƒ­ ãƒ­ ãƒ­ 0 ãƒ» ã‚¯ â–  0 ãƒ­ ãƒ­ ãƒ­ 0 ãƒ­ ãƒ­ ãƒ­ 0 ã‚ 0 ãƒ­ ãƒ­ ãƒ­ ãƒ­ 00 ãƒ­ 0 ã‚¯ â–  ã‚ â–  0 ãƒ­ ãƒ­ 00 ãƒ­ ãƒ­ ãƒ­ 0 ãƒ­ 000 ãƒ­ 00 â€² 2 ã€ - â–  â–  å› ãƒ­ ãƒ­ ãƒ­ 0 000 ãƒ­ ãƒ­ 0 â€² ãƒ­ 0 ãƒ­ ãƒ¬ 0 ãƒ­ ãƒ­ 0 ãƒ­ ãƒ­ ãƒ­ 0 0 ãƒ­ ãƒ­ å› å›º ãƒ­ ãƒ­ 00 ãƒ­ ãƒ­ 000 ãƒ­ 000 ãƒ­ PERCENTAGE OF HOUSEHOLDS 100 ](media/Other-Statistics-image7.jpeg)\n<https://www.investopedia.com/terms/l/lorenz-curve.asp>\n\n## Rank Order Scale / Rank Ordering / Ranking scale**\n\nRank order items are analyzed using[Spearman](http://changingminds.org/explanations/research/analysis/spearman.htm)or[Kendall](http://changingminds.org/explanations/research/analysis/kendall.htm)correlation.\n**Statistical Power**\n\n<https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python>\n\n## Effect Size**\n\n<https://machinelearningmastery.com/effect-size-measures-in-python>\n\n"},{"fields":{"slug":"/Mathematics/Statistics/Sampling/","title":"Sampling"},"frontmatter":{"draft":false},"rawBody":"# Sampling\n\nCreated: 2021-10-02 14:33:50 +0500\n\nModified: 2022-11-23 23:12:03 +0500\n\n---\n-   Sampling is the main technique employed for data selection\n    -   It is often used for both the preliminary investigation of the data and the final data analysis\n-   Statisticians samples because obtaining the entire set of data of interest is too expensive or time consuming\n-   Sampling is used in data mining because processing the entire set of data of interest is too expensive or time consuming\n-   The key principle for effective sampling is the following:\n    -   Using a sample will work almost as well as using the entire data sets, if the sample is representative\n    -   A sample is representative if it has approximately the same property (of interest) as the original set of data\n**Probability vs non-probability sampling**\n\nIn a probability-based sample, the observations/subjects are picked at random. In a non-probability sample, the researcher hand picks the sample, based upon criteria that he or she picks\n**Types of Sampling**\n-   **Simple random sampling**\n    -   There is an equal probability of selecting any particular item\n-   **Samling without replacement**\n    -   As each item is selected, it is removed from the population\n-   **Sampling with replacement**\n    -   Objects are not removed from the population as they are selected for the sample\n        -   In sampling with replacement, the same objects can be picked up more than once\n-   **Stratified sampling**\n    -   Split the data into several partitions; then draw random samples from each paritition\n-   **Cluster Random**\n    -   You break the population into groups, and then randomly select some of these groups and collect data on each group member\nData sampling is a[statistical analysis](https://whatis.techtarget.com/definition/statistical-analysis)technique used to select, manipulate and analyze a representative subset of data points to identify patterns and trends in the larger[data set](https://whatis.techtarget.com/definition/data-set)being examined. It enables[data scientists](https://searchenterpriseai.techtarget.com/definition/data-scientist), predictive modelers and other data analysts to work with a small, manageable amount of data about a statistical[population](https://whatis.techtarget.com/definition/population)to build and run analytical models more quickly, while still producing accurate findings.\n**Advantages and challenges of data sampling**\n\nSampling can be particularly useful with data sets that are too large to efficiently analyze in full -- for example, in[big data analytics](https://searchbusinessanalytics.techtarget.com/definition/big-data-analytics)applications or surveys. Identifying and analyzing a representative sample is more efficient and cost-effective than surveying the entirety of the data or population.\nAn important consideration, though, is the size of the required data sample and the possibility of introducing a[sampling error](https://whatis.techtarget.com/definition/sampling-error). In some cases, a small sample can reveal the most important information about a data set. In others, using a larger sample can increase the likelihood of accurately representing the data as a whole, even though the increased size of the sample may impede ease of manipulation and interpretation.\n**Types of data sampling methods**\n\nThere are many different methods for drawing samples from data; the ideal one depends on the data set and situation. Sampling can be based on[probability](https://whatis.techtarget.com/definition/probability), an approach that uses[random numbers](https://whatis.techtarget.com/definition/random-numbers)that correspond to points in the data set to ensure that there is no correlation between points chosen for the sample. Further variations in probability sampling include:\n-   Simple random sampling:Software is used to randomly select subjects from the whole population.\n-   Stratified sampling:Subsets of the data sets or population are created based on a common factor, and samples are randomly collected from each subgroup.\n-   Cluster sampling:The larger data set is divided into subsets (clusters) based on a defined factor, then a random sampling of clusters is analyzed.\n-   Stratified random sampling - splits the population into groups of interest and randomly selects people from each of the \"stratas\" so that each group in the overall sample is represented appropriately\n-   Multistage sampling:A more complicated form of cluster sampling, this method also involves dividing the larger population into a number of clusters. Second-stage clusters are then broken out based on a secondary factor, and those clusters are then sampled and analyzed. Thisstagingcould continue as multiple subsets are identified, clustered and analyzed.\n-   Systematic sampling:A sample is created by setting an interval at which to extract data from the larger population -- for example, selecting every 10th row in a spreadsheet of 200 items to create a sample size of 20 rows to analyze.\n-   Snowball sampling - When current respondants are asked to help recruit people they know from the population of interest\n-   Census - A survey that samples an entire population\nSampling can also be based on nonprobability, an approach in which a data sample is determined and extracted based on the judgment of the analyst. As inclusion is determined by the analyst, it can be more difficult to extrapolate whether the sample accurately represents the larger population than when probability sampling is used.\nNonprobability data sampling methods include:\n-   Convenience sampling:Data is collected from an easily accessible and available group.\n-   Consecutive sampling:Data is collected from every subject that meets the criteria until the predetermined sample size is met.\n-   Purposive or judgmental sampling:The researcher selects the data to sample based on predefined criteria.\n-   Quota sampling:The researcher ensures equal representation within the sample for all subgroups in the data set or population.\nOnce generated, a sample can be used for[predictive analytics](https://searchbusinessanalytics.techtarget.com/definition/predictive-analytics). For example, a retail business might use data sampling to uncover patterns about customer behavior and[predictive modeling](https://searchenterpriseai.techtarget.com/definition/predictive-modeling)to create more effective sales strategies.\n**Sample Size**\n\n![8000 points 2000 Points 500 Points ](media/Sampling-image1.jpeg)\n**Sampling Sins**\n-   Bias: The sample has to be representative of the population. If the sampling method creates bias, the results from the sample cannot be extrapolated to the population\n    -   Exclusion: Some parts of the population may not even make into the sampling universe\n    -   Self-selection: Some parts of the population may be more easily accessible than other parts, given how you collect data\n    -   Non-response: Some parts of the population may be less likely to respond to requests for data\n    -   Survivorship: Success (of failure, sometimes) may make an observation more likely to be sampled\n-   Noise: Even if the sample is representative, the results that you obtain will have statistical error or noise that can muddy your conclusions.\n**Why do we need sampling?**\n\n1.  **Practicality:** If the population is too large to collect data on, and/or a subset (small or large) of the population is inaccessible, you have no choice but to sample the data\n\n2.  **Costs:** Even if you could collect data on the entire population, the costs (in time and money) may outweigh the benefits of doing so\n\n3.  **Time trade off:** Related to the second point is the question of how frequently you want to update the data. It is easier to update sampled data than data on the entire population\n**Independence + Identical Distributions (ID) (IID)**\n-   In almost any discussion of sampling and statistics, the words independence and identical distributions thrown in as pre-requisites or at least good qualities in a sample\n    -   Independence: Events are independent when whether an event occurs or not is not determined by other events occuring\n        -   Coin tosses are a classic example of independence\n        -   Are stock price changes independent\n    -   Identical Distributions: Each event draws from the same probability distribution\n        -   Coin tosses draw from the same distribution\n        -   Do stock price changes draw from the same distribution\n-   In finance, researchers often assume independence and identical distributions, in making assertions based upon samples, but the truth is that both characteristics are hard to find\n**Stratified Random Sampling**\n\nIn the context of sampling,stratifiedmeans splitting the population into smaller groups or strata based on a characteristic. To put it another way, you divide a population into groups based on their features\nRandomsamplingentails randomly selecting subjects (entities) from a population. Each subject has an equal probability of being chosen from the population to form a sample (subpopulation) of the overall population\nSo therefore,stratified random samplingis a sampling approach in which the population is separated into groups or strata depending on a particular characteristic. Then subjects from each stratum (the singular of strata) are randomly sampled\nYou divide the population into groups based on a characteristic and then choose a subject or entity at random from each group\n**Types of Stratified Random Sampling**\n\nStratified sampling is divided into two categories, which are:\n-   **Proportionate stratified random sampling**\n-   **Disproportionate stratified random sampling**\n<https://www.freecodecamp.org/news/what-is-stratified-random-sampling-definition-and-python-example>\n\n"},{"fields":{"slug":"/Databases/Concepts/MVCC,-MultiVersion-Concurrency-Control/","title":"MVCC, MultiVersion Concurrency Control"},"frontmatter":{"draft":false},"rawBody":"# MVCC, MultiVersion Concurrency Control\n\nCreated: 2019-06-26 23:35:49 +0500\n\nModified: 2021-07-26 16:55:00 +0500\n\n---\n\nThe DBMS maintans physical versions of a single logical object in the database:\n-   When a txn writes to an object, the DBMS creates a new version of that object\n-   When a txn reads an object, it reads the newest version that existed when the txn started.\nMVCC means that while querying a database each transaction sees a snapshot of data (adatabase version) as it was some time ago, regardless of the current state of the underlying data. This protects the transaction from viewing inconsistent data that could be caused by (other) concurrent transaction updates on the same data rows, providingtransaction isolationfor each database session.\nThe main difference between multiversion and lock models is that in MVCC locks acquired for querying (reading) data don't conflict with locks acquired for writing data and so reading never blocks writing and writing never blocks reading.\nMultiversion concurrency control(MCCorMVCC), is a[concurrency control](https://en.wikipedia.org/wiki/Concurrency_control)method commonly used by[database management systems](https://en.wikipedia.org/wiki/Database_management_system)to provide concurrent access to the database and in programming languages to implement [transactional memory](https://en.wikipedia.org/wiki/Transactional_memory).\nWithout concurrency control, if someone is reading from a database at the same time as someone else is writing to it, it is possible that the reader will see a half-written or[inconsistent](https://en.wikipedia.org/wiki/Consistency_(database_systems))piece of data. For instance, when making a wire transfer between two bank accounts if a reader reads the balance at the bank when the money has been withdrawn from the original account and before it has deposited in the destination account, it would seem that money has disappeared from the bank.[Isolation](https://en.wikipedia.org/wiki/ACID#Isolation)is the property that provides guarantees in the concurrent accesses to data. Isolation is implemented by means of a[concurrency control](https://en.wikipedia.org/wiki/Concurrency_control)protocol. The simplest way is to make all readers wait until the writer is done, which is known as a **read-write[lock](https://en.wikipedia.org/wiki/Lock_(database)).** Locks are known to create contention especially between long read transactions and update transactions. MVCC aims at solving the problem by keeping **multiple copies of each data item.** In this way, each user connected to the database sees asnapshotof the database at a particular instant in time. Any changes made by a writer will not be seen by other users of the database until the changes have been completed (or, in database terms: until the[transaction](https://en.wikipedia.org/wiki/Database_transaction)has been committed.)\n**When an MVCC database needs to update a piece of data, it will not overwrite the original data item with new data, but instead creates a newer version of the data item.** Thus there are multiple versions stored. The version that each transaction sees depends on the isolation level implemented. The most common isolation level implemented with MVCC is[snapshot isolation](https://en.wikipedia.org/wiki/Snapshot_isolation). With snapshot isolation, a transaction observes a state of the data as when the transaction started. MVCC introduces the challenge of how to remove versions that become obsolete and will never be read. In some cases, a process to periodically sweep through and delete the obsolete versions is implemented. This is often a stop-the-world process that traverses a whole table and rewrites it with the last version of each data item.[PostgreSQL](https://en.wikipedia.org/wiki/PostgreSQL)adopts this approach with its VACUUM process. Other databases split the storage blocks into two parts: the data part and an undo log. The data part always keeps the last committed version. The undo log enables recreation of older versions of data. The main inherent limitation of this latter approach is that when there are update-intensive workloads, the undo log part runs out of space and then transactions are aborted as they cannot be given their snapshot. For a[document-oriented database](https://en.wikipedia.org/wiki/Document-oriented_database)it also allows the system to optimize documents by writing entire documents onto contiguous sections of disk---when updated, the entire document can be re-written rather than bits and pieces cut out or maintained in a linked, non-contiguous database structure.\nMVCC provides[point-in-time consistent](https://en.wikipedia.org/wiki/Data_consistency#Point-in-time_consistency)views. Read transactions under MVCC typically use a timestamp or transaction ID to determine what state of the DB to read, and read these versions of the data. Read and write transactions are thus[isolated](https://en.wikipedia.org/wiki/Isolation_(database_systems))from each other without any need for locking. However, despite locks being unnecessary, they are used by some MVCC databases such as Oracle. Writes create a newer version, while concurrent reads access an older version.\n<https://en.wikipedia.org/wiki/Multiversion_concurrency_control>\n\n## Lock Contention**\n\nLock contention occurs **when many database sessions all require frequent access to the same lock.** This is also often called a \"hot lock\". The locks in question are only held for a short time by each accessing session, then released. This creates a \"single lane bridge\" situation. Problems are not noticeable when traffic is low (i.e. non-concurrent or low-concurrency situations). However, as traffic (i.e. concurrency) increases, a bottleneck is created.\nOverall, Lock Contention problems have a relatively low impact. They manifest themselves by impacting and limiting scalability. As concurrency increases, system throughput does not increase and may even degrade (as shown in Figure 1 below). Lock contention may also lead to high CPU usage on the database server.\nThe best way to identify a lock contention problem is through analysis of statistical information on locks provided by the DBMS\n**Long Term Blocking**\n\nLong Term Blocking is similar to Lock Contention in that **it involves an object or lock that is frequently accessed by a large number of database sessions**. Where it differs is that in this case, one session does not release the lock immediately. Instead, the lock is held for a long period of time and while that lock is held, all dependent sessions will be blocked.\nLong Term Blocking tends to be a much bigger problem than Lock Contention. It can bring an entire area of functionality or even a whole system to a stand still. The locks involved in these scenarios may not be \"hot\" enough to lead to Lock Contention problems under normal circumstances. As such, these problems may be intermittent and very dependent on certain coincidental activity. These are the most likely to lead to \"disasters\" in production due to the combination of devastating impact and difficulty to reproduce.\nThe consequences of long term blocking problems may be abandonment. However, these problems can also often lead to further problems as frustrated users re-submit their requests. This can compound and exacerbate the problem by leading to a larger queue and consuming additional resource. In this way, the impact can expand to consume an entire system.\n"},{"fields":{"slug":"/Databases/Concepts/Isolation-Levels/","title":"Isolation Levels"},"frontmatter":{"draft":false},"rawBody":"# Isolation Levels\n\nCreated: 2019-07-02 22:58:00 +0500\n\nModified: 2022-02-23 15:36:43 +0500\n\n---\n\nSELECT @@TX_ISOLATION; - REPEATABLE-READ\n\nSHOW ENGINE INNODB STATUS;\nIn[database](https://en.wikipedia.org/wiki/Database)systems,isolationdetermines how[transaction](https://en.wikipedia.org/wiki/Database_transaction)integrity is visible to other users and systems. For example, when a user is creating a Purchase Order and has created the header, but not the Purchase Order lines, is the header available for other systems/users (carrying out[concurrent](https://en.wikipedia.org/wiki/Concurrency_(computer_science))operations, such as a report on Purchase Orders) to see? (Refers to current, not past database systems)\nA lower isolation level increases the ability of many users to access the same data at the same time, but increases the number of concurrency effects (such as dirty reads or lost updates) users might encounter. Conversely, a higher isolation level reduces the types of concurrency effects that users may encounter, but requires more system resources and increases the chances that one transaction will block another.\nIsolation is typically defined at database level as a property that defines how/when the changes made by one operation become visible to other. On older systems, it may be implemented systemically, for example through the use of temporary tables. In two-tier systems, a Transaction Processing (TP) manager is required to maintain isolation. In n-tier systems (such as multiple websites attempting to book the last seat on a flight), a combination of stored procedures and transaction management is required to commit the booking and send confirmation to the customer.\nIsolation is one of the[ACID](https://en.wikipedia.org/wiki/ACID)([Atomicity](https://en.wikipedia.org/wiki/Atomicity_(database_systems)),[Consistency](https://en.wikipedia.org/wiki/Consistency_(database_systems)), Isolation,[Durability](https://en.wikipedia.org/wiki/Durability_(database_systems))) properties.\n**Read Phenomenon (DLNP)**\n\nThe ANSI/ISO standard SQL 92 refers to three differentread phenomenawhen Transaction 1 reads data that Transaction 2 might have changed.\n-   **Dirty reads**\n\n*Adirty read(akauncommitted dependency)* occurs when a transaction is allowed to read data from a row that has been modified by another running transaction and not yet committed.\n-   **Lost Update**\n\nA lost update occurs when two different transactions are trying to update the same column on the same row within a database at the same time. Typically, one transaction updates a particular column in a particular row, while another that began very shortly afterward did not see this update before updating the same value itself. The result of the first transaction is then \"lost\", as it is simply overwritten by the second transaction.\n-   **Non-repeatable reads**\n\nA*non-repeatable read*occurs, when during the course of a transaction, a row is retrieved twice and the values within the row differ between reads.\n-   **Phantom reads**\n\nA*phantom read*occurs when, in the course of a transaction, new rows are added or removed by another transaction to the records being read.\n**The incorrect summary problem**\n\nWhile one transaction takes a summary over the values of all the instances of a repeated data-item, a second transaction updates some instances of that data-item. The resulting summary does not reflect a correct result for any (usually needed for correctness) precedence order between the two transactions (if one is executed before the other), but rather some random result, depending on the timing of the updates, and whether certain update results have been included in the summary or not.\n**Isolation Levels (SRRR)**\n\nOf the four[ACID](https://en.wikipedia.org/wiki/ACID)properties in a[DBMS](https://en.wikipedia.org/wiki/Database_management_system)(Database Management System), the isolation property is the one most often relaxed. When attempting to maintain the highest level of isolation, a DBMS usually acquires[locks](https://en.wikipedia.org/wiki/Lock_(database))on data which may result in a loss of[concurrency](https://en.wikipedia.org/wiki/Concurrency_(computer_science))or implements [multiversion concurrency](https://en.wikipedia.org/wiki/Multiversion_concurrency_control) control. This requires adding logic for the[application](https://en.wikipedia.org/wiki/Software_application)to function correctly.\nMost DBMSs offer a number oftransaction isolation levels, which control the degree of locking that occurs when selecting data. For many database applications, the majority of database transactions can be constructed to avoid requiring high isolation levels (e.g. SERIALIZABLE level), thus reducing the locking overhead for the system. The programmer must carefully analyze database access code to ensure that any relaxation of isolation does not cause software bugs that are difficult to find. Conversely, if higher isolation levels are used, the possibility of[deadlock](https://en.wikipedia.org/wiki/Deadlock)is increased, which also requires careful analysis and programming techniques to avoid.\nThe isolation levels defined by the[ANSI](https://en.wikipedia.org/wiki/American_National_Standards_Institute)/[ISO](https://en.wikipedia.org/wiki/International_Organization_for_Standardization)[SQL](https://en.wikipedia.org/wiki/SQL)standard are listed as follows.\n\n1.  **Serializable**\n\nThis is the*highest*isolation level.\nWith a lock-based[concurrency control](https://en.wikipedia.org/wiki/Concurrency_control)DBMS implementation, [serializability](https://en.wikipedia.org/wiki/Serializability) requires read and write locks (acquired on selected data) to be released at the end of the transaction. Alsorange-locks must be acquired when a [SELECT](https://en.wikipedia.org/wiki/Select_(SQL)) query uses a rangedWHEREclause, especially to avoid the[phantom reads](https://en.wikipedia.org/wiki/Isolation_(database_systems)#Phantom_reads) phenomenon.\nWhen using non-lock based concurrency control, no locks are acquired; however, if the system detects awrite collisionamong several concurrent transactions, only one of them is allowed to commit. See[snapshot isolation](https://en.wikipedia.org/wiki/Snapshot_isolation) for more details on this topic.\n2.  **Repeatable reads**\n\nIn this isolation level, a lock-based[concurrency control](https://en.wikipedia.org/wiki/Concurrency_control)DBMS implementation keeps read and write locks (acquired on selected data) until the end of the transaction. However,range-locksare not managed, so[phantom reads](https://en.wikipedia.org/wiki/Isolation_(database_systems)#Phantom_reads)can occur.\nWrite skew is possible at this isolation level, a phenomenon where two writes are allowed to the same column(s) in a table by two different writers (who have previously read the columns they are updating), resulting in the column having data that is a mix of the two transactions\n3.  **Read committed**\n\nIn this isolation level, a lock-based[concurrency control](https://en.wikipedia.org/wiki/Concurrency_control)DBMS implementation keeps write locks (acquired on selected data) until the end of the transaction, but read locks are released as soon as the[SELECT](https://en.wikipedia.org/wiki/Select_(SQL))operation is performed (so the[non-repeatable reads phenomenon](https://en.wikipedia.org/wiki/Isolation_(database_systems)#Non-repeatable_reads)can occur in this isolation level). As in the previous level,range-locksare not managed.\nPutting it in simpler words, **read committed is an isolation level that guarantees that any data read is committed at the moment it is read**. It simply restricts the reader from seeing any intermediate, uncommitted, 'dirty' read. It makes no promise whatsoever that if the transaction re-issues the read, it will find the same data; data is free to change after it is read.\n4.  **Read uncommitted**\n\nThis is the*lowest*isolation level. In this level,[dirty reads](https://en.wikipedia.org/wiki/Isolation_(database_systems)#Dirty_reads)are allowed, so one transaction may see*not-yet-committed*changes made by other transactions.\n| **Isolation level** | **Dirty reads** | **Lost updates** | **Non-repeatable reads** | **Phantoms** |\n|------------------|------------|-------------|--------------------|-----------|\n| Read Uncommitted    | may occur       | may occur        | may occur                | may occur    |\n| Read Committed      | don't occur    | may occur        | may occur                | may occur    |\n| Repeatable Read     | don't occur    | don't occur     | don't occur             | may occur    |\n| Serializable        | don't occur    | don't occur     | don't occur             | don't occur |\nIsolation levels in distributed systems get more complicated. Many distributed systems implement variations of the serializable isolation level, such as**one copy-serializability (1SR),strict serializability (strict 1SR)orupdate serializability (US)**. Of those,[**\"strict serializability\"** is the most perfect](https://fauna.com/blog/serializability-vs-strict-serializability-the-dirty-secret-of-database-isolation-levels)of those serializable options.\nThe isolation levels defined as part of SQL-92 standard only focused on anomalies that can occur in a 2PL-based DBMS.\nThere are two additional isolation levels:\n\n1.  CURSOR STABILITY\n    -   Between repeatable reads and read committed\n    -   Prevents \"Lost Update\" Anomaly.\n    -   Default isolation level in IBM DB2.\n2.  SNAPSHOT ISOLATION\n    -   Guarantees that all reads made in a transaction see a consistent snapshot of the database that existed at the time the transaction started.\n    -   A transaction will commit only if its writes do not conflict with any concurrent updates made since that snapshot.\n    -   Susceptible to write skew anomaly.\n![Linearizable Sequential Repeatable Read Cursor Stability Read Committed Read U ncommitted Strict Serializable Serializable Snapshot Isolation Monotonic Atomic View Writes Follow Reads Monotonic Reads Causal Monotonic Writes Read Your Writes ](media/Isolation-Levels-image1.png)\n<https://en.wikipedia.org/wiki/Isolation_(database_systems)>\n\n<http://highscalability.com/blog/2011/2/10/database-isolation-levels-and-their-effects-on-performance-a.html>\n\n<https://fauna.com/blog/introduction-to-transaction-isolation-levels>\n\n## Demystifying Database Systems: Correctness Anomalies Under SerializableIsolation**\n\n**What Does \"Serializable\" Mean in a Distributed/ReplicatedSystem?**\n\nWe defined \"serializable isolation\" above as a guarantee that even though a database system is allowed to run transactions in parallel, the final result is equivalent to as if they were running one after the other. In a replicated system, this guarantee must be strengthened in order to avoid the anomalies that would only occur at lower levels of isolation in non-replicated systems.-   The Immortal Write\n-   The Stale Read\n-   The Casual Reverse\n**Classification of SerializableSystems**\n\n| **System Guarantee**        | **Immortal write**                     | **Stale read**                         | **Causal reverse**                     |\n|-----------------|-------------------|-------------------|------------------|\n| ONE COPY SERIALIZABLE       | Possible                               | Possible                               | Possible                               |\n| STRONG SESSION SERIALIZABLE | Possible(but not within same session) | Possible(but not within same session) | Possible(but not within same session) |\n| ASYNCHRONOUS SERIALIZABLE   | Not Possible                           | Possible                               | Not Possible                           |\n| PARTITIONED SERIALIZABLE    | Not Possible                           | Not Possible                           | Possible                               |\n| STRICT SERIALIZABLE         | Not Possible                           | Not Possible                           | Not Possible                           |\n<https://fauna.com/blog/demystifying-database-systems-correctness-anomalies-under-serializable-isolation>\n\n## Isolation Table with Anomalies**\n\n| **System Guarantee**                                               | **Dirty read** | **Non-repeatable read** | **Phantom Read** | **Write Skew** | **Immortal write** | **Stale read** | **Causal reverse** |\n|--------------|--------|----------|---------|--------|---------|--------|--------|\n| READ UNCOMMITTED                                                   | Possible       | Possible                | Possible         | Possible       | Possible           | Possible       | Possible           |\n| READ COMMITTED                                                     | Not Possible   | Possible                | Possible         | Possible       | Possible           | Possible       | Possible           |\n| REPEATABLE READ                                                    | Not Possible   | Not Possible            | Possible         | Possible       | Possible           | Possible       | Possible           |\n| SNAPSHOT ISOLATION                                                 | Not Possible   | Not Possible            | Not Possible     | Possible       | Possible           | Possible       | Possible           |\n| SERIALIZABLE / ONE COPY SERIALIZABLE / STRONG SESSION SERIALIZABLE | Not Possible   | Not Possible            | Not Possible     | Not Possible   | Possible           | Possible       | Possible           |\n| ASYNCHRONOUS SERIALIZABLE                                          | Not Possible   | Not Possible            | Not Possible     | Not Possible   | Not Possible       | Possible       | Not Possible       |\n| PARTITIONED SERIALIZABLE                                           | Not Possible   | Not Possible            | Not Possible     | Not Possible   | Not Possible       | Not Possible   | Possible           |\n| STRICT SERIALIZABLE                                                | Not Possible   | Not Possible            | Not Possible     | Not Possible   | Not Possible       | Not Possible   | Not Possible       |\n**Distributed Locking**\n-   **Efficiency:**Taking a lock saves you from unnecessarily doing the same work twice (e.g. some expensive computation). If the lock fails and two nodes end up doing the same piece of work, the result is a minor increase in cost (you end up paying 5 cents more to AWS than you otherwise would have) or a minor inconvenience (e.g. a user ends up getting the same email notification twice)\n-   **Correctness:**Taking a lock prevents concurrent processes from stepping on each others' toes and messing up the state of your system. If the lock fails and two nodes concurrently work on the same piece of data, the result is a corrupted file, data loss, permanent inconsistency, the wrong dose of a drug administered to a patient, or some other serious problem\n<https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html>\n\n## SLOG - Serializable, Low-Latency, Geo-Replicated transactions**\n**MySQL**\n\nYou can enforce a high degree of consistency with the default[REPEATABLE READ](https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html#isolevel_repeatable-read)level, for operations on crucial data where[ACID](https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_acid)compliance is important. Or you can relax the consistency rules with[READ COMMITTED](https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html#isolevel_read-committed)or even[READ UNCOMMITTED](https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html#isolevel_read-uncommitted), in situations such as bulk reporting where precise consistency and repeatable results are less important than minimizing the amount of overhead for locking. [SERIALIZABLE](https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html#isolevel_serializable)enforces even stricter rules than[REPEATABLE READ](https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html#isolevel_repeatable-read), and is used mainly in specialized situations, such as with[XA](https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_xa)transactions and for troubleshooting issues with concurrency and[deadlocks](https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_deadlock).\n**MySQL Lost Update problem**\n\n<https://stackoverflow.com/questions/53562850/mysql-repeatable-read-isolation-level-and-lost-update-phenomena>\n\n<https://forums.mysql.com/read.php?22,56420,57733>\n\n<https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html>\n\n## References**\n\n<https://dbmsmusings.blogspot.com/2019/08/an-explanation-of-difference-between.html>\n\n<http://dbmsmusings.blogspot.com/2019/10/introducing-slog-cheating-low-latency.html>\n\n"},{"fields":{"slug":"/Databases/Concepts/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2020-03-01 14:46:21 +0500\n\nModified: 2021-07-24 17:22:38 +0500\n\n---\n\n**Transactions**\n\nA transaction is defined as a sequence of actionsthat are executed on a shared database to perform some higher-level function. It is a basic unit of change in the DBMS. No partial transactions are allowed.\nThere are three categories of actions that the DBMS can execute.\n-   Unprotected Actions\n\nLow-level operations on physical resources (e.g., disk, memory). These lack all of the ACID properties except for consistency. Their effects cannot be depended upon.\n-   Protected Actions\n\nThese are the high-level changes that the application wants to perform on the database. The DBMS does not externalize their results before they are completely done. Fully ACID.\n-   Real Actions\n\nThese affect the physical world in a way that is hard or impossible to reverse. For example, if the application sends out an email, then the DBMS cannot retract it.\n**Transaction Models**\n\nA transaction model specifies the execution semantics of protected actions.\n**Flat Transactions**\n\nStandard transaction model that starts with BEGIN, followed by one or more actions, and then completed with either COMMIT or ROLLBACK. This is what most people think of when discussing transaction support in a DBMS.\nThere are several limitations to flat transactions that motivate us to consider other models. Foremost is that the application can only rollback the entire transaction (i.e., no partial rollbacks). All of a transaction's work is lost if the DBMS fails before that transaction finishes. Each transaction takes place at a single point in time.\n**Transaction Savepoints**\n\nSave the current state of processing for the transaction and provide a handle for the application to refer to that savepoint.The application can control the state of the transaction through these savepoints. The application can create a handle with the SAVEPOINT command during a transaction. It can use ROLLBACK to revert all changes back to the state of the database at a given savepoint. It can also use RELEASE to destroy a savepoint previously defined in the transaction.\n**Nested Transactions**\n\nThe invocation of a transaction during the execution of another transaction. The nested transactions form a hierarchy of work. The outcome of a child transaction depends on the outcome of its parent transaction.\n**Chained Transactions**\n\nThe ability to link multiple transactions one after each other. The combined COMMIT and BEGIN operations between two transactions is atomic. This means that no other transaction can change the state of the databaseas seen by the second transaction from the time that the first transaction commits and the second transaction begins.\n**Compensating Transactions**\n\nA special type of transaction that is designed to semantically reverse the effects of another already committed transaction. Such a reversal has to be logical instead of physical.\n**Saga Transactions**\n\nA sequence of chained transactions T1-Tn and compensating transactions C1-Cnâˆ’1 where one of the following is guaranteed: The transactions will commit in the order T1,. . .Tj,Cj. . .C1 (where j < n).\n**Schedule**\n\nA schedule is a series of operations from one or more transactions. A schedule can be of two types:\n-   **Serial Schedule:**\n\nWhen one transaction completely executes before starting another transaction, the schedule is called serial schedule. A serial schedule is always consistent. e.g.; If a schedule S has debit transaction T1 and credit transaction T2, possible serial schedules are T1 followed by T2 (T1->T2) or T2 followed by T1 ((T1->T2). A serial schedule has low throughput and less resource utilization.-   **Concurrent Schedule:**\n\nWhen operations of a transaction are interleaved with operations of other transactions of a schedule, the schedule is called Concurrent schedule. But concurrency can lead to inconsistency in the database.\n**Conflict Serializability in DBMS**\n\nSerial schedules have less resource utilization and low throughput. To improve it, two are more transactions are run concurrently. But concurrency of transactions may lead to inconsistency in database. To avoid this, we need to check whether these concurrent schedules are serializable or not.\n**Conflict Serializable:**A schedule is called conflict serializable if it can be transformed into a serial schedule by swapping non-conflicting operations.\n**Conflicting operations:**Two operations are said to be conflicting if all conditions satisfy:\n-   They belong to different transactions\n-   They operate on the same data item\n-   At Least one of them is a write operation\nExample\n-   **Conflicting**operations pair (R1(A), W2(A)) because they belong to two different transactions on same data item A and one of them is write operation.\n-   Similarly, (W1(A), W2(A)) and (W1(A), R2(A)) pairs are also**conflicting**.\n-   On the other hand, (R1(A), W2(B)) pair is**non-conflicting**because they operate on different data item.\n-   Similarly, ((W1(A), W2(B)) pair is**non-conflicting**.\n**Topics**\n-   Concurrency Control\n-   Indexing\n-   Storage Models, Compression\n-   Parallel Join Algorithms\n-   Networking Protocols\n-   Logging & Recovery Methods\n-   Query Optimization, Execution, Compilation\n**Others**\n\n<https://docs.oracle.com/cd/B19306_01/server.102/b14220/toc.htm>\n\n<https://medium.com/@rakyll/things-i-wished-more-developers-knew-about-databases-2d0178464f78>\n\n<https://littlekendra.com>\n\n[**https://www.freecodecamp.org/news/watch-a-cornell-university-database-course-for-free/**](https://www.freecodecamp.org/news/watch-a-cornell-university-database-course-for-free/)\n\n[**https://www.youtube.com/watch?v=BQBGORBPytw**](https://www.youtube.com/watch?v=BQBGORBPytw)\n\n[**https://www.youtube.com/watch?v=ER8oKX5myE0**](https://www.youtube.com/watch?v=ER8oKX5myE0)\n-   **Database modeling**\n-   **Isolation levels**\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Client,-Broker-&-Connection-Establishment/","title":"Client, Broker & Connection Establishment"},"frontmatter":{"draft":false},"rawBody":"# Client, Broker & Connection Establishment\n\nCreated: 2019-03-12 17:24:21 +0500\n\nModified: 2019-03-12 18:20:07 +0500\n\n---\n\n**Client**\n\nBoth publishers and subscribers are MQTT clients.\n\nThe publisher and subscriber labels refer to whether the client is currently publishing messages or subscribing to messages (publish and subscribe functionality can also be implemented in the same MQTT client)\n\nBasically, any device that speaks MQTT over a TCP/IP stack can be called an MQTT client.\n**Broker**\n\nThe broker is responsible for receiving all messages, filtering the messages, determining who is subscribed to each message, and sending the message to these subscribed clients.\n\nThe broker also holds the sessions of all persisted clients, including subscriptions and missed messages\n**MQTT Connection**\n\nThe MQTT protocol is based on TCP/IP. Both the client and the broker need to have a TCP/IP stack.\nThe MQTT connection is always between one client and the broker. Clients never connect to each other directly. To initiate a connection,**the client sends a CONNECT message to the broker. The broker responds with a CONNACK message**and a status code. Once the connection is established, the broker keeps it open until the client sends a disconnect command or the connection breaks.\n\n![MQTT Connection Flow](media/Client,-Broker-&-Connection-Establishment-image1.gif)\n**MQTT Connect**\n\nTo initiate a connection, the client sends a command message to the broker. If this CONNECT message is malformed (according to the MQTT specification) or too much time passes between opening a network socket and sending the connect message, the broker closes the connection.\n\n![MQTT Connect message content](media/Client,-Broker-&-Connection-Establishment-image2.png)\n\n**ClientID**\n\nThe client identifier (ClientId)identifies each MQTT clientthat connects to an MQTT broker. The broker uses the ClientID to identify the client and the current state of the client.Therefore, this ID should be unique per client and broker. In MQTT 3.1.1 (the current standard), you can send an empty ClientId, if you don't need a state to be held by the broker. The empty ClientID results in a connection without any state. In this case, the clean session flag must be set to true or the broker will reject the connection.\n**Clean Session**\n\nThe clean session flag tells the broker whether the client wants to establish a persistent session or not. In a persistent session (CleanSession = false), the broker stores all subscriptions for the client and all missed messages for the client that subscribed with a[Quality of Service (QoS)](https://www.hivemq.com/blog/mqtt-essentials-part-6-mqtt-quality-of-service-levels/)level 1 or 2. If the session is not persistent (CleanSession = true), the broker does not store anything for the client and purges all information from any previous persistent session.\n**Username/Password**\n\nMQTT can send auser name and password for client authentication and authorization. However, if this information isn't encrypted or hashed (either by implementation or TLS), the password is sent in plain text. We highly recommend the use of user names and passwords together with a secure transport.\n**Will Message**\n\nThe last will message is part of the Last Will and Testament (LWT) feature of MQTT.This message notifies other clients when a client disconnects ungracefully.When a client connects, it can provide the broker with a last will in the form of an MQTT message and topic within the CONNECT message. If the client disconnects ungracefully, the broker sends the LWT message on behalf of the client.\n**KeepAlive**\n\nThe keep alive isa time interval in secondsthat the client specifies and communicates to the broker when the connection established. This interval defines the longest period of time that the broker and client can endure without sending a message.The client commits to sending regular PING Request messages to the broker. The broker responds with a PING response. This method allows both sides to determine if the other one is still available.\n**CONNACK Message**\n\nWhen a broker receives a CONNECT message, it is obligated to respond with a CONNACK message.\n\nThe CONNACK message contains two data entries:\n-   The session present flag\n-   A connect acknowledge flag\n\n![MQTT Connack contents](media/Client,-Broker-&-Connection-Establishment-image3.png)\n**Session Presentflag**\n\nThesession present flag tells the client whether the broker already has a persistent session available from previous interactions with the client. When a client connects with Clean Session set to true, the session present flag is always false because there is no session available. If a client connects with Clean Session set to false, there are two possibilities: If session information is available for the client Id. and the broker has stored session information, the session present flag is true. Otherwise, if the broker does not have any session information for the client ID, the session present flag is false. This flag was added in MQTT 3.1.1 to help clients determine whether they need to subscribe to topics or if the topics are still stored in a persistent session.\n**Connect acknowledge flag**\n\nThe second flag in the[CONNACK](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718033)message is the connect acknowledge flag. This flag containsa return code that tells the client whether the connection attempt was successful.\n\n| Return Code | Return Code Response                              |\n|-------------|---------------------------------------------------|\n| 0           | Connection accepted                               |\n| 1           | Connection refused, unacceptable protocol version |\n| 2           | Connection refused, identifier rejected           |\n| 3           | Connection refused, server unavailable            |\n| 4           | Connection refused, bad user name or password     |\n| 5           | Connection refused, not authorized                |\n**References**\n\n<https://www.hivemq.com/blog/mqtt-essentials-part-3-client-broker-connection-establishment>\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-04-24 11:27:12 +0500\n\nModified: 2021-06-06 16:48:35 +0500\n\n---\n\n**Outline**\n\n**Basic**\n-   Publish and Subscribe Basics\n-   Client, Broker and Connection Establishment\n-   Publish, Subscribe and Unsubscribe\n-   Topics and Best Practices\n**Features**\n-   Quality of Service Levels\n-   Persistent Sessions and Queuing messages\n-   Retained Messages\n-   Last Will and Testament\n-   Keep Alive and Client Take-Over\n**Specials**\n-   MQTT over Websockets\nMQTT is a Client Server publish/subscribe messaging transport protocol. It is light weight, open, simple, and designed so as to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in Machine to Machine (M2M) and Internet of Things (IoT) contexts where a small code footprint is required and/or network bandwidth is at a premium.\n**MQTT**(**MQ Telemetry Transport**) is an[ISO standard](https://en.wikipedia.org/wiki/International_Organization_for_Standardization)(ISO/IEC PRF 20922)[publish-subscribe](https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern)-based messaging protocol. It works on top of the[TCP/IP protocol](https://en.wikipedia.org/wiki/TCP/IP). The[publish-subscribe messaging pattern](https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern)requires a[message broker](https://en.wikipedia.org/wiki/Message_broker).\nMQTT Utilizes many characteristics of the TCP transport, so the minimum requirement for using MQTT is a working TCP stack, which is now available for even the smallest microcontrollers. A variant,MQTT-SN, is used over other transports such asUDPor Bluetooth.MQTTsends connection credentials in plain text format and does not include any measures for security or authentication.\nIt is a very light weight and binary protocol, and due to its minimal packet overhead, MQTTexcels when transferring data over the wire in comparison to protocols like HTTP.\n**Features**\n-   Open source\n-   simple and lightweight protocol\n-   small code footprint\n-   Designed for resource-contrained devices and low bandwidth, high latency networks such as dial up lines and satellite links (embedded systems)\n-   publish-and-subscribe messaging (no queues)\n-   3 quality of services\n    -   fire-and-forget / unreliable\n    -   \"at least once\" to ensure it is sent a minimum of one time (but can be sent more than one time)\n    -   \"exactly once\"\n-   ideal for machine-to-machine (M2M) or Internet of Things and for mobile applications\n**Usage**\n\nMQTT excels in scenarios where reliable message delivery is crucial for an application but a reliable network connection is not necessarily available, e.g. mobile networks. Typical use cases of MQTT include:\n-   Telemetry\n-   Automotive\n-   Smart Home\n-   Energy Monitoring\n-   Chat Applications\n-   Notification Services\n-   Healthcare Applications\n**History**\n\nThe MQTT protocol was invented in 1999 by Andy Stanford-Clark (IBM) and Arlen Nipper (Arcom, now Cirrus Link). They needed a protocol for minimal battery loss and minimal bandwidth to[connect with oil pipelines via satellite](http://www.ibm.com/podcasts/software/websphere/connectivity/piper_diaz_nipper_mq_tt_11182011.pdf).\n![MOTT History 2010 MQTT 3.1 released royalty free 2014 MQTT 3.1.1 officially released 1999 MQTT Invention 2013 HiveMQ 1.3 released 2018 MQTT 5 released ](media/Intro-image1.png)\n**Real world deployments**\n-   Facebook mobile application\n**See also**\n-   Message Oriented Architecture (MOM)\n**References**\n-   Getting started with MQTT by DZone Refcardz\n-   <https://www.hivemq.com/blog/6-facts-why-its-worth-upgrading-to-mqtt-3-1-1>\n-   <https://www.hivemq.com/mqtt-essentials>\n-   <https://www.hivemq.com/blog/mqtt-essentials-part-1-introducing-mqtt>\n\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Keep-Alive-&-Client-Take-Over/","title":"Keep Alive & Client Take-Over"},"frontmatter":{"draft":false},"rawBody":"# Keep Alive & Client Take-Over\n\nCreated: 2019-03-12 22:57:11 +0500\n\nModified: 2019-04-16 12:43:12 +0500\n\n---\n\n**The problem of half-open TCP connections**\n\n[MQTT is based on the Transmission Control Protocol (TCP)](https://www.hivemq.com/blog/mqtt-essentials-part-3-client-broker-connection-establishment/). This protocol ensures that packets are transferred over the internet in a[\"reliable, ordered, and error-checked\"](http://en.wikipedia.org/wiki/Transmission_Control_Protocol)way. Nevertheless, from time to time, the transfer between communicating parties can get out of sync. For example, if one of the parties crashes or has transmission errors. In TCP, this state of incomplete connection is called a[half-open connection](http://en.wikipedia.org/wiki/TCP_half-open). The important point to remember is that one side of the communication continues to function and is not notified about the failure of the other side. The side that is still connected keeps trying to send messages and waits for acknowledgements.\nAs Andy Stanford-Clark (the inventor of the MQTT protocol) points out, the problem with half-open connections increases in mobile networks:\n\n\"Although TCP/IP in theory notifies you when a socket breaks, in practice, particularly on things like mobile and satellite links, which often \"fake\" TCP over the air and put headers back on at each end, it's quite possible for a TCP session to \"black hole\", i.e. it appears to be open still, but in fact is just dumping anything you write to it onto the floor.\"\n**MQTT Keep Alive**\n\nMQTT includes a keep alive function that provides a workaround for the issue of half-open connections (or at least makes it possible to assess if the connection is still open).\n\nKeep alive ensures that the connection between the broker and client is still open and that the broker and the client are aware of being connected.When the client establishes a connection to the broker, the client communicates a time interval in seconds to the broker. This interval defines the maximum length of time that the broker and client may not communicate with each other.\nThe MQTT specification says the following:\n\n\"The Keep Alive ... is the maximum time interval that is permitted to elapse between the point at which the Client finishes transmitting one Control Packet and the point it starts sending the next. It is the responsibility of the Client to ensure that the interval between Control Packets being sent does not exceed the Keep Alive value. In the absence of sending any other Control Packets, the Client MUST send a PINGREQ Packet.\"\nAs long as messages are exchanged frequently and the keep-alive interval is not exceeded, there is no need to send an extra message to establish whether the connection is still open.\n\nIf the client does not send a messages during the keep-alive period, it must send a PINGREQ packet to the broker to confirm that it is available and to make sure that the broker is also still available.\nThe broker must disconnect a client that does not send a message or a PINGREQ packet in one and a half times the keep alive interval.Likewise, the client is expected to close the connection if it does not receive a response from the broker in a reasonable amount of time.\nThis allows the broker to detect a half-open connection and close the connection to the (already disconnected) client if the keepAlive value is exceeded by more than 150% of the value.\n**Keep Alive Flow**\n\n**PINGREQ**\n\n![pingreq](media/Keep-Alive-&-Client-Take-Over-image1.png)\n\nThe PINGREQ is sent by the client and indicates to the broker that the client is still alive. If the client does not send any other type of packets (for example, a PUBLISH or SUBSCRIBE packet), the client must send a PINGREQ packet to the broker. The client can send a PINGREQ packet any time it wants to confirm that the network connection is still alive. The PINGREQ packet does not contain a payload.\n**PINGRESP**\n\n![pingresp](media/Keep-Alive-&-Client-Take-Over-image2.png)\n\nWhen the broker receives a PINGREQ packet, the broker must reply with a PINGRESP packet to show the client that it is still available. The PINGRESP packet also does not contain a payload.\n**Good to Know**\n-   If the broker does not receive a PINGREQ or any other packet from a client, the broker closes the connection and sends the[last will and testament message](https://www.hivemq.com/blog/mqtt-essentials-part-9-last-will-and-testament/)(if the client specified an LWT).\n-   It is the responsibility of the MQTT client to set an appropriate keep alive value. For example, the client can adjust the keep-alive interval to its current signal strength.\n-   The maximum keep alive is 65535 seconds (18h 12min 15 sec).\n-   If the keep alive interval is 0, the keep alive mechanism is deactivated.\n**Client Take-Over**\n\nUsually, a disconnected client tries to reconnect. Sometimes, the broker still has an half-open connection for the client. In MQTT, if the broker detects a half-open connection, it performs a 'client take-over'.The broker closes the previous connection to the same client (determined by the client identifier), and establishes a new connection with the client.This behavior ensures that the half-open connection does not stop the disconnected client from re-establishing a connection.\n**References**\n\n<https://www.hivemq.com/blog/mqtt-essentials-part-10-alive-client-take-over>"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Last-Will-and-Testament/","title":"Last Will and Testament"},"frontmatter":{"draft":false},"rawBody":"# Last Will and Testament\n\nCreated: 2019-03-12 22:53:14 +0500\n\nModified: 2019-03-12 22:56:53 +0500\n\n---\n\nBecause MQTT is often used in scenarios that include unreliable networks, it's reasonable to assume that some of the MQTT clients in these scenarios will occasionally disconnect ungracefully. An ungraceful disconnect can occur due to loss of connection, empty batteries, or many other reasons. Knowing whether a client disconnected gracefully (with an MQTTDISCONNECTmessage) or ungracefully (without a disconnect message), helps you respond correctly. The Last Will and Testament feature provides a way for clients to respond to ungraceful disconnects in an appropriate way.\nIn MQTT, you use the Last Will and Testament (LWT) feature to notify other clients about an ungracefully disconnected client. Each client can specify its last will message when it connects to a broker. The last will message is a normal MQTT message with a topic, retained message flag, QoS, and payload. The broker stores the message until it detects that the client has disconnected ungracefully. In response to the ungraceful disconnect, the broker sends the last-will message to all subscribed clients of the last-will message topic. If the client disconnects gracefully with a correct DISCONNECT message, the broker discards the stored LWT message.\n\n![disconnect](media/Last-Will-and-Testament-image1.png)\n\nLWT helps you implement various strategies when the connection of a client drops (or at least inform other clients about the offline status).\n**How do you specify a LWT message for a client?**\n\nClients can specify an LWT message in the CONNECT message that initiates the connection between the client and the broker.\n\n![MQTT Connect message content](media/Last-Will-and-Testament-image2.png)\n**When does a broker send the LWT message?**\n\nAccording to the[MQTT 3.1.1 specification](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html), the broker must distribute the LWT of a client in the following situations:\n-   The broker detects an I/O error or network failure.\n-   The client fails to communicate within the defined Keep Alive period.\n-   The client does not send a DISCONNECT packet before it closes the network connection.\n-   The broker closes the network connection because of a protocol error.\n**Best Practices - When should you use LWT?**\n\nLWT is a great way to notify other subscribed clients about the unexpected loss of connection of another client. In real-world scenarios, LWT is often combined with[retained messages](https://www.hivemq.com/blog/mqtt-essentials-part-8-retained-messages/)to store the state of a client on a specific topic. For example, client1 first sends a CONNECT message to the broker with a lastWillMessage that has \"Offline\" as the payload, the lastWillRetain flag set to true, and the lastWillTopic set toclient1/status. Next, the client sends a PUBLISH message with the payload \"Online\" and the retained flag set to true to the same topic (client1/status). As long as client1 stays connected, newly-subscribed clients to the client1/status topic receive the \"Online\" retained message. If client1 disconnects unexpectedly, the broker publishes the LWT message with the payload \"Offline\" as the new retained message. Clients that subscribe to the topic while client1 is offline, receive the LWT retained message (\"Offline\") from the broker. This pattern of retained messages keeps other clients up to date on the current status of client1 on a specific topic.\n**References**\n\n<https://www.hivemq.com/blog/mqtt-essentials-part-9-last-will-and-testament>"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Libraries/","title":"Libraries"},"frontmatter":{"draft":false},"rawBody":"# Libraries\n\nCreated: 2019-03-12 23:41:55 +0500\n\nModified: 2019-03-12 23:42:03 +0500\n\n---\n\n**MQTT BROKER IMPLEMENTATIONS**\n\n<table>\n<colgroup>\n<col style=\"width: 16%\" />\n<col style=\"width: 83%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>BROKER</th>\n<th>DESCRIPTION</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>mosquitto</td>\n<td><p>[Opensource] mosquitto is an open source MQTT broker written in C.</p>\n<p>It fully supports MQTT 3.1 and MQTT 3.1.1 and is very lightweight. Due to its small size, this broker can be used on constrained devices.</p></td>\n</tr>\n<tr class=\"even\">\n<td>Apache ActiveMQ</td>\n<td>ActiveMQ is an open-source multi-protocol message broker with a core written around JMS. It supports MQTT and maps MQTT semantics over JMS.</td>\n</tr>\n<tr class=\"odd\">\n<td>HiveMQ</td>\n<td>[Enterprise] HiveMQ is a scalable, high-performance MQTT broker suitable for mission critical deployments. It fully supports MQTT 3.1 and MQTT 3.1.1 and has features like websockets, clustering, and an open-source plugin system for Java developers.</td>\n</tr>\n<tr class=\"even\">\n<td>RabbitMQ</td>\n<td>[Opensource] RabbitMQ is a scalable, open-source message queue implementation, written in Erlang. It is an AMQP message broker but has an MQTT plugin available. Does not support all MQTT features (e.g. QoS 2).</td>\n</tr>\n<tr class=\"odd\">\n<td>mosca</td>\n<td>mosca is an open-source MQTT broker written in Node.js. It can operate as standalone or be embedded into any Node. js application. Does not implement all MQTT features (e.g. QoS 2).</td>\n</tr>\n<tr class=\"even\">\n<td>RSMB</td>\n<td>RSMB is a message broker by IBM available for personal use. It is written in C and is one of the oldest MQTT broker implementations available.</td>\n</tr>\n<tr class=\"odd\">\n<td>WebsphereMQ / IBM MQ</td>\n<td>Websphere MQ is a commercial message-oriented middleware by IBM. Fully supports MQTT.</td>\n</tr>\n<tr class=\"even\">\n<td>emqtt</td>\n<td>[Opensource, enterprise] Erlang MQTT Broker is a distributed, massively scalable, highly extensible MQTT message broker written in Erlang/OTP.</td>\n</tr>\n<tr class=\"odd\">\n<td>VerneMQ</td>\n<td>[Opensource] VerneMQ is a high-performance, distributed MQTT broker. It scales horizontally and vertically on commodity hardware to support a high number of concurrent publishers and consumers while maintaining low latency and fault tolerance. VerneMQ is the reliable message hub for your IoT platform or smart products.</td>\n</tr>\n</tbody>\n</table>\n\n<https://blog.autsoft.hu/choosing-an-mqtt-broker-for-your-iot-project>\n\n<https://en.wikipedia.org/wiki/Comparison_of_MQTT_Implementations>\n\nPublic mqtt brokers for testing - <https://github.com/mqtt/mqtt.github.io/wiki/public_brokers>\n\n## MQTT Client Libraries**\n\n| LIBRARY                | LANGUAGE                                 | DESCRIPTION                                                                                                                |\n|--------------|------------------|-----------------------------------------|\n| Eclipse Paho           | C, C++, Java, Javascript, Python, Go, C# | Paho clients are among the most popular client library implementations.                                                    |\n| M2MQTT                 | C#                                       | M2MQTT is an MQTT client library for .NET and WinRT.                                                                       |\n| Fusesource MQTT Client | Java                                     | The Fusesource MQTT client is a Java MQTT client with 3 different API styles: Blocking, Future-based, and Callback- based. |\n| Machine Head           | Clojure                                  | Machine Head is an MQTT client for Clojure. It implements the basic MQTT 3.1 features.                                     |\n| MQTT.js                | Javascript                               | MQTT.js is an MQTT client library for Node.js and web applications, available as a npm module.                             |\n| ruby-mqtt              | Ruby                                     | ruby-mqtt is an MQTT client available as a Ruby gem. It does not support QoS > 0.                                         |\n**MQTT Client Tools**\n\n<table>\n<colgroup>\n<col style=\"width: 19%\" />\n<col style=\"width: 15%\" />\n<col style=\"width: 65%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>CLIENT TOOL</th>\n<th>OS</th>\n<th>DESCRIPTION</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>MQTT.fx</td>\n<td>Windows, Linux, MacOSX</td>\n<td>MQTT.fx is a JavaFX application with a clean interface and advanced features like scripting, broker statistics, and templates.</td>\n</tr>\n<tr class=\"even\">\n<td>mqtt-spy</td>\n<td>Windows, Linux, MacOSX</td>\n<td><p>mqtt-spy is a JavaFX application that is</p>\n<p>easy to use and focused on analyzing MQTT subscriptions. There is also a CLI-based daemon application available, which does not need a graphic interface.</p></td>\n</tr>\n<tr class=\"odd\">\n<td>MQTT Inspector</td>\n<td>iOS</td>\n<td>MQTT Inspector is an iOS app that allows detailed analysis of MQTT traffic. Use of the publish/subscribe message types, and complex filterings of received messages, are available.</td>\n</tr>\n<tr class=\"even\">\n<td>HiveMQ Websocket Client</td>\n<td>Web browser</td>\n<td>The HiveMQ websocket client runs on any modern browser and connects to MQTT brokers via websockets. Very useful if itâ€™s not possible to install a client application on the machine in use, as well as for quick MQTT tests.</td>\n</tr>\n<tr class=\"odd\">\n<td>MyMQTT</td>\n<td>Android</td>\n<td>MyMQTT is an MQTT test application for Android devices. It allows the creation of templates for publishing, which makes it very useful for testing MQTT â€œon-the-go.â€</td>\n</tr>\n<tr class=\"even\">\n<td>MQTTLens</td>\n<td>Google Chrome</td>\n<td>MQTTLens is a Chrome Webapp that can connect to MQTT brokers via TCP and over websockets. This app is easy to grasp and equipped with all the basic MQTT features needed for quick tests.</td>\n</tr>\n<tr class=\"odd\">\n<td>mosquitto_pub / mosquitto_sub</td>\n<td>Linux, Windows, MacOSX</td>\n<td>mosquitto_pub and mosquitto_sub are the best options for publish/subscribe on servers without GUI. It is also great for MQTT task automation.</td>\n</tr>\n</tbody>\n</table>\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/MQTT---SN/","title":"MQTT - SN"},"frontmatter":{"draft":false},"rawBody":"# MQTT - SN\n\nCreated: 2018-12-11 12:45:38 +0500\n\nModified: 2019-04-25 11:27:29 +0500\n\n---\n\nMQTT for Sensor Networks\n-   MQTT-SNusesUDPand notTCPfor its transport.\n-   UDPis a connection less protocol whereasTCPis connection orientated.\n-   MQTT-SNis designed,as far as possible., to work in the same way as MQTT\n-   In that regard MQTT-SN usually requires a connection to the broker before it can send and receive messages.\n-   This connection is in effect avirtual connection.\n**QOS Levels**\n\nMQTT-SN supports QOS 0,1,2 as per MQTT, but it also supports a special publish QOS of 3 or -1.\n\nNote: it is known as QOS -1 but the QOS flag in the message is set to 11 or decimal 3.\n\nPublishing messages with a QOS of -1 or 3 doesn't require an initial connection to have been set up.\n\nYou can publish using the topic id or short topic name.\n**Subscribing to MQTT-SN Topics**\n\nYou cansubscribe to a topicsusing 3 different formats:\n-   A long topic name as per MQTT e.g.house/sensor1\n-   A short topic name of 2 characters only e.g.s1\n-   A pre-defined topic id (integer) e.g. 1\n\nWildcardscan be used as per MQTT, but they only make sense for long topic names.\n**MQTT-SN clients have the ability to discover brokers.**\n\nThere are two mechanisms used:\n-   Advertising by a broker or Gateway\n-   A Search by the client\n\nBoth methods use a multicast packet. Currently there is no standardized multicast packet address.\n**MQTT-SN Architecture**\nThe architecture of MQTT-SN can be represented by the following figure -\n![MQTT-SN Architecture](media/MQTT---SN-image1.jpg)\nThere are three kinds of MQTT-SN components -\n\n1.  MQTT-SN clients,\n\n2.  MQTT-SN gateways (GW) and\n\n3.  MQTT-SN forwarders.\nMQTT-SN clients connect themselves to a MQTT server via a MQTT-SN GW using the MQTT-SN protocol. A MQTT-SN GW may or may not be integrated with a MQTT server. In case of a stand-alone GW, the MQTT protocol is used between the MQTT server and the MQTT-SN GW. Its main function is the translation between MQTT and MQTT-SN. MQTT-SN clients can also access a GW via a forwarder in case the GW is not directly attached to their network. The forwarder simply encapsulates the MQTT-SN frames it receives on the wireless side and forwards them unchanged to the GW. In the opposite direction, it releases the frames it receives from the gateway and sends them to the clients, unchanged too.\nThere are two types of Gateways depending on how a Gateway performs the protocol translation between MQTT and MQTT-SN:\n1.  Transparent Gateway - For each MQTT-SN client, transparent gateway will form an individual MQTT connection to the MQTT broker.\n\n2.  Aggregating Gateway - In this, for all MQTT-SN clients, the Aggregating gateway will form only one MQTT connection to the MQTT broker. The advantage of using aggregating gateway is shown where WSN network has large number of sensor nodes because it helps in reducing the number of MQTT connections that broker has to create with individual client.\n**MQTT-SN vs MQTT**\n\n**Advantages**\n\n1.  MQTT-SN supports topic ID instead of topic name. First client sends a registration request with topic name and topic ID (2 octets) to a broker. After the registration is accepted, client uses topic ID to refer the topic name. This saves media bandwidth and device memory - it is quite expensive to keep and send topic name e.g:home/livingroom/socket2/meterin memory for each publish message.\n\n2.  Topic name to topic ID can be preconfigured in MQTT-SN gateway, so that even registration message can be skipped before publish.\n\n3.  MQTT-SN does not require TCP/IP stack. It can be used over a serial link (preferred way), where with simple link protocol (to distinguish different devices on the line) overhead is really small. Alternatively it can be used over UDP, which is less hungry than TCP.\n\n**Disadvantages**\n\n1.  You need some sort of gateway, which is nothing else than a TCP or UDP stack moved to a different device. This can also be a simple device (e.g.: Arduino Uno) just serving multiple MQTT-SN devices without doing other job.\n\n2.  MQTT-SN is not well supported.\n**References**\n\n<http://www.steves-internet-guide.com/mqtt-sn>\n\n<https://www.bevywise.com/blog/benefits-of-mqtt-sn-over-mqtt>\nClients -\n\n<https://github.com/eclipse/paho.mqtt-sn.embedded-c>\n\n<https://www.eclipse.org/paho/clients/c/embedded-sn>\n\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/MQTT-5.0/","title":"MQTT 5.0"},"frontmatter":{"draft":false},"rawBody":"# MQTT 5.0\n\nCreated: 2019-03-12 23:56:27 +0500\n\nModified: 2019-04-17 14:52:21 +0500\n\n---\n\n**MQTT 5.0**\n-   Enhancements for scalability and large scale systems in respect to setups with 1000s and millions of devices.\n-   Improved error reporting (Reason Code & Reason String)\n-   Formalize common patterns including capability discovery and request response\n-   Extensibility mechanisms includinguser properties, payload format and content type\n-   Performance improvements and improved support for small clients-   Session expiry: Split the Clean Session flag into a Clean Start flag which indicates that the session should start without using an existing session, and a Session Expiry interval which says how long to retain the session after a disconnect. The session expiry interval can be modified at disconnect. Setting of Clean Start to 1 and Session Expiry Interval to 0 is equivalent in MQTT v3.1.1 of setting Clean Session to 1.\n-   Message expiry: Allow an expiry interval to be set when a message is published.\n-   Reason code on all ACKs: Change all response packets to contain a reason code. This include CONNACK, PUBACK, PUBREC, PUBREL, PUBCOMP, SUBACK, UNSUBACK, DISCONNECT, and AUTH. This allows the invoker to determine whether the requested function succeeded.\n-   Reason string on all ACKs: Change most packets with a reason code to also allow an optional reason string. This is designed for problem determination and is not intended to be parsed by the receiver.\n-   Server disconnect: Allow DISCONNECT to be sent by the Server to indicate the reason the connection is closed.\n-   Payload format and content type: Allow the payload format (binary, text) and a MIME style content type to be specified when a message is published. These are forwarded on to the receiver of the message.\n-   Request / Response: Formalize the request/response pattern within MQTT and provide the Response Topic and Correlation Data properties to allow response messages to be routed back to the publisher of a request. Also, add the ability for the Client to get configuration information from the Server about how to construct the response topics.\n-   **Shared Subscriptions:** Add shared subscription support allowing for load balanced consumers of a subscription (easily handle massive fan-in scenario)\n-   Subscription ID: Allow a numeric subscription identifier to be specified on a SUBSCRIBE, and returned on the message when it is delivered. This allows the Client to determine which subscription or subscriptions caused the message to be delivered.\n-   Topic Alias: Decrease the size of the MQTT packet overhead by allowing the topic name to be abbreviated to a small integer. The Client and Server independently specify how many topic aliases they allow.\n-   Flow control: Allow the Client and Server to independently specify the number of outstanding reliable messages (QoS>0) they allow. The sender pauses sending such messages to stay below this quota. This is used to limit the rate of reliable messages, and to limit how many are in flight at one time.\n-   User properties: Add User Properties to most packets. User properties on PUBLISH are included with the message and are defined by the Client applications. The user properties on PUBLISH and Will Properties are forwarded by the Server to the receiver of the message. User properties on the CONNECT, SUBSCRIBE, and UNSUBSCRIBE packets are defined by the Server implementation. The user properties on CONNACK PUBACK, PUBREC, PUBREL, PUBCOMP, SUBACK, UNSUBACK and AUTH packets are defined by the sender, and are unique to the sender implementation. The meaning of user properties is not defined by MQTT.\n-   Maximum Packet Size: Allow the Client and Server to independently specify the maximum packet size they support. It is an error for the session partner to send a larger packet.\n-   Optional Server feature availability: Define a set of features which the Server does not allow and provide a mechanism for the Server to specify this to the Client. The features which can be specified in this way are: Maximum QoS, Retain Available, Wildcard Subscription Available, Subscription Identifier Available, and Shared Subscription Available. It is an error for the Client to use features that the Server has declared are not available. It is possible in earlier versions of MQTT for a Server to not implement a feature by declaring that the Client is not authorized for that function. This feature allows such optional behavior to be declared and adds specific Reason Codes when the Client uses one of these features anyway.\n-   Enhanced authentication: Provide a mechanism to enable challenge/response style authentication including mutual authentication. This allows SASL style authentication to be used if supported by both Client and Server, and includes the ability for a Client to re-authenticate within a connection.\n-   Subscription options: Provide subscription options primarily defined to allow for message bridge applications. These include an option to not send messages originating on this Client (noLocal), and options for handling retained messages on subscribe.\n-   Will delay: Add the ability to specify a delay between the end of the connection and sending the will message. This is designed so that if a connection to the session is re-established then the will message is not sent. This allows for brief interruptions of the connection without notification to others.\n-   Server Keep Alive: Allow the Server to specify the value it wishes the Client to use as a keep alive. This allows the Server to set a maximum allowed keepalive and still have the Client honor it.\n-   Assigned ClientID: In cases where the ClientID is assigned by the Server, return the assigned ClientID. This also lifts the restriction that Server assigned ClientIDs can only be used with Clean Session=1 connections.\n-   Server reference: Allow the Server to specify an alternate Server to use on CONNACK or DISCONNECT. This can be used as a redirect or to do provisioning.\n**References**\n\n<https://github.com/mqtt/mqtt.github.io/wiki/Differences-between-3.1.1-and-5.0>\n\n<https://blog.codecentric.de/en/2017/11/hello-mqtt-version-5-0>\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/MQTT-over-WebSockets/","title":"MQTT over WebSockets"},"frontmatter":{"draft":false},"rawBody":"# MQTT over WebSockets\n\nCreated: 2019-03-12 23:22:12 +0500\n\nModified: 2019-03-12 23:43:23 +0500\n\n---\n\nWe've seen that MQTT is ideal for constrained devices and unreliable networks and that it is perfect for sending messages with a very low overhead. Naturally, it would be quite nice to send and receive MQTT messages directly in a browser. For example, on a mobile phone.MQTT over WebSocketsis the answer. MQTT over WebSockets enables the browser to leverage all MQTT features. You can use these capabilities for many interesting use cases:\n-   Display live information from a device or sensor.\n-   Receive push notifications (for example, an alert or critical condition warning).\n-   See the current status of devices with LWT and retained messages.\n-   Communicate efficiently with mobile web applications.\nHTML5 websockets provide a full-duplex communication over a TCP connection. Most modern web browsers implement this specification, even on mobile devices. MQTT can be used in\n\nconjunction with websockets to allow any web application to behave like a full-featured MQTT client. A library that utilizes websockets for MQTT like the Paho Javascript Client is needed.\n**The advantages of using MQTT in web applications are**\n-   Quality of Service semantics: With QoS 1 and 2, there's an assurance that a message arrives on the client or broker at least once/exactly once, even if the Internet connection dropped in the meantime.\n-   Queuing: When using QoS 1 or 2 and a persistent session, the broker will queue all messages a client misses from its subscriptions when it is not connected. On reconnect, all messages are delivered instantly to that client.\n-   Retained messages: Messages that are retained on the server are delivered instantly when a web application subscribes to one of these topics.\n-   Last Will and Testament: If a client doesn't disconnect gracefully, it's possible to publish a message to a topic in order to notify all subscribers that the client went offline.\n**What does all this mean from a technical point of view?**\n\n[Every modern browser that supports WebSockets](http://caniuse.com/#feat=websockets)can be a full-fledged MQTT client and offer all the features described in the MQTT Essentials.The Keep Alive, Last Will and Testament, Quality of Service, and Retained Messages features work the same way in the browser as in a native MQTT client. All you need is a[JavaScript library](https://eclipse.org/paho/clients/js/)that enables MQTT over WebSockets and a broker that supports MQTT over webSockets. Of course, the HiveMQ broker offers this capability straight out-of-the-box.\n**How does it work?**\n\nWebSocket is a network protocol that provides bi-directional communication between a browser and a web server. The protocol was standardized in 2011 and all modern browsers provide built-in support for it. Similar to MQTT, the WebSocket protocol is based on TCP.\n\n![websockets](media/MQTT-over-WebSockets-image1.png)\n\nInMQTT over WebSockets, the MQTT message (for example, a CONNECT or PUBLISH packet) is transferred over the network and encapsulated by one or more WebSocket frames. WebSockets are a good transport method for MQTT because they provide bi-directional, ordered, and lossless communication (WebSockets also leverage TCP). To communicate with an MQTT broker over WebSockets, the broker must be able to handle native WebSockets. Occasionally, people use a webserver and bridge WebSockets to the MQTT broker, but we don't recommend this method. When using HiveMQ, it is very easy to get started with WebSockets. Simply enable the native support in the configuration. For more information, read[MQTT over WebSockets with HiveMQ](https://www.hivemq.com/mqtt-over-websockets-with-hivemq/).\n**Why not use MQTT directly?**\n\nCurrently, it is not possible to speak pure MQTT in a browser because it is not possible to open a raw TCP connection.[Socket API](http://www.w3.org/TR/raw-sockets/)will change that situation; however, few browsers implement this API yet.\n**Secure WebSockets**\n\nYou can leverage Transport Layer Security (TLS) to use secure WebSockets with encryption of the whole connection. This method works seamlessly with HiveMQ. However, there are a few points that you need to keep in mind.\n**References**\n\n<https://www.hivemq.com/blog/mqtt-essentials-special-mqtt-over-websockets>\n\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Messages/","title":"Messages"},"frontmatter":{"draft":false},"rawBody":"# Messages\n\nCreated: 2019-03-12 23:26:33 +0500\n\nModified: 2019-10-04 14:48:12 +0500\n\n---\n\nMQTT has 14 different message types. Typically, end users only need to employ the CONNECT, PUBLISH, SUBSCRIBE, and UNSUBSCRIBE message types. The other message types are used for internal mechanisms and message flows.\n\n| **MESSAGE TYPE**       | **DESCRIPTION**                                                   |\n|----------------------|--------------------------------------------------|\n| CONNECT                | Client request to connect to Server                               |\n| CONNACK                | Connection Acknowledgement                                        |\n| PUBLISH                | A message which represents a new/separate publish                 |\n| PUBACK                 | QoS 1 Response to a PUBLISH message                               |\n| PUBREC (Pub Received)  | First part of QoS 2 message flow                                  |\n| PUBREL (Pub Release)   | Second part of QoS 2 message flow                                 |\n| PUBCOMP (Pub Complete) | Last part of the QoS 2 message flow                               |\n| SUBSCRIBE              | A message used by clients to subscribe to specific topics         |\n| SUBACK                 | Acknowledgement of a SUBSCRIBE message                            |\n| UNSUBSCRIBE            | A message used by clients to unsubscribe from specific topics     |\n| UNSUBACK               | Acknowledgement of an UNSUBSCRIBE message                         |\n| PINGREQ                | Heartbeat message                                                 |\n| PINGRESP               | Heartbeat message acknowledgement                                 |\n| DISCONNECT             | Graceful disconnect message sent by clients before disconnecting. |\r\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-10-05 23:29:06 +0500\n\nModified: 2020-02-13 23:33:49 +0500\n\n---\n\n**MQTT vs HTTP**\n-   MQTT transfers data as a byte array and publish/subscribe model, which makes it perfect for resource-constrained devices and help to save battery.\n-   Besides, publish/subscribe model provides clients with independent existence from one another and enhance the reliability of the whole system. When one client is out of order the whole system can keep on working properly.\n-   throughput of MQTT is 93 times faster than HTTP's.\n-   Besides, in comparison to HTTP, MQTT Protocol ensures high delivery guarantees. There are 3 levels of Quality of Services\n-   MQTT also provides users with options of Last will & Testament and Retained messages. The first means that in case of unexpected disconnection of a client all subscribed clients will get a message from a broker. Retained message means that a newly subscribed client will get an immediate status update.\n-   MQTT has pretty short specification. There are only CONNECT, PUBLISH, SUBSCRIBE, UNSUBSCRIBE and DISCONNECT types that are significant for developers. Whereas HTTP specifications are much longer.\n-   MQTT has a very short message header and the smallest packet message size of 2 bytes. Using text message format by HTTP protocol allows it to compose lengthy headers and messages. It helps to eliminate troubles because it can be read by humans, but at the same time it's needless for resource-constrained devices.-   The real advantage of MQTT over HTTP occurs when we reuse the single connection for sending multiple messages in which the average response per message converges to around 40 ms and the data amount per message converges to around 400 bytes. Note that in the case of HTTP, these reductions simply aren't possible.\n-   The conclusion we can draw is that when choosing MQTT over HTTP, it's really important to reuse the same connection as much as possible. If connections are set up and torn down frequently just to send individual messages, the efficiency gains are not significant compared to HTTP.\n    -   The greatest efficiency gains can be achieved through MQTT's increase in information density for each payload message.\n    -   The most straightforward approach is to reduce the payload size where more data can be transmitted in each payload, which can be achieved through choosing proper compression and package methods based on the type of the data being generated. For instance, protobuf is an efficient way to serialize structured data."},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Paho-Client/","title":"Paho Client"},"frontmatter":{"draft":false},"rawBody":"# Paho Client\n\nCreated: 2018-12-11 17:52:02 +0500\n\nModified: 2020-01-07 19:08:39 +0500\n\n---\n\n```\nimport paho.mqtt.client as mqtt\n\n# The callback for when the client receives a CONNACK response from the server.\ndef on_connect(client, userdata, flags, rc):\nprint(\"Connected with result code \"+str(rc))\n\n# Subscribing in on_connect() means that if we lose the connection and\n# reconnect then subscriptions will be renewed.\nclient.subscribe(\"$SYS/#\")\n\n# The callback for when a PUBLISH message is received from the server.\ndef on_message(client, userdata, msg):\nprint(msg.topic+\" \"+str(msg.payload))\n\nclient = mqtt.Client()\nclient.on_connect = on_connect\nclient.on_message = on_message\n\nclient.connect(\"localhost\", 1883, 60)\n\n# Blocking call that processes network traffic, dispatches callbacks and\n# handles reconnecting.\n# Other loop*() functions are available that give a threaded interface and a\n# manual interface.\nclient.loop_forever()\nPublish single message\n\nimport paho.mqtt.publish as publish\n\npublish.single(\"paho/test/single\", \"payload\", hostname=\"localhost\")\n```\n\n**References**\n<https://github.com/eclipse/paho.mqtt.python>\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Persistent-Session-&-Queuing-Messages/","title":"Persistent Session & Queuing Messages"},"frontmatter":{"draft":false},"rawBody":"# Persistent Session & Queuing Messages\n\nCreated: 2019-03-12 22:32:58 +0500\n\nModified: 2019-03-12 23:39:39 +0500\n\n---\n\nTo receive messages from an MQTT broker, a client connects to the broker and creates[subscriptions to the topics](https://www.hivemq.com/blog/mqtt-essentials-part-5-mqtt-topics-best-practices/)in which it is interested. If the connection between the client and broker is interrupted during a non-persistent session, these topics are lost and the client needs to subscribe again on reconnect. Re-subscribing every time the connection is interrupted is a burden for constrained clients with limited resources. To avoid this problem, the client can request a persistent session when it connects to the broker. Persistent sessions save all information that is relevant for the client on the broker. TheclientIdthat the client provides when it establishes connection to the broker identifies the session\n**What's stored in a persistent session?**\n\nIn a persistent session, the broker stores the following information (even if the client is offline). When the client reconnects the information is available immediately.\n-   Existence of a session (even if there are no subscriptions).\n-   All the subscriptions of the client.\n-   All messages in a[Quality of Service (QoS)](https://www.hivemq.com/blog/mqtt-essentials-part-6-mqtt-quality-of-service-levels/)1 or 2 flow that the client has not yet confirmed.\n-   All new QoS 1 or 2 messages that the client missed while offline.\n-   All QoS 2 messages received from the client that are not yet completely acknowledged.\n**How do you start or end a persistent session?**\n\nWhen the client connects to the broker, it can request a persistent session. The client uses acleanSessionflag to tell the broker what kind of session it needs:\n-   When the clean session flag is set to true, the client does not want a persistent session. If the client disconnects for any reason, all information and messages that are queued from a previous persistent session are lost.\n-   When the clean session flag is set to false, the broker creates a persistent session for the client. All information and messages are preserved until the next time that the client requests a clean session. If the clean session flag is set to false and the broker already has a session available for the client, it uses the existing session and delivers previously queued messages to the client.\n**How does the client know if a session is already stored?**\n\nSince MQTT 3.1.1, theCONNACKmessage from the broker contains asession present flag. This flag tells the client if a previously established session is still available on the broker.\n**Persistent session on the client side**\n\nSimilar to the broker, each MQTT client must also store a persistent session. When a client requests the server to hold session data, the client is responsible for storing the following information:\n-   All messages in a QoS 1 or 2 flow, that are not yet confirmed by the broker.\n-   All QoS 2 messages received from the broker that are not yet completely acknowledged.\n**Best practices**\n\nHere are some guidelines that can help you decide when to use a persistent session or a clean session:\n\n**Persistent Session**\n-   The client must get all messages from a certain topic, even if it is offline. You want the broker to queue the messages for the client and deliver them as soon as the client is back online.\n-   The client has limited resources. You want the broker to store the subscription information of the client and restore the interrupted communication quickly.\n-   The client needs to resume all QoS 1 and 2 publish messages after a reconnect.\n**Clean session**\n-   The client needs only to publish messages to topics, the client does not need to subscribe to topics.You don't want the broker to store session information or retry transmission of QoS 1 and 2 messages.\n-   The client does not need to get messages that it misses offline.\nPersistent sessions are often used for MQTT clients on constrained devices and clients who must not miss any messages for certain topics---not even when they are disconnected. When a client reconnects, the broker will send all missed messages for a subscription with a QoS Level of 1 or 2. Persistent sessions are most useful for clients that subscribe to topics; publishing-only clients don't profit from persistent sessions.\n**How long does the broker store messages?**\n\nPeople often ask how long the broker stores the session. The easy answer is: The broker stores the session until the clients comes back online and receives the message. However,what happens if a client does not come back online for a long time?Usually, the memory limit of the operating system is the primary constraint on message storage. There is no standard answer for this scenario. The right solution depends on your use case.\n**References**\n\n<https://www.hivemq.com/blog/mqtt-essentials-part-7-persistent-session-queuing-messages>\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Publish,-Subscribe-&-Unsubscribe/","title":"Publish, Subscribe & Unsubscribe"},"frontmatter":{"draft":false},"rawBody":"# Publish, Subscribe & Unsubscribe\n\nCreated: 2019-03-12 18:05:41 +0500\n\nModified: 2019-03-12 18:19:41 +0500\n\n---\n\n**Publish**\n\nAn MQTT client can publish messages as soon as it connects to a broker. MQTT utilizes topic-based filtering of the messages on the broker.Each message must contain a topic that the broker can use to forward the message to interested clients. Typically, each message has a payload which contains the data to transmit in byte format.MQTT is data-agnostic. The use case of the client determines how the payload is structured. The sending client (publisher) decides whether it wants to send binary data, text data, or even full-fledged XML or JSON.\n\n![MQTT Publish attributes](media/Publish,-Subscribe-&-Unsubscribe-image1.png)\n\n**Topic Name**\n\nThe topic name is a simple string that is hierarchically structured with forward slashes as delimiters. For example, \"myhome/livingroom/temperature\" or \"Germany/Munich/Octoberfest/people\".\n**QoS**\n\nThis number indicates the Quality of Service Level (QoS) of the message. There are three levels: 0,1, and 2. The service level determines what kind of guarantee a message has for reaching the intended recipient (client or broker).\n**Retain Flag**\n\nThis flag defines whether the message is saved by the broker as the last known good value for a specified topic. When a new client subscribes to a topic, they receive the last message that is retained on that topic.\n**Payload**\n\nThis is the actual content of the message. MQTT is data-agnostic. It is possible to send images, text in any encoding, encrypted data, and virtually every data in binary.\n**Packet Identifier**\n\nThe packet identifier uniquely identifies a message as it flows between the client and broker. The packet identifier is only relevant for QoS levels greater than zero. The client library and/or the broker is responsible for setting this internal MQTT identifier.\n**DUP flag**\n\nThe flag indicates that the message is a duplicate and was resent because the intended recipient (client or broker) did not acknowledge the original message. This is only relevant for QoS greater than 0. Usually, the resend/duplicate mechanism is handled by the MQTT client library or the broker as an implementation detail.\nWhen a client sends a message to an MQTT broker for publication,the broker reads the message, acknowledges the message (according to the QoS Level), and processes the message. Processing by the broker includes determining which clients have subscribed to the topic and sending the message to them.\n\n![MQTT Publish Message Flow](media/Publish,-Subscribe-&-Unsubscribe-image2.gif)\n\nThe client that initially publishes the message is only concerned about delivering the PUBLISH message to the broker. Once the broker receives the PUBLISH message, it is the responsibility of the broker to deliver the message to all subscribers. The publishing client does not get any feedback about whether anyone is interested in the published message or how many clients received the message from the broker.\n**Subscribe**\n\nPublishing a message doesn't make sense if no one ever receives it. In other words, if there are no clients to subscribe to the topics of the messages. To receive messages on topics of interest, the client sends a[SUBSCRIBE](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718063)message to the MQTT broker. This subscribe message is very simple, it contains a unique packet identifier and a list of subscriptions.\n\n![MQTT Subscribe attributes](media/Publish,-Subscribe-&-Unsubscribe-image3.png)\n\n**Packet Identifier**\n\nThe packet identifier uniquely identifies a message as it flows between the client and broker. The client library and/or the broker is responsible for setting this internal MQTT identifier.\n**List of Subscriptions**\n\nA SUBSCRIBE message can contain multiple subscriptions for a client. Each subscription is made up of a topic and a QoS level. The topic in the subscribe message can contain wildcards that make it possible to subscribe to a topic pattern rather than a specific topic. If there are overlapping subscriptions for one client, the broker delivers the message that has the highest QoS level for that topic.\n**Suback**\n\nTo confirm each subscription, the broker sends a[SUBACK](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718068)acknowledgement message to the client. This message contains the packet identifier of the original Subscribe message (to clearly identify the message) and a list of return codes.\n\n![MQTT SUBACK attributes](media/Publish,-Subscribe-&-Unsubscribe-image4.png)\n\n**Packet Identifier**\n\nThe packet identifier is a unique identifier used to identify a message. It is the same as in the SUBSCRIBE message.\n**Return Code**\n\nThe broker sends one return code for each topic/QoS-pair that it receives in the SUBSCRIBE message. For example, if the SUBSCRIBE message has five subscriptions, the SUBACK message contains five return codes. The return code acknowledges each topic and shows the QoS level that is granted by the broker. If the broker refuses a subscription, the SUBACK message conains a failure return code for that specific topic. For example, if the client has insufficient permission to subscribe to the topic or the topic is malformed.\n\n| Return Code | Return Code Response    |\n|-------------|-------------------------|\n| 0           | Success - Maximum QoS 0 |\n| 1           | Success - Maximum QoS 1 |\n| 2           | Success - Maximum QoS 2 |\n| 128         | Failure                 |\n![MQTT Subscribe Flow](media/Publish,-Subscribe-&-Unsubscribe-image5.gif)\n\nAfter a client successfully sends the SUBSCRIBE message and receives the SUBACK message, it gets every published message that matches a topic in the subscriptions that the SUBSCRIBE message contained.\n**Unsubscribe**\n\nThe counterpart of the SUBSCRIBE message is the[UNSUBSCRIBE](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718072)message. This message deletes existing subscriptions of a client on the broker. The UNSUBSCRIBE message is similar to the SUBSCRIBE message and has a packet identifier and a list of topics.\n\n![MQTT Unsubscribe attributes](media/Publish,-Subscribe-&-Unsubscribe-image6.png)\n\n**Packet Identifier**\n\nThe packet identifier uniquely identifies a message as it flows between the client and broker. The client library and/or the broker is responsible for setting this internal MQTT identifier.\n**List of Topic**\n\nThe list of topics can contain multiple topics from which the client wants to unsubscribe. It is only necessary to send the topic (without QoS). The broker unsubscribes the topic, regardless of the QoS level with which it was originally subscribed.\n**Unsuback**\n\nTo confirm the unsubscribe, the broker sends an[UNSUBACK](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718077)acknowledgement message to the client. This message contains only the packet identifier of the original UNSUBSCRIBE message (to clearly identify the message).\n\n![MQTT UNSUBACK attributes](media/Publish,-Subscribe-&-Unsubscribe-image7.png)\n\n**Packet Identifier**\n\nThe packet identifier uniquely identifies the message. As already mentioned, this is the same packet identifier that is in the UNSUBSCRIBE message.\n\n![Unsubscribe Flow](media/Publish,-Subscribe-&-Unsubscribe-image8.gif)\n\nAfter receiving the UNSUBACK from the broker, the client can assume that the subscriptions in the UNSUBSCRIBE message are deleted.\n**References**\n\n<https://www.hivemq.com/blog/mqtt-essentials-part-4-mqtt-publish-subscribe-unsubscribe>"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Publish-Subscribe-Pattern/","title":"Publish Subscribe Pattern"},"frontmatter":{"draft":false},"rawBody":"# Publish Subscribe Pattern\n\nCreated: 2019-03-12 17:01:50 +0500\n\nModified: 2019-03-12 18:20:46 +0500\n\n---\n\nThe publish/subscribe pattern (also known as pub/sub) provides an alternative to traditional client-server architecture. In the client-sever model, a client communicates directly with an endpoint.The pub/sub model**decouples the client that sends a message (the publisher) from the client or clients that receive the messages (the subscribers)**. The publishers and subscribers never contact each other directly. In fact, they are not even aware that the other exists.**The connection between them is handled by a third component (the broker)**. The job of the broker is to filter all incoming messages and distribute them correctly to subscribers.\nThe most important aspect of pub/sub is the decoupling of the publisher of the message from the recipient (subscriber). This decoupling has several dimensions:\n-   **Space decoupling:**Publisher and subscriber do not need to know each other (for example, no exchange of IP address and port).\n-   **Time decoupling:**Publisher and subscriber do not need to run at the same time.\n-   **Synchronization decoupling:**Operations on both components do not need to be interrupted during publishing or receiving.\n**Message Filtering**\n\nIt's clear that the broker plays a pivotal role in the pub/sub process. But how does the broker manage to filter all the messages so that each subscriber receives only messages of interest? As you'll see, the broker has several filtering options:\n**Option 1: Subject-based filtering**\n\nThis filtering is based on the subject or topic that is part of each message. The receiving client subscribes to the broker for topics of interest. From that point on, the broker ensures that the receiving client gets all message published to the subscribed topics. In general, topics are strings with a hierarchical structure that allow filtering based on a limited number of expressions.\n**Option 2: Content-based filtering**\n\nIn content-based filtering, the broker filters the message based on a specific content filter-language. The receiving clients subscribe to filter queries of messages for which they are interested. A significant downside to this method is that the content of the message must be known beforehand and cannot be encrypted or easily changed.\n**Option 3: Type-based filtering**\n\nWhen object-oriented languages are used, filtering based on the type/class of a message (event) is a common practice. For example,, a subscriber can listen to all messages, which are of type Exception or any sub-type.\n**Distinction from Message Queues**\n\n**A message queue stores message until they are consumed**\n\nWhen you use a message queue, each incoming message is stored in the queue until it is picked up by a client (often called a consumer). If no client picks up the message, the message remains stuck in the queue and waits to be consumed. In a message queue, it is not possible for a message not to be processed by any client, as it is in MQTT if nobody subscribes to a topic.\n\n**A message is only consumed by one client**\n\nAnother big difference is that in a traditional message queue a message can be processed by one consumer only. The load is distributed between all consumers for a queue. In MQTT the behavior is quite the opposite: every subscriber that subscribes to the topic gets the message.\n\n**Queues are named and must be created explicitly**\n\nA queue is far more rigid than a topic. Before a queue can be used, the queue must be created explicitly with a separate command. Only after the queue is named and created is it possible to publish or consume messages. In contrast, MQTT topics are extremely flexible and can be created on the fly.\n**References**\n\n<https://www.hivemq.com/blog/mqtt-essentials-part2-publish-subscribe>\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/QoS-Levels/","title":"QoS Levels"},"frontmatter":{"draft":false},"rawBody":"# QoS Levels\n\nCreated: 2019-03-12 18:21:07 +0500\n\nModified: 2019-09-23 10:48:16 +0500\n\n---\n\n**What is Quality of Service?**\n\nTheQuality of Service(QoS) level is an agreement between the sender of a message and the receiver of a message that defines the guarantee of delivery for a specific message.\n| QOS LEVEL | Name                                     | DESCRIPTION                                                                                                                                                                                                                          |\n|----------|----------------|----------------------------------------------|\n| 0         | At most once (fire and forget)           | The sender tries with best effort to send the message and relies on the reliability of TCP. No retransmission takes place.                                                                                                           |\n| 1         | At least once (acknowledged delivery)    | The receiver will get the message at least once. If the receiver does not acknowledge the message or the acknowledge gets lost on the way, it will be resent until the sender gets an acknowledgement. Duplicate messages can occur. |\n| 2         | Exactly once delivery (assured delivery) | The protocol makes sure that the message will arrive exactly once at the receiver. This increases communication overhead but is the best option when neither loss nor duplication of messages are acceptable.                        |\nWhen you talk about QoS in MQTT, you need to consider the two sides of message delivery:\n\na.  **Message delivery form the publishing client to the broker.**\n\nb.  **Message delivery from the broker to the subscribing client.**\nWe will look at the two sides of the message delivery separately because there are subtle differences between the two. The client that publishes the message to the broker defines the QoS level of the message when it sends the message to the broker. The broker transmits this message to subscribing clients using the QoS level that each subscribing client defines during the subscription process. If the subscribing client defines a lower QoS than the publishing client, the broker transmits the message with the lower quality of service.\n**Why is Quality of Service important?**\n\nQoS is a key feature of the MQTT protocol. QoS gives the client the power to choose a level of service that matches its network reliability and application logic. Because MQTT manages the re-transmission of messages and guarantees delivery (even when the underlying transport is not reliable), QoS makes communication in unreliable networks a lot easier.\n**QoS 0 - at most once**\n\nThe minimal QoS level is zero. This service level guarantees a best-effort delivery. There is no guarantee of delivery. The recipient does not acknowledge receipt of the message and the message is not stored and re-transmitted by the sender. QoS level 0 is often called \"fire and forget\" and provides the same guarantee as the underlying TCP protocol.\n\n![publish_qos0_flow](media/QoS-Levels-image1.png)\n\n**QoS 1 - at least once**\n\nQoS level 1 guarantees that a message is delivered at least one time to the receiver. The sender stores the message until it gets a[PUBACK](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718043)packet from the receiver that acknowledges receipt of the message. It is possible for a message to be sent or delivered multiple times.\n\n![publish_qos1_flow](media/QoS-Levels-image2.png)\n![puback_packet](media/QoS-Levels-image3.png)\n\nThe sender uses the packet identifier in each packet to match the PUBLISH packet to the corresponding PUBACK packet. If the sender does not receive a PUBACK packet in a reasonable amount of time, the sender resends the PUBLISH packet. When a receiver gets a message with QoS 1, it can process it immediately. For example, if the receiver is a broker, the broker sends the message to all subscribing clients and then replies with a PUBACK packet. If the publishing client sends the message again it sets a duplicate (DUP) flag. In QoS 1, this DUP flag is only used for internal purposes and is not processed by broker or client. The receiver of the message sends a PUBACK, regardless of the DUP flag.\n**QoS 2 - exactly once**\n-   PUBLISH\n-   PUBREC (Publish Received)\n-   PUBREL (Publish Release)\n-   PUBCOMP (Publish Complete)\n\nQoS 2 is the highest level of service in MQTT. This level guarantees that each message is received only once by the intended recipients. QoS 2 is the safest and slowest quality of service level. The guarantee is provided by at least two request/response flows (a four-part handshake) between the sender and the receiver. The sender and receiver use the packet identifier of the original PUBLISH message to coordinate delivery of the message.\n\n![publish_qos2_flow](media/QoS-Levels-image4.png)\n\nWhen a receiver gets a QoS 2 PUBLISH packet from a sender, it processes the publish message accordingly and replies to the sender with a[PUBREC](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718048)packet that acknowledges the PUBLISH packet. If the sender does not get a PUBREC packet from the receiver, it sends the PUBLISH packet again with a duplicate (DUP) flag until it receives an acknowledgement.\n\n![pubrec_packet](media/QoS-Levels-image5.png)\n\nOnce the sender receives a PUBREC packet from the receiver, the sender can safely discard the initial PUBLISH packet. The sender stores the PUBREC packet from the receiver and responds with a[PUBREL](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718053)packet.\n\n![pubrel_packet](media/QoS-Levels-image6.png)\n\nAfter the receiver gets the PUBREL packet, it can discard all stored states and answer with a[PUBCOMP](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718058)packet (the same is true when the sender receives the PUBCOMP). Until the receiver completes processing and sends the PUBCOMP packet back to the sender, the receiver stores a reference to the packet identifier of the original PUBLISH packet. This step is important to avoid processing the message a second time. After the sender receives the PUBCOMP packet, the packet identifier of the published message becomes available for reuse.\n\n![pubcomp_packet](media/QoS-Levels-image7.png)\n\nWhen the QoS 2 flow is complete, both parties are sure that the message is delivered and the sender has confirmation of the delivery..\n\nIf a packet gets lost along the way, the sender is responsible to retransmit the message within a reasonable amount of time. This is equally true if the sender is an MQTT client or an MQTT broker. The recipient has the responsibility to respond to each command message accordingly.\n**Good to know**\n\nSome aspects of QoS are not very obvious at first glance. Here are a few things to keep in mind when you use QoS:\n**Downgrade of QoS**\n\nAs we already mentioned, the QoS definition and levels between the client that sends (publishes) the message and the client that receives the message are two different things. The QoS levels of these two interactions can also be different. The client that sends the PUBLISH message to the broker defines the QoS of the message. However, when the broker delivers the message to recipients (subscribers), the broker uses the QoS that the receiver (subscriber) defined during subscription. For example, client A is the sender of the message. Client B is the receiver of the message. If client B subscribes to the broker with QoS 1 and client A sends the message to the broker with QoS 2, the broker delivers the message to client B (receiver/subscriber) with QoS 1. The message can be delivered more than once to client B, because QoS 1 guarantees delivery of the message at least one time and does not prevent multiple deliveries of the same message.\n**Packet identifiers are unique per client**\n\nThe packet identifier that MQTT uses for QoS 1 and QoS 2 is unique between a specific client and a broker within an interaction. This identifier is not unique between all clients. Once the flow is complete, the packet identifier is available for reuse. This reuse is the reason why the packet identifier does not need to exceed 65535. It is unrealistic that a client can send more than this number of messages without completing an interaction.\n**Best Practice**\n\n**Use QoS 0 when**\n-   You have a completely or mostly stable connection between sender and receiver. A classic use case for QoS 0 is connecting a test client or a front end application to an MQTT broker over a wired connection.\n-   You don't mind if a few messages are lost occasionally. The loss of some messages can be acceptable if the data is not that important or when data is sent at short intervals\n-   You don't need message queuing. Messages are only queued for disconnected clients if they have QoS 1 or 2 and a[persistent session](https://www.hivemq.com/blog/mqtt-essentials-part-7-persistent-session-queuing-messages).\n**Use QoS 1 when**\n-   You need to get every message and your use case can handle duplicates. QoS level 1 is the most frequently used service level because it guarantees the message arrives at least once but allows for multiple deliveries. Of course, your application must tolerate duplicates and be able to process them accordingly.\n-   You can't bear the overhead of QoS 2. QoS 1 delivers messages much faster than QoS 2.\n**Use QoS 2 when**\n-   It is critical to your application to receive all messages exactly once. This is often the case if a duplicate delivery can harm application users or subscribing clients. Be aware of the overhead and that the QoS 2 interaction takes more time to complete.\n**Queuing of QoS 1 and 2 messages**\n\nAll messages sent with QoS 1 and 2 are queued for offline clients until the client is available again. However, this queuing is only possible if the client has a[persistent session](https://www.hivemq.com/blog/mqtt-essentials-part-7-persistent-session-queuing-messages/).\n**References**\n\n<https://www.hivemq.com/blog/mqtt-essentials-part-6-mqtt-quality-of-service-levels>\n\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Retained-Messages/","title":"Retained Messages"},"frontmatter":{"draft":false},"rawBody":"# Retained Messages\n\nCreated: 2019-03-12 22:47:13 +0500\n\nModified: 2019-03-12 23:36:44 +0500\n\n---\n\nIn MQTT, the client that publishes a message has no guarantee that a subscribing client actually receives the message. The publishing client can only make sure that the message gets delivered safely to the broker. Basically, the same is true for a subscribing client. The client that connects and subscribes to topics has no guarantee on when the publishing client will publish a message in one of their topics of interest. It can take a few seconds, minutes, or hours for the publisher to send a new message in one of the subscribed topics. Until the next message is published, the subscribing client is totally in the dark about the current status of the topic. This situation is where retained messages come into play.\n**A retained message is a normal MQTT message with the retained flag set to true. The broker stores the last retained message and the corresponding QoS for that topic.**Each client that **subscribes to a topic pattern that matches the topic of the retained message receives the retained message immediately after they subscribe**. The broker stores only one retained message per topic.\n**Retained messages help newly-subscribed clients get a status update immediately after they subscribe to a topic. The retained message eliminates the wait for the publishing clients to send the next update.**\nIn other words, a retained message on a topic is thelast known good value. The retained message doesn't have to be the last value, but it must be the last message with the retained flag set to true.\n\nIt is important to understand that a retained message has nothing to do withpersitant session. Once a retained message is stored by the broker, there's only one way to remove it.\n**Send a retained message**\n\nFrom the perspective of a developer, sending a retained message is quite simple and straight-forward. You just set theretained flagof a[MQTT publish message](https://www.hivemq.com/blog/mqtt-essentials-part-4-mqtt-publish-subscribe-unsubscribe/)to true. Typically, your client library provides an easy way to set this flag.\n**Delete a retained message**\n\nThere is also a very simple way to delete the retained message of a topic: send a retained message with a zero-byte payload on the topic where you want to delete the previous retained message. The broker deletes the retained message and new subscribers no longer get a retained message for that topic. Frequently, it is not even necessary to delete, because each new retained message overwrites the previous one.\n**Why and when should you use Retained Messages?**\n\n**A retained message makes sense when you want newly-connected subscribers to receive messages immediately (without waiting until a publishing client sends the next message)**.This is extremely helpful for status updates of components or devices on individual topics. For example, the status of device1 is on the topicmyhome/devices/device1/status. When retained messages are used, new subscribers to the topic get the status (online/offline) of the device immediately after they subscribe. The same is true for clients that send data in intervals, temperature, GPS coordinates, and other data.**Without retained messages, new subscribers are kept in the dark between publish intervals.**Using retained messages helps provide the last good value to a connecting client immediately.**References**\n\n<https://www.hivemq.com/blog/mqtt-essentials-part-8-retained-messages>\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Scaling/","title":"Scaling"},"frontmatter":{"draft":false},"rawBody":"# Scaling\n\nCreated: 2019-03-12 23:48:02 +0500\n\nModified: 2019-04-06 16:21:12 +0500\n\n---\n\nIn a brokered architecture it's critical to avoid a single point of failure and to think about scaling out, since typically only one broker node is used. In the context of MQTT there are two different popular strategies applicable:\n**BRIDGING**\n\nSome brokers implement an unofficial bridging protocol which makes it possible to chain brokers together. Bridging allows forwarding messages on specific topics to other MQTT brokers. Bridge connections between brokers can be uni- or bidirectional. Technically, a bridge connection to another broker is a connection where the broker behaves like an MQTT client and subscribes to specific topics.\n**Pros:**\n-   Great for forwarding messages on specific topics\n-   Different broker products can be chained\n-   Hierarchical broker architectures possible\n**Cons:**\n-   No shared state between brokers\n-   Bridge protocol is not officially specified\n\nBrokers which implement bridging: HiveMQ, mosquitto, RSMB, Websphere MQ / IBM MQ\n**CLUSTERING**\n\nMany enterprise MQTT brokers implement clustering, which supports high availability configurations and also allows for scaling out by adding more broker nodes. When a cluster node is no longer available, other cluster nodes can take over so that no data or messages are lost. Often brokers implement elastic clustering, and nodes can be added or removed any time.\n**Pros:**\n-   High availability and scalability\n-   MQTT semantics across cluster nodes\n**Cons:**\n-   No standard\n-   Broker-specific\nBrokers which implement clustering: Apache ActiveMQ, HiveMQ, RabbitMQ\n*If broker implementation allows, clustering and bridging can be used together, enabling messages from one broker cluster to be forwarded to another isolated cluster.*\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Security/","title":"Security"},"frontmatter":{"draft":false},"rawBody":"# Security\n\nCreated: 2019-03-12 23:47:37 +0500\n\nModified: 2020-11-17 23:18:04 +0500\n\n---\n\nSecurity is a very important part of any communication. MQTT itself keeps everything as simple as possible and relies on other proven technologies for safeguards instead of reinventing the wheel.\n\n**USERNAME / PASSWORD AUTHENTICATION**\n\nAn MQTT CONNECT message can contain a username and password. The broker can authenticate and authorize with this information if such a mechanism is implemented. Many open-source brokers rely on Access Control Lists while other enterprise brokers allow coupling with user databases and/or LDAP systems.\n**TRANSPORT SECURITY: TLS**\n\nA best practice when using MQTT is to add transport layer security if possible. With TLS, the complete communication between client and broker is encrypted, and no attacker can read any message exchanged. If feasible, X509 client certificate authentication adds an additional layer of security to the clients: trust. Some MQTT brokers, like HiveMQ, allow the use of X509 certificates in the plugin system for further processing (e.g. authorization).\n**OTHER SECURITY MECHANISMS**\n\nMost enterprise MQTT brokers add additional security mechanisms, e.g. a plugin system where concrete logic can be hooked in. Additional security for MQTT communications can be gained when adding the following to clients / brokers:\n-   **Payload encryption:** This is application-specific. Clients can encrypt the payload of their PUBLISH messages. The shared secret has to be provisioned to all communication participants beforehand.\n-   **Payload signing:** If the MQTT broker of choice supports intercepting MQTT messages (e.g. with a plugin system), every received message payload can be intercepted\n    and signed with a private key before distributing. The distributed messages can then be verified by the MQTT clients to make sure no one has modified the message.\n-   **Complex authentication protocols:** For many enterprise MQTT brokers, additional authentication methods can be implemented (e.g. OAuth 2, Kerberos, OpenID Connect, etc.).\n-   **Authorization / Topic Permissions:** Securing access to topics is often done with a permission concept. Some brokers offer restricting publish / subscribe permissions with a plugin system. This makes sure no one can subscribe to more information than needed, and that only specific clients can publish on specific topics.\n\n"},{"fields":{"slug":"/Computer-Science/Networking/MQTT/Topics-&-Best-Practices/","title":"Topics & Best Practices"},"frontmatter":{"draft":false},"rawBody":"# Topics & Best Practices\n\nCreated: 2019-03-12 18:14:51 +0500\n\nModified: 2019-03-12 23:30:56 +0500\n\n---\n\nIn MQTT, the word topic refers to an UTF-8 string that the broker uses to filter messages for each connected client. The topic consists of one or more topic levels. Each topic level is separated by a forward slash (topic level separator).\n\n![topic_basics](media/Topics-&-Best-Practices-image1.png)\n\nIn comparison to a message queue, MQTT topics are very lightweight. The client does not need to create the desired topic before they publish or subscribe to it. The broker accepts each valid topic without any prior initialization.\nHere are some examples of topics:\n\nmyhome/groundfloor/livingroom/temperature\n\nUSA/California/San Francisco/Silicon Valley\n\n5ff4a2ce-e485-40f4-826c-b1a5d81be9b6/status\n\nGermany/Bavaria/car/2382340923453/latitude\nNote that each topic must containat least 1 characterand that the topic string permits empty spaces.Topics are case-sensitive.For example, _myhome/temperature and _MyHome/Temperature are two different topics. Additionally, the forward slash alone is a valid topic.\n**Wildcards**\n\nWhen a client subscribes to a topic, it can subscribe to the exact topic of a published message or it can use wildcards to subscribe to multiple topics simultaneously. A wildcard can only be used to subscribe to topics, not to publish a message. There are two different kinds of wildcards: _single-level and _multi-level.\n\nWildcards **are not allowed in topic names when publishing messages**. The wildcard characters are reserved and must not be used in the topic. These characters cannot be escaped.\n| **WILDCARD**          | **SYMBOL** | **MEANING**                                                                                                                                               |\n|-------------|----------|-------------------------------------------------|\n| Single-level Wildcard | +         | A wildcard that matches one complete topic level. It must occupy an entire topic level. This wildcard can be used more than once in a topic subscription. |\n| Multi-level Wildcard  | #         | A wildcard that matches any number of levels within a topic. It must be the last character of a topic subscription.                                       |\n**VALID MQTT TOPIC EXAMPLES**\n\nâ€¢ my/test/topic\n\nâ€¢ my/+/topic\n\nâ€¢ my/#\n\nâ€¢ my/+/+ (subscribe all topics that has two levels)\n\nâ€¢ +/# â€¢#\n**Single Level: +**\n\nAs the name suggests, a single-level wildcard replaces one topic level. The plus symbol represents a single-level wildcard in a topic.\n\n![topic_wildcard_plus](media/Topics-&-Best-Practices-image2.png)\n\nAny topic matches a topic with single-level wildcard if it contains an arbitrary string instead of the wildcard. For example a subscription to _myhome/groundfloor/+/temperature can produce the following results:\n\n![topic_wildcard_plus_example](media/Topics-&-Best-Practices-image3.png)\n**Multi Level: #**\n\nThe multi-level wildcard covers many topic levels. The hash symbol represents the multi-level wild card in the topic. For the broker to determine which topics match, the multi-level wildcard must be placed as the last character in the topic and preceded by a forward slash.\n\n![topic_wildcard_hash](media/Topics-&-Best-Practices-image4.png)\n![topic_wildcard_hash_example](media/Topics-&-Best-Practices-image5.png)\n\nWhen a client subscribes to a topic with a multi-level wildcard, it receives all messages of a topic that begins with the pattern before the wildcard character, no matter how long or deep the topic is. If you specify only the multi-level wildcard as a topic (_#), you receive all messages that are sent to the MQTT broker. If you expect high throughput, subscription with a multi-level wildcard alone is an anti-pattern (see the best practices below).\n**Topics beginning with $**\n\nGenerally, you can name your MQTT topics as you wish. However, there is one exception: __ Topics that start with a $ symbol have a different purpose.__ These topics are not part of the subscription when you subscribe to the multi-level wildcard as a topic (#).The $-symbol topics are reserved for internal statistics of the MQTT broker.Clients cannot publish messages to these topics. At the moment, there is no official standardization for such topics. Commonly,$SYS/is used for all the following information, but broker implementations varies.\n\n$SYS/broker/clients/connected\n\n$SYS/broker/clients/disconnected\n\n$SYS/broker/clients/total\n\n$SYS/broker/messages/sent\n\n$SYS/broker/uptime\n**Best practices**\n\n**Never use a leading forward slash**\n\nA leading forward slash is permitted in MQTT. For example,/myhome/groundfloor/livingroom. However, the leading forward slash introduces an unnecessary topic level with a zero character at the front. The zero does not provide any benefit and often leads to confusion.\n**Never use spaces in a topic**\n\nA space is the natural enemy of every programmer. When things are not going the way they should, spaces make it much harder to read and debug topics. As with leading forward slashes, just because something is allowed, doesn't mean it should be used.[UTF-8 has many different white space types](http://www.cs.tut.fi/~jkorpela/chars/spaces.html), such uncommon characters should be avoided.\n**Keep the topic short and concise**\n\nEach topic is included in every message in which it is used. Make your topics as short and concise as possible. When it comes to small devices, every byte counts and topic length has a big impact.\n**Use only ASCII characters, avoid non printable characters**\n\nBecause non-ASCII UTF-8 characters often display incorrectly, it is very difficult to find typos or issues related to the character set. Unless it is absolutely necessary, we recommend avoiding the use of non-ASCII characters in a topic.\n**Embed a unique identifier or the Client Id into the topic**\n\nIt can be very helpful to include the unique identifier of the publishing client in the topic. The unique identifier in the topic helps you identify who sent the message. The embedded ID can be used to enforce authorization. Only a client that has the same client ID as the ID in the topic is allowed to publish to that topic. For example, a client with the _client1 ID is allowed to publish to _client1/status, but not permitted to publish to _client2/status.\n**Don't subscribe to #**\n\nSometimes, it is necessary to subscribe to all messages that are transferred over the broker. For example, to persist all messages into a database.Do not subscribe to all messages on a broker by using an MQTT client and subscribing to a multi-level wildcard.Frequently, the subscribing client is not able to process the load of messages that results from this method (especially if you have a massive throughput). Our recommendation is to implement an extension in the MQTT broker. For example, with the[plugin system of HiveMQ](https://www.hivemq.com/extensions)you can hook into the behavior of HiveMQ and add an asynchronous routine to process each incoming message and persist it to a database.\n**Don't forget extensibility**\n\nTopics are a flexible concept and there is no need to preallocate them in any way. However, both the publisher and the subscriber need to be aware of the topic. It is important to think about how topics can be extended to allow for new features or products. For example, if your smart-home solution adds new sensors, it should be possible to add these to your topic tree without changing the whole topic hierarchy.\n**Use specific topics, not general ones**\n\nWhen you name topics, don't use them in the same way as in a queue. Be as specific topics as possible. For example, if you have three sensors in your living room, create topics for _myhome/livingroom/temperature, _myhome/livingroom/brightness and _myhome/livingroom/humidity. Do not send all values over _myhome/livingroom. Use of a single topic for all messages is a anti pattern. Specific naming also makes it possible for you to use other MQTT features such as retained messages.\n**References**\n\n<https://www.hivemq.com/blog/mqtt-essentials-part-5-mqtt-topics-best-practices>"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/Addressing-Methods---cast-protocols/","title":"Addressing Methods / cast protocols"},"frontmatter":{"draft":false},"rawBody":"# Addressing Methods / cast protocols\n\nCreated: 2018-12-11 12:37:01 +0500\n\nModified: 2021-06-08 00:30:14 +0500\n\n---\n\nThe**cast**term here signifies some data(stream of packets) is being transmitted to the recipient(s) from client(s) side over the communication channel that help them to communicate. Let's see some of the \"cast\" concepts that are prevailing in the computer networks field.\n1.  **Unicast**\n\nThis type of information transfer is useful when there is a participation of single sender and single recipient. So, in short you can term it as a one-to-one transmission. For example, a device having IP address 10.1.2.0 in a network wants to send the traffic stream(data packets) to the device with IP address 20.12.4.2 in the other network,then unicast comes into picture. This is the most common form of data transfer over the networks.\n2.  **Broadcast**\n\nBroadcasting transfer (one-to-all) techniques can be classified into two types :\n-   **Limited Broadcasting**\n    Suppose you have to send stream of packets to all the devices over the network that you reside, this broadcasting comes handy. For this to achieve,it will append 255.255.255.255 (all the 32 bits of IP address set to 1) called asLimited Broadcast Addressin the destination address of the datagram (packet) header which is reserved for information tranfer to all the recipients from a single client (sender) over the network.\n\n![NETWORK CLUSTER ](media/Addressing-Methods---cast-protocols-image1.png)\n-   **Direct Broadcasting**\n    This is useful when a device in one network wants to transfer packet stream to all the devices over the other network.This is achieved by translating all the Host ID part bits of the destination address to 1,referred asDirect Broadcast Addressin the datagram header for information transfer.\n\n![NETWORK Î‘ NETWORK B ](media/Addressing-Methods---cast-protocols-image2.png)\n\nThis mode is mainly utilized by television networks for video and audio distribution.\n\nOne important protocol of this class in Computer Networks is[Address Resolution Protocol (ARP)](https://www.geeksforgeeks.org/computer-network-arp-works/)that is used for resolving IP address into physical address which is necessary for underlying communication.\n\n3.  **Multicast**\n\nIn multicasting, one/more senders and one/more recipients participate in data transfer traffic. In this method traffic recline between the boundaries of unicast (one-to-one) and broadcast (one-to-all). Multicast lets server's direct single copies of data streams that are then simulated and routed to hosts that request it. IP multicast requires support of some other protocols likeIGMP (Internet Group Management Protocol), Multicast routingfor its working. Also in Classful IP addressingClass Dis reserved for multicast groups.\n\n![](media/Addressing-Methods---cast-protocols-image3.png)\n4.  Anycast\n\nAnycastis a network[addressing](https://en.wikipedia.org/wiki/Addressing)and[routing](https://en.wikipedia.org/wiki/Routing)methodology in which a single destination address has multiple routing paths to two or more endpoint destinations. Routers will select the desired path on the basis of number of hops, distance, lowest cost, latency measurements or based on the least congested route. Anycast networks are widely used for[content delivery network](https://en.wikipedia.org/wiki/Content_delivery_network)(CDN) products to bring their content closer to the end user.\n<https://en.wikipedia.org/wiki/Anycast>\n5.  **Geocast**\n\nRefers to the delivery of information to a group of destinations in a network identified by their geographical locations. It is a specialized form of multicast addressing used by some routing protocols for mobile ad hoc networks.![Routing schemes Unicast Broadcast Multicast Anycast Geocast ](media/Addressing-Methods---cast-protocols-image4.png)\n**Automatic Repeat Request / Automatic Repeat Query (ARQ)**\n\nAutomatic repeat request(ARQ), also known asautomatic repeat query, is an[error-control](https://en.wikipedia.org/wiki/Error_control)method for[data transmission](https://en.wikipedia.org/wiki/Data_transmission)that uses[acknowledgements](https://en.wikipedia.org/wiki/Acknowledgement_(data_networks))(messages sent by the receiver indicating that it has correctly received a[packet](https://en.wikipedia.org/wiki/Packet_(information_technology))) and[timeouts](https://en.wikipedia.org/wiki/Timeout_(computing))(specified periods of time allowed to elapse before an acknowledgment is to be received) to achieve[reliable data transmission](https://en.wikipedia.org/wiki/Reliability_(computer_networking))over an unreliable service. If the sender does not receive an acknowledgment before the timeout, it usually[re-transmits](https://en.wikipedia.org/wiki/Retransmission_(data_networks))the packet until the sender receives an acknowledgment or exceeds a predefined number of retransmissions.\nThe types of ARQ protocols include**[Stop-and-wait ARQ](https://en.wikipedia.org/wiki/Stop-and-wait_ARQ),[Go-Back-N ARQ](https://en.wikipedia.org/wiki/Go-Back-N_ARQ), and[Selective Repeat ARQ/Selective Reject ARQ](https://en.wikipedia.org/wiki/Selective_Repeat_ARQ).** All three protocols usually use some form of[sliding window protocol](https://en.wikipedia.org/wiki/Sliding_window_protocol)to tell the transmitter to determine which (if any) packets need to be retransmitted. These protocols reside in the[data link](https://en.wikipedia.org/wiki/Data_link_layer)or[transport layers](https://en.wikipedia.org/wiki/Transport_layer)(layers 2 and 4) of the[OSI model](https://en.wikipedia.org/wiki/OSI_model).\n<https://en.wikipedia.org/wiki/Automatic_repeat_request>\n\n## References**\n\n<https://www.geeksforgeeks.org/computer-network-difference-unicast-broadcast-multicast>\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/Book---Computer-Networks/","title":"Book - Computer Networks"},"frontmatter":{"draft":false},"rawBody":"# Book - Computer Networks\n\nCreated: 2020-01-01 21:59:23 +0500\n\nModified: 2020-01-01 22:24:49 +0500\n\n---\n\n**Andrew S Tanenbaum and DAVID J. WETHERALL - 5th Edition**\n**Acronyms - 28/119**\n**1INTRODUCTION**\n\n1.1USES OF COMPUTER NETWORKS, 3\n\n1.1.1Business Applications, 3\n\n1.1.2Home Applications, 6\n\n1.1.3Mobile Users, 10\n\n1.1.4Social Issues, 14\n1.2NETWORK HARDWARE, 17\n\n1.2.1Personal Area Networks, 18\n\n1.2.2Local Area Networks, 19\n\n1.2.3Metropolitan Area Networks, 23\n\n1.2.4Wide Area Networks, 23\n\n1.2.5Internetworks, 28\n1.3NETWORK SOFTWARE, 29\n\n1.3.1Protocol Hierarchies, 29\n\n1.3.2Design Issues for the Layers, 33\n\n1.3.3Connection-Oriented Versus Connectionless Service, 35\n\n1.3.4Service Primitives, 38\n\n1.3.5The Relationship of Services to Protocols, 40\n1.4REFERENCE MODELS, 41\n\n1.4.1The OSI Reference Model, 41\n\n1.4.2The TCP/IP Reference Model, 45\n\n1.4.3The Model Used in This Book, 48\n\n1.4.4A Comparison of the OSI and TCP/IP Reference Models*, 49\n\n1.4.5A Critique of the OSI Model and Protocols*, 51\n\n1.4.6A Critique of the TCP/IP Reference Model*, 53\n1.5EXAMPLE NETWORKS\n\n1.5.1The Internet, 54\n\n1.5.2Third-Generation Mobile Phone Networks*, 65\n\n1.5.3Wireless LANs: 802.11*, 70\n\n1.5.4RFID and Sensor Networks*, 73\n1.6NETWORK STANDARDIZATION*, 75\n\n1.6.1Who's Who in the Telecommunications World, 77\n\n1.6.2Who's Who in the International Standards World, 78\n\n1.6.3Who's Who in the Internet Standards World, 80\n1.7METRIC UNITS, 82\n1.8OUTLINE OF THE REST OF THE BOOK, 83\n1.9SUMMARY, 84\n**2THE PHYSICAL LAYER**\n\n2.1THE THEORETICAL BASIS FOR DATA COMMUNICATION, 90\n\n2.1.1Fourier Analysis, 90\n\n2.1.2Bandwidth-Limited Signals, 90\n\n2.1.3The Maximum Data Rate of a Channel, 94\n2.2GUIDED TRANSMISSION MEDIA, 95\n\n2.2.1Magnetic Media, 95\n\n2.2.2Twisted Pairs, 96\n\n2.2.3Coaxial Cable, 97\n\n2.2.4Power Lines, 98\n\n2.2.5Fiber Optics, 99\n2.3WIRELESS TRANSMISSION, 105\n\n2.3.1The Electromagnetic Spectrum, 105\n\n2.3.2Radio Transmission, 109\n\n2.3.3Microwave Transmission, 110\n\n2.3.4Infrared Transmission, 114\n\n2.3.5Light Transmission, 114\n\n2.4COMMUNICATION SATELLITES*, 116\n\n2.4.1Geostationary Satellites, 117\n\n2.4.2Medium-Earth Orbit Satellites, 121\n\n2.4.3Low-Earth Orbit Satellites, 121\n\n2.4.4Satellites Versus Fiber, 123\n2.5DIGITAL MODULATION AND MULTIPLEXING, 125\n\n2.5.1Baseband Transmission, 125\n\n2.5.2Passband Transmission, 130\n\n2.5.3Frequency Division Multiplexing, 132\n\n2.5.4Time Division Multiplexing, 135\n\n2.5.5Code Division Multiplexing, 135\n2.6THE PUBLIC SWITCHED TELEPHONE NETWORK, 138\n\n2.6.1Structure of the Telephone System, 139\n\n2.6.2The Politics of Telephones, 142\n\n2.6.3The Local Loop: Modems, ADSL, and Fiber, 144\n\n2.6.4Trunks and Multiplexing, 152\n\n2.6.5Switching, 161\n2.7THE MOBILE TELEPHONE SYSTEM*, 164\n\n2.7.1First-Generation (coco1G) Mobile Phones: Analog Voice, 166\n\n2.7.2Second-Generation (2G) Mobile Phones: Digital Voice, 170\n\n2.7.3Third-Generation (3G) Mobile Phones: Digital Voice and Data, 174\n2.8CABLE TELEVISION*, 179\n\n2.8.1Community Antenna Television, 179\n\n2.8.2Internet over Cable, 180\n\n2.8.3Spectrum Allocation, 182\n\n2.8.4Cable Modems, 183\n\n2.8.5ADSL Versus Cable, 185\n2.9SUMMARY, 186\n**3THE DATA LINK LAYER**\n\n3.1DATA LINK LAYER DESIGN ISSUES, 194\n\n3.1.1Services Provided to the Network Layer, 194\n\n3.1.2Framing, 197\n\n3.1.3Error Control, 200\n\n3.1.4Flow Control, 201\n\n3.2ERROR DETECTION AND CORRECTION, 202\n\n3.2.1Error-Correcting Codes, 204\n\n3.2.2Error-Detecting Codes, 209\n3.3ELEMENTARY DATA LINK PROTOCOLS, 215\n\n3.3.1A Utopian Simplex Protocol, 220\n\n3.3.2A Simplex Stop-and-Wait Protocol for an Error-Free Channel, 221\n\n3.3.3A Simplex Stop-and-Wait Protocol for a Noisy Channel, 222\n3.4SLIDING WINDOW PROTOCOLS, 226\n\n3.4.1A One-Bit Sliding Window Protocol, 229\n\n3.4.2A Protocol Using Go-Back-N, 232\n\n3.4.3A Protocol Using Selective Repeat, 239\n3.5EXAMPLE DATA LINK PROTOCOLS, 244\n\n3.5.1Packet over SONET, 245\n\n3.5.2ADSL (Asymmetric Digital Subscriber Loop), 248\n3.6SUMMARY, 251\n**4THE MEDIUM ACCESS CONTROL SUBLAYER**\n\n4.1THE CHANNEL ALLOCATION PROBLEM, 258\n\n4.1.1Static Channel Allocation, 258\n\n4.1.2Assumptions for Dynamic Channel Allocation, 260\n4.2MULTIPLE ACCESS PROTOCOLS, 261\n\n4.2.1ALOHA, 262\n\n4.2.2Carrier Sense Multiple Access Protocols, 266\n\n4.2.3Collision-Free Protocols, 269\n\n4.2.4Limited-Contention Protocols, 274\n\n4.2.5Wireless LAN Protocols, 277\n4.3ETHERNET, 280\n\n4.3.1Classic Ethernet Physical Layer, 281\n\n4.3.2Classic Ethernet MAC Sublayer Protocol, 282\n\n4.3.3Ethernet Performance, 286\n\n4.3.4Switched Ethernet, 288\n\n4.3.5Fast Ethernet, 290\n\n4.3.6Gigabit Ethernet, 293\n\n4.3.710-Gigabit Ethernet, 296\n\n4.3.8Retrospective on Ethernet, 298\n4.4WIRELESS LANS, 299\n\n4.4.1The 802.11 Architecture and Protocol Stack, 299\n\n4.4.2The 802.11 Physical Layer, 301\n\n4.4.3The 802.11 MAC Sublayer Protocol, 303\n\n4.4.4The 802.11 Frame Structure, 309\n\n4.4.5Services, 311\n4.5BROADBAND WIRELESS*, 312\n\n4.5.1Comparison of 802.16 with 802.11 and 3G, 313\n\n4.5.2The 802.16 Architecture and Protocol Stack, 314\n\n4.5.3The 802.16 Physical Layer, 316\n\n4.5.4The 802.16 MAC Sublayer Protocol, 317\n\n4.5.5The 802.16 Frame Structure, 319\n4.6BLUETOOTH*, 320\n\n4.6.1Bluetooth Architecture, 320\n\n4.6.2Bluetooth Applications, 321\n\n4.6.3The Bluetooth Protocol Stack, 322\n\n4.6.4The Bluetooth Radio Layer, 324\n\n4.6.5The Bluetooth Link Layers, 324\n\n4.6.6The Bluetooth Frame Structure, 325\n4.7 RFID*, 327\n\n4.7.1EPC Gen 2 Architecture, 327\n\n4.7.2EPC Gen 2 Physical Layer, 328\n\n4.7.3EPC Gen 2 Tag Identiï¬cation Layer, 329\n\n4.7.4Tag Identiï¬cation Message Formats, 331\n4.8DATA LINK LAYER SWITCHING, 332\n\n4.8.1Uses of Bridges, 332\n\n4.8.2Learning Bridges, 334\n\n4.8.3Spanning Tree Bridges, 337\n\n4.8.4Repeaters, Hubs, Bridges, Switches, Routers, and Gateways, 340\n\n4.8.5Virtual LANs, 342\n4.9SUMMARY, 349\n\n**5THE NETWORK LAYER**\n\n5.1NETWORK LAYER DESIGN ISSUES, 355\n\n5.1.1Store-and-Forward Packet Switching, 356\n\n5.1.2Services Provided to the Transport Layer, 356\n\n5.1.3Implementation of Connectionless Service, 358\n\n5.1.4Implementation of Connection-Oriented Service, 359\n\n5.1.5Comparison of Virtual-Circuit and Datagram Networks, 361\n5.2ROUTING ALGORITHMS, 362\n\n5.2.1The Optimality Principle, 364\n\n5.2.2Shortest Path Algorithm, 366\n\n5.2.3Flooding, 368\n\n5.2.4Distance Vector Routing, 370\n\n5.2.5Link State Routing, 373\n\n5.2.6Hierarchical Routing, 378\n\n5.2.7Broadcast Routing, 380\n\n5.2.8Multicast Routing, 382\n\n5.2.9Anycast Routing, 385\n\n5.2.10Routing for Mobile Hosts, 386\n\n5.2.11Routing in Ad Hoc Networks, 389\n5.3CONGESTION CONTROL ALGORITHMS, 392\n\n5.3.1Approaches to Congestion Control, 394\n\n5.3.2Trafï¬c-Aware Routing, 395\n\n5.3.3Admission Control, 397\n\n5.3.4Trafï¬c Throttling, 398\n\n5.3.5Load Shedding, 401\n5.4QUALITY OF SERVICE, 404\n\n5.4.1Application Requirements, 405\n\n5.4.2Trafï¬c Shaping, 407\n\n5.4.3Packet Scheduling, 411\n\n5.4.4Admission Control, 415\n\n5.4.5Integrated Services, 418\n\n5.4.6Differentiated Services, 421\n5.5INTERNETWORKING, 424\n\n5.5.1How Networks Differ, 425\n\n5.5.2How Networks Can Be Connected, 426\n\n5.5.3Tunneling, 429\n\n5.5.4Internetwork Routing, 431\n\n5.5.5Packet Fragmentation, 432\n5.6THE NETWORK LAYER IN THE INTERNET, 436\n\n5.6.1The IP Version 4 Protocol, 439\n\n5.6.2IP Addresses, 442\n\n5.6.3IP Version 6, 455\n\n5.6.4Internet Control Protocols, 465\n\n5.6.5Label Switching and MPLS, 470\n\n5.6.6OSPF---An Interior Gateway Routing Protocol, 474\n\n5.6.7BGP---The Exterior Gateway Routing Protocol, 479\n\n5.6.8Internet Multicasting, 484\n\n5.6.9Mobile IP, 485\n5.7SUMMARY, 488\n**6THE TRANSPORT LAYER**\n\n6.1THE TRANSPORT SERVICE, 495\n\n6.1.1Services Provided to the Upper Layers, 496\n\n6.1.2Transport Service Primitives, 498\n\n6.1.3Berkeley Sockets, 500\n\n6.1.4An Example of Socket Programming: An Internet File Server, 503\n6.2ELEMENTS OF TRANSPORT PROTOCOLS, 507\n\n6.2.1Addressing, 509\n\n6.2.2Connection Establishment, 512\n\n6.2.3Connection Release, 517\n\n6.2.4Error Control and Flow Control, 522\n\n6.2.5Multiplexing, 527\n\n6.2.6Crash Recovery, 527\n6.3CONGESTION CONTROL, 530\n\n6.3.1Desirable Bandwidth Allocation, 531\n\n6.3.2Regulating the Sending Rate, 535\n\n6.3.3Wireless Issues, 539\n6.4THE INTERNET TRANSPORT PROTOCOLS: UDP, 541\n\n6.4.1Introduction to UDP, 541\n\n6.4.2Remote Procedure Call, 543\n\n6.4.3Real-Time Transport Protocols, 546\n\n6.5THE INTERNET TRANSPORT PROTOCOLS: TCP, 552\n\n6.5.1Introduction to TCP, 552\n\n6.5.2The TCP Service Model, 553\n\n6.5.3The TCP Protocol, 556\n\n6.5.4The TCP Segment Header, 557\n\n6.5.5TCP Connection Establishment, 560\n\n6.5.6TCP Connection Release, 562\n\n6.5.7TCP Connection Management Modeling, 562\n\n6.5.8TCP Sliding Window, 565\n\n6.5.9TCP Timer Management, 568\n\n6.5.10TCP Congestion Control, 571\n\n6.5.11The Future of TCP, 581\n6.6PERFORMANCE ISSUES*, 582\n\n6.6.1Performance Problems in Computer Networks, 583\n\n6.6.2Network Performance Measurement, 584\n\n6.6.3Host Design for Fast Networks, 586\n\n6.6.4Fast Segment Processing, 590\n\n6.6.5Header Compression, 593\n\n6.6.6Protocols for Long Fat Networks, 595\n6.7DELAY-TOLERANT NETWORKING*, 599\n\n6.7.1DTN Architecture, 600\n\n6.7.2The Bundle Protocol, 603\n6.8SUMMARY, 605\n**7THE APPLICATION LAYER**\n\n7.1DNS---THE DOMAIN NAME SYSTEM, 611\n\n7.1.1The DNS Name Space, 612\n\n7.1.2Domain Resource Records, 616\n\n7.1.3Name Servers, 619\n7.2ELECTRONIC MAIL*, 623\n\n7.2.1Architecture and Services, 624\n\n7.2.2The User Agent, 626\n\n7.2.3Message Formats, 630\n\n7.2.4Message Transfer, 637\n\n7.2.5Final Delivery, 643\n7.3THE WORLD WIDE WEB, 646\n\n7.3.1Architectural Overview, 647\n\n7.3.2Static Web Pages, 662\n\n7.3.3Dynamic Web Pages and Web Applications, 672\n\n7.3.4HTTP---The HyperText Transfer Protocol, 683\n\n7.3.5The Mobile Web, 693\n\n7.3.6Web Search, 695\n7.4STREAMING AUDIO AND VIDEO, 697\n\n7.4.1Digital Audio, 699\n\n7.4.2Digital Video, 704\n\n7.4.3Streaming Stored Media, 713\n\n7.4.4Streaming Live Media, 721\n\n7.4.5Real-Time Conferencing, 724\n7.5CONTENT DELIVERY, 734\n\n7.5.1Content and Internet Trafï¬c, 736\n\n7.5.2Server Farms and Web Proxies, 738\n\n7.5.3Content Delivery Networks, 743\n\n7.5.4Peer-to-Peer Networks, 748\n7.6SUMMARY, 757\n**8NETWORK SECURITY**\n\n8.1CRYPTOGRAPHY, 766\n\n8.1.1Introduction to Cryptography, 767\n\n8.1.2Substitution Ciphers, 769\n\n8.1.3Transposition Ciphers, 771\n\n8.1.4One-Time Pads, 772\n\n8.1.5Two Fundamental Cryptographic Principles, 776\n8.2SYMMETRIC-KEY ALGORITHMS, 778\n\n8.2.1DES---The Data Encryption Standard, 780\n\n8.2.2AES---The Advanced Encryption Standard, 783\n\n8.2.3Cipher Modes, 787\n\n8.2.4Other Ciphers, 792\n\n8.2.5Cryptanalysis, 792\n\n8.3PUBLIC-KEY ALGORITHMS, 793\n\n8.3.1 RSA, 794\n\n8.3.2 Other Public-Key Algorithms, 796\n8.4DIGITAL SIGNATURES, 797\n\n8.4.1Symmetric-Key Signatures, 798\n\n8.4.2Public-Key Signatures, 799\n\n8.4.3Message Digests, 800\n\n8.4.4The Birthday Attack, 804\n8.5MANAGEMENT OF PUBLIC KEYS, 806\n\n8.5.1Certiï¬cates, 807 8.5.2 X.509, 809\n\n8.5.3 Public Key Infrastructures, 810\n8.6COMMUNICATION SECURITY, 813\n\n8.6.1IPsec, 814\n\n8.6.2Firewalls, 818\n\n8.6.3Virtual Private Networks, 821\n\n8.6.4Wireless Security, 822\n8.7AUTHENTICATION PROTOCOLS, 827\n\n8.7.1Authentication Based on a Shared Secret Key, 828\n\n8.7.2Establishing a Shared Key: The Difï¬e-Hellman Key Exchange, 833\n\n8.7.3Authentication Using a Key Distribution Center, 835\n\n8.7.4Authentication Using Kerberos, 838\n\n8.7.5Authentication Using Public-Key Cryptography, 840\n8.8EMAIL SECURITY*, 841\n\n8.8.1PGP---Pretty Good Privacy, 842\n\n8.8.2S/MIME, 846\n8.9WEB SECURITY, 846\n\n8.9.1Threats, 847\n\n8.9.2Secure Naming, 848\n\n8.9.3SSL---The Secure Sockets Layer, 853\n\n8.9.4Mobile Code Security, 857\n8.10SOCIAL ISSUES, 860\n\n8.10.1Privacy, 860\n\n8.10.2Freedom of Speech, 863\n\n8.10.3Copyright, 867\n8.11SUMMARY, 869\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/CIDR/","title":"CIDR"},"frontmatter":{"draft":false},"rawBody":"# CIDR\n\nCreated: 2018-06-19 13:05:35 +0500\n\nModified: 2018-06-19 13:07:24 +0500\n\n---\n\n**Classless Inter-Domain Routing**\n\nis a method for allocating[IP addresses](https://en.wikipedia.org/wiki/IP_address)and[IP routing](https://en.wikipedia.org/wiki/IP_routing). The[Internet Engineering Task Force](https://en.wikipedia.org/wiki/Internet_Engineering_Task_Force)introduced CIDR in 1993 to replace the previous addressing architecture of[classful network](https://en.wikipedia.org/wiki/Classful_network)design in the[Internet](https://en.wikipedia.org/wiki/Internet). Its goal was to slow the growth of[routing tables](https://en.wikipedia.org/wiki/Routing_table)on[routers](https://en.wikipedia.org/wiki/Router_(computing))across the Internet, and to help slow the rapid[exhaustion of IPv4 addresses](https://en.wikipedia.org/wiki/IPv4_address_exhaustion).\nIP addresses are described as consisting of two groups of[bits](https://en.wikipedia.org/wiki/Bit)in the address: the[most significant bits](https://en.wikipedia.org/wiki/Most_significant_bit)are the[network prefix](https://en.wikipedia.org/wiki/Network_prefix), which identifies a whole network or[subnet](https://en.wikipedia.org/wiki/Subnetwork), and the[least significant](https://en.wikipedia.org/wiki/Least_significant_bit)set forms the*host identifier*, which specifies a particular interface of a host on that network. This division is used as the basis of traffic routing between IP networks and for address allocation policies.\nWhereas classful network design for[IPv4](https://en.wikipedia.org/wiki/IPv4)sized the network prefix as one or more 8-bit groups, resulting in the blocks of Class A, B, or C addresses, Classless Inter-Domain Routing allocates address space to[Internet service providers](https://en.wikipedia.org/wiki/Internet_service_provider)and end users on any address[bit](https://en.wikipedia.org/wiki/Bit)boundary. In[IPv6](https://en.wikipedia.org/wiki/IPv6), however, the interface identifier has a fixed size of 64 bits by convention, and smaller subnets are never allocated to end users.\nCIDR encompasses several concepts. It is based on the**variable-length subnet masking**(**VLSM**) technique, which allows the specification of arbitrary-length prefixes. CIDR introduced a new method of representation for IP addresses, now commonly known as**CIDR notation**, in which an address or routing prefix is written with a suffix indicating the number of bits of the prefix, such as*192.0.2.0/24*for IPv4, and*2001:db8::/32*for IPv6. CIDR introduced an administrative process of allocating address blocks to organizations based on their actual and short-term projected needs. The aggregation of multiple contiguous prefixes resulted in[supernets](https://en.wikipedia.org/wiki/Supernet)in the larger Internet, which whenever possible are advertised as aggregates, thus reducing the number of entries in the global routing table.\n**Resources**\n\n<https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing>\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/Data-Center-Networking/","title":"Data Center Networking"},"frontmatter":{"draft":false},"rawBody":"# Data Center Networking\n\nCreated: 2019-12-29 01:40:58 +0500\n\nModified: 2020-01-09 01:34:10 +0500\n\n---\n\nData centeris a pool of resources (computational, storage, network) interconnected using a[communication network](https://en.wikipedia.org/wiki/Communication_network). Data Center Network (DCN) holds a pivotal role in a[data center](https://en.wikipedia.org/wiki/Data_center), as it interconnects all of the data center resources together. DCNs need to be scalable and efficient to connect tens or even hundreds of thousands of servers to handle the growing demands of[Cloud computing](https://en.wikipedia.org/wiki/Cloud_computing).Today's data centers are constrained by the interconnection network.\n**Types of Data center network**\n\n**Three-tier DCN**\n\nThe[legacy](https://en.wikipedia.org/wiki/Legacy_system)three-tier DCN architecture follows a multi-rooted[tree based network topology](https://en.wikipedia.org/wiki/Tree_network)composed of three layers of network switches, namely access, aggregate, and core layers.The[servers](https://en.wikipedia.org/wiki/Server_(computing))in the lowest layers are connected directly to one of the edge layer switches. The aggregate layer switches interconnect together multiple access layer switches. All of the aggregate layer switches are connected to each other by core layer switches. Core layer switches are also responsible for connecting the data center to the[Internet](https://en.wikipedia.org/wiki/Internet). The three-tier is the common network architecture used in data centers.However, three-tier architecture is unable to handle the growing demand of cloud computing.The higher layers of the three-tier DCN are highly oversubscribed.Moreover, scalability is another major issue in three-tier DCN. Major problems faced by the three-tier architecture include, scalability, fault tolerance, energy efficiency, and cross-sectional bandwidth. The three-tier architecture uses enterprise-level network devices at the higher layers of topology that are very expensive and power hungry.\n**Fat tree DCN**\n\nFat tree DCN architecture handles the oversubscription and cross section bandwidth problem faced by the legacy three-tier DCN architecture. Fat tree DCN employs commodity network switches based architecture using[Clos topology](https://en.wikipedia.org/wiki/Clos_network).The network elements in fat tree topology also follows hierarchical organization of network switches in access, aggregate, and core layers. However, the number of network switches is much larger than the three-tier DCN. The architecture is composed ofkpods, where each pod contains, (k/2)2servers, k/2 access layer switches, and k/2 aggregate layer switches in the topology. The core layers contain (k/2)2core switches where each of the core switches is connected to one aggregate layer switch in each of the pods. The fat tree topology offers 1:1 oversubscription ratio and full bisection bandwidth.The fat tree architecture uses a customized addressing scheme and[routing algorithm](https://en.wikipedia.org/wiki/Routing_algorithm). The scalability is one of the major issues in fat tree DCN architecture and maximum number of pods is equal to the number of ports in each switch.\n**DCell**\n\nDCell is a server-centric hybrid DCN architecture where one server is directly connected to many other servers.A server in the DCell architecture is equipped with multiple[Network Interface Cards](https://en.wikipedia.org/wiki/Network_Interface_Card)(NICs). The DCell follows a recursively built hierarchy of cells. A cell0is the basic unit and building block of DCell topology arranged in multiple levels, where a higher level cell contains multiple lower layer cells. The cell0is building block of DCell topology, which containsnservers and one commodity network switch. The network switch is only used to connect the server within a cell0. A cell1containk=n+1cell0cells, and similarly a cell2contains k * n + 1 dcell1. The DCell is a highly scalable architecture where a four level DCell with only six servers in cell0can accommodate around 3.26 million servers. Besides very high scalability, the DCell architecture depicts very high structural robustness.However, cross section bandwidth and network latency is a major issue in DCell DCN architecture.\n**Others**\n\nSome of the other well-known DCNs include BCube,Camcube,FiConn,Jelly fishand Scafida. A qualitative discussion of different DCNs along with benefits and drawbacks associated with each one has been made available.\n<https://en.wikipedia.org/wiki/Data_center_network_architectures>\n\n## Torus Interconnect**\n\nA**[torus](https://en.wikipedia.org/wiki/Torus)interconnect**is a switch-less[network topology](https://en.wikipedia.org/wiki/Network_topology)for connecting processing nodes in a[parallel computer](https://en.wikipedia.org/wiki/Parallel_computer)system.\n<https://en.wikipedia.org/wiki/Torus_interconnect>\n\n## InfiniBand**\n\n**InfiniBand**(**IB**) is a computer networking communications standard used in[high-performance computing](https://en.wikipedia.org/wiki/High-performance_computing)that features very high[throughput](https://en.wikipedia.org/wiki/Throughput)and very low[latency](https://en.wikipedia.org/wiki/Latency_(engineering)). It is used for data interconnect both among and within computers. InfiniBand is also used as either a direct or switched interconnect between servers and storage systems, as well as an interconnect between storage systems.\n<https://en.wikipedia.org/wiki/InfiniBand>\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/Forward-Error-Correction/","title":"Forward Error Correction"},"frontmatter":{"draft":false},"rawBody":"# Forward Error Correction\n\nCreated: 2019-12-07 23:08:13 +0500\n\nModified: 2021-07-30 23:47:50 +0500\n\n---\n\n**Error Detection and Correction Codes**\n**Parity Bit**\n\nAparity bit, orcheck bit, is a[bit](https://en.wikipedia.org/wiki/Bit)added to a string of[binary code](https://en.wikipedia.org/wiki/Binary_code)to ensure that the total number of 1-bits in the string is[even](https://en.wikipedia.org/wiki/Even_number)or[odd](https://en.wikipedia.org/wiki/Odd_number).Parity bits are used as the simplest form of[error detecting code](https://en.wikipedia.org/wiki/Error_detection_and_correction).\nThere are two variants of parity bits:even parity bitandodd parity bit.\nIn the case of even parity, for a given set of bits, the occurrences of bits whose value is 1 is counted. If that count is odd, the parity bit value is set to 1, making the total count of occurrences of 1s in the whole set (including the parity bit) an even number. If the count of 1s in a given set of bits is already even, the parity bit's value is 0.\nIn the case of odd parity, the coding is reversed. For a given set of bits, if the count of bits with a value of 1 is even, the parity bit value is set to 1 making the total count of 1s in the whole set (including the parity bit) an odd number. If the count of bits with a value of 1 is odd, the count is already odd so the parity bit's value is 0.\nEven parity is a special case of a[cyclic redundancy check](https://en.wikipedia.org/wiki/Cyclic_redundancy_check)(CRC), where the 1-bit CRC is generated by the[polynomial](https://en.wikipedia.org/wiki/Polynomial)x+1.\nIf a bit is present at a point otherwise dedicated to a parity bit, but is not used for parity, it may be referred to as amark parity bitif the parity bit is always 1, or aspace parity bitif the bit is always 0. In such cases where the value of the bit is constant, it may be called astick parity biteven though its function has nothing to do with parity.The function of such bits varies with the system design, but examples of functions for such bits include timing management, or identification of a packet as being of data or address significance. If its actual bit value is irrelevant to its function, the bit amounts to a[don't-care term](https://en.wikipedia.org/wiki/Don%27t-care_term).\nParity bits are generally applied to the smallest units of a communication protocol, typically 8-bit[octets](https://en.wikipedia.org/wiki/Octet_(computing))(bytes), although they can also be applied separately to an entire message string of bits.\n<https://en.wikipedia.org/wiki/Parity_bit>\n\n## Forward Error Correction**\n\nForward error correction (FEC) is an error correction technique to detect and correct a limited number of errors in transmitted data without the need for retransmission.\nIn this method, the sender sends a redundant error-correcting code along with the data frame. The receiver performs necessary checks based upon the additional redundant bits. If it finds that the data is free from errors, it executes error-correcting code that generates the actual frame. It then removes the redundant bits before passing the message to the upper layers.\n**Advantages and Disadvantages**\n-   Because FEC does not require handshaking between the source and the destination, it can be used for broadcasting of data to many destinations simultaneously from a single source.\n-   Another advantage is that FEC saves bandwidth required for retransmission. So, it is used in real time systems.\n-   Its main limitation is that if there are too many errors, the frames need to be retransmitted.\n**Error Correction Codes for FEC**\n\nError correcting codes for forward error corrections can be broadly categorized into two types, namely, block codes and convolution codes.\n-   **Block codesâˆ’** The message is divided into fixed-sized blocks of bits to which redundant bits are added for error correction.\n-   **Convolutional codesâˆ’** The message comprises of data streams of arbitrary length and parity symbols are generated by the sliding application of a Boolean function to the data stream.\nThere are four popularly used error correction codes.\n\n![Hamming Codes Binary Convolution Code Reed --- Solomon Code Low-Density Parity Check Code ](media/Forward-Error-Correction-image1.jpg)\n-   **Hamming Codesâˆ’** It is a block code that is capable of detecting up to two simultaneous bit errors and correcting single-bit errors.\nIn[telecommunication](https://en.wikipedia.org/wiki/Telecommunication),Hamming codesare a family of[linear error-correcting codes](https://en.wikipedia.org/wiki/Linear_code). Hamming codes can detect up to two-bit errors or correct one-bit errors without detection of uncorrected errors. By contrast, the simple[parity code](https://en.wikipedia.org/wiki/Parity_bit)cannot correct errors, and can detect only an odd number of bits in error. Hamming codes are[perfect codes](https://en.wikipedia.org/wiki/Perfect_code), that is, they achieve the highest possible[rate](https://en.wikipedia.org/wiki/Block_code#The_rate_R)for codes with their[block length](https://en.wikipedia.org/wiki/Block_code#The_block_length_n)and[minimum distance](https://en.wikipedia.org/wiki/Block_code#The_distance_d)of three.[Richard W. Hamming](https://en.wikipedia.org/wiki/Richard_Hamming)invented Hamming codes in 1950 as a way of automatically correcting errors introduced by[punched card](https://en.wikipedia.org/wiki/Punched_card)readers. In his original paper, Hamming elaborated his general idea, but specifically focused on the[Hamming(7,4)](https://en.wikipedia.org/wiki/Hamming(7,4))code which adds three parity bits to four bits of data.\n<https://en.wikipedia.org/wiki/Hamming_code>-   **Binary Convolution Codeâˆ’** Here, an encoder processes an input sequence of bits of arbitrary length and generates a sequence of output bits.\n-   **Reed-Solomon Codeâˆ’** They are block codes that are capable of correcting burst errors in the received data block.\n-   **Low-Density Parity Check Codeâˆ’** It is a block code specified by a parity-check matrix containing a low density of 1s. They are suitable for large block sizes in very noisy channels.\n![Error correction codes Hamming codes Reed-Solomon Turbo codes 1930 1940 950 codes 1960 1970 1980 1990 2000 2010 Shannon's paper on information theory Shor codes (quantum) ](media/Forward-Error-Correction-image2.png)\n\n<https://www.tutorialspoint.com/forward-error-correction-fec>\n\n<https://en.wikipedia.org/wiki/Forward_error_correction>\n\n[Hamming codes, hâ– w to ovâ– rcoâ– e nâ– ise.](https://www.youtube.com/watch?v=X8jsijhllIA)\n![One Bit is Wrong (according to an extended Hamming Code) oooooooo Can you tell which? ](media/Forward-Error-Correction-image3.jpg)\n**Binary Goley Code**\n\nIn[mathematics](https://en.wikipedia.org/wiki/Mathematics)and[electronics engineering](https://en.wikipedia.org/wiki/Electronics_engineering), abinary Golay codeis a type of linear[error-correcting code](https://en.wikipedia.org/wiki/Error-correcting_code)used in[digital communications](https://en.wikipedia.org/wiki/Digital_communication). The binary Golay code, along with the[ternary Golay code](https://en.wikipedia.org/wiki/Ternary_Golay_code), has a particularly deep and interesting connection to the theory of[finite sporadic groups](https://en.wikipedia.org/wiki/Finite_sporadic_group)in mathematics.These codes are named in honor of[Marcel J. E. Golay](https://en.wikipedia.org/wiki/Marcel_J._E._Golay)whose 1949 paperintroducing them has been called, by[E. R. Berlekamp](https://en.wikipedia.org/wiki/E._R._Berlekamp), the \"best single published page\" in coding theory.\nThere are two closely related binary Golay codes. Theextended binary Golay code,G24(sometimes just called the \"Golay code\" in finite group theory) encodes 12 bits of data in a 24-bit word in such a way that any 3-bit errors can be corrected or any 7-bit errors can be detected. The other, theperfect binary Golay code,G23, has codewords of length 23 and is obtained from the extended binary Golay code by deleting one coordinate position (conversely, the extended binary Golay code is obtained from the perfect binary Golay code by adding a[parity bit](https://en.wikipedia.org/wiki/Parity_bit)). In standard coding notation the codes have parameters [24, 12, 8] and [23, 12, 7], corresponding to the length of the codewords, the[dimension](https://en.wikipedia.org/wiki/Dimension_(vector_space))of the code, and the minimum[Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance)between two codewords, respectively.\n<https://en.wikipedia.org/wiki/Binary_Golay_code>\n\n## Data Scrubbing**\n\nData scrubbingis an[error correction](https://en.wikipedia.org/wiki/Error_correction)technique that uses a background task to periodically inspect[main memory](https://en.wikipedia.org/wiki/Main_memory)or[storage](https://en.wikipedia.org/wiki/Computer_data_storage)for errors, then correct detected errors using[redundant data](https://en.wikipedia.org/wiki/Data_redundancy)in the form of different[checksums](https://en.wikipedia.org/wiki/Checksum)or copies of data. Data scrubbing reduces the likelihood that single correctable errors will accumulate, leading to reduced risks of uncorrectable errors.\n[Data integrity](https://en.wikipedia.org/wiki/Data_integrity)is a high-priority concern in writing, reading, storage, transmission, or processing of the[computer](https://en.wikipedia.org/wiki/Computer)[data](https://en.wikipedia.org/wiki/Data)in computer[operating systems](https://en.wikipedia.org/wiki/Operating_system)and in computer storage and data transmission systems. However, only a few of the currently existing and used file systems provide sufficient protection against[data corruption](https://en.wikipedia.org/wiki/Data_corruption).\nTo address this issue, data scrubbing provides routine checks of all[inconsistencies](https://en.wikipedia.org/wiki/Inconsistency)in data and, in general, prevention of hardware or software failure. This \"scrubbing\" feature occurs commonly in memory, disk arrays,[file systems](https://en.wikipedia.org/wiki/File_system), or[FPGAs](https://en.wikipedia.org/wiki/Field-programmable_gate_array)as a mechanism of error detection and correction.\n<https://en.wikipedia.org/wiki/Data_scrubbing>\n\n## Checksum**\n\n**Verhoeff algorithm**\n\nTheVerhoeff algorithmis a[checksum](https://en.wikipedia.org/wiki/Checksum)formula for[error detection](https://en.wikipedia.org/wiki/Error_detection)developed by the Dutch mathematician [Jacobus Verhoeff](https://en.wikipedia.org/wiki/Jacobus_Verhoeff)and was first published in 1969.[[2]](https://en.wikipedia.org/wiki/Verhoeff_algorithm#cite_note-Kirtland_2001-2)[[3]](https://en.wikipedia.org/wiki/Verhoeff_algorithm#cite_note-Salomon_2005-3)It was the first decimal[check digit](https://en.wikipedia.org/wiki/Check_digit)algorithm which detects all single-digit errors, and all transposition errors involving two adjacent digits,which was at the time thought impossible with such a code.\nEx - used in aadhaar validation\n<https://en.wikipedia.org/wiki/Verhoeff_algorithm>\n\n<https://medium.com/@krs.sharath03/how-aadhar-number-is-generated-and-validated-3c3e7172e606>\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/IP/","title":"IP"},"frontmatter":{"draft":false},"rawBody":"# IP\n\nCreated: 2019-05-01 10:05:15 +0500\n\nModified: 2022-01-30 23:30:30 +0500\n\n---\n\nIP addresses are typically made of two separate components. The first part of the address is used to identify the network that the address is a part of. The part that comes afterwards is used to specify a specific host within that network.\nWhere the network specification ends and the host specification begins depends on how the network is configured. We will discuss this more thoroughly momentarily.\nIPv4 addresses were traditionally divided into five different \"classes\", named A through E, meant to differentiate segments of the available addressable IPv4 space. These are defined by the first four bits of each address. You can identify what class an IP address belongs to by looking at these bits.\nHere is a translation table that defines the addresses based on their leading bits:\n-   **Class A**\n\n0---: If the first bit of an IPv4 address is \"0\", this means that the address is part of class A. This means that any address from0.0.0.0 to 127.255.255.255is in class A.\n-   **Class B**\n\n10--: Class B includes any address from128.0.0.0 to 191.255.255.255. This represents the addresses that have a \"1\" for their first bit, but don't have a \"1\" for their second bit.\n**Private IP** - IANA - RFC1918 (reserved for private subnets)\n\n10.x.x.x\n\n172.16.0.0\n\n192.168.0.0-   **Class C**\n\n110-: Class C is defined as the addresses ranging from192.0.0.0 to 223.255.255.255. This represents all of the addresses with a \"1\" for their first two bits, but without a \"1\" for their third bit.\n-   **Class D**\n\n1110: This class includes addresses that have \"111\" as their first three bits, but a \"0\" for the next bit. This address range includes addresses from224.0.0.0 to 239.255.255.255.\n-   **Class E**\n\n1111: This class defines addresses between240.0.0.0 and 255.255.255.255. Any address that begins with four \"1\" bits is included in this class.\nClass D addresses are reserved for multi-casting protocols, which allow a packet to be sent to a group of hosts in one movement. Class E addresses are reserved for future and experimental use, and are largely not used.\nTraditionally, each of the regular classes (A-C) divided the networking and host portions of the address differently to accommodate different sized networks. Class A addresses used the remainder of the first octet to represent the network and the rest of the address to define hosts. This was good for defining a few networks with a lot of hosts each.\nThe class B addresses used the first two octets (the remainder of the first, and the entire second) to define the network and the rest to define the hosts on each network. The class C addresses used the first three octets to define the network and the last octet to define hosts within that network.\nThe division of large portions of IP space into classes is now almost a legacy concept. Originally, this was implemented as a stop-gap for the problem of rapidly depleting IPv4 addresses (you can have multiple computers with the same host if they are in separate networks). This was replaced largely by later schemes that we will discuss below.\n**Reserved IP Addresses**\n\nOne of the most useful reserved ranges is the loopback range specified by addresses from 127.0.0.0 to 127.255.255.255. This range is used by each host to test networking to itself. Typically, this is expressed by the first address in this range: 127.0.0.1.\n<https://en.wikipedia.org/wiki/Reserved_IP_addresses>\n\n## LoopBack Address**\n\nThelocalhostis the default name describing the local computer address also known as theloopback address. For example, typing:ping localhostwould ping the local IP address of 127.0.0.1 (the loopback address). When setting up a web server or software on a web server, 127.0.0.1 is used to point the software to the local machine.\ncat /etc/hosts\n\n127.0.0.1localhost\n\n255.255.255.255broadcasthost\n\n::1 localhost\nUsing the loopback interface bypasses any local network interface hardware. The local loopback mechanism is useful for testing software during development, independently of any networking configurations. For example, if a computer has been configured to provide a website, directing a locally running web browser to <http://localhost> may display its home page.\nThe IP**0.0.0.0**is commonly used to mean that the program listens on all the IPs available in that machine/server\n\nThe address0.0.0.0is a non-routable meta-address used to designate an invalid, unknown or non-applicable target. This address is assigned specific meanings in a number of contexts, such as on[clients](https://en.wikipedia.org/wiki/Client_(computing))or on[servers](https://en.wikipedia.org/wiki/Server_(computing)).\n\n<https://en.wikipedia.org/wiki/0.0.0.0>\n<https://whatismyipaddress.com/localhost>\n\n## Netmasks and Subnets**\n\nThe process of dividing a network into smaller network sections is calledsubnetting. This can be useful for many different purposes and helps isolate groups of hosts together and deal with them easily.\n**CIDR Notation**\n\nA system calledClassless Inter-Domain Routing, or CIDR, was developed as an alternative to traditional subnetting. The idea is that you can add a specification in the IP address itself as to the number of significant bits that make up the routing or networking portion.\nFor example, we could express the idea that the IP address192.168.0.15is associated with the netmask255.255.255.0by using the CIDR notation of192.168.0.15/24. This means that the first 24 bits of the IP address given are considered significant for the network routing.\n**IPv4 vs IPv6 / IP routing**\n**IPAM**\n\n[IPAM (IP Address Management)](https://www.infoblox.com/products/ipam-dhcp/)is the administration of DNS and DHCP, which are the network services that assign and resolve IP addresses to machines in a TCP/IP network. Simply put, IPAM is a means of planning, tracking, and managing the Internet Protocol address space used in a network. Most commonly, tools such as DNS and DHCP are used in tandem to perform this task, though true IPAM will glue these services together so that each is aware of changes in the other (for instance DNS knowing of the IP address taken by a client via DHCP, and updating itself accordingly).\nIP Address Management (IPAM) is an integrated suite of tools to enable end-to-end planning, deploying, managing and monitoring of your IP address infrastructure, with a rich user experience. IPAM automatically discovers IP address infrastructure servers and Domain Name System (DNS) servers on your network and enables you to manage them from a central interface.\n<https://docs.microsoft.com/en-us/windows-server/networking/technologies/ipam/ipam-top>\n\n## Ip address examples**\n\n| ping 0                               | 127.0.0.1 (Linux)     |\n|--------------------------------------|-----------------------|\n| ping 0                               | 0.0.0.0 (Mac)         |\n| ping 127.1                           | 127.0.0.1             |\n| ping 10.50.1                         | 10.50.0.1             |\n| ping 10.0.513                        | 10.0.2.1 (2x 256 + 1) |\n| Decimal ip notation - ping 167772673 | 10.0.2.1              |\n| Hex ip notation - ping 0xA000201     | 10.0.2.1              |\n| Octal ip notation - ping 10.0.2.010  | 10.0.2.8              |\n<https://ma.ttias.be/theres-more-than-one-way-to-write-an-ip-address>\n\n## References**\n\n<https://www.digitalocean.com/community/tutorials/understanding-ip-addresses-subnets-and-cidr-notation-for-networking>\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2018-06-23 12:42:58 +0500\n\nModified: 2021-08-08 12:40:38 +0500\n\n---\n\n**Connection**\n\nIn networking, a connection refers to pieces of related information that are transfered through a network. This generally infers that a connection is built before the data transfer (by following the procedures laid out in a protocol) and then is deconstructed at the at the end of the data transfer.\n**Packet**\n\nA packet is, generally speaking, the most basic unit that is transfered over a network. When communicating over a network, packets are the envelopes that carry your data (in pieces) from one end point to the other.\nPackets have a header portion that contains information about the packet including the source and destination, timestamps, network hops, etc. The main portion of a packet contains the actual data being transfered. It is sometimes called the body or the payload.\n**Network Interface**\n\nA network interface can refer to any kind of software interface to networking hardware. For instance, if you have two network cards in your computer, you can control and configure each network interface associated with them individually.\nA network interface may be associated with a physical device, or it may be a representation of a virtual interface. The \"loopback\" device, which is a virtual interface to the local machine, is an example of this.\n**LAN**\n\nLAN stands for \"local area network\". It refers to a network or a portion of a network that is not publicly accessible to the greater internet. A home or office network is an example of a LAN.\n**WAN**\n\nWAN stands for \"wide area network\". It means a network that is much more extensive than a LAN. While WAN is the relevant term to use to describe large, dispersed networks in general, it is usually meant to mean the internet, as a whole.\nIf an interface is said to be connected to the WAN, it is generally assumed that it is reachable through the internet.\n**Protocol**\n\nA protocol is a set of rules and standards that basically define a language that devices can use to communicate. There are a great number of protocols in use extensively in networking, and they are often implemented in different layers.\nSome low level protocols are TCP, UDP, IP, and ICMP. Some familiar examples of application layer protocols, built on these lower protocols, are HTTP (for accessing web content), SSH, TLS/SSL, and FTP.\n**Port**\n\nA port is an address on a single machine that can be tied to a specific piece of software. It is not a physical interface or location, but it allows your server to be able to communicate using more than one application.\n**Firewall**\n\nA firewall is a program that decides whether traffic coming into a server or going out should be allowed. A firewall usually works by creating rules for which type of traffic is acceptable on which ports. Generally, firewalls block ports that are not used by a specific application on a server.\n**NAT (Network Address Translation)**\n\nIt is a way to translate requests that are incoming into a routing server to the relevant devices or servers that it knows about in the LAN. This is usually implemented in physical LANs as a way to route requests through one IP address to the necessary backend servers.\nA system called*Network Address Translation*, allows the addresses to be rewritten when packets traverse network borders to allow them to continue on to their correct destination. This allows the same IP address to be used on multiple, isolated networks while still allowing these to communicate with each other if configured correctly.\n**IP masquerading**is a technique that hides an entire IP address space, usually consisting of private IP addresses, behind a single IP address in another, usually public address space. The hidden addresses are changed into a single (public) IP address as the source address of the outgoing IP packets so they appear as originating not from the hidden host but from the routing device itself. Because of the popularity of this technique to conserve IPv4 address space, the term*NAT*has become virtually synonymous with IP masquerading.\n**NAT Implementations**\n-   Full-cone NAT\n-   (Address)-restricted-cone NAT\n-   Port-restricted-cone NAT\n-   Symmetric NAT\nPAT - Port Address Translation\n\nDNAT - Destination Network Address Translation\n\nSNAT - Source Network Address Translation\n<https://en.wikipedia.org/wiki/Network_address_translation>\n\n## VPN**\n\nVPN stands for virtual private network. It is a means of connecting separate LANs through the internet, while maintaining privacy. This is used as a means of connecting remote systems as if they were on a local network, often for security reasons.\n**DMVPN**\n\nDMVPN (Dynamic Multipoint VPN) is a routing technique we can use to build a VPN network with multiple sites without having to statically configure all devices. It's a \"hub and spoke\" network where the spokes will be able to communicate with each other directly without having to go through the hub. Encryption is supported through IPsec which makes DMVPN a popular choice for connecting different sites using regular Internet connections. It's a great backup or alternative to private networks like MPLS VPN.\nThere are four pieces to the DMVPN puzzle:\n-   Multipoint GRE (mGRE)\n-   NHRP (Next Hop Resolution Protocol)\n-   Routing (RIP, EIGRP, OSPF, BGP, etc.)\n-   IPsec (not required but recommended)\n<https://networklessons.com/cisco/ccie-routing-switching/introduction-to-dmvpn>\n\n## Bridge**\n\nA**network bridge**is a[computer networking device](https://en.wikipedia.org/wiki/Networking_hardware)that creates a single aggregate network from multiple[communication networks](https://en.wikipedia.org/wiki/Communication_network)or[network segments](https://en.wikipedia.org/wiki/Network_segment). This function is called**network bridging.**Bridging is distinct from[routing](https://en.wikipedia.org/wiki/Routing). Routing allows multiple networks to communicate independently and yet remain separate, whereas bridging connects two separate networks as if they were a single network.In the[OSI model](https://en.wikipedia.org/wiki/OSI_model), bridging is performed in the[data link layer](https://en.wikipedia.org/wiki/Data_link_layer)(layer 2). If one or more segments of the bridged network are[wireless](https://en.wikipedia.org/wiki/Wireless_network), the device is known as a**wireless bridge.**\nThere are four main types of network bridging technologies: simple bridging, multiport bridging, learning or transparent bridging, and[source route bridging](https://en.wikipedia.org/wiki/Source_route_bridging).\n<https://en.wikipedia.org/wiki/Bridging_(networking)>\n\n## References**\n\n<https://www.digitalocean.com/community/tutorials/an-introduction-to-networking-terminology-interfaces-and-protocols>\n<https://www.freecodecamp.org/news/free-computer-networking-course>\n-   Intro to Network Devices\n-   Networking Services and Applications\n-   DHCP in the Network\n-   Introduction to the DNS Service\n-   Introducing Network Address Translation\n-   WAN Technologies\n-   Network Cabling\n-   Network Topologies\n-   Network Infrastructure Implementations\n-   Introduction to IPv4\n-   Introduction to IPv6\n-   Special IP Networking Concepts\n-   Introduction to Routing Concepts\n-   Introduction to Routing Protocols\n-   Basic Elements of Unified Communications\n-   Virtualization Technologies\n-   Storage Area Networks\n-   Basic Cloud Concepts\n-   Implementing a Basic Network\n-   Analyzing Monitoring Reports\n-   Network Monitoring\n-   Supporting Configuration Management\n-   The Importance of Network Segmentation\n-   Applying Patches and Updates\n-   Configuring Switches\n-   Wireless LAN Infrastructure\n-   Risk and Security Related Concepts\n-   Common Network Vulnerabilities\n-   Common Network Threats\n-   Network Hardening Techniques\n-   Physical Network Security Control\n-   Firewall Basics\n-   Network Access Control\n-   Basic Forensic Concepts\n-   Network Troubleshooting Methodology\n-   Troubleshooting Connectivity with Utilities\n-   Troubleshooting Connectivity with Hardware\n-   Troubleshooting Wireless Networks\n-   Troubleshooting Copper Wire Networks\n-   Troubleshooting Fiber Cable Networks\n-   Network Troubleshooting Common Network Issues\n-   Common Network Security Issues\n-   Common WAN Components and Issues\n-   The OSI Networking Reference Model\n-   The Transport Layer Plus ICMP\n-   Basic Network Concepts\n-   Introduction to Wireless Network Standards\n-   Introduction to Wired Network Standards\n-   Security Policies and other Documents\n-   Introduction to Safety Practices\n-   Rack and Power Management\n-   Cable Management\n-   Basics of Change Management\n-   Common Networking Protocols\n<https://www.freecodecamp.org/news/how-does-the-internet-work>\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/Network-Sockets-Ports/","title":"Network Sockets/Ports"},"frontmatter":{"draft":false},"rawBody":"# Network Sockets/Ports\n\nCreated: 2019-09-28 13:27:16 +0500\n\nModified: 2020-05-11 23:51:24 +0500\n\n---\n\n# Network Sockets\n\nA**network socket**is an internal endpoint for sending or receiving data within a[node](https://en.wikipedia.org/wiki/Node_(networking))on a[computer network](https://en.wikipedia.org/wiki/Computer_network). Concretely, it is a representation of this endpoint in networking software ([protocol stack](https://en.wikipedia.org/wiki/Protocol_stack)), such as an entry in a table (listing communication protocol, destination, status, etc.), and is a form of[system resource](https://en.wikipedia.org/wiki/System_resource).\nA[process](https://en.wikipedia.org/wiki/Process_(computing))can refer to a socket using asocket descriptor, a type of[handle](https://en.wikipedia.org/wiki/Handle_(computing)). A process first requests that the protocol stack create a socket, and the stack returns a descriptor to the process so it can identify the socket. The process then passes the descriptor back to the protocol stack when it wishes to send or receive data using this socket.\nUnlike[ports](https://en.wikipedia.org/wiki/Port_(computer_networking)), sockets are specific to one node; they are local resources and cannot be referred to directly by other nodes. Further, sockets are not necessarily associated with a persistent connection ([channel](https://en.wikipedia.org/wiki/Channel_(communications))) for communication between two nodes, nor is there necessarily some single other endpoint. For example, a[datagram socket](https://en.wikipedia.org/wiki/Datagram_socket)can be used for[connectionless communication](https://en.wikipedia.org/wiki/Connectionless_communication), and a[multicast](https://en.wikipedia.org/wiki/Multicast)socket can be used to send to multiple nodes. However, in practice for[internet](https://en.wikipedia.org/wiki/Internet)communication, sockets are generally used to connect to a specific endpoint and often with a persistent connection.\n**Use**\n\nA[process](https://en.wikipedia.org/wiki/Process_(computing))can refer to a socket using asocket descriptor, a type of[handle](https://en.wikipedia.org/wiki/Handle_(computing)). A process first requests that the protocol stack create a socket, and the stack returns a descriptor to the process so it can identify the socket. The process then passes the descriptor back to the protocol stack when it wishes to send or receive data using this socket.\nUnlike[ports](https://en.wikipedia.org/wiki/Port_(computer_networking)), sockets are specific to one node; they are local resources and cannot be referred to directly by other nodes. Further, sockets are not necessarily associated with a persistent connection ([channel](https://en.wikipedia.org/wiki/Channel_(communications))) for communication between two nodes, nor is there necessarily some single other endpoint. For example, a[datagram socket](https://en.wikipedia.org/wiki/Datagram_socket)can be used for[connectionless communication](https://en.wikipedia.org/wiki/Connectionless_communication), and a[multicast](https://en.wikipedia.org/wiki/Multicast)socket can be used to send to multiple nodes. However, in practice for[internet](https://en.wikipedia.org/wiki/Internet)communication, sockets are generally used to connect to a specific endpoint and often with a persistent connection.\n**Socket addresses**\n\nIn practice,socketusually refers to a socket in an[Internet Protocol](https://en.wikipedia.org/wiki/Internet_Protocol)(IP) network (where a socket may be called anInternet socket), in particular for the[Transmission Control Protocol](https://en.wikipedia.org/wiki/Transmission_Control_Protocol)(TCP), which is a protocol for one-to-one connections. In this context, sockets are assumed to be associated with a specificsocket address, namely the[IP address](https://en.wikipedia.org/wiki/IP_address)and a[port number](https://en.wikipedia.org/wiki/Port_number)for the local node, and there is a corresponding socket address at the foreign node (other node), which itself has an associated socket, used by the foreign process. Associating a socket with a socket address is calledbinding.\nNote that while a local process can communicate with a foreign process by sending or receiving data to or from a foreignsocket address, it does not have access to the foreignsocketitself, nor can it use the foreignsocket descriptor, as these are both internal to the foreign node. For example, in a connection between 10.20.30.40:4444 and 50.60.70.80:8888 (local IP address:local port, foreign IP address:foreign port), there will also be an associated socket at each end, corresponding to the internal representation of the connection by the protocol stack on that node. These are referred to locally by numerical socket descriptors, say 317 at one side and 922 at the other. A process on node 10.20.30.40 can request to communicate with node 50.60.70.80 on port 8888 (request that the protocol stack create a socket to communicate with that destination), and once it has created a socket and received a socket descriptor (317), it can communicate via this socket by using the descriptor (317). The protocol stack will then forward data to and from node 50.60.70.80 on port 8888. However, a process on node 10.20.30.40 cannot request to communicate based on the foreign socket descriptor, (e.g. \"socket 922\" or \"socket 922 on node 50.60.70.80\") as these are internal to the foreign node and are not usable by the protocol stack on node 10.20.30.40.\n<https://en.wikipedia.org/wiki/Network_socket>\n# Ports\n\nIn[computer networking](https://en.wikipedia.org/wiki/Computer_networking), aportis a communication endpoint. Physical as well as wireless connections are terminated at ports of hardware devices. At the software level, within an[operating system](https://en.wikipedia.org/wiki/Operating_system), a port is a logical construct that identifies a specific[process](https://en.wikipedia.org/wiki/Process_(computing))or a type of[network service](https://en.wikipedia.org/wiki/Network_service). Ports are identified for each protocol and address combination by 16-bit unsigned numbers, commonly known as theport number. The most common protocols that use port numbers are the[Transmission Control Protocol](https://en.wikipedia.org/wiki/Transmission_Control_Protocol)(TCP) and the[User Datagram Protocol](https://en.wikipedia.org/wiki/User_Datagram_Protocol)(UDP).\nA port number is always associated with an[IP address](https://en.wikipedia.org/wiki/IP_address)of a host and the[protocol](https://en.wikipedia.org/wiki/Network_protocol)type of the communication. It completes the destination or origination[network address](https://en.wikipedia.org/wiki/Network_address)of a message. Specific port numbers are commonly reserved to identify specific services, so that an arriving packet can be easily forwarded to a running application. For this purpose, the lowest numbered 1024 port numbers identify the historically most commonly used services, and are called the[well-known port numbers](https://en.wikipedia.org/wiki/Well-known_port_numbers). Higher-numbered ports are available for general use by applications and are known as[ephemeral ports](https://en.wikipedia.org/wiki/Ephemeral_port).\nWhen used as a service enumeration, ports provide a[multiplexing](https://en.wikipedia.org/wiki/Multiplexing)service for multiple services or multiple communication sessions at one network address. In the[client--server model](https://en.wikipedia.org/wiki/Client%E2%80%93server_model)of application architecture multiple simultaneous communication sessions may be initiated for the same service.\n**Port number**\n\nA port number is a 16-bit unsigned integer, thus ranging from 0 to 65535. For[TCP](https://en.wikipedia.org/wiki/Transmission_Control_Protocol), port number 0 is reserved and cannot be used, while for[UDP](https://en.wikipedia.org/wiki/User_Datagram_Protocol), the source port is optional and a value of zero means no port. A[process](https://en.wikipedia.org/wiki/Process_(computing))associates its input or output channels via an[Internet socket](https://en.wikipedia.org/wiki/Internet_socket), which is a type of[file descriptor](https://en.wikipedia.org/wiki/File_descriptor), with a[transport protocol](https://en.wikipedia.org/wiki/Transport_protocol), an[IP address](https://en.wikipedia.org/wiki/IP_address), and a port number. This is known asbinding, and enables the process to send and receive data via the network. The operating system's networking software has the task of transmitting outgoing data from all application ports onto the network, and forwarding arriving[network packets](https://en.wikipedia.org/wiki/Network_packet)to processes by matching the packet's IP address and port number. For TCP, only one process may bind to a specific IP address and port combination. Common application failures, sometimes calledport conflicts, occur when multiple programs attempt to use the same port number on the same IP address with the same protocol.\nApplications implementing common services often use specifically reserved[well-known port numbers](https://en.wikipedia.org/wiki/Well-known_port_numbers)for receiving service requests from clients. This process is known aslistening, and involves the receipt of a request on the well-known port and establishing a one-to-one server-client dialog, using the same local port number. Other clients may continue to connect to the listening port; this works because a TCP connection is identified by a tuple consisting of the local address, the local port, the remote address, and the remote port.The well-known ports are defined by convention overseen by the[Internet Assigned Numbers Authority](https://en.wikipedia.org/wiki/Internet_Assigned_Numbers_Authority)(IANA). The core network services, such as the[World-Wide Web](https://en.wikipedia.org/wiki/World-Wide_Web), typically use well-known port numbers. In many operating systems special privileges are required for applications to bind to these ports, because these are often deemed critical to the operation of IP networks. Conversely, the client end of a connection typically uses a high port number allocated for short term use, therefore called an[ephemeral port](https://en.wikipedia.org/wiki/Ephemeral_port).\n**Common port numbers**\n\nThe[Internet Assigned Numbers Authority](https://en.wikipedia.org/wiki/Internet_Assigned_Numbers_Authority)(IANA) is responsible for the global coordination of the DNS Root, IP addressing, and other Internet protocol resources. This includes the registration of commonly used port numbers for well-known Internet services.\nThe port numbers are divided into three ranges: thewell-known ports, theregistered ports, and thedynamicorprivate ports.\n1.  **Well Known Ports/System ports (0-1023)**\n\nThese are allocated toserver servicesby theInternet Assigned Numbers Authority(IANA).\n<table>\n<colgroup>\n<col style=\"width: 18%\" />\n<col style=\"width: 81%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>Port number</th>\n<th>Assignment</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>20</td>\n<td><a href=\"https://en.wikipedia.org/wiki/File_Transfer_Protocol\">File Transfer Protocol</a>(FTP) Data Transfer</td>\n</tr>\n<tr class=\"even\">\n<td>21</td>\n<td><a href=\"https://en.wikipedia.org/wiki/File_Transfer_Protocol\">File Transfer Protocol</a>(FTP) Command Control</td>\n</tr>\n<tr class=\"odd\">\n<td>22</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Secure_Shell\">Secure Shell</a>(SSH) Secure Login</td>\n</tr>\n<tr class=\"even\">\n<td>23</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Telnet\">Telnet</a>remote login service, unencrypted text messages</td>\n</tr>\n<tr class=\"odd\">\n<td>25</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol\">Simple Mail Transfer Protocol</a>(SMTP) E-mail routing</td>\n</tr>\n<tr class=\"even\">\n<td>53</td>\n<td><p><strong><a href=\"https://en.wikipedia.org/wiki/Domain_Name_System\">Domain Name System</a>(DNS) service</strong></p>\n<p>Mainly uses UDP but can use TCP for Zone Transfers</p></td>\n</tr>\n<tr class=\"odd\">\n<td>80</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol\">Hypertext Transfer Protocol</a>(HTTP) used in the<a href=\"https://en.wikipedia.org/wiki/World_Wide_Web\">World Wide Web</a></td>\n</tr>\n<tr class=\"even\">\n<td>110</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Post_Office_Protocol\">Post Office Protocol</a>(POP3)</td>\n</tr>\n<tr class=\"odd\">\n<td>119</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Network_News_Transfer_Protocol\">Network News Transfer Protocol</a>(NNTP)</td>\n</tr>\n<tr class=\"even\">\n<td>123</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Network_Time_Protocol\">Network Time Protocol</a>(NTP)</td>\n</tr>\n<tr class=\"odd\">\n<td>143</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Internet_Message_Access_Protocol\">Internet Message Access Protocol</a>(IMAP) Management of digital mail</td>\n</tr>\n<tr class=\"even\">\n<td>161</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Simple_Network_Management_Protocol\">Simple Network Management Protocol</a>(SNMP)</td>\n</tr>\n<tr class=\"odd\">\n<td>194</td>\n<td><a href=\"https://en.wikipedia.org/wiki/Internet_Relay_Chat\">Internet Relay Chat</a>(IRC)</td>\n</tr>\n<tr class=\"even\">\n<td>443</td>\n<td><a href=\"https://en.wikipedia.org/wiki/HTTP_Secure\">HTTP Secure</a>(HTTPS) HTTP over TLS/SSL</td>\n</tr>\n<tr class=\"odd\">\n<td>3389</td>\n<td>RDP, Remote Desktop Protocol</td>\n</tr>\n</tbody>\n</table>\n2.  **Registered Ports (1024-49151)**\n\nThese can be registered for services with theIANAand should be treated assemi-reserved.User written programs should not use these ports.\n3.  **Ephermeral Ports (49152-65535)**\n\nThese are used byclient programsand you are free to use these in client programs. When a Web browser connects to a web server the browser will allocate itself a port in this range. Also known asephemeral ports.\n<https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers>\n\n## Network behavior**\n\n[Transport layer](https://en.wikipedia.org/wiki/Transport_layer)protocols, such as the[Transmission Control Protocol](https://en.wikipedia.org/wiki/Transmission_Control_Protocol)(TCP) and the[User Datagram Protocol](https://en.wikipedia.org/wiki/User_Datagram_Protocol)(UDP), transfer data using[protocol data units](https://en.wikipedia.org/wiki/Protocol_data_unit)(PDUs). For TCP, the PDU is a[segment](https://en.wikipedia.org/wiki/Transmission_Control_Protocol#TCP_segment_structure), and a[datagram](https://en.wikipedia.org/wiki/Datagram)for UDP. Both protocols use a[header](https://en.wikipedia.org/wiki/Header_(computing))field for recording the source and destination port number. The port numbers are encoded in the transport protocol[packet header](https://en.wikipedia.org/wiki/Packet_header), and they can be readily interpreted not only by the sending and receiving computers, but also by other components of the networking infrastructure. In particular,[firewalls](https://en.wikipedia.org/wiki/Firewall_(networking))are commonly configured to differentiate between packets based on their source or destination port numbers.[Port forwarding](https://en.wikipedia.org/wiki/Port_forwarding)is an example application of this.\n**Port scanning**\n\nThe practice of attempting to connect to a range of ports in sequence on a single computer is commonly known as[port scanning](https://en.wikipedia.org/wiki/Port_scanning). This is usually associated either with malicious[cracking](https://en.wikipedia.org/wiki/Security_cracking)attempts or with network administrators looking for possible vulnerabilities to help prevent such attacks. Port connection attempts are frequently monitored and logged by computers. The technique of[port knocking](https://en.wikipedia.org/wiki/Port_knocking)uses a series of port connections (knocks) from a client computer to enable a server connection.\n**Port Knocking**\n\n[Port knocking](https://en.wikipedia.org/wiki/Port_knocking)is something nobody actually uses in the real world, but is a lot of fun to set up. In short, port knocking is a sequence of hits to various closed network ports, and if you get that sequence right, the \"real\" port opens up for use to your IP. It's neat, but impractical in an actual enterprise.\n<https://en.wikipedia.org/wiki/Port_(computer_networking)>"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/Networking-Fabric/","title":"Networking Fabric"},"frontmatter":{"draft":false},"rawBody":"# Networking Fabric\n\nCreated: 2019-12-02 23:50:51 +0500\n\nModified: 2019-12-02 23:52:21 +0500\n\n---\n\nThe networking switch fabric is the basic topology of how a network is laid out and connected to switch traffic on a data or circuit-switched network.\nThe switch fabric concept, in which many connections are made in a matrix, has a long history in communications, dating back to the circuit-switched voice telephone days when connections were switched by humans operating a switchboard. The term \"fabric\" comes from the criss-cross nature of a networking fabric, which is often visualized as a checkerboard of connections\nAlthough the basic concept of switch fabrics dates back to the origins of the telephone nearly 150 years ago, data networking has pushed theconcept to new levels. We've come a long way from the local telephone switchboard. Now, a myriad of physical connectivity, chip, and switching technologies are used to connect bits in a networking fabric. In some cases, a switching fabric can be boiled down to a single switch, known as a \"switch on a chip.\"\nAs data networks were developed, switch fabrics took on many variations and evolutions as technology competitors tried to introduce innovations to the market. As Ethernet has gained primary market-share for moving data around the world, the Ethernet switch has become the most popular switching machine to create networking fabrics. However, over the years, many other technologies have been developed to build switching fabrics and many of these are still used in production, including Time Division Multiplexing (TDM),[Frame](https://www.sdxcentral.com/listings/frame/)Relay, and[storage](https://www.sdxcentral.com/data-center/storage/)protocols such as Fibre Channel. All of these technologies can be used to build their own networking fabrics. Optical networking technologies such as Dense Wave Division Multiplexing (DWDM) can also be used to build networking fabrics, though some optical networking technologies are used to create networking \"rings,\" which don't necessarily resemble fabrics but can be used to connect networks using a different topology (for example, a \"metro ring\" is a common optical technology used to connect metro geographic areas).\nMany of the innovations around switching fabrics come down to how technologies can be used to mitigate potential problems in switching data in a fabric, including latency (how fast connections can be made), collisions (data packets or frames \"colliding\" with each other inside the switch), and power and heat limitations of the switch design.\n![What is Networking Fabric?](media/Networking-Fabric-image1.jpg)\n**From Physical to the Virtual**\n\nThe latest development in switch fabrics is to build them in a flexible, virtual way, using[software-defined networking (SDN)](https://www.sdxcentral.com/networking/sdn/). Using[SDN](https://www.sdxcentral.com/networking/sdn/definitions/what-the-definition-of-software-defined-networking-sdn/), physical network switches can be managed and automated using software, rather than requiring physical changes. Virtual switches are most often implemented by a hypervisor technology that can be placed on a server or a switch and allow it to be controlled by emerging[SDN](https://www.sdxcentral.com/networking/sdn/definitions/what-is-software-defined-compute/)technology.\nThis move to large, virtualized networks and[SDN](https://www.sdxcentral.com/networking/sdn/definitions/why-sdn-software-defined-networking-or-nfv-network-functions-virtualization-now/)has given \"switch fabric\" new life as technology vendors have injected the term into their marketing descriptions for creating large, virtualized switched networks, which tie together physical networking devices. In addition, large \"hyperscale\" providers such as Facebook have created new \"switch fabric architectures\" describing how all of the pieces of a data center can be meshed together to move data (see diagram above).\n<https://www.sdxcentral.com/data-center/what-is-networking-switch-fabric>\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/OSI-Layers/","title":"OSI Layers"},"frontmatter":{"draft":false},"rawBody":"# OSI Layers\n\nCreated: 2018-05-07 20:32:26 +0500\n\nModified: 2022-10-30 23:59:24 +0500\n\n---\n\nMnemonic - All people seem to need data processing\nOSI Model (Open Systems Interconnection)\n\nIt was designed to be a reference model for describing the functions of a communication system. It has been developed by ISO -- '**International Organization of Standardization**', in the year 1974.\n\n![Application Layer Sender Presentation Layer Session Layer Receiver Transport Layer Network Layer Data Link Layer Physical Layer Software Layers Heart of OSI Hardware Layers ](media/OSI-Layers-image1.png)\n\n<table>\n<colgroup>\n<col style=\"width: 8%\" />\n<col style=\"width: 16%\" />\n<col style=\"width: 21%\" />\n<col style=\"width: 53%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th></th>\n<th><strong>OSI Model</strong></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><strong>Layer</strong></td>\n<td></td>\n<td><strong><a href=\"https://en.wikipedia.org/wiki/Protocol_data_unit\">Protocol data unit</a>(PDU)</strong></td>\n<td><strong>Function</strong><a href=\"https://en.wikipedia.org/wiki/OSI_model#cite_note-3\"></a></td>\n</tr>\n<tr class=\"even\">\n<td><p><strong>Host</strong></p>\n<p><strong>layers</strong></p></td>\n<td>7.<a href=\"https://en.wikipedia.org/wiki/Application_layer\">Application</a></td>\n<td><a href=\"https://en.wikipedia.org/wiki/Data_(computing)\">Data</a></td>\n<td>High-level<a href=\"https://en.wikipedia.org/wiki/API\">APIs</a>, including resource sharing, remote file access</td>\n</tr>\n<tr class=\"odd\">\n<td></td>\n<td>6.<a href=\"https://en.wikipedia.org/wiki/Presentation_layer\">Presentation</a></td>\n<td></td>\n<td>Translation of data between a networking service and an application; including<a href=\"https://en.wikipedia.org/wiki/Character_encoding\">character encoding</a>,<a href=\"https://en.wikipedia.org/wiki/Data_compression\">data compression</a>and<a href=\"https://en.wikipedia.org/wiki/Encryption\">encryption/decryption</a></td>\n</tr>\n<tr class=\"even\">\n<td></td>\n<td>5.<a href=\"https://en.wikipedia.org/wiki/Session_layer\">Session</a></td>\n<td></td>\n<td>Managing communication<a href=\"https://en.wikipedia.org/wiki/Session_(computer_science)\">sessions</a>, i.e. continuous exchange of information in the form of multiple back-and-forth transmissions between two nodes</td>\n</tr>\n<tr class=\"odd\">\n<td></td>\n<td>4.<a href=\"https://en.wikipedia.org/wiki/Transport_layer\">Transport</a></td>\n<td><a href=\"https://en.wikipedia.org/wiki/Packet_segmentation\">Segment</a>,<a href=\"https://en.wikipedia.org/wiki/Datagram\">Datagram</a></td>\n<td>Reliable transmission of data segments between points on a network, including<a href=\"https://en.wikipedia.org/wiki/Packet_segmentation\">segmentation</a>,<a href=\"https://en.wikipedia.org/wiki/Acknowledgement_(data_networks)\">acknowledgement</a>and<a href=\"https://en.wikipedia.org/wiki/Multiplexing\">multiplexing</a></td>\n</tr>\n<tr class=\"even\">\n<td><p><strong>Media</strong></p>\n<p><strong>layers</strong></p></td>\n<td>3.<a href=\"https://en.wikipedia.org/wiki/Network_layer\">Network</a></td>\n<td><a href=\"https://en.wikipedia.org/wiki/Network_packet\">Packet</a></td>\n<td>Structuring and managing a multi-node network, including<a href=\"https://en.wikipedia.org/wiki/Address_space\">addressing</a>,<a href=\"https://en.wikipedia.org/wiki/Routing\">routing</a>and<a href=\"https://en.wikipedia.org/wiki/Network_traffic_control\">traffic control</a></td>\n</tr>\n<tr class=\"odd\">\n<td></td>\n<td>2.<a href=\"https://en.wikipedia.org/wiki/Data_link_layer\">Data link</a></td>\n<td><a href=\"https://en.wikipedia.org/wiki/Frame_(networking)\">Frame</a></td>\n<td>Reliable transmission of data frames between two nodes connected by a physical layer</td>\n</tr>\n<tr class=\"even\">\n<td></td>\n<td>1.<a href=\"https://en.wikipedia.org/wiki/Physical_layer\">Physical</a></td>\n<td><a href=\"https://en.wikipedia.org/wiki/Bit\">Bit</a></td>\n<td>Transmission and reception of raw bit streams over a physical medium</td>\n</tr>\n</tbody>\n</table>\n1.  **Physical Layer (Layer 1)**\n\nThe lowest layer of the OSI reference model is the physical layer. It is responsible for the actual physical connection between the devices. The physical layer contains information in the form ofbits. When receiving data, this layer will get the signal received and convert it into 0s and 1s and send them to the Data Link layer, which will put the frame back together.\n\n![1100 0111 0011 ](media/OSI-Layers-image2.png)\n\nThe functions of the physical layer are :\n\ni.  Bit synchronization:The physical layer provides the synchronization of the bits by providing a clock. This clock controls both sender and receiver thus providing synchronization at bit level.\n\nii. Bit rate control:The Physical layer also defines the transmission rate i.e. the number of bits sent per second.\n\niii. Physical topologies:Physical layer specifies the way in which the different, devices/nodes are arranged in a network i.e. bus, star or mesh topolgy.\n\niv. Transmission mode:Physical layer also defines the way in which the data flows between the two connected devices. The various transmission modes possible are: Simplex, half-duplex and full-duplex.\n\n* Hub, Repeater, Modem, Cables are Physical Layer devices.\n\n** Network Layer, Data Link Layer and Physical Layer are also known asLower LayersorHardware Layers.\n2.  **Data Link Layer (DLL) (Layer 2)**\n\nThe data link layer is responsible for the node to node delivery of the message. The main function of this layer is to make sure data transfer is error free from one node to another, over the physical layer. When a packet arrives in a network, it is the responsibility of DLL to transmit it to the Host using its MAC address.\nData Link Layer is divided into two sub layers :\n\ni.  Logical Link Control (LLC)\n\nii. Media Access Control (MAC)\nMAC determines how devices in a network gain access to a medium and permission to transmit data. LLC identifies and encapsulates network layer protocols and controls error checking and frame synchronization.\nPacket received from Network layer is further divided into frames depending on the frame size of NIC(Network Interface Card). DLL also encapsulates Sender and Receiver's MAC address in the header.\nThe Receiver's MAC address is obtained by placing an ARP(Address Resolution Protocol) request onto the wire asking \"Who has that IP address?\" and the destination host will reply with its MAC address.\nThe functions of the data Link layer are :\n\ni.  Framing:Framing is a function of the data link layer. It provides a way for a sender to transmit a set of bits that are meaningful to the receiver. This can be accomplished by attaching special bit patterns to the beginning and end of the frame.\n\nii. Physical addressing:After creating frames, Data link layer adds physical addresses (MAC address) of sender and/or receiver in the header of each frame.\n\niii. Error control:Data link layer provides the mechanism of error control in which it detects and retransmits damaged or lost frames.\n\niv. Flow Control:The data rate must be constant on both sides else the data may get corrupted thus , flow control coordinates that amount of data that can be sent before receiving acknowledgement.\n\nv.  Access control:When a single communication channel is shared by multiple devices, MAC sub-layer of data link layer helps to determine which device has control over the channel at a given time.\n* Packet in Data Link layer is referred asFrame.\n\n** Data Link layer is handled by the NIC (Network Interface Card) and device drivers of host machines.\n\n*** Switch & Bridge are Data Link Layer devices.\n3.  **Network Layer (Layer 3)**\n\nNetwork layer works for the transmission of data from one host to the other located in different networks. It also takes care of packet routing i.e. selection of shortest path to transmit the packet, from the number of routes available. **The sender & receiver's IP address are placed in the header by network layer.**\n\nThe functions of the Network layer are :\n\ni.  Routing:The network layer protocols determine which route is suitable from source to destination. This function of network layer is known as routing.\n\nii. Logical Addressing:In order to identify each device on internetwork uniquely, network layer defines an addressing scheme. The sender & receiver's IP address are placed in the header by network layer. Such an address distinguishes each device uniquely and universally.\nIf a packet is too large to be transmitted, it can be split into several fragments which are shipped out and then reassembled on the receiving end. Layer 3 also contains network firewalls and 3-layer switches.\n* Segmentin Network layer is referred asPacket.\n\n** Network layer is implemented by networking devices such as routers.\n4.  **Transport Layer (Layer 4)**\n\nTransport layer provides services to application layer and takes services from network layer. The data in the transport layer is referred to asSegments. It is responsible for the End to End delivery of the complete message. Transport layer also provides the acknowledgement of the successful data transmission and re-transmits the data if error is found.\n\nâ€¢ At sender's side:\n\nTransport layer receives the formatted data from the upper layers, performsSegmentationand also implementsFlow & Error controlto ensure proper data transmission. It also adds Source and Destination port number in its header and forwards the segmented data to the Network Layer.\n\nNote:The sender need to know the port number associated with the receiver's application.\n\nGenerally this destination port number is configured, either by default or manually. For example, when a web application makes a request to a web server, it typically uses port number 80, because this is the default port assigned to web applications. Many applications have default port assigned.\n\nâ€¢ At receiver's side:\n\nTransport Layer reads the port number from its header and forwards the Data which it has received to the respective application. It also performs sequencing and reassembling of the segmented data.\nThe functions of the transport layer are :\n\ni.  Segmentation and Reassembly:This layer accepts the message from the (session) layer , breaks the message into smaller units . Each of the segment produced has a header associated with it. The transport layer at the destination station reassembles the message.\n\nii. Service Point Addressing:In order to deliver the message to correct process, transport layer header includes a type of address called service point address or port address. Thus by specifying this address, transport layer makes sure that the message is delivered to the correct process.\n\nThe services provided by transport layer :\n\niii. Connection Oriented Service:It is a three phase process which include\n     -- Connection Establishment\n     -- Data Transfer\n     -- Termination / disconnection\n     In this type of transmission the receiving device sends an acknowledgment, back to the source after a packet or group of packet is received. This type of transmission is reliable and secure.\n\niv. Connection less service:It is a one phase process and includes Data Transfer. In this type of transmission the receiver does not acknowledge receipt of a packet. This approach allows for much faster communication between devices. Connection oriented Service is more reliable than connection less Service.\n\n* Data in the Transport Layer is called asSegments.\n\n** Transport layer is operated by the Operating System. It is a part of the OS and communicates with the Application Layer by making system calls.\n\nTransport Layer is called asHeart of OSImodel.\n5.  **Session Layer (Layer 5)**\n\nThis layer is responsible for establishment of connection, maintenance of sessions, authentication and also ensures security.\n\nThe functions of the session layer are :\n\ni.  Session establishment, maintenance and termination:The layer allows the two processes to establish, use and terminate a connection.\n\nii. Synchronization :This layer allows a process to add checkpoints which are considered as synchronization points into the data. These synchronization point help to identify the error so that the data is re-synchronized properly, and ends of the messages are not cut prematurely and data loss is avoided.\n\niii. Dialog Controller :The session layer determines which device will communicate first and the amount of data that will be sent.\n\n**All the above 3 layers are integrated as a single layer in TCP/IP model as \"Application Layer\".\n\n**Implementation of above 3 layers is done by the network application itself. These are also known asUpper LayersorSoftware Layers.\n\nSCENARIO:\n\nLet's consider a scenario where a user wants to send a message through some Messenger application running in his browser. The \"Messenger\" here acts as the application layer which provides the user with an interface to create the data. This message or so called Data is compressed, encrypted (if any secure data) and converted into bits (0's and 1's) so that it can be transmitted.\n\n![Sender Message Receiver ](media/OSI-Layers-image3.png)\n6.  **Presentation Layer (Layer 6)**\n\nPresentation layer is also called theTranslation layer.The data from the application layer is extracted here and manipulated as per the required format to transmit over the network.\n\nThe functions of the presentation layer are :\n\ni.  Translation :For example, ASCII to EBCDIC.\n\nii. Encryption/ Decryption :Data encryption translates the data into another form or code. The encrypted data is known as the cipher text and the decrypted data is known as plain text. A key value is used for encrypting as well as decrypting data.\n\niii. Compression:Reduces the number of bits that need to be transmitted on the network.\n7.  **Application Layer (Layer 7)**\n\nAt the very top of the OSI Reference Model stack of layers, we find Application layer which is implemented by the network applications. These applications produce the data, which has to be transferred over the network. This layer also serves as window for the application services to access the network and for displaying the received information to the user.\n\nEx: Application -- Browsers, Skype Messenger etc.\n\n**Application Layer is also called as Desktop Layer.\n\n![](media/OSI-Layers-image4.png)\n\nThe functions of the Application layer are :\n\ni.  Network Virtual Terminal\n\nii. FTAM-File transfer access and management\n\niii. Mail Services\n\niv. Directory Services\nOSI model acts as a reference model and is not implemented in Internet because of its late invention. Current model being used is the TCP/IP model.\n**References**\n\n<https://www.digitalocean.com/community/tutorials/an-introduction-to-networking-terminology-interfaces-and-protocols>\n\n<https://www.freecodecamp.org/news/osi-model-networking-layers-explained-in-plain-english>\n\n<https://www.freecodecamp.org/news/osi-model-computer-networking-for-beginners>\n\n<https://www.freecodecamp.org/news/the-five-layers-model-explained>\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2018-03-27 01:07:52 +0500\n\nModified: 2022-01-27 18:29:02 +0500\n\n---\n\n**Interface Definition Languages**\n-   Apache Thrift (donated by Facebook)\n-   Google Protocol Buffer (ProtoBuf)\n-   AIDL (Android Interface Definition Language)\n**Spine Switches**\n\nIn recent years, an architecture known as leaf and spine, or distributed core, has emerged as a leading design for data centers. This design requires spine specially designed[spine switches](https://www.sdxcentral.com/data-center/what-is-networking-switch-fabric/)in order to work.\nAll spine switches can handle Layer 3 (L3) with high port density, which allows for scalability. In a[software-defined network](https://www.sdxcentral.com/networking/sdn/)([SDN](https://www.sdxcentral.com/networking/sdn/definitions/what-the-definition-of-software-defined-networking-sdn/)), the spine switch is directly connected to a network control system with a virtual Layer 2 switch on top of the leaf-spine system. This controller is not a part of the data path but remains the central network engine, also known as an[SDN Controller](https://www.sdxcentral.com/networking/sdn/definitions/sdn-controllers/).\n\n![Spine Switches Diagram](media/Others-image1.jpg)\n![](media/Others-image2.jpg)\n<https://www.sdxcentral.com/data-center/definitions/what-are-spine-switches>\n\n## NAS**\n\nNetwork-attached storage(NAS) is a file-level (as opposed to[block-level](https://en.wikipedia.org/wiki/Block_device))[computer data storage](https://en.wikipedia.org/wiki/Computer_data_storage)server connected to a[computer network](https://en.wikipedia.org/wiki/Computer_network)providing data access to a[heterogeneous](https://en.wikipedia.org/wiki/Heterogeneous_computing)group of clients. NAS is specialized for[serving files](https://en.wikipedia.org/wiki/File_server)either by its hardware, software, or configuration. It is often manufactured as a[computer appliance](https://en.wikipedia.org/wiki/Computer_appliance)-- a purpose-built specialized computer.[[nb 1]](https://en.wikipedia.org/wiki/Network-attached_storage#cite_note-1)NAS systems are networked appliances which contain one or more[storage drives](https://en.wikipedia.org/wiki/Hard_disk_drive), often arranged into logical, redundant storage containers or[RAID](https://en.wikipedia.org/wiki/RAID). Network-attached storage removes the responsibility of file serving from other servers on the network. They typically provide access to files using network file sharing protocols such as[NFS](https://en.wikipedia.org/wiki/Network_File_System_(protocol)),[SMB](https://en.wikipedia.org/wiki/Server_Message_Block), or[AFP](https://en.wikipedia.org/wiki/Apple_Filing_Protocol). From the mid-1990s, NAS devices began gaining popularity as a convenient method of sharing files among multiple computers. Potential benefits of dedicated network-attached storage, compared to general-purpose servers also serving files, include faster data access, easier administration, and simple configuration.\nThe hard disk drives with \"NAS\" in their name are functionally similar to other drives but may have different firmware, vibration tolerance, or power dissipation to make them more suitable for use in RAID arrays, which are often used in NAS implementations.For example, some NAS versions of drives support a command extension to allow extended error recovery to be disabled. In a non-RAID application, it may be important for a disk drive to go to great lengths to successfully read a problematic storage block, even if it takes several seconds. In an appropriately configured RAID array, a single bad block on a single drive can be recovered completely via the redundancy encoded across the RAID set. If a drive spends several seconds executing extensive retries it might cause the RAID controller to flag the drive as \"down\" whereas if it simply replied promptly that the block of data had a checksum error, the RAID controller would use the redundant data on the other drives to correct the error and continue without any problem. Such a \"NAS\" SATA hard disk drive can be used as an internal PC hard drive, without any problems or adjustments needed, as it simply supports additional options and may possibly be built to a higher quality standard (particularly if accompanied by a higher quoted[MTBF](https://en.wikipedia.org/wiki/MTBF)figure and higher price) than a regular consumer drive.\n<https://en.wikipedia.org/wiki/Network-attached_storage>\n\n## Peer to Peer Networks**\n\n<https://skerritt.blog/designing-effective-peer-to-peer-networks>\n\n## VPC / RDP**\n\nssh -i ec2_ssh_key.pem -X ubuntu@13.233.36.211\nmac - cmd+space\n\nvnc://13.233.36.211:5901\n<https://ubuntu.com/tutorials/tutorial-ubuntu-desktop-aws#1-overview>\n\n<https://datawookie.netlify.app/blog/2017/08/remote-desktop-on-an-ubuntu-ec2-instance>\n\n## NoMachine**\n\n<https://www.nomachine.com>\n\n## Windows remote desktop (RDP)**\n\n**Multi user access**\n\nThis will launch theGroup Policy Editor(gpedit.msc), which is a management console through which you can configure many Windows system properties or run scripts.\nOnce the Group Policy Editor is running, navigate to:\n\nComputer Configuration > Administrative Templates > Windows Components > Remote Desktop Services > Remote Desktop Session Host > Connections.\nFrom here, first set theRestrict Remote Desktop Services user to a single Remote Desktop Services sessionparameter toDisabled.\nNext, double-click onLimit number of connectionsand then set theRD Maximum Connections allowedto 999999.\n<https://www.serverwatch.com/server-tutorials/multiple-remote-desktop-connections-on-windows-server-2016-and-windows-server-2012.html>\n\n## Download - chrome using Powershell (not cmd)**\n\n$LocalTempDir = $env:TEMP; $ChromeInstaller = \"ChromeInstaller.exe\"; (new-object System.Net.WebClient).DownloadFile('http://dl.google.com/chrome/install/375.126/chrome_installer.exe', \"$LocalTempDir$ChromeInstaller\"); & \"$LocalTempDir$ChromeInstaller\" /silent /install; $Process2Monitor = \"ChromeInstaller\"; Do { $ProcessesFound = Get-Process | ?{$Process2Monitor -contains $_.Name} | Select-Object -ExpandProperty Name; If ($ProcessesFound) { \"Still running: $($ProcessesFound -join ', ')\" | Write-Host; Start-Sleep -Seconds 2 } else { rm \"$LocalTempDir$ChromeInstaller\" -ErrorAction SilentlyContinue -Verbose } } Until (!$ProcessesFound)\n**ONAP**\n\nONAP is a comprehensive platform for orchestration, management, and automation of network and edge computing services for network operators, cloud providers, and enterprises. Real-time, policy-driven orchestration and automation of physical and virtual network functions enables rapid automation of new services and complete lifecycle management critical for 5G and next-generation networks.\n<https://www.onap.org>\n\n## IPVS**\n\nIPVS(IP Virtual Server) implements transport-layer[load balancing](https://en.wikipedia.org/wiki/Load_balancing_(computing)), usually called[Layer 4](https://en.wikipedia.org/wiki/Layer_4)[LAN switching](https://en.wikipedia.org/wiki/LAN_switching), as part of the[Linux kernel](https://en.wikipedia.org/wiki/Linux_kernel). It's configured via the user-space utility [ipvsadm(8)](https://man.cx/?page=ipvsadm(8)) tool.\nIPVS is incorporated into the[Linux Virtual Server](https://en.wikipedia.org/wiki/Linux_Virtual_Server)(LVS), where it runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single[IP address](https://en.wikipedia.org/wiki/IP_address). IPVS is built on top of the[Netfilter](https://en.wikipedia.org/wiki/Netfilter).\nIPVS is merged into versions 2.4.x and newer of the Linux kernel mainline.\nIn a nutshell, IPVS is used to expose an entrypoint service with a unique virtual IP. All TCP/UPD traffic going through thisendpoint is load-balanced between physical servers.\n<https://en.wikipedia.org/wiki/IP_Virtual_Server>\n\n## REST GraphQL Grpc**\n\n<https://www.redhat.com/architect/apis-rest-graphql-grpc>\n\n## RFID vs NFC**\n\nRFID is the process by which items are uniquely identified using radio waves, and NFC is a specialized subset within the family of RFID technology. Specifically, NFC is a branch of High-Frequency (HF) RFID, and both operate at the 13.56 MHz frequency. NFC is designed to be a secure form of data exchange, and an NFC device is capable of being both an NFC reader and an[NFC tag](https://www.atlasrfidstore.com/near-field-communication/). This unique feature allows NFC devices to communicate peer-to-peer.\nRFID tags are either[Active or Passive](https://blog.atlasrfidstore.com/active-rfid-vs-passive-rfid).\n-   **Active RFID tags** contain their own power source giving them the ability to broadcast with a read range of up to 100 meters. Their long read range makes active RFID tags ideal for many industries where asset location and other improvements in logistics are important.-   [**Passive RFID tags**](https://www.atlasrfidstore.com/rfid-tags/)do not have their own power source. Instead, they are powered by the electromagnetic energy transmitted from the RFID reader. Because the radio waves must be strong enough to power the tags, passive RFID tags have a read range from near contact and up to 25 meters.\n<https://www.atlasrfidstore.com/rfid-insider/rfid-vs-nfc>\n\n## Mobile Cell Service**\n\n[How Cell Service Actually Works](https://youtu.be/0faCad2kKeg)"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/Questions/","title":"Questions"},"frontmatter":{"draft":false},"rawBody":"# Questions\n\nCreated: 2019-10-15 20:09:14 +0500\n\nModified: 2020-01-03 19:27:43 +0500\n\n---\n\n1.  Where would you use exact-match packet forwarding? Where would you use longest-prefix? Whyis 2-choice better than standard hashing?\n\n2.  What are the different types of queueing structures in a router? What are the pros and cons of each? Explain the HoL blocking problem in input-queued routers.\n\n3.  Describe the ALOHA protocol. Derive the optimal transmission probability in ALOHA. What is the overall utilization of the medium with this probability?\n\n4.  How are the operating conditions of WiFi different from ALOHA? How is WiFi different fromEthernet?\n\n5.  What is modulation? What is the signal-to-noise ratio? What is a checksum?\n\n6.  Explain the problem of wireless bit-rate adaptation. Explain the SampleRate algorithm.\n\n7.  What are some performance metrics you might care about during video streaming? Give an example of a design that optimizes for each metric in isolation\n\n8.  What distinguishes a peer-to-peer application from a client-server application? Compare and contrast the networking characteristics of Bitcoin and BitTorrent.\n\n9.  How many hosts can a leaf-spine topology made up of k switches with k ports each support? Describe the load balancing algorithm between the leaf and spine switches.\n\n10. Describe some security and privacy concerns with IoT things. What is a data mule? Is it more efficient to send 100 PB of data across the country using a truck or using the Internet? Explain why.\n\n11. Describe the two core concepts behind software-defined networking. Explain what policy routing is.\n\n12. What are the three properties that a correct implementation of TLS guarantees? What is one property it explicitly does not guarantee? Explain the difference between public-private key encryption/decryption and symmetric key encryption/decryption.\n\n13. What is the main difference between the security adversary in network surveillance and censorship and the security in standard discussions of security protocols? Explain a few ways in which a network censor can censor access to the Internet for its customers.\n**References**\n\n<https://cs.nyu.edu/courses/fall17/CSCI-UA.0480-009>\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/Routing/","title":"Routing"},"frontmatter":{"draft":false},"rawBody":"# Routing\n\nCreated: 2018-03-11 07:33:26 +0500\n\nModified: 2022-01-30 23:29:24 +0500\n\n---\n\nIP addresses are classified into several classes of operational characteristics: unicast, multicast, anycast and broadcast addressing.\n-   **Unicast addressing**\n\nThe most common concept of an IP address is in[unicast](https://en.wikipedia.org/wiki/Unicast)addressing, available in both IPv4 and IPv6. It normally refers to a single sender or a single receiver, and can be used for both sending and receiving. Usually, a unicast address is associated with a single device or host, but a device or host may have more than one unicast address. Sending the same data to multiple unicast addresses requires the sender to send all the data many times over, once for each recipient.-   **Broadcast addressing**\n\n[Broadcasting](https://en.wikipedia.org/wiki/Broadcasting_(computing))is an addressing technique available in IPv4 to address data to all possible destinations on a network in one transmission operation as anall-hosts broadcast. All receivers capture the network packet. The address255.255.255.255is used for network broadcast. In addition, a more limited directed broadcast uses the all-ones host address with the network prefix. For example, the destination address used for directed broadcast to devices on the network192.0.2.0/24is192.0.2.255.\nIPv6 does not implement broadcast addressing, and replaces it with multicast to the specially defined all-nodes multicast address.-   **Multicast addressing**\n\nA[multicast address](https://en.wikipedia.org/wiki/Multicast_address)is associated with a group of interested receivers. In IPv4, addresses224.0.0.0through239.255.255.255(the former[Class D](https://en.wikipedia.org/wiki/Classful_network)addresses) are designated as multicast addresses.[[21]](https://en.wikipedia.org/wiki/IP_address#cite_note-rfc5771-21)IPv6 uses the address block with the prefixff00::/8for multicast. In either case, the sender sends a single datagram from its unicast address to the multicast group address and the intermediary routers take care of making copies and sending them to all interested receivers (those that have joined the corresponding multicast group).-   **Anycast addressing**\n\nLike broadcast and multicast,[anycast](https://en.wikipedia.org/wiki/Anycast)is a one-to-many routing topology. However, the data stream is not transmitted to all receivers, just the one which the router decides is closest in the network. Anycast addressing is an built-in feature of IPv6.In IPv4, anycast addressing is implemented with[Border Gateway Protocol](https://en.wikipedia.org/wiki/Border_Gateway_Protocol)using the shortest-path[metric](https://en.wikipedia.org/wiki/Metrics_(networking))to choose destinations. Anycast methods are useful for global load balancing and are commonly used in distributed[DNS](https://en.wikipedia.org/wiki/Domain_name_system)systems.\n<https://en.wikipedia.org/wiki/IP_address>\n\n## Routing Table**\n\nIn[computer networking](https://en.wikipedia.org/wiki/Computer_networking)a**routing table, orrouting information base (RIB)**, is a[data table](https://en.wikipedia.org/wiki/Data_table)stored in a[router](https://en.wikipedia.org/wiki/Router_(computing))or a networked[computer](https://en.wikipedia.org/wiki/Computer)that lists the routes to particular network destinations, and in some cases,[metrics](https://en.wikipedia.org/wiki/Metrics_(networking))(distances) associated with those routes. The routing table contains information about the[topology of the network](https://en.wikipedia.org/wiki/Network_topology)immediately around it. The construction of routing tables is the primary goal of[routing protocols](https://en.wikipedia.org/wiki/Routing_protocol).[Static routes](https://en.wikipedia.org/wiki/Static_route)are entries made in a routing table by non-automatic means and which are fixed rather than being the result of some network topology \"discovery\" procedure.\nThe routing table consists of at least three information fields:\n\n1.  network ID: The destination subnet\n\n2.  metric: The[routing metric](https://en.wikipedia.org/wiki/Routing_metric)of the path through which the packet is to be sent. The route will go in the direction of the gateway with the lowest metric.\n\n3.  next hop: The next hop, or gateway, is the address of the next station to which the packet is to be sent on the way to its final destination\nDepending on the application and implementation, it can also contain additional values that refine path selection:\n\n1.  quality of serviceassociated with the route. For example, the U flag indicates that an IP route is up.\n\n2.  links to filtering criteria/access lists associated with the route\n\n3.  interface: such as eth0 for the first Ethernet card, eth1 for the second Ethernet card, etc.\nRouting tables are also a key aspect of certain security operations, such as[unicast reverse path forwarding](https://en.wikipedia.org/wiki/Unicast_reverse_path_forwarding) (uRPF). In this technique, which has several variants, the router also looks up, in the routing table, thesource addressof the packet. If there exists no route back to the source address, the packet is assumed to be malformed or involved in a network attack, and is dropped.\n<https://en.wikipedia.org/wiki/Routing_table>\n\n## Forwarding Table**\n\nRouting tables are generally not used directly for[packet forwarding](https://en.wikipedia.org/wiki/Packet_forwarding)in modern router architectures; instead, they are used to generate the information for a smaller[forwarding table](https://en.wikipedia.org/wiki/Forwarding_table). This forwarding table contains only the routes which are chosen by the[routing algorithm](https://en.wikipedia.org/wiki/Routing_algorithm)as preferred routes for packet forwarding. It is often in a compressed or pre-compiled format that is[optimized](https://en.wikipedia.org/wiki/Optimisation_(computer_science))for hardware storage and[lookup](https://en.wikipedia.org/wiki/Lookup).\n\nThis router architecture separates the[Control Plane](https://en.wikipedia.org/wiki/Control_Plane)function of the routing table from the[Forwarding Plane](https://en.wikipedia.org/wiki/Forwarding_Plane)function of the forwarding table.[[3]](https://en.wikipedia.org/wiki/Routing_table#cite_note-3)This separation of control and forwarding provides uninterrupted performance.\n**Routing Protocols**\n\nOSPF - Open Shortest Path First\n\nRouting protocol for IP networks.\n\nIt uses link state routing (LSR) algorithm and falls into the group of interior gateway protocols (IGPs), operating within a single autonomous system (AS)\n\nBGP - Border Gateway Protocol\n\nRIP - Routing Information Protocol\n\nIS-IS - Intermediate System to Intermediate System\nDistance vector routing - thread (similar to RIP)\n\nMaintain and advertise best next hop towards each Thread Router\n**Tools**\n\n[Quagga](http://www.quagga.net/)is a routing software suite, providing implementations of OSPFv2, OSPFv3, RIP v1 and v2, RIPng and BGP-4 for Unix platforms, particularly FreeBSD, Linux, Solaris and NetBSD. Quagga is a fork of[GNU Zebra](http://www.zebra.org/)which was developed by Kunihiro Ishiguro.\nThe Quagga architecture consists of a core daemon,zebra, which acts as an abstraction layer to the underlying Unix kernel and presents the Zserv API over a Unix or TCP stream to Quagga clients. It is these Zserv clients which typically implement a routing protocol and communicate routing updates to the zebra daemon. Existing Zserv implementations are:\n\n| IPv4  | IPv6   |                                                                    |\n|----------|----------|-----------------------------------------------------|\n| zebra |       | - kernel interface, static routes, zserv server                    |\n| ripd  | ripngd | - RIPv1/RIPv2 for IPv4 and RIPng for IPv6                          |\n| ospfd | ospf6d | - OSPFv2 and OSPFv3                                                |\n| bgpd  |       | - BGPv4+ (including address family support for multicast and IPv6) |\n| isisd |       | - IS-IS with support for IPv4 and IPv6                             |\nQuagga daemons are each configurable via a network accessible CLI (called a 'vty'). The CLI follows a style similar to that of other routing software. There is an additional tool included with Quagga called 'vtysh', which acts as a single cohesive front-end to all the daemons, allowing one to administer nearly all aspects of the various Quagga daemons in one place.\n<https://www.quagga.net>\n\n## Administrative Distance**\n\nAdministrative distance (AD)orroute preferenceis a number of[arbitrary unit](https://en.wikipedia.org/wiki/Arbitrary_unit)assigned to[dynamic routes](https://en.wikipedia.org/wiki/Dynamic_route),[static routes](https://en.wikipedia.org/wiki/Static_route)and directly-connected routes. The value is used in[routers](https://en.wikipedia.org/wiki/Router_(computing))to rank routes from most preferred (low administrative distance value) to least preferred (high administrative distance value).When multiple paths to the same destination are available in its[routing table](https://en.wikipedia.org/wiki/Routing_table), the router uses the route with the lowest administrative distance.\nRouter vendors typically design their routers to assign a default administrative distance to each kind of route. For example Cisco routers, routes issued by[OSPF](https://en.wikipedia.org/wiki/OSPF)have a lower default administrative distance than routes issued by the[Routing Information Protocol](https://en.wikipedia.org/wiki/Routing_Information_Protocol). By default, OSPF has a default administrative distance of 110 and RIP has a default administrative distance of 120. Administrative distance values can, however, usually be adjusted manually by a[network administrator](https://en.wikipedia.org/wiki/Network_administrator).\n<https://en.wikipedia.org/wiki/Administrative_distance>\n\n## Virtual Routing and Forwarding (VRF)**\n\nIn[IP-based](https://en.wikipedia.org/wiki/Internet_Protocol)[computer networks](https://en.wikipedia.org/wiki/Computer_network),virtual routing and forwarding(VRF) is a technology that allows multiple instances of a[routing table](https://en.wikipedia.org/wiki/Routing_table)to co-exist within the same router at the same time. One or more logical or physical interfaces may have a VRF and these VRFs do not share routes therefore the packets are only forwarded between interfaces on the same VRF. VRFs are the[TCP/IP](https://en.wikipedia.org/wiki/Internet_Protocol)layer 3 equivalent of a[VLAN](https://en.wikipedia.org/wiki/VLAN). Because the routing instances are independent, the same or overlapping[IP addresses](https://en.wikipedia.org/wiki/IP_address)can be used without conflicting with each other. Network functionality is improved because network paths can be segmented without requiring multiple routers.\n<https://en.wikipedia.org/wiki/Virtual_routing_and_forwarding>\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/Sockets/","title":"Sockets"},"frontmatter":{"draft":false},"rawBody":"# Sockets\n\nCreated: 2019-11-10 13:18:21 +0500\n\nModified: 2019-11-10 13:46:48 +0500\n\n---\n\n**Socket**\n\nThe first function is socket(), which creates an object called a socket. A socket is a number that a program can use to communicate with another program. In UNIX terms, it is no different from a file descriptor, which is a number that is used for reading or writing from an open file. Instead, with a socket, a program is reading (receiving) or writing (sending) from or to the network.\nnc -l 8000 # for tcp listen\n\nnc -l 8000 -u\n\nThis just tells nc to create a UDP receiver (the argument -u) that is expecting data on port 8000. Now, let's use sock object to send data to nc.\n**UDP socket server in python**\n\n```\nfrom socket import *\n\nsock_receiver = socket(AF_INET, SOCK_DGRAM)\n\nsock_receiver.bind((\"127.0.0.1\", 8000))\n\nsock_receiver.recv(4096)\n```\n\n**UDP socket client in python**\n```\nfrom socket import *\n\nsock_object=socket(AF_INET, SOCK_DGRAM)\n\nsock_object.sendto(b\"hello\", (\"127.0.0.1\", 8000))\n```\n\n**TCP socket**\n\ntcp_socket = socket(AF_INET, SOCK_STREAM)\n<https://cs.nyu.edu/courses/fall17/CSCI-UA.0480-009>\n"},{"fields":{"slug":"/Computer-Science/Networking/Networking-Concepts/TCP-IP/","title":"TCP/IP"},"frontmatter":{"draft":false},"rawBody":"# TCP/IP\n\nCreated: 2018-05-07 20:32:29 +0500\n\nModified: 2022-03-10 12:07:35 +0500\n\n---\n\nTheOSI Modelwe just looked at is just a reference/logical model. It was designed to describe the functions of the communication system by dividing the communication procedure into smaller and simpler components. But when we talk about the TCP/IP model, it was designed and developed by Department of Defense (DoD) in 1960s and is based on standard protocols. It stands for Transmission Control Protocol/Internet Protocol. TheTCP/IP modelis a concise version of the OSI model. It contains four layers, unlike seven layers in the OSI model. The layers are:\n\n1.  Process/Application Layer\n\n2.  Host-to-Host/Transport Layer\n\n3.  Internet Layer\n\n4.  Network Access/Link Layer\n**TCP is built on top of IP. TCP is obliged to somehow send data reliably using only an unreliable tool (IP).**\nThe diagrammatic comparison of the TCP/IP and OSI model is as follows\n\n![TCP/IP MODEL Application Layer Transport Layer Internet Layer Network Access Layer MODEL Application Layer Presentation Layer Session Layer Transport Layer Network Layer Data Link Layer Physical Layer ](media/TCP-IP-image1.png)\n\n**1. Network Access Layer / Link layer**\n\nThis layer corresponds to the combination of Data Link Layer and Physical Layer of the OSI model. It looks out for hardware addressing and the protocols present in this layer allows for physical transmission of data.\nWe just talked about ARP being a protocol of Internet layer, but there is a conflict about declaring it as a protocol of Internet Layer or Network access layer. It is described as residing in layer 3, being encapsulated by layer 2 protocols.\n\na.  The**Multiple Spanning Tree Protocol**(**MSTP**) and[algorithm](https://en.wikipedia.org/wiki/Algorithm), provides both simple and full connectivity assigned to any given[Virtual LAN](https://en.wikipedia.org/wiki/Virtual_LAN)(VLAN) throughout a Bridged Local Area Network. MSTP uses[BPDUs](https://en.wikipedia.org/wiki/Bridge_Protocol_Data_Unit)to exchange information between spanning-tree compatible devices, to prevent loops in each[MSTI](https://en.wikipedia.org/wiki/Multiple_Spanning_Tree_Protocol#Multiple_Spanning_Tree_Instances_(MSTI))(Multiple Spanning Tree Instances) and in the[CIST](https://en.wikipedia.org/wiki/Multiple_Spanning_Tree_Protocol#Common_and_Internal_Spanning_Tree(CST/CIST))(Common and Internal Spanning Tree), by selecting active and blocked paths. This is done as well as in[STP](https://en.wikipedia.org/wiki/Spanning_Tree_Protocol)without the need of manually enabling backup links and getting rid of[bridge](https://en.wikipedia.org/wiki/Bridging_(networking))[loops](https://en.wikipedia.org/wiki/Switching_loop)danger.\n**2. Internet Layer**\n\nThis layer parallels the functions of OSI's Network layer. It defines the protocols which are responsible for logical transmission of data over the entire network. The main protocols residing at this layer are :\n\na.  **IP (IPV4/IPV6) --**stands for Internet Protocol and it is responsible for delivering packets from the source host to the destination host by looking at the IP addresses in the packet headers. IP has 2 versions:\n    IPv4 and IPv6. IPv4 is the one that most of the websites are using currently. But IPv6 is growing as the number of IPv4 addresses are limited in number when compared to the number of users.\n\nb.  **ICMP --**stands for Internet Control Message Protocol. It is encapsulated within IP datagrams and is responsible for providing hosts with information about network problems.\n\nc.  **ARP --**stands for Address Resolution Protocol. It's job is to find the hardware address of a host from a known IP address. ARP has several types: Reverse ARP, Proxy ARP, Gratituous ARP and Inverse ARP.\n**3. Host-to-Host Layer / Transport layer**\n\nThis layer is analogous to the transport layer of the OSI model. It is responsible for end-to-end communication and error-free delivery of data. It shields the upper-layer applications from the complexities of data. The two main protocols present in this layer are :\n\na.  **Transmission Control Protocol (TCP) --**It is known to provide reliable and error-free communication between end systems. It performs sequencing and segmentation of data. It also has acknowledgement feature and controls the flow of the data through flow control mechanism. It is a very effective protocol but has a lot of overhead due to such features. Increased overhead leads to increased cost.\n\nb.  User Datagram Protocol (UDP) --On the other hand does not provide any such features. It is the go to protocol if your application does not require reliable transport as it is very cost-effective. Unlike TCP, which is connection-oriented protocol, UDP is connectionless.\n**4. Process Layer / Application layer**\n\nThis layer performs the functions of top three layers of the OSI model: Application, Presentation and Session Layer. It is responsible for node-to-node communication and controls user-interface specifications. Some of the protocols present in this layer are : HTTP, HTTPS, FTP, TFTP, Telnet, SSH, SMTP, SNMP, NTP, DNS, DHCP, NFS, X Window, LPD.\n\na.  **HTTP and HTTPS --**HTTP stands for Hyper-text transfer protocol. It is used by the World Wide Web to manage communications between web browsers and servers. HTTPS stands for HTTP-Secure. It is a combination of HTTP with SSL(Secure Socket Layer). It is efficient in cases where the browser need to fill out forms, sign in, authenticate and carry out bank transactions.\n\nb.  **SSH --**SSH stands for Secure Shell. It is a terminal emulations software similar to Telnet. The reason SSH is more preferred is because of its ability to maintain encrypted connection. It sets up a secure session over a TCP/IP connection.\n\nc.  **NTP --**NTP stands for Network Time Protocol. It is used to synchronize the clocks on our computer to one standard time source. It is very useful in situations like bank transactions. Assume the following situation without the presence of NTP. Suppose you carry out a transaction,where your computer reads the time at 2:30 PM while the server records it at 2:28 PM. The server can crash very badly if it's out of sync.\n\n![](media/TCP-IP-image2.png)\n\n**Is UDP port 80 the same as TCP port 80?**\n-   UDP and TCP both support the same port numbers (1-65535) but they're different protocols. You can run 2 different servers on UDP port 80 and TCP port 80 at the same time.\n<https://blog.cloudflare.com/unimog-cloudflares-edge-load-balancer>"},{"fields":{"slug":"/Computer-Science/Networking/Others/5G-Wireless-Networking/","title":"5G Wireless Networking"},"frontmatter":{"draft":false},"rawBody":"# 5G Wireless Networking\n\nCreated: 2019-04-07 08:35:22 +0500\n\nModified: 2019-04-07 08:48:53 +0500\n\n---\n\n**Wireless Network Design**\n-   Rethinking cellular system design\n-   Software-defined wireless networking\n**PHY/MAC Techniques**\n-   Utilizing more spectrum (mmWave/THz)\n-   (Massive) MIMO\n-   New modulation, coding, and detection\n-   New MAC strategies\n**ML in Wireless Systems**\n\n**We have shown that ML \"trumps theory\":**\n-   In equalization of unknown/complex channels\n-   In joint source and channel coding of text\n**Application of ML to wireless system design**\n-   Detection in unknown channels (molecular, mmW, nonlinear)\n-   Modulation and detection\n-   Encoding and decoding\n-   MIMO transmission and reception\n-   Joint source and channel encoding/decoding\n-   Network resource allocation\n**ML algorithm and training optimization needed**\n-   That is where comm/network theory come in\n**Rethinking Cellular System Design**\n\n**Cellular systems reuse channels/timeslots in different cells**\n\nTraditional design assumes system is \"interference-limited\"\n\nCapacity unknown; upper bound based on BC/MAC with pooled antennas\n**No longer the case with recent technology advances:**\n\nMIMO, multiuser detection, cooperating BSs (CoMP) and relays\n\nRaises interesting questions such as \"what is a cell?\"\n**Dynamic self-organizing networking (SoN) needed for optimization**\n**Small cells are the solution to increasing cellular system capacity**\n-   Cellular networks are increasingly hierarchical\n-   Large cells for coverage\n-   Small cells for capacity and power efficiency\n-   Cell resource optimization is best done in the cloud\n**Software Defined Wireless Networking**\n**Drastic energy reduction needed for IoT devices**\n\n1.  New Infrastuctures: cell size, BS placement, DAS, Picos, relays\n\n2.  New Protocols: Cell Zooming, Coop MIMO, RRM, Scheduling, Sleeping, Relaying\n\n3.  Low-Power (Green) Radios: Radio Architectures, Modulation, coding, MIMO\n**Where should energy come from?**\n\n**Batteries and traditional charging mechanisms**\n\nWell-understood devices and systems\n**Wireless-power transfer**\n\nPoorly understood, especially at large distances and with high efficiency\n**Communication with Energy Harvesting Devices**\n\nIntermittent and random energy arrivals\n\nCommunication becomes energy-dependent\n\nCan combine information and energy transmission\n\nNew principles for communication system design needed\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/Apache-Avro/","title":"Apache Avro"},"frontmatter":{"draft":false},"rawBody":"# Apache Avro\n\nCreated: 2018-09-08 15:53:32 +0500\n\nModified: 2020-05-04 03:26:03 +0500\n\n---\n\n*Apache Avro is a **language-neutral fast data serialization system**.* It was developed by Doug Cutting, the father of Hadoop. Since Hadoop writable classes lack language portability, Avro becomes quite helpful, as it deals with data formats that can be processed by multiple languages. Avro is a preferred tool to serialize data in Hadoop.\nAvro has a schema-based system. A language-independent schema is associated with its read and write operations. Avro serializes the data which has a built-in schema. Avro serializes the data into a compact binary format, which can be deserialized by any application.\nAvro uses JSON format to declare the data structures. Presently, it supports languages such as Java, C, C++, C#, Python, and Ruby.\n*One of the most interesting features of Avro, and what makes it a good fit for use in a messaging system like Kafka, is that when the application that is writing messages switches to a new schema, the application reading the data can continue processing messages without requiring any change or update.*\nAvro provides:\n-   Rich data structures.\n-   A compact, fast, binary data format.\n-   A container file, to store persistent data.\n-   Remote procedure call (RPC).\n-   Simple integration with dynamic languages. Code generation is not required to read or write data files nor to use or implement RPC protocols. Code generation as an optional optimization, only worth implementing for statically typed languages.\n**Avro Schemas**\n\nAvro depends heavily on itsschema. It allows every data to be written with no prior knowledge of the schema. It serializes fast and the resulting serialized data is lesser in size. Schema is stored along with the Avro data in a file for any further processing.\nIn RPC, the client and the server exchange schemas during the connection. (This can be optimized so that, for most calls, no schemas are actually transmitted.) Since both client and server both have the other's full schema, correspondence between same named fields, missing fields, extra fields, etc. can all be easily resolved.\nAvro schemas are defined with JSON that simplifies its implementation in languages with JSON libraries.\nLike Avro, there are other serialization mechanisms in Hadoop such asSequence Files, Protocol Buffers,andThrift.\n**Schemas**\n\nAvro relies onschemas. When Avro data is read, the schema used when writing it is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. This also facilitates use with dynamic, scripting languages, since data, together with its schema, is fully self-describing.\nWhen Avro data is stored in a file, its schema is stored with it, so that files may be processed later by any program. If the program reading the data expects a different schema this can be easily resolved, since both schemas are present.\nWhen Avro is used in RPC, the client and server exchange schemas in the connection handshake. (This can be optimized so that, for most calls, no schemas are actually transmitted.) Since both client and server both have the other's full schema, correspondence between same named fields, missing fields, extra fields, etc. can all be easily resolved.\nAvro schemas are defined with[JSON](https://www.json.org/). This facilitates implementation in languages that already have JSON libraries.\n**Comparison with other systems**\n\nAvro provides functionality similar to systems such as[Thrift](https://thrift.apache.org/),[Protocol Buffers](https://code.google.com/p/protobuf/), etc. Avro differs from these systems in the following fundamental aspects.\n-   **Dynamic typing:** Avro does not require that code be generated. Data is always accompanied by a schema that permits full processing of that data without code generation, static datatypes, etc. This facilitates construction of generic data-processing systems and languages.\n-   **Untagged data:** Since the schema is present when data is read, considerably less type information need be encoded with data, resulting in smaller serialization size.\n-   **No manually-assigned field IDs:** When a schema changes, both the old and new schema are always present when processing data, so differences may be resolved symbolically, using field names.\n**Features of Avro**\n-   Avro is alanguage-neutraldata serialization system.\n-   It can be processed by many languages (currently C, C++, C#, Java, Python, and Ruby).\n-   Avro creates binary structured format that is bothcompressibleandsplittable. Hence it can be efficiently used as the input to Hadoop MapReduce jobs.\n-   Avro providesrich data structures. For example, you can create a record that contains an array, an enumerated type, and a sub record. These datatypes can be created in any language, can be processed in Hadoop, and the results can be fed to a third language.\n-   Avroschemasdefined inJSON, facilitate implementation in the languages that already have JSON libraries.\n-   Avro creates a self-describing file namedAvro Data File,in which it stores data along with its schema in the metadata section.\n-   Avro is also used in Remote Procedure Calls (RPCs). During RPC, client and server exchange schemas in the connection handshake.\n-   It has a direct mapping to and from JSON\n-   It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage.\n-   It is very fast.\n-   It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream.\n-   It has a rich, extensible schema language defined in pure JSON\n-   It has the best notion of compatibility for evolving your data over time.\n**Effective Avro**\n\nHere are some recommendations specific to Avro:\n-   Use enumerated values whenever possible instead of magic strings. Avro allows specifying the set of values that can be used in the schema as an enumeration. This avoids typos in data producer code making its way into the production data set that will be recorded for all time.\n-   Require documentation for all fields. Even seemingly obvious fields often have non-obvious details. Try to get them all written down in the schema so that anyone who needs to really understand the meaning of the field need not go any further.\n-   Avoid non-trivial union types and recursive types. These are Avro features that map poorly to most other systems. Since our goal is an intermediate format that maps well to other systems we want to avoid any overly advanced features.\n-   Enforce reasonable schema and field naming conventions. Since these schemas will map into Hadoop having common fields like customer_id named the same across events will be very helpful in making sure that joins between these are easy to do. A reasonable scheme might be something like PageViewEvent, OrderEvent, ApplicationBounceEvent, etc.-   Avro Support Athena as Deser for querying\n\n<https://docs.aws.amazon.com/athena/latest/ug/avro.html>-   Disadvantage\n    -   Can't directly see files in S3, only csv, json and parquet supported in s3\n\n**Tools**\n\n<https://github.com/sksamuel/avro4s>\n\n## References**\n\n<https://www.tutorialspoint.com/avro/avro_overview.htm>\n\n<http://cloudurable.com/blog/avro/index.html>\n\n<http://avro.apache.org/docs/current>\n\n<https://docs.oracle.com/database/nosql-12.1.3.0/GettingStartedGuide/avroschemas.html>\n\n<https://www.sderosiaux.com/articles/2017/03/02/serializing-data-efficiently-with-apache-avro-and-dealing-with-a-schema-registry>\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/Apache-Parquet/","title":"Apache Parquet"},"frontmatter":{"draft":false},"rawBody":"# Apache Parquet\n\nCreated: 2020-02-26 23:07:20 +0500\n\nModified: 2021-07-11 02:05:58 +0500\n\n---\n\n**Apache Parquet**is a[free and open-source](https://en.wikipedia.org/wiki/Free_and_open-source)[column-oriented](https://en.wikipedia.org/wiki/Column-oriented_DBMS)data store of the[Apache Hadoop](https://en.wikipedia.org/wiki/Apache_Hadoop)ecosystem. It is similar to the other columnar-storage file formats available in[Hadoop](https://en.wikipedia.org/wiki/Apache_Hadoop)namely[RCFile](https://en.wikipedia.org/wiki/RCFile)and Optimized RCFile (ORC). It is compatible with most of the data processing frameworks in the[Hadoop](https://en.wikipedia.org/wiki/Hadoop)environment. It provides efficient[data compression](https://en.wikipedia.org/wiki/Data_compression)and[encoding](https://en.wikipedia.org/wiki/Encoding)schemes with enhanced performance to handle complex data in bulk.\nApache Parquet is a self-describing data format which embeds the schema, or structure, within the data itself.-   Columnar format\n-   Schema segregated into footer\n-   Column major format\n-   All data is pushed to the leaf\n-   Integrated compression and indexes\n-   Support for **predicate pushdown**\n**Design Goals**\n-   Interoperability\n-   Space efficiency\n-   Query efficiency\n**Features**\n-   Improved read performance at the cost of slower writes.\n-   Apache Parquet is implemented using the **record-shredding and assembly algorithm**,which accommodates the complex[data structures](https://en.wikipedia.org/wiki/Data_structures)that can be used to store the data.The values in each column are physically stored in contiguous memory locations and this columnar storage provides the following benefits:\n    -   Column-wise compression is efficient and saves storage space\n    -   Compression techniques specific to a type can be applied as the column values tend to be of the same type\n    -   Queries that fetch specific column values need not read the entire raw data thus improving performance\n    -   Different encoding techniques can be applied to different columns\n![Dataset Data stored as CSV files Data stored in Apache Parquet format* Savings / Speedup Size on Amazon S3 130 GB 87% less with Parquet Query Run time 236 seconds 6.78 seconds 34x faster Data Scanned 1.15 TB 2.51 GB 99% less data scanned Cost $5.75 $0.01 99.7% savings ](media/Apache-Parquet-image1.png)\n![Dataset Data stored as CSV file Data stored as GZIP CSV file Data stored as Parquet file Columns 4 4 4 Size on Amazon S3 4 TB ITB ITB Data Scanned 4 TB ITB .25TB Cost $20 (4TB x $5/TB) $5 (ITB x $5/TB) $1.25 (.25TB x $5/TB) ](media/Apache-Parquet-image2.png)\n**Compression and encoding**\n\nIn Parquet, compression is performed column by column, which enables different encoding schemes to be used for text and integer data. This strategy also keeps the door open for newer and better encoding schemes to be implemented as they are invented.\n**Dictionary encoding**\n\nParquet has an automatic dictionary encoding enabled dynamically for data with asmallnumber of unique values (i.e. below 105) that enables significant compression and boosts processing speed.\n**Bit packing**\n\nStorage of integers is usually done with dedicated 32 or 64 bits per integer. For small integers, packing multiple integers into the same space makes storage more efficient.\n**Run-length encoding (RLE)**\n\nTo optimize storage of multiple occurrences of the same value, a single value is stored once along with the number of occurrences.\nParquet implements a hybrid of bit packing and RLE, in which the encoding switches based on which produces the best compression results. This strategy works well for certain types of integer data and combines well with dictionary encoding.\n**Parquet Cares About Your Schema**\n\nOne limitation of CSV/TSV data is that you don't know what the exact schema is supposed to be, or the desired type of each field.\nUsing our example above, without the schema, should the 'True' values be cast to boolean? How can we be sure without knowing the schema beforehand?\nJSON improves upon CSV as each row provides some indication of schema, but without a special header-row, there's no way to derive a schema for every record in the file, and it isn't always clear what type a 'null' value should be interpreted as.\nAvro and Parquet on the other hand understand the schema of the data they store. When you write a file in these formats, you need to specify your schema. When you read the file back, it tells you the schema of the data stored within. This is super useful for a framework like Spark, which can use this information to give you a fully formed data-frame with minimal effort.\nEven ignoring the runtime of your production jobs, let me outline some of my favorite ways to use Parquet outside of analytics workloads:\n\n1.  Data validation- need to do some rough counts to verify your data is complete? Such checks can be run in a few seconds with Parquet, even with a 1TB dataset.\n\n2.  Debugging- did your pipeline do the right thing? Did it add/remove/modify the right records? With parquet you can capture quick and easy information (such as all unique values of a column) in a few seconds without scanning the whole file.\n\n3.  Quick Metrics Extraction- want to record in your monitoring system a count of a subset of records in your dataset? Previously I captured this information by running a follow-up pipeline, but with Parquet it is a very fast query through either Hive or Spark SQL.\n\n4.  Less redundancy- Need a similar dataset for two different pipelines? Instead of building a distinct dataset for each, Parquet lets you just dynamically query a larger, comprehensive dataset without the penalties of scanning a whole file.\n\n5.  Analytics- Ok, I cheated and put it in anyway. Yes, Parquet is AMAZING for analytics, anyone running SQL queries will thank you for saving them hours a day in front of a SQL prompt when their queries run up to 1000x faster.\n**Working**\n\n![Parquet File Layout Diagram](media/Apache-Parquet-image3.gif)\n**Comparison**\n\nApache Parquet is comparable to[RCFile](https://en.wikipedia.org/wiki/RCFile)and[Optimized Row Columnar (ORC)](https://en.wikipedia.org/wiki/Apache_ORC)file formats---all three fall under the category of columnar data storage within the Hadoop ecosystem. They all have better compression and encoding with improved read performance at the cost of slower writes. In addition to these features, Apache Parquet supports limited[schema evolution](https://en.wikipedia.org/wiki/Schema_evolution), i.e., the schema can be modified according to the changes in the data. It also provides the ability to add new columns and merge schemas that don't conflict.\n**Negatives of Columnar Formats**\n\nThe biggest negative of columnar formats is that re-constructing a complete record is slower and requires reading segments from each row, one-by-one. It is for this reason that columnar-file-formats initially hit their groove for analytics-style workflows, rather than Map/Reduce style workflows --- which by default operate on whole rows of data at a time.\nFor real columnar file formats (like[Parquet](http://parquet.apache.org/)), this downside is minimized by some clever tricks like breaking the file up into 'row groups' and building extensive metadata, although for particularly wide datasets (like 200+ columns), the speed impact can be fairly significant.\nThe other downside, is that they are more CPU and ram intensive to write, as the file writer needs to collect a whole bunch of metadata, and reorganize the rows before it can write the file.\nAs an aside - I still almost always recommend still using a columnar file format, it's just so useful to be able to quickly peek into a file and gather some simple metrics.\n**Tools**\n\n<https://github.com/apache/parquet-mr/tree/master/parquet-tools>\n\n## References**\n\n<https://en.wikipedia.org/wiki/Apache_Parquet>\n\n<https://parquet.apache.org/documentation/latest>\n\n<https://blog.matthewrathbone.com/2019/12/20/parquet-or-bust.html>\n\n<https://github.com/julienledem/redelm/wiki/The-striping-and-assembly-algorithms-from-the-Dremel-paper>\n\n<https://blog.twitter.com/engineering/en_us/a/2013/dremel-made-simple-with-parquet.html>\n\n[**https://www.youtube.com/watch?v=rVC9F1y38oU**](https://www.youtube.com/watch?v=rVC9F1y38oU)\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/File-Formats/","title":"File Formats"},"frontmatter":{"draft":false},"rawBody":"# File Formats\n\nCreated: 2018-10-04 13:25:36 +0500\n\nModified: 2021-05-07 23:41:15 +0500\n\n---\n\nCSV, TSV, JSON, and Avro, are traditional row-based file formats. Parquet, and ORC file are columnar file formats.\n# SequenceFile\n\nSequence files are introduced in Hadoop. Sequence files act as a container to store the small files. Sequence files are flat files consisting of binary key-value pairs. When Hive converts queries to MapReduce jobs, it decides on the appropriate key-value pairs to be used for a given record.Sequence files are in the binary format which can be split and the main use of these files is to club two or more smaller files and make them as a one sequence file.\n\nThere are three types of sequence files:\n\nâ€¢ Uncompressed key/value records.\n\nâ€¢ Record compressed key/value records -- only 'values' are compressed here\n\nâ€¢ Block compressed key/value records -- both keys and values are collected in 'blocks' separately and compressed. The size of the 'block' is configurable.\n# RCFile\n-   RCFILE stands of Record Columnar File which is another type of binary file format which offers high compression rate on the top of the rows.\n-   RCFILE is used when we want to perform operations on multiple rows at a time.\n-   RCFILEs are flat files consisting of binary key/value pairs, which shares many similarities with SEQUENCEFILE. RCFILE stores columns of a table in form of record in a columnar manner. It first partitions rows horizontally into row splits and then it vertically partitions each row split in a columnar way. RCFILE first stores the metadata of a row split, as the key part of a record, and all the data of a row split as the value part. This means that RCFILE encourages column oriented storage rather than row oriented storage.\n-   This column oriented storage is very useful while performing analytics. It is easy to perform analytics when we \"hive' a column oriented storage type.\n# ORCFile\n-   ORC stands for **Optimized Row Columnar** which means it can store data in an optimized way than the other file formats. ORC reduces the size of the original data up to 75%(eg: 100GB file will become 25GB). As a result the speed of data processing also increases. ORC shows better performance than Text, Sequence and RC file formats.\n-   An ORC file contains rows data in groups called as Stripes along with a file footer. ORC format improves the performance when Hive is processing the data.\n**Choosing File Formats**\n-   If your data is delimited by some parameters then you can useTEXTFILEformat.\n-   If your data is in small files whose size is less than the block size then you can useSEQUENCEFILEformat.\n-   If you want to perform analytics on your data and you want to store your data efficiently for that then you can useRCFILEformat.\n-   If you want to store your data in an optimized way which lessens your storage and increases your performance then you can useORCFILEformat.\n<https://acadgild.com/blog/apache-hive-file-formats>\n\n## Amazon Ion**\n\nAmazon Ionis a[richly-typed](http://amzn.github.io/ion-docs/guides/why.html#rich-type-system),[self-describing](http://amzn.github.io/ion-docs/guides/why.html#self-describing), hierarchical data serialization format offering[interchangeable binary and text](http://amzn.github.io/ion-docs/guides/why.html#dual-format-interoperability)representations. The[text format](http://amzn.github.io/ion-docs/docs/spec.html)(a superset of[JSON](http://json.org/)) is easy to read and author, supporting rapid prototyping. The[binary representation](http://amzn.github.io/ion-docs/docs/binary.html)is[efficient to store, transmit, and skip-scan parse](http://amzn.github.io/ion-docs/guides/why.html#read-optimized-binary-format). The rich type system provides unambiguous semantics for long-term preservation of data which can survive multiple generations of software evolution.\nIon was built to address rapid development, decoupling, and efficiency challenges faced every day while engineering large-scale, service-oriented architectures. It has been addressing these challenges within Amazon for nearly a decade, and we believe others will benefit as well.\nThe Ion text format is a superset of JSON; thus, any valid JSON document is also a valid Ion document.\n<http://amzn.github.io/ion-docs>\n\n<http://amzn.github.io/ion-docs/docs/spec.html>\n\n## File Format Benchmarks - Avro, JSON, ORC, Parquet**\n\nAvro\n-   Cross-language file format for Hadoop\n-   Schema evolution was primary goal\n-   Schema segregated from data\n    -   Unlike Protobuf and Thrift\n-   Row major format\nJSON\n-   Serialization format for HTTP & Javascript\n-   Text-format with many parsers\n-   Schema completely integrated with data\n-   Row major format\n-   Compression applied on top\nORC\n-   Originally part of Hive to replace RCFile\n    -   Now top-level project\n-   Schema segregated into footer\n-   Column major format with stripes\n-   Rich type mode, stored top-down\n-   Integrated compression, indexes, & stats\nParquet\n-   Design based on Google's Dremel paper\n-   Schema segregated into footer\n-   Column major format with stripes\n-   Simpler type-model with logical types\n-   All data pushed to leaves of the tree\n-   Integrated compression and indexes\n![Image for post](media/File-Formats-image1.png)\n**DataSets**\n-   NYC Taxi Data\n    -   18 columns with no null values\n    -   Doubles, integers, decimal & strings\n    -   2 months of data - 22.7 million rows\n-   Github Logs\n    -   704 columns with a lot of structure & nulls\n    -   1/2 month of data - 10.5 million rows\n    -   Schema is huge (12k)\n-   Sales\n    -   55 columns with lots of nulls\n    -   A little structure\n    -   Timestamps, strings, longs, booleans, list & struct\n    -   23 million rows\n**Compression**\n-   ORC and Parquet use RLE & Dictionaries\n-   All the formats have general compression\n    -   ZLIB (GZip) - tight compression, slower\n    -   Snappy - Some compression, faster\n![ĞĞ½Ğ¾Ğ¼ Ğ¼Ğ¾Ñ… Ğ²Ğ¾Ğ½ Ğ¼Ğ¾Ğµ - Aaaeus Ğ±Ğ¾â€¢Ğ¿ 0t.31 !Ñ…ĞµÑˆ ](media/File-Formats-image2.png)\n**Taxi size analysis**\n-   Don' use JSON\n-   Use either Snappy or Zlib compression\n-   Avor's small compression window hurts\n-   Parquet Zlib is smaller than ORC\n    -   Group the column sizes by type\n![19 Parquet & ORC Taxi Column Sizes â€¢parquet â€¢orc Inc, Rights HORTONWORKS ](media/File-Formats-image3.png)\n\n![Sales Size 2.SE.10 1.5010 1 E.to parq tlib snap\" none snap\" avro snappy parquet none 20 Inc, Rights HORTONWORKS ](media/File-Formats-image4.png)\n**Taxi size analysis**\n-   ORC did better than expected\n    -   String columns have small cardinality\n    -   Lots of timestamp columns\n    -   No doubles\n![Github Size 3.5010 2010 1SEâ€¢10 1010 s E.09 22 Inc, Rights avro snapcr,' Ott avo snappy parq uet snappv HORTONWORKS ](media/File-Formats-image5.png)\n**Github Size Analysis**\n-   Surprising win for JSON and Avro\n    -   Worst when uncompressed\n    -   Best with zlib\n-   Many partially shared strings\n    -   ORC and Parquet don't compress across columns\n**Use Case - Full Table Scans**\n-   Read all columns & rows\n-   All formats except JSON are splitable\n    -   Different workers do different parts of file\n![Taxi gs/record none avro 26 none Inc, Rights snappy orc zlib orc snappy avro zlib avro snappy none zlib none parquetlparquet parquet json snappy json zlib HORTONWORKS ](media/File-Formats-image6.png)\n**Taxi read performance analysis**\n-   JSON is very slow to read\n    -   Large storage size for this data set\n    -   Needs to do a lot of string parsing\n-   Tradeoff between space & time\n    -   Less compression is sometimes faster\n![Sales gs/record none avro snappy avro awo none snappy parquet parquet parquet json snappy json t lib J son 28 Inc, Rights HORTONWORKS ](media/File-Formats-image7.png)\n**Sales read performance analysis**\n-   Read performance is dominated by format\n    -   Compression matters less for this data set\n    -   Straight ordering: ORC, Avro, Parquet & JSON\n-   Garbage collection is important\n    -   ORC 0.3 to 1.4% of time\n    -   Avro < 0.1% of time\n    -   Parquet 4 to 8% of time\n![Github gs/record 30 snappy Snappy Inc, Rights orc ppy j son j son snappy none ison parquet parquet parquet HORTONWORKS ](media/File-Formats-image8.png)\n**Github read performance analysis**\n-   Garbage collection is critical\n    -   ORC 2.1 to 3.4% of time\n    -   Avro 0.1% of time\n    -   Parquet 11.4 to 12.8% of time\n-   A lot of columns needs more space\n    -   Suspect that we need bigger stripes\n    -   Rows/stripe - ORC: 18.6k, Parquet: 88.1k\n![Column Projection Often just need a few columns ---Only ORC & Parquet are columnar ---Only read, decompress, & deserialize some columns 'thub hub ales ales 32 Inc, AA Rights ar ar uet uet uet zlib zlib 21.319 72.494 1.866 12.893 2.766 3.496 o. 185 0.585 0.05 0.329 0.063 0.718 0.87 0.81 3. 2.55 2. 20.54 HORTONWORKS ](media/File-Formats-image9.png)\n**Projection & Predicate Pushdown**\n-   Sometimes have a filter predicate on table\n    -   Select a superset of rows that match\n    -   Selective filters have a huge impact\n-   Improves data layout options\n    -   Better than partition pruning with sorting\n-   ORC has added optional bloom filters\n**Metadata Access**\n-   ORC & Parquet store metadata\n    -   Stored in filt footer\n    -   File schema\n    -   Number of records\n    -   Min, max, count of each column\n-   Provides O(1) access\n**Recommendations**\n-   Don't use JSON for processing\n-   If your use case needs column projection or predicate push down use ORC or Parquet\n-   For complex tables with common strings - Avro with Snappy is a good fit (w/o projection)\n-   For other tables - ORC with Zlib or Snappy is a good fit\n**Key conclusions -** <https://eng.uber.com/trip-data-squeeze-json-encoding-compression>\n\n1.  Simply compressing JSON with zlib would yield a reasonable tradeoff in size and speed. The result would be just a little bigger, but execution was much faster than using BZ2 on JSON.\n\n2.  Going with IDL-based protocols, Thrift and Protocol Buffers compressed with zlib or Snappy would give us the best gain in size and/or speed.\n![ParetoFront](media/File-Formats-image10.png)\n\n<https://www.slideshare.net/oom65/file-format-benchmarks-avro-json-orc-parquet>\n\n## Compression (zlib)**\n\n**In 1 mb of memory**\n\nraw 7304 (19 kv pair)\n\nsnappy 3656 (40 kv pair)\n\nzlib 2580 (62 kv pair)\nnumber of messages in json - 52 messages\n\nRaw - 18733 B = 18.7 KB\n\nzlib - 3846 B - 3.8 KB\n\n50 messages - 18 KB\n\n150 messages - 54 KB\n\nZlib compression - 150 messages - 11.4 KB\n\n100000 * 150 = 15000000 = 15 millions / day\n\n100000 * 54 = 5400000 KB / day = 5.4 GB / day = 162 GB / month\n\n100000 * 11.4 = 1140000 KB / day = 1.14 GB / day = 34.2 GB / month\n1356011 * 4 = 5424044 * 10 = 54240440 = 54240440 = 54 GB / month\nDaily hits: 50000*52 = 2600000 messages / day = 2.6 Million msgs/day\n\n50000 * 3.8 = 190000 KB = 190 MB / day\n\n190 * 30 = 5,700 MB / month = 5.7 GB / month\n\n5.7 * 6 = 34.2\n50000 * 18.7 = 935000 = 935 MB /day\n\n935 * 30 = 28,050 / month\n2600000 * 3.8 = 9880000 KB = 9.9 GB / day\n\n9.9 * 30 = 297 GB / month\n**Things to consider**\n-   **The structure of your data:** Some formats accept nested data such as JSON, Avro or Parquet and others do not. Even, the ones that do, may not be highly optimized for it. Avro is the most efficient format for nested data, I recommend not to use Parquet nested types because they are very inefficient. Process nested JSON is also very CPU intensive. In general, it is recommended to flat the data when ingesting it.\n-   **Performance:** Some formats such as Avro and Parquet perform better than other such JSON. Even between Avro and Parquet for different use cases one will be better than others. For example, since Parquet is a column based format it is great to query your data lake using SQL whereas Avro is better for ETL row level transformation.\n-   **Easy to read:** Consider if you need people to read the data or not. JSON or CSV are text formats and are human readable whereas more performant formats such parquet or Avro are binary.\n-   **Compression:** Some formats offer higher compression rates than others.\n-   **Schema evolution:** Adding or removing fields is far more complicated in a data lake than in a database. Some formats like Avro or Parquet provide some degree of schema evolution which allows you to change the data schema and still query the data. Tools such[Delta Lake](https://delta.io/)format provide even better tools to deal with changes in Schemas.\n-   **Compatibility:** JSON or CSV are widely adopted and compatible with almost any tool while more performant options have less integration points.\n**h5 File / h5py**\n\n**Hierarchical Data Format**(**HDF**) is a set of file formats (**HDF4**,**HDF5**) designed to store and organize large amounts of data. Originally developed at the[National Center for Supercomputing Applications](https://en.wikipedia.org/wiki/National_Center_for_Supercomputing_Applications), it is supported by The HDF Group, a non-profit corporation whose mission is to ensure continued development of HDF5 technologies and the continued accessibility of data stored in HDF.\nFilename extensions - .hdf,.h4,.hdf4,.he2,.h5,.hdf5,.he5\nHETEROGENEOUS DATA\n\nHDFÂ®supports n-dimensional datasets and each element in the dataset may itself be a complex object.\n\nEASY SHARING\n\nHDFÂ®is portable, with no vendor lock-in, and is a self-describing file format, meaning everything all data and metadata can be passed along in one file.\n\nCROSS PLATFORM\n\nHDFÂ®is a software library that runs on a range of computational platforms, from laptops to massively parallel systems, and implements a high-level API with C, C++, Fortran 90, and Java interfaces. HDF has a large ecosystem with 700+ Github projects.\n\nFAST I/O\n\nHDFÂ®is high-performance I/O with a rich set of integrated performance features that allow for access time and storage space optimizations.\n\nBIG DATA\n\nThere is no limit on the number or size of data objects in the collection, giving great flexibility for big data.\n\nKEEP METADATA WITH DATA\n\nHDF5Â®allows you to keep the metadata with the data, streamlining data lifecycles and pipelines.\n<https://en.wikipedia.org/wiki/Hierarchical_Data_Format>\n\n<https://github.com/h5py/h5py>\n\n[**https://docs.h5py.org/en/stable/**](https://docs.h5py.org/en/stable/)\n\n<https://www.hdfgroup.org>\n\n<https://www.geeksforgeeks.org/hdf5-files-in-python>\n[**https://realpython.com/storing-images-in-python/**](https://realpython.com/storing-images-in-python/)\n\n[**https://github.com/realpython/materials/blob/storing-images/storing-images/storing_images.ipynb**](https://github.com/realpython/materials/blob/storing-images/storing-images/storing_images.ipynb)\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-04-25 11:29:11 +0500\n\nModified: 2021-04-26 10:26:33 +0500\n\n---\n\n**OpenConnect**\n\n**OpenConnect**is an[open-source software](https://en.wikipedia.org/wiki/Open-source_software)application for connecting to[virtual private networks](https://en.wikipedia.org/wiki/Virtual_private_network)(VPN), which implement secure[point-to-point](https://en.wikipedia.org/wiki/Point-to-point_(telecommunications))connections.\n<https://en.wikipedia.org/wiki/OpenConnect>\n\n## OpenSSH**\n\n**OpenSSH**(also known as**OpenBSD Secure Shell**) is a suite of[secure](https://en.wikipedia.org/wiki/Computer_security)[networking](https://en.wikipedia.org/wiki/Computer_network)utilities based on the[Secure Shell](https://en.wikipedia.org/wiki/Secure_Shell)(SSH) protocol, which provides a[secure channel](https://en.wikipedia.org/wiki/Secure_channel)over an unsecured network in a[client--server](https://en.wikipedia.org/wiki/Client%E2%80%93server_model)architecture.\n\nOpenSSH started as a[fork](https://en.wikipedia.org/wiki/Fork_(software_development))of the[free](https://en.wikipedia.org/wiki/Free_software)SSH program developed by[Tatu YlÃ¶nen](https://en.wikipedia.org/wiki/Tatu_Yl%C3%B6nen); later versions of YlÃ¶nen's SSH were[proprietary software](https://en.wikipedia.org/wiki/Proprietary_software)offered by[SSH Communications Security](https://en.wikipedia.org/wiki/SSH_Communications_Security).OpenSSH was first released in 1999, and is currently developed as part of the[OpenBSD](https://en.wikipedia.org/wiki/OpenBSD)[operating system](https://en.wikipedia.org/wiki/Operating_system).\n\nOpenSSH is not a single computer program, but rather a suite of programs that serve as alternatives to unencrypted protocols like[Telnet](https://en.wikipedia.org/wiki/Telnet)and[FTP](https://en.wikipedia.org/wiki/FTP). OpenSSH is integrated into several operating systems,while the[portable](https://en.wikipedia.org/wiki/Porting)version is available as a package in other systems.\n<https://en.wikipedia.org/wiki/OpenSSH>\n\n## GStreamer**\n\n**GStreamer**is a[pipeline](https://en.wikipedia.org/wiki/Pipeline_(computing))-based[multimedia framework](https://en.wikipedia.org/wiki/Multimedia_framework)that links together a wide variety of media processing systems to complete complex workflows. For instance, GStreamer can be used to build a system that reads files in one format, processes them, and exports them in another. The formats and processes can be changed in a plug and play fashion.\nGStreamer supports a wide variety of media-handling components, including simple[audio](https://en.wikipedia.org/wiki/Audio_frequency)playback, audio and video playback,[recording](https://en.wikipedia.org/wiki/Sound_recording_and_reproduction),[streaming](https://en.wikipedia.org/wiki/Streaming_media)and editing. The pipeline design serves as a base to create many types of[multimedia](https://en.wikipedia.org/wiki/Multimedia)applications such as[video editors](https://en.wikipedia.org/wiki/Video_editing),[transcoders](https://en.wikipedia.org/wiki/Transcoding), streaming media broadcasters and[media players](https://en.wikipedia.org/wiki/Media_player_(application_software)).\n<https://en.wikipedia.org/wiki/GStreamer>\n\n<https://gstreamer.freedesktop.org>\n\n## Others**\n\nNNStreamer - Neural Network (NN) Streamer, Stream Processing Paradigm for Neural Network Apps/Devices.\n<https://github.com/nnsuite/nnstreamer>\n\n## Zeroconf**\n\n**Zero-configuration networking**(**zeroconf**) is a set of technologies that automatically creates a usable[computer network](https://en.wikipedia.org/wiki/Computer_network)based on the[Internet Protocol Suite](https://en.wikipedia.org/wiki/Internet_Protocol_Suite)(TCP/IP) when computers or network peripherals are interconnected. It does not require manual operator intervention or special configuration servers. Without zeroconf, a network administrator must set up[network services](https://en.wikipedia.org/wiki/Network_service), such as[Dynamic Host Configuration Protocol](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol)(DHCP) and[Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System)(DNS), or configure each computer's network settings manually.\nZeroconf is built on three core technologies: automatic assignment of numeric[network addresses](https://en.wikipedia.org/wiki/Network_address)for networked devices, automatic distribution and resolution of computer[hostnames](https://en.wikipedia.org/wiki/Hostname), and automatic[location of network services](https://en.wikipedia.org/wiki/Service_discovery), such as printing devices.\n<https://en.wikipedia.org/wiki/Zero-configuration_networking>\n\n## pmacct**\n\npmacct is a small set of multi-purpose passive network monitoring tools [NetFlow IPFIX sFlow libpcap BGP BMP RPKI IGP Streaming Telemetry]\n[pmacct](https://github.com/pmacct/pmacct)is a set of network monitoring tools that can collect network traffic via libpcap and export it to a variety of places. Can be used to do network flow analysis.\npmacct is a small set of multi-purpose passive network monitoring tools. It can account, classify, aggregate, replicate and export forwarding-plane data, ie. IPv4 and IPv6 traffic; collect and correlate control-plane data via BGP and BMP; collect and correlate RPKI data; collect infrastructure data via Streaming Telemetry. Each component works both as a standalone daemon and as a thread of execution for correlation purposes (ie. enrich NetFlow with BGP data). pmacct main features are:\n-   Suitable to ISP, IXP, CDN, IP carrier, Cloud, DC and hot-spots enviroments and SDN solutions\n-   Runs on Linux, BSDs, Solaris and embedded systems\n-   Support for both IPv4 and IPv6\n-   Collects data through libpcap, Netlink/NFLOG, NetFlow v1/v5/v7/v8/v9, sFlow v2/v4/v5 and IPFIX\n-   Collects Streaming Telemetry data. Read more[here](https://github.com/pmacct/pmacct/blob/master/telemetry/README.telemetry).\n-   Supports Cisco NEL for CGNAT scenarios and Cisco NSEL\n-   Saves data to a number of backends including:\n    -   Relational databases: MySQL, PostgreSQL and SQLite\n    -   noSQL databases: MongoDB and BerkeleyDB\n    -   AMQP message exchanges: RabbitMQ\n    -   Kafka message brokers\n    -   memory tables\n    -   flat files\n-   Exports data to remote collectors through IPFIX, NetFlow v5/v9 and sFlow v5\n-   Replicates incoming IPFIX, NetFlow and sFlow packets to remote collectors\n-   Flexible architecture to tag, filter, redirect, aggregate and split captured data\n-   Comes with:\n    -   a BGP daemon/thread for efficient visibility into the inter-domain routing plane. Read more[here](http://www.pmacct.net/lucente_pmacct_uknof14.pdf).\n        -   Supports BGP/MPLS VPNs rfc4364, Label Unicast rfc3107\n        -   Supports BGP ADD-PATHs (draft-ietf-idr-add-paths) for visibility of BGP multi-path routes\n        -   Can log live BGP messaging and/or dump BGP tables per peer at regular time interval\n    -   a BMP daemon/thread to gain insight in BGP data, events and statistics\n        -   Supports draft-ietf-grow-bmp-loc-rib and draft-ietf-grow-bmp-adj-rib-out\n    -   an IS-IS/IGP thread for visibility of internal routes\n    -   a RPKI thread to associate Route Origin Validation (ROV) state to BGP data (from1.7.3)\n-   Packet classification via nDPI\n-   Inspection of tunnelled traffic (ie. GTP)\n-   GeoIP lookups leveraging Maxmind library\n-   Pluggable architecture for easy integration of new capturing environments and data backends\n-   Careful SQL support: data pre-processing, triggers, dynamic table naming\n-   It's free, open-source, developed and supported with passion and open mind for more than 10 years\n<https://brooks.sh/2019/11/17/network-flow-analysis-with-prometheus>\n\n<https://github.com/pmacct/pmacct>\n\n## Ribbon**\n\nRibbon is a Inter Process Communication (remote procedure calls) library with built in software load balancers. The primary usage model involves REST calls with various serialization scheme support.\nRibbon is a client side IPC library that is battle-tested in cloud. It provides the following features\n-   Load balancing\n-   Fault tolerance\n-   Multiple protocol (HTTP, TCP, UDP) support in an asynchronous and reactive model\n-   Caching and batching\n<https://github.com/Netflix/ribbon>\n\n## mininet**\n\nEmulator for rapid prototyping of Software Defined Networks (SDN)\n<https://github.com/mininet/mininet>\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/Protocol-Buffers-Protobuf/","title":"Protocol Buffers Protobuf"},"frontmatter":{"draft":false},"rawBody":"# Protocol Buffers Protobuf\n\nCreated: 2018-03-26 21:37:20 +0500\n\nModified: 2020-07-23 12:07:39 +0500\n\n---\n\nProtocol Buffers are a way of encoding structured data in an efficient yet extensible format. Google uses Protocol Buffers for almost all of its internal RPC protocols and file formats.\nProtocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for **serializing structured data.**\n**Protocol Buffers**is a method of[serializing](https://en.wikipedia.org/wiki/Serialization)structured data. It is useful in developing programs to communicate with each other over a wire or for storing data. The method involves an[interface description language](https://en.wikipedia.org/wiki/Interface_description_language)that describes the structure of some data and a program that generates source code from that description for generating or parsing a stream of bytes that represents the structured data.\nThough the primary purpose of Protocol Buffers is to facilitate network communication, its simplicity and speed make Protocol Buffers an alternative to data-centric C++ classes and structs, especially where interoperability with other languages or systems might be needed in the future.-   IDL (Interface Definition Language)\n\nDescibe only and generate interfaces for any language\n-   Data Model\n\nStructure of the request and response\n-   Wire Format\n\nBinary format for network transmission\n![message HelloRequest { string greeting â€¢ 1; message HeiloResponse { string reply service HeiloService { rpc SayHeiio(HelloRequest) returns (HeiloResponse); ](media/Protocol-Buffers-Protobuf-image1.png)\n<https://github.com/google/protobuf>\n\n<https://developers.google.com/protocol-buffers>\n\n## Advantages over XML**\n\n1.  are simpler\n\n2.  are 3 to 10 times smaller\n\n3.  are 20 to 100 times faster\n\n4.  are less ambiguous\n\n5.  generate data access classes that are easier to use programmatically\n**protoc (protobuf compiler)**\n-   Generates data structure that correspond to IDL data model\n-   Request and response types\ndocker run --rm -v $(pwd):$(pwd) -w $(pwd) znly/protoc --plugin=protoc-gen-grpc=/usr/bin/grpc_python_plugin --python_out=./messages/ --grpc_out=./messages/ --proto_path=./protobuf ./protobuf/*.proto\n# generate pb files from proto files\n\npython -m grpc_tools.protoc -I=pcbook/proto --python_out=pcbook/pb --grpc_python_out=pcbook/pb pcbook/proto/*.proto\nprotoc --proto_path=. *.proto --python_out=.\n**Plugins for protoc**\n-   Generate interfaces and client stubs\n**How to define a protocol message**\n-   Name of the message: UpperCamelCase\n-   Name of the field: lower_snake_case\n-   Some scalar-value data types:\n    -   string, bool, bytes\n    -   float, double\n    -   int32, int64, uint32, uint64, sint32, sint64, etc.\n-   Data types can be user-defined enums or other messages\n-   Tags are more important than field names\n    -   Is an arbitrary integer\n        -   From 1 to 536,870,911 (or 2^29-1)\n        -   Except from 19000 to 19999 (reserved)\n    -   From 1 to 15 take 1 byte\n    -   From 16 to 2047 take 2 bytes\n    -   Don't need to be in-order or sequential\n    -   Must be unique for same-level fields\n**Others**\n\nA Protobuf message definition consists of fields defined by a name, a type, and an integer field number. The field number in particular is sacred, as this is what is actually transmitted in a serialised message (as opposed to the field name). All producers and consumers rely on this integer having a consistent meaning, and altering it can cause havoc if a consumer processes old data with a new understanding of what data belongs to a field number.\nThe tests we've implemented cover the following aspects:\n-   Field numbers must not be amended.\n-   Fields must not have their type amended.\n-   Fields that have been removed from a message must have an entry added to a reserved statement within the message, both for the deleted field and the deleted field number. This ensures that the protoc compiler will complain if someone attempts to add either of these back in to a subsequent version.\n-   Fields must not have their name amended (this would not break Protobuf compatibility, but we have the test in place to help maintain the evolvable schemas for JSON derived from Protobuf models).\n**Python apis**\n\n<https://developers.google.com/protocol-buffers/docs/reference/python-generated>\n\n<https://developers.google.com/protocol-buffers/docs/pythontutorial>\n\n<https://developers.google.com/protocol-buffers/docs/proto3>-   **SerializeToString():** serializes the message and returns it as a string. Note that the bytes are binary, not text; we only use thestrtype as a convenient container\n-   **ParseFromString(data):** parses a message from the given string\n**Specification**\n\nprotobuf.common.messageData\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/RPC/","title":"RPC"},"frontmatter":{"draft":false},"rawBody":"# RPC\n\nCreated: 2017-12-04 21:16:13 +0500\n\nModified: 2020-05-12 01:12:57 +0500\n\n---\n\n<https://sbdevel.wordpress.com/2009/12/17/the-case-rpc-vs-messaging>\n\n## RPC stands for Remote Procedure Call**\nRPC has a greater learning curve than REST\n**RPC** is a mechanism that allows you to call methods on remote services as though they were methods on a local object.\nRemote Procedure Call is a protocol that one program can use to request a service from a program located in another computer on a network\nIn[distributed computing](https://en.wikipedia.org/wiki/Distributed_computing), a**remote procedure call**(**RPC**) is when a[computer program](https://en.wikipedia.org/wiki/Computer_program)causes a procedure ([subroutine](https://en.wikipedia.org/wiki/Subroutine)) to execute in a different[address space](https://en.wikipedia.org/wiki/Address_space)(commonly on another computer on a shared network), which is coded as if it were a normal (local) procedure call, without the programmer explicitly coding the details for the remote interaction. That is, the programmer writes essentially the same code whether the subroutine is local to the executing program, or remote. This is a form of[client--server](https://en.wikipedia.org/wiki/Client%E2%80%93server_model)interaction (caller is client, executor is server), typically implemented via a[request--response](https://en.wikipedia.org/wiki/Request%E2%80%93response)message-passing system. In the[object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming)paradigm, RPC calls are represented by[remote method invocation](https://en.wikipedia.org/wiki/Remote_method_invocation)(RMI).\nRPCs are a form of[inter-process communication](https://en.wikipedia.org/wiki/Inter-process_communication)(IPC), in that different processes have different address spaces: if on the same host machine, they have distinct[virtual address spaces](https://en.wikipedia.org/wiki/Virtual_address_space), even though the physical address space is the same; while if they are on different hosts, the[physical address](https://en.wikipedia.org/wiki/Physical_address)space is different. Many different (often incompatible) technologies have been used to implement the concept.\n**Message Passing in RPC**\n\nRPC is a[request--response](https://en.wikipedia.org/wiki/Request%E2%80%93response)protocol. An RPC is initiated by theclient, which sends a request message to a known remoteserverto execute a specified procedure with supplied parameters. The remote server sends a response to the client, and the application continues its process. While the server is processing the call, the client is blocked (it waits until the server has finished processing before resuming execution), unless the client sends an asynchronous request to the server, such as an XHTTP call. There are many variations and subtleties in various implementations, resulting in a variety of different (incompatible) RPC protocols.\nAn important difference between remote procedure calls and local calls is that remote calls can fail because of unpredictable network problems. Also, callers generally must deal with such failures without knowing whether the remote procedure was actually invoked.[Idempotent](https://en.wikipedia.org/wiki/Idempotent)procedures (those that have no additional effects if called more than once) are easily handled, but enough difficulties remain that code to call remote procedures is often confined to carefully written low-level subsystems.\n**Sequence of events**\n\n1.  The client calls the client[stub](https://en.wikipedia.org/wiki/Stub_(distributed_computing)). The call is a local procedure call, with parameters pushed on to the stack in the normal way.\n\n2.  The[client stub](https://en.wikipedia.org/wiki/Class_stub)packs the parameters into a message and makes a system call to send the message. Packing the parameters is called[marshalling](https://en.wikipedia.org/wiki/Marshalling_(computer_science)).\n\n3.  The client's local[operating system](https://en.wikipedia.org/wiki/Operating_system)sends the message from the client machine to the server machine.\n\n4.  The local[operating system](https://en.wikipedia.org/wiki/Operating_system)on the server machine passes the incoming packets to the[server stub](https://en.wikipedia.org/wiki/Class_skeleton).\n\n5.  The server stub unpacks the parameters from the message. Unpacking the parameters is called[unmarshalling](https://en.wikipedia.org/wiki/Unmarshalling).\n\n6.  Finally, the server stub calls the server procedure. The reply traces the same steps in the reverse direction.\n**Problems with RPC -**\n\n1.  **Non-L**ocal Exceptions Problem2.  Indirect Memory Allocation Problem\n\n3.  Blocking Calls Problem\n\n4.  Static Interface Problem\n**Example - RPC Systems**\n\n1.  Java RMI\n\n2.  SOAP\n\n3.  CORBA\n\n4.  NFS (Network File System)\n\nNetwork File System(NFS) is a[distributed file system](https://en.wikipedia.org/wiki/Distributed_file_system)protocol originally developed by[Sun Microsystems](https://en.wikipedia.org/wiki/Sun_Microsystems) (Sun) in 1984,allowing a user on a client[computer](https://en.wikipedia.org/wiki/Computer)to access files over a[computer network](https://en.wikipedia.org/wiki/Computer_network)much like local storage is accessed. NFS, like many other protocols, builds on the[Open Network Computing Remote Procedure Call](https://en.wikipedia.org/wiki/Open_Network_Computing_Remote_Procedure_Call)(ONC RPC) system. The NFS is an open standard defined in a[Request for Comments](https://en.wikipedia.org/wiki/Request_for_Comments)(RFC), allowing anyone to implement the protocol.\n5.  Finagle\n\nA fault tolerant, protocol-agnostic RPC system\nFinagle is an extensible RPC system for the JVM, used to construct high-concurrency servers. Finagle implements uniform client and server APIs for several protocols, and is designed for high performance and concurrency. Most of Finagle's code is protocol agnostic, simplifying the implementation of new protocols.\nFinagle provides a robust implementation of:\n-   connection pools, with throttling to avoid TCP connection churn;\n-   failure detectors, to identify slow or crashed hosts;\n-   failover strategies, to direct traffic away from unhealthy hosts;\n-   load-balancers, including \"least-connections\" and other strategies; and\n-   back-pressure techniques, to defend servers against abusive clients and dogpiling.\n<https://github.com/twitter/finagle>\n\n<https://blog.twitter.com/engineering/en_us/a/2011/finagle-a-protocol-agnostic-rpc-system.html>\n\n<https://twitter.github.io/finagle>\n\n## Tools**\n\n**BloomRPC**\n\nBloomRPCaim to give the simplest and efficient developer experience for exploring and querying your GRPC services.\n**Features**\n-   Native GRPC calls\n-   Unary Calls and Server Side Streaming Support\n-   Client side and Bi-directional Streaming\n-   Automatic Input recognition\n-   Multi tabs operations\n-   Metadata support\n-   Persistent Workspace\n-   Request Cancellation\nbrew cask install bloomrpc\n<https://github.com/uw-labs/bloomrpc>\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/Serialization-Deserialization/","title":"Serialization/Deserialization"},"frontmatter":{"draft":false},"rawBody":"# Serialization/Deserialization\n\nCreated: 2020-05-02 23:14:51 +0500\n\nModified: 2020-05-03 02:06:51 +0500\n\n---\n\nSerialization is the process of turning structured in-memory objects into a byte stream for transmission over a network or for writing to persistent storage\nDeserialization is the reverse process from a byte stream back to a series of structured in-memory objects\nWhen selecting a data serialization format, the following characteristic should be evaluated:\n-   Schema support and Schema evolution\n-   Code generation\n-   Language support / Interoperability\n-   Transparent compression\n-   Splitability\n-   Support in Big Data / Fast Data Ecosystem\n\n![](media/Serialization-Deserialization-image1.png)\n**Forward Compatibility**\n\nForwards compatibility means that consumers can read data produced from a client using a later version of the schema than that consumer. In the case where a new field is added to a Protobuf message, the message will be decoded by the consumer but it will have no knowledge of that new field until it moves to the later version.\nFields that have been deleted in the new schema will be deserialised as default values for the relevant types in the consumer programming language. In both cases no deserialisation errors occur as a result of the schema mismatch.\n**Backward Compatiblity**\n\nBackwards compatibility means that consumers using a newer version of the schema can read the data produced by a client with an earlier version of the schema. In a similar but reversed fashion as described above, fields that have been added in the newer version will be deserialised, but because the producer has no knowledge of the new fields, messages are transmitted with no data in those fields, and are subsequently deserialised with default values in the consumer.\nFields that have been deleted in the new schema will naturally require that any subsequent code that was in place to handle that data be refactored to cope.\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/VPN/","title":"VPN"},"frontmatter":{"draft":false},"rawBody":"# VPN\n\nCreated: 2018-12-09 12:24:08 +0500\n\nModified: 2022-01-10 18:41:51 +0500\n\n---\n\nA**virtual private network**(**VPN**) extends a[private network](https://en.wikipedia.org/wiki/Private_network)across a public network, and enables users to send and receive data across shared or public networks as if their computing devices were directly connected to the private network. Applications running on a computing device, e.g., a laptop, desktop, smartphone, across a VPN may therefore benefit from the functionality, security, and management of the private network. Encryption is a common, though not an inherent, part of a VPN connection.\nFull Tunnel\n**Split Tunnel**\n\nSplit tunnelingis a computer networking concept which allows a mobile user to access dissimilar security domains like a public network (e.g., the Internet) and a local LAN or WAN at the same time, using the same or different network connections. This connection state is usually facilitated through the simultaneous use of a Local Area Network (LAN) Network Interface Card (NIC), radio NIC, Wireless Local Area Network (WLAN) NIC, and VPN client software application without the benefit of access control.\n**Advantages**\n-   One advantage of using split tunneling is that it alleviates bottlenecks and conserves bandwidth as Internet traffic does not have to pass through the VPN server.\n-   Another advantage is in the case where a user works at a supplier or partner site and needs access to network resources on both networks throughout the day. Split tunneling prevents the user from having to continually connect and disconnect.\n**Disadvantages**\n-   A disadvantage is that when split tunneling is enabled, users bypass gateway level security that might be in place within the company infrastructure.[[1]](https://en.wikipedia.org/wiki/Split_tunneling#cite_note-1)For example, if web or[content filtering](https://en.wikipedia.org/wiki/Content_filtering)is in place, this is something usually controlled at a gateway level, not the client PC.\n-   ISPs that implement[DNS hijacking](https://en.wikipedia.org/wiki/DNS_hijacking)break name resolution of private addresses with a split tunnel.\n**Variant - Inverse Split Tunneling**\n\nA variant of this split tunneling is called \"inverse\" split tunneling. By default all datagrams enter the tunnel except those destination IPs explicitly allowed by VPN gateway. The criteria for allowing datagrams to exit the local network interface (outside the tunnel) may vary from vendor to vendor (i.e.: port, service, etc.) This keeps control of network gateways to a centralized policy device such as the VPN terminator. This can be augmented by endpoint policy enforcement technologies such as an interface firewall on the endpoint device's network interface driver,[group policy](https://en.wikipedia.org/wiki/Group_policy)object or anti-malware agent. This is related in many ways to[network access control](https://en.wikipedia.org/wiki/Network_access_control)(NAC).\n<https://en.wikipedia.org/wiki/Split_tunneling>\n\n## IPSec VPN vs SSL VPN**\n\nThe major difference between anIPsec VPNand an SSLVPNcomes down to the network layers at which encryption and authentication are performed.IPsecoperates at the network layer and can be used to encrypt data being sent between any systems that can be identified by IP addresses.\n**IPSec**\n\nIn[computing](https://en.wikipedia.org/wiki/Computing),Internet Protocol Security(IPsec) is a secure network[protocol suite](https://en.wikipedia.org/wiki/Protocol_suite)that[authenticates](https://en.wikipedia.org/wiki/Authentication)and[encrypts](https://en.wikipedia.org/wiki/Encryption)the[packets](https://en.wikipedia.org/wiki/Packet_(information_technology))of data to provide secure encrypted communication between two computers over an[Internet Protocol](https://en.wikipedia.org/wiki/Internet_Protocol)network. It is used in[virtual private networks](https://en.wikipedia.org/wiki/Virtual_private_network)(VPNs).\nIPsec includes protocols for establishing[mutual authentication](https://en.wikipedia.org/wiki/Mutual_authentication)between agents at the beginning of a session and negotiation of[cryptographic keys](https://en.wikipedia.org/wiki/Key_(cryptography))to use during the session. IPsec can protect data flows between a pair of hosts (host-to-host), between a pair of security gateways (network-to-network), or between a security gateway and a host (network-to-host).IPsec uses cryptographic security services to protect communications over Internet Protocol (IP) networks. It supports network-level peer authentication, data-origin authentication, data integrity, data confidentiality (encryption), and replay protection.\nThe initial[IPv4](https://en.wikipedia.org/wiki/IPv4)suite was developed with few security provisions. As a part of the IPv4 enhancement, IPsec is a layer 3[OSI model](https://en.wikipedia.org/wiki/OSI_model)or[internet layer](https://en.wikipedia.org/wiki/Internet_layer)end-to-end security scheme, while some other Internet security systems in widespread use operate above layer 3, such as[Transport Layer Security](https://en.wikipedia.org/wiki/Transport_Layer_Security)(TLS) and[Secure Shell](https://en.wikipedia.org/wiki/Secure_Shell)(SSH), both of which operate at the Transport layer. IPsec can automatically secure applications at the IP layer.\n**Modes of operation**\n\nThe IPsec protocols AH and ESP can be implemented in a host-to-host transport mode, as well as in a network tunneling mode.\n<https://en.wikipedia.org/wiki/IPsec>\n\n## Openvpn**\n\n**OpenVPN**is a[free and open-source](https://en.wikipedia.org/wiki/Free_and_open-source)software application that implements[virtual private network](https://en.wikipedia.org/wiki/Virtual_private_network)(VPN) techniques to create secure point-to-point or site-to-site connections in routed or bridged configurations and remote access facilities. It uses a custom security protocolthat utilizes[SSL/TLS](https://en.wikipedia.org/wiki/Transport_Layer_Security)for key exchange. It is capable of traversing[network address translators](https://en.wikipedia.org/wiki/Network_address_translator)(NATs) and[firewalls](https://en.wikipedia.org/wiki/Firewall_(computing)).\nOpenVPN allows[peers](https://en.wikipedia.org/wiki/Peer-to-peer)to[authenticate](https://en.wikipedia.org/wiki/Authentication)each other using[pre-shared secret keys](https://en.wikipedia.org/wiki/Pre-shared_key),[certificates](https://en.wikipedia.org/wiki/Public_key_certificate)or[username](https://en.wikipedia.org/wiki/User_(computing))/[password](https://en.wikipedia.org/wiki/Password). When used in a multiclient-server configuration, it allows the server to release an[authentication certificate](https://en.wikipedia.org/wiki/Public_key_certificate)for every client, using[signatures](https://en.wikipedia.org/wiki/Digital_signature)and[certificate authority](https://en.wikipedia.org/wiki/Certificate_authority). It uses the[OpenSSL](https://en.wikipedia.org/wiki/OpenSSL)encryption[library](https://en.wikipedia.org/wiki/Library_(computing))extensively, as well as the[TLS](https://en.wikipedia.org/wiki/Transport_Layer_Security)protocol, and contains many security and control features.\n<https://en.wikipedia.org/wiki/OpenVPN>\n\n<https://openvpn.net>\n\n<https://medium.com/swlh/creating-a-vpn-with-2-factor-authentication-using-openvpn-and-docker-9569e609151a>\n\n## WireGuard**\n\nWireGuard is a novel VPN that runs inside the Linux Kernel and utilizesstate-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is designed as a general purpose VPN for running on embedded interfaces and super computers alike, fit for many different circumstances. It runs over UDP.\n<https://www.wireguard.com>\n\n<https://www.freecodecamp.org/news/how-to-set-up-a-vpn-server-at-home>\n\n## NGrok**\n\nngrok is a reverse proxy that creates a secure tunnel from a public endpoint to a locally running web service. ngrok captures and analyzes all traffic over the tunnel for later inspection and replay.\n<https://ngrok.com>\n\n<https://github.com/inconshreveable/ngrok>\n\n## Secure access service edge (SASE) model**\n\n<https://pages.awscloud.com/AWSMP-SEC-NetworkSecurity-Edge-SASE-en.html>\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/gRPC/","title":"gRPC"},"frontmatter":{"draft":false},"rawBody":"# gRPC\n\nCreated: 2018-05-26 01:15:07 +0500\n\nModified: 2022-04-18 18:28:52 +0500\n\n---\n-   RPC - Remote Procedure Call framework that uses HTTP 2.0 and Protocol Buffers\n-   A high performance, open-source universal RPC framework\n**gRPC**(**gRPC****Remote Procedure Calls**) is an[open source](https://en.wikipedia.org/wiki/Open-source_software)[remote procedure call](https://en.wikipedia.org/wiki/Remote_procedure_call)(RPC) system initially developed at[Google](https://en.wikipedia.org/wiki/Google). It uses[HTTP/2](https://en.wikipedia.org/wiki/HTTP/2)for transport,[Protocol Buffers](https://en.wikipedia.org/wiki/Protocol_Buffers)as the[interface description language](https://en.wikipedia.org/wiki/Interface_description_language), and provides features such as authentication, bidirectional streaming and flow control, blocking or nonblocking bindings, and cancellation and timeouts. It generates cross-platform client and server bindings for many languages.\n**Overview**\n\nIn gRPC a client application can directly call methods on a server application on a different machine as if it was a local object, making it easier for you to create distributed applications and services. As in many RPC systems, gRPC is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types. On the server side, the server implements this interface and runs a gRPC server to handle client calls. On the client side, the client has a stub (referred to as just a client in some languages) that provides the same methods as the server.\n\n![gRPC Server C++ Service Bequest proto Bespo Proto Request roto Response(s) I gRPC Stub Ruby Client gRPC stub Android-Java Client ](media/gRPC-image1.png)\n\ngRPC clients and servers can run and talk to each other in a variety of environments - from servers inside Google to your own desktop - and can be written in any of gRPC's supported languages. So, for example, you can easily create a gRPC server in Java with clients in Go, Python, or Ruby. In addition, the latest Google APIs will have gRPC versions of their interfaces, letting you easily build Google functionality into your applications.\n**Types of gRPC service**\n\ngRPC lets you define four kinds of service method:-   **Unary RPCs** where the client sends a single request to the server and gets a single response back, just like a normal function call.\nrpc SayHello(HelloRequest) returns (HelloResponse){\n}-   **Server streaming RPCs** where the client sends a request to the server and gets a stream to read a sequence of messages back. The client reads from the returned stream until there are no more messages.\nrpc LotsOfReplies(HelloRequest) returns (stream HelloResponse){\n}-   **Client streaming RPCs** where the client writes a sequence of messages and sends them to the server, again using a provided stream. Once the client has finished writing the messages, it waits for the server to read them and return its response.\nrpc LotsOfGreetings(stream HelloRequest) returns (HelloResponse) {\n}-   **Bidirectional streaming RPCs** where both sides send a sequence of messages using a read-write stream. The two streams operate independently, so clients and servers can read and write in whatever order they like: for example, the server could wait to receive all the client messages before writing its responses, or it could alternately read a message then write a message, or some other combination of reads and writes. The order of messages in each stream is preserved.\nrpc BidiHello(stream HelloRequest) returns (stream HelloResponse){\n}\n**Specifications for RPCs**\n-   Language-agnostic Semantics\n    -   Unary RPCs vs Streaming RPCs\n    -   Metadata\n    -   **Cancellation/Deadlines**\n    -   Response messages and errors\n-   Spec for wire protocol\n    -   Maps RPC semantics to HTTP/2 protocol\n**Runtime Libraries**\n-   Implements Wire Protocol\n-   Server support\n    -   Expose service implementations via gRPC\n-   Client support\n    -   Machinery for connecting to servers, sending RPCs\n    -   Service discovery, load balancing, connection management\n**Benefits & Tradeoffs of gRPC**\n\n1.  Developer productivity\n    -   Abstracts away networking details\n    -   \"Procedure Call\" syntax\n    -   Action-centric, not resource-centric\n\n2.  Strongly Typed Message Schemas\n    -   clear \"contract\" of what messages can look like\n    -   Compile-time type checking (in languages that allow it)\n    -   Object-oriented APIs facilitate IDE support\n    -   Tradeoff: less flexibility\n\n3.  Efficiency/Performance\n    -   HTTP/2\n    -   Compact binary format\n    -   Tradeoff: less human-consumable than JSON+REST\n    -   Tradeoff: lack of browser support\n\n4.  Language agnostic\n\n5.  Many modern features\n    -   Flow control\n    -   Full-duplex bidirectional streams; not just request-response\n    -   Request and response metadata (headers, trailers)\n    -   Call cancellation, deadline propagation\n    -   Interceptors (middleware)*\n    -   **Service discovery**\n    -   **load balancing (Client-side / Look-aside load balancing)**\n\n![Lookaside Load Balancing Server List LB Load Reports server _vtâ€¢.zabcn ](media/gRPC-image2.png)-   Proxyless RPC Mesh\n<https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears>\n\n## Client side load balancing**\n\nTwo main components needed for the gRPC client-side load balancing to work\n-   [name resolver](https://github.com/grpc/grpc/blob/master/doc/naming.md)\n-   [load balancing policy](https://github.com/grpc/grpc/blob/master/doc/load-balancing.md).\n\n![](media/gRPC-image3.png)\n<https://github.com/jtattermusch/grpc-loadbalancing-kubernetes-examples>-   **Automatic retries, hedging* (Retry hedging)**\n\n![Ğ’Ğ°Ğ¾Ñ Hedgi09 Pathway ](media/gRPC-image4.png)\n-   Hit all backend\n-   If you get response from one client, cancel all requests\n-   Problem - Cascading Failures\n    -   Retry throttling\n    -   Pushback-   Transports\n-   Auth & Security\n    -   Plugin auth mechanism for extensibility\n-   Stats, Monitoring and Tracing\n    -   Prometheus, Zipkin, OpenCensus, Opentracing integrations\n-   Service Discovery\n    -   Consul, Zookeeper, Eureka\n-   Supported with Proxies\n    -   Envoy, Nginx, linkerd, nghttp2, haproxy\n6.  Opinionated\n    -   gRPC is end-to-end service communications framework\n    -   Few decisions to make\n    -   Tradeoff: must buy in 100%\n\n7.  Ecosystem\n    -   grpc-gateway\n        -   Exposes gRPC services as REST APIs\n    -   grpc-web\n        -   Adapts gRPC to work with browser clients\n**Limitations**\n\n1.  grpc server not available in php\n    -   Spiral framework\n\n<https://spiral.dev>\n\n2.  Load Balancing\n\n3.  Error handling is really bad\n\n4.  No support for browser JS\n\n5.  Breaking API changes\n\n6.  Poor documentation for some languages\n\n7.  No standardization across languages\n**Development Flow**\n\n1.  Define the API in \"language agnostic\" proto sources\n\n2.  Implement server\n\n3.  Implement clients\n**Architecture**\n\nThe first thing to note is that the architecture of gRPC is layered:\n-   **The lowest layer is the transport:**gRPC uses HTTP/2 as its transport protocol. HTTP/2 provides the same basic semantics as HTTP 1.1 (the version with which nearly all developers are familiar), but aims to be more efficient and more secure. The new features in HTTP/2 that are most obvious at first glance are (1) that it can multiplex many parallel requests over the same network connection and (2) that it allows full-duplex bidirectional communication. We'll learn more about\n-   **The next layer is the channel:**This is a thin abstraction over the transport. The channel defines calling conventions and implements the mapping of an RPC onto the underlying transport. At this layer, a gRPC call consists of a client-provided service name and method name, optional request metadata (key-value pairs), and zero or more request messages. A call is completed when the server provides optional response header metadata, zero or more response messages, and response trailer metadata. The trailer metadata indicates the final disposition of the call: whether it was a success or a failure. At this layer, there is no knowledge of interface constraints, data types, or message encoding. A message is just a sequence of zero or more bytes. A call may have any number of request and response messages.\n-   **The last layer is the stub:**The stub layer is where interface constraints and data types are defined. Does a method accept exactly one request message or a stream of request messages? What kind of data is in each response message and how is it encoded? The answers to these questions are provided by the stub. The stub marries the IDL-defined interfaces to a channel. The stub code is generated from the IDL. The channel layer provides the ABI that these generated stubs use.\n**grpc-web**\n\ngRPC-Web provides a Javascript library that lets browser clients access a gRPC service.\n<https://www.npmjs.com/package/grpc-web>\n\n[**https://blog.envoyproxy.io/envoy-and-grpc-web-a-fresh-new-alternative-to-rest-6504ce7eb880**](https://blog.envoyproxy.io/envoy-and-grpc-web-a-fresh-new-alternative-to-rest-6504ce7eb880)\n**Alternatives for protocol buffer for grpc**\n-   Google flatbuffers\n-   Microsoft bond\n**Alternatives**\n\n1.  Rsocket\n**Getting Started**\n\n<https://www.semantics3.com/blog/a-simplified-guide-to-grpc-in-python-6c4e25f0c506>\n\n## POC**\n-   load balancing (gRPC will not work with Kubernetes service load balancing out of the box)\n-   Istio sidecar proxy load balancing of grpc services\n-   working in Kubernetes (using services/multiple pods)\n-   proxy\n-   authentication\n-   packaging and using proto files in other projects\n-   sharing client side and server side stubs and servicers\n-   responses and error handling\n-   How is HTTP/2 support over our network and applications\n-   **Security - Keys, pem files, secure connection**\n-   What if streaming request from client fails in between (like when sending sms as stream)\n-   Load Testing server (number of rps)\n-   onboarding for other devs\n-   Sharing of proto files\n**References**\n\n<https://en.wikipedia.org/wiki/GRPC>\n\n<https://grpc.io/docs/guides>\n\n<https://alexandreesl.com/tag/grpc>\n\n<https://medium.com/@EdgePress/is-grpc-the-future-of-client-server-communication-b112acf9f365>\n\nOnline book - Practical gRPC\n\n<https://bleedingedgepress.com/practical-grpc>\n\n<https://www.semantics3.com/blog/a-simplified-guide-to-grpc-in-python-6c4e25f0c506>\n<https://dev.to/techschoolguru/is-grpc-better-than-rest-where-to-use-it-3blg>\n\n<https://grpc.io/blog/optimizing-grpc-part-1>\n\n<https://github.com/grpc-ecosystem/awesome-grpc>\n\n<https://www.udemy.com/course/protocol-buffers>\n\n[The Story of Why We Migrate to gRPC and How We Go About It - Matthias GrÃ¼ter, Spotify](https://www.youtube.com/watch?v=fMq3IpPE3TU)\n[**The complete gRPC course [Protobuf, Go, Java]**](https://www.youtube.com/playlist?list=PLy_6D98if3UJd5hxWNfAqKMr15HZqFnqf)\n\n[gRPC Crash Course - Modes, Examples, Pros & Cons and more](https://www.youtube.com/watch?v=Yw4rkaTc0f8)\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/AMQP/","title":"AMQP"},"frontmatter":{"draft":false},"rawBody":"# AMQP\n\nCreated: 2018-04-24 11:34:42 +0500\n\nModified: 2019-12-22 16:48:01 +0500\n\n---\n\n**Advanced Message Queuing Protocol (AMQP)**\n-   open standard application layer protocol\n-   Features -\n\n    1.  Reliability\n\n    2.  Interoperability\n\n    3.  Message orientation\n\n    4.  Reliable queuing\n\n    5.  Routing/Message delivery (including point-to-point, topic based publish-and-subscribe, trasactions and store-and-forward)\n\n    6.  Security-   AMQP exchanges route messages directly - in fanout form, by topic, and also based on headers.\nAMQP is an open standard for enterprise messaging, designed to support messaging for almost any distributed or business application. It works like instant messaging or email, and the difference towards these available solutions is that AMQP comprises both a network protocol, which speciï¬es the entities (producer/consumer, broker) to interoperate with each other, and a protocol model, which speciï¬es the representation of messages, and the commands to interoperate among the entities. Furthermore, AMQP messages are self-contained, and data content in a message is opaque and immutable. Also, there is no limit for the size of a message; it can either support a 4 GByte message or a 4 KByte one, in any case ensuring security, reliability, and performance.\nAMQP mandates the behavior of the messaging provider and client to the extent that implementations from different vendors are interoperable, in the same way as SMTP, HTTP, FTP, etc. have created interoperable systems. Previous standardizations of[middleware](https://en.wikipedia.org/wiki/Middleware)have happened at the API level (e.g.[JMS](https://en.wikipedia.org/wiki/Java_Message_Service)) and were focused on standardizing programmer interaction with different middleware implementations, rather than on providing interoperability between multiple implementations.Unlike JMS, which defines an API and a set of behaviors that a messaging implementation must provide, AMQP is a[wire-level protocol](https://en.wikipedia.org/wiki/Wire_protocol). A wire-level protocol is a description of the format of the data that is sent across the network as a stream of[bytes](https://en.wikipedia.org/wiki/Octet_(computing)). Consequently, any tool that can create and interpret messages that conform to this data format can interoperate with any other compliant tool irrespective of implementation language.\nUsage -\n-   Aadhaar Project - 1.2 billion identities\n-   Ocean Observatories Initiative - 8 terabytes of data per day\n**See also**\n-   Message Oriented Architecture (MOM)\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/DNS-Domain-Name-System/","title":"DNS Domain Name System"},"frontmatter":{"draft":false},"rawBody":"# DNS Domain Name System\n\nCreated: 2019-11-13 21:43:58 +0500\n\nModified: 2022-03-08 21:55:31 +0500\n\n---\n\n<https://dnschecker.org>\nDNS stands for domain name system. It is an application layer protocol used to provide a human-friendly naming mechanism for internet resources. It is what ties a domain name to an IP address and allows you to access sites by name in your browser.\nDNS is the directory of the Internet. Whenever you click on a link, send an email, open a mobile app, often one of the first things that has to happen is your device needs to look up the address of a domain.\n**There are two sides of the DNS network:**\n\n1.  **Authoritative (the content side)**\n\nEvery domain needs to have an Authoritative DNS provider.\n\n2.  **Resolver (the consumer side)**\n\nOn the other side of the DNS system are resolvers. Every device that connects to the Internet needs a DNS resolver.\nCloudfare DNS = 1.1.1.1\n\n<https://blog.cloudflare.com/announcing-1111>\n![](media/DNS-Domain-Name-System-image1.png)\nYoutube - [What is DNS? - Introduction to Domain Name System](https://www.youtube.com/watch?v=e2xLV7pCOLI)\n**Recursive Nameserver**\n\nA recursive nameserver is different because if it does not know the ip address for the asked dns name. It will do the work of finding the answer, instead of merely re-directing the query.\nThe recursive nameserver will check its cache first. If the IP address is not there, it will ask a root nameserver (root nameservers do not know IP addresses, but they can read requests and tell the recursive nameserver where to go next). All recursive nameservers come with 13 root nameservers' IP addresses pre-configured. The recursive nameserver picks one and asks it the same question (\"what is the IP address for [www.google.com](http://www.google.com)?\").\nThe root nameserver reads the top-level domain (the end of the request), in this case .com, ([www.google.com](http://www.google.com)) and will tell the recursive nameserver to ask the Global Top Level Domain Servers (GTLD). GTLDs are essentially reference lists for each type of domainâ€Š---â€Š.com, .net., .edu, etc. While they don't know the IP addresses for websites, they do know which nameservers will have that information.\nThe GTLD nameserver will read the next part of your request, reading from right to left (in this case the 'google' of[www.google.com](http://www.google.com))and will send back a message with the authoritative nameserver to contact. An authoritative nameserver is a nameserver that is responsible for the domain (and is the primary source of information).\n<https://www.freecodecamp.org/news/what-is-dns-anyway>\n\n## Fully Qualified Domain Name**\n\nA**fully qualified domain name**(**FQDN**), sometimes also referred to as an*absolute domain name*,is a[domain name](https://en.wikipedia.org/wiki/Domain_name)that specifies its exact location in the tree hierarchy of the[Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System)(DNS). It specifies all domain levels, including at least a[second-level domain](https://en.wikipedia.org/wiki/Second-level_domain)and a[top-level domain](https://en.wikipedia.org/wiki/Top-level_domain).A fully qualified domain name is distinguished by its lack of ambiguity: it can be interpreted only in one way.\n**DNS Lookup**\n**Defining DNS Records**\n\nTYPE\n\nA, AAAA, CNAME, ALIAS, ANAME, TXT, MX\nHOST\n\nThe root (@ or blank) or subdomain (www, app, blog, etc) where you want to place the record\nVALUE\n\nCan be an IP address (A, AAAA) another domain (CNAME, ALIAS, ANAME, MX) or arbitrary value (TXT)\nPRIORITY\n\nOnly for MX records you will be given what value and priority to use by your email provider\nTTL (Time to Live)\n\nHow long to let record values be cached Shorter = better for fast changing values Longer = faster resolution time and less traffic for your DNS server\n**Most Common Types**\n\n**A**\n\nMap domain name to IPv4 address. Ex: example.com => 127.0.0.1\n**AAAA**\n\nMap domain name to IPv6 address. Ex: example.com => ::1\n**CNAME**\n\nMap domain name to another domain name. CAUTION! Don't do this on the root (@). Ex: [www.example.com](http://www.example.com) => example.com\n**ALIAS**\n\nMap domain name to another domain name CAN do this on the root. Ex: example.com => example.herokudns.com\n**ANAME**\n\nAnother name for ALIAS (different providers name it differently; also \"virtual CNAME\") Ex: example.com => example.netlify.com\n**TXT**\n\nSet arbitrary data on your domain record Ex: @ => my-domain-is-awesome-123\n**MX**\n\nSetup custom email for your domain Ex: @ => ASPMX.L.GOOGLE.COM. 1\n**SOA**\n\nA**Start of Authority record**(abbreviated as**SOA record**) is a type of[resource record](https://en.wikipedia.org/wiki/Resource_record)in the[Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System)(DNS) containing administrative information about the zone, especially regarding[zone transfers](https://en.wikipedia.org/wiki/DNS_zone_transfer). The SOA record format is specified in[RFC 1035](https://tools.ietf.org/html/rfc1035)\n\n<https://en.wikipedia.org/wiki/SOA_record>\n\n## SRV**\n\nA**Service record**(**SRV record**) is a specification of data in the[Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System)defining the location, i.e. the[hostname](https://en.wikipedia.org/wiki/Hostname)and[port number](https://en.wikipedia.org/wiki/Port_number), of servers for specified services. It is defined in[RFC 2782](https://tools.ietf.org/html/rfc2782), and its type code is 33. Some Internet protocols such as the[Session Initiation Protocol](https://en.wikipedia.org/wiki/Session_Initiation_Protocol)(SIP) and the[Extensible Messaging and Presence Protocol](https://en.wikipedia.org/wiki/Extensible_Messaging_and_Presence_Protocol)(XMPP) often require SRV support by network elements.\n| **Commonly used record types** | **Description**             | **Examples**                        |\n|------------------------|-----------------------|--------------------------|\n| A                              | Host address                | host -t a nixcraft.com              |\n| AAAA                           | IPv6 host address           | host -t aaaa nixcraft.com           |\n| ALIAS                          | Auto resolved alias         | host -t alias portal2.cyberciti.biz |\n| CNAME                          | Canonical name for an alias | host -t cname feeds.cyberciti.biz   |\n| MX                             | Mail eXchange               | host -t mx google.com               |\n| NS                             | Name Server                 | host -t ns nixcraft.com             |\n| PTR                            | Pointer                     | host -t ptr 216.58.200.147          |\n| SOA                            | Start Of Authority          | host -t soa nixcraft.com            |\n| SRV                            | Location of service         | host -t srv server1.cyberciti.biz   |\n| TXT                            | Descriptive text            | host -t txt nixcraft.com            |\n<https://dev.to/chrisachard/dns-record-crash-course-for-web-developers-35hn>\n\n## Dissecting a domain name (URL - Uniform Resource Locator)**\n\n![](media/DNS-Domain-Name-System-image2.png)\n![](media/DNS-Domain-Name-System-image3.png)\n**Domain name hierarchy**\n\nRoot Name Server (13)\n\nTop Level Domain\n\nAuthoritative Name Servers\n\nDNS Zone Files (collection of related dns names)\n\nResolver / Recursive DNS\n\n![Do the com. Resolvers Know? amazon.com. reinvent- 2017.com. \".\" (The Roots) org. wikipedia.org. kernel.org. ](media/DNS-Domain-Name-System-image4.png)\n![Internet Corporation for Assigned Names and Numbers Allocates IP addresses to Regional Internet Registries (RIRs) ICANN RIPS: Allocate IP addresses to Network Providers in their respective regions â€¢ARIN (Canada, United States, some Caribbean nations) â€¢RIPE NCC (Europe, Russia, Middle East, Central Asia) â€¢APNIC (Asia-Pacific region) â€¢LACNIC (Latin America, some Caribbean nations) .AFRINIC (Africa) Network Providers (Verizon, CenturyLink) hand out IP Addresses to their customers End Customer can provide IP address for their servers ](media/DNS-Domain-Name-System-image5.png)\n![Contracts with Domain Name Registries Domain Name Registries Maintain the database of Domain Names registered within their respective TLDs (e.g. The registry for .com is Verisign) Accredits Domain Name Registrars ICANN Domain Name Reqistrars Registers new domain names on behalf of applicants. Checks with Peqistrv that the name is available Domain Name Reqistrant Requests (Buys) a domain name from the Registrar Informs the Registrar the names and IP address of their Authoritative DNS Name Server ](media/DNS-Domain-Name-System-image6.png)[https://ns1.com/academy#](https://ns1.com/academy)\n**Punycode**\n\n**Punycode**is a representation of[Unicode](https://en.wikipedia.org/wiki/Unicode)with the limited[ASCII](https://en.wikipedia.org/wiki/ASCII)character subset used for Internet[host names](https://en.wikipedia.org/wiki/Host_(network)). Using Punycode, host names containing Unicode characters are transcoded to a subset of ASCII consisting of letters, digits, and hyphen, which is called the Letter-Digit-Hyphen (LDH) subset. For example,*MÃ¼nchen*([German](https://en.wikipedia.org/wiki/German_language)name for[Munich](https://en.wikipedia.org/wiki/Munich)) is encoded as*Mnchen-3ya*.\nWhile the[Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System)(DNS) technically supports arbitrary sequences of octets in domain name labels, the DNS standards recommend the use of the LDH subset of ASCII conventionally used for host names, and require that string comparisons between DNS domain names should be case-insensitive. The Punycode syntax is a method of encoding strings containing Unicode characters, such as[internationalized domain names](https://en.wikipedia.org/wiki/Internationalized_domain_name)(IDNA), into the LDH subset of ASCII favored by DNS. It is specified in[IETF](https://en.wikipedia.org/wiki/IETF)[Request for Comments](https://en.wikipedia.org/wiki/Request_for_Comments)3492.\n<https://en.wikipedia.org/wiki/Punycode>\n\n## mDNS (multicast DNS)**\n\nIn[computer networking](https://en.wikipedia.org/wiki/Computer_networking), themulticast DNS(mDNS) protocol resolves[hostnames](https://en.wikipedia.org/wiki/Hostname)to IP addresses within small networks that do not include a local[name server](https://en.wikipedia.org/wiki/Name_server). It is a[zero-configuration](https://en.wikipedia.org/wiki/Zero_configuration_networking)service, using essentially the same programming interfaces, packet formats and operating semantics as the unicast[Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System)(DNS). Although[Stuart Cheshire](https://en.wikipedia.org/wiki/Stuart_Cheshire)designed mDNS as a stand-alone protocol, it can work in concert with standard DNS servers.\nThe mDNS protocol is published as[RFC](https://en.wikipedia.org/wiki/Request_for_Comments)[6762](https://tools.ietf.org/html/rfc6762), uses IP multicast[User Datagram Protocol](https://en.wikipedia.org/wiki/User_Datagram_Protocol)(UDP) packets, and is implemented by the Apple[Bonjour](https://en.wikipedia.org/wiki/Bonjour_(software))and open source[Avahi](https://en.wikipedia.org/wiki/Avahi_(software))software packages.[Android](https://en.wikipedia.org/wiki/Android_(operating_system))contains an mDNS implementation.mDNS has also been implemented in[Windows 10](https://en.wikipedia.org/wiki/Windows_10), initially limited to discovering networked printers, later becoming capable of resolving hostnames as well.\nmDNS can work in conjunction with[DNS Service Discovery](https://en.wikipedia.org/wiki/DNS_Service_Discovery)(DNS-SD), a companion zero-configuration technique specified separately in[RFC 6763](https://tools.ietf.org/html/rfc6763).[[4]](https://en.wikipedia.org/wiki/Multicast_DNS#cite_note-rfc6763-4)\n<https://en.wikipedia.org/wiki/Multicast_DNS>\n\n## What is DNS encryption?**\n\nDNS encryption ensures that only you and your DNS provider know what DNS queries are being performed, and therefore which websites you are visiting. Also, it enforces your choice of DNS provider. So you can choose a provider that offers features and guarantees which differ from your ISP, such as speed and privacy.\n**Is Cloudflare DNS-encrypted?**\n\nCloudflare DNS, available at 1.1.1.1, is a free public DNS service run by the CDN provider Cloudflare. It supports encryption using DNS over HTTPS (DoH) and DNS over TLS (DoT). As it uses the existing CDN, it can provide very fast response times.\n\n**DoT (DNS over TLS) and DoH (DNS over HTTPS)**\n\n<https://blog.cloudflare.com/dns-encryption-explained>\n\n## Which services can use SNI?**\n\nSNI is an extension to TLS that provides support for multiple hostnames on a single IP address. The most common use of TLS is HTTPS for secure websites. However, it is present in all protocols that use TLS for security.\n**How does SNI work?**\n\nThe TLS extension SNI works by requiring the client to transmit the hostname of the service it wishes to securely communicate with before the encryption is established. An updated version of SNI called ESNI allows this hostname to be encrypted, which protects privacy and helps to avoid censorship.\n<https://www.toptal.com/web/encrypted-safe-with-esni-doh-dot>\n\n## Global Traffic Management**\n-   **Geoproximity**\n-   **Route53**\n-   **POPs, Authoritative name servers, recursive name servers**\n-   **Sideways delegation**\n\n![Step reinvent-2017.com 600 IN NS ns-1084 awsdns-07 org. reinvent-2017.com 600 IN NS ms 1831 awsans-36 uk reinvent-2017.com 600 IN NS ns- 190 awsdns-23 com reinvent-2017.com 600 IN NS ns-634 awsdns- 15 NS com. reinvent-2017.com (DNS Provider) F:lnvent 0 2017. Amazon Web Services, Inc. or its Affiliates Alt rights reserved. In-Zone (t = 2 Days) reinvent-2017.com 172800 IN NS nsl.provider.com. reinvent-2017.com 172800 IN NS ns2.provider.com. reinvent-2017.com 172800 IN NS ns3.provider.com. reinvent-2017.com 172800 IN NS ns4.provider.com. reinvent-2017.com. (Route 53) reinvent-2017.com 600 IN NS ns-1084.awsdns-07.org. reinvent-2017.com 600 IN NS ns-1831.awsdns-36.co.uk. reinvent-2017.com 600 IN NS ns-190.awsdns-23.com. reinvent-2017.com 600 IN NS ns-634.awsdns-15.net. ](media/DNS-Domain-Name-System-image7.png)\n<https://aws.amazon.com/blogs/aws/latency-based-multi-region-routing-now-available-for-aws>\n\n[AWS re:Invent 2017: DNS Demystified: Global Traffic Management with Amazon Route 53 (NET302)](https://www.youtube.com/watch?v=PVBC1gb78r8)\n![DÃ…X9{tified Global Tra fic Manageme t'Mith Amazon Route 53 Develop Xuan Shi, Software Developer, Ine. November 28, 2017 Flnvent ](media/DNS-Domain-Name-System-image8.jpg)\n**DNS Subdomain Names**\n\nMost resource types require a name that can be used as a DNS subdomain name as defined in[RFC 1123](https://tools.ietf.org/html/rfc1123). This means the name must:\n-   contain no more than 253 characters\n-   contain only lowercase alphanumeric characters, '-' or '.'\n-   start with an alphanumeric character\n-   end with an alphanumeric character\n**DNS Label Names**\n\nSome resource types require their names to follow the DNS label standard as defined in[RFC 1123](https://tools.ietf.org/html/rfc1123). This means the name must:\n-   contain at most 63 characters\n-   contain only lowercase alphanumeric characters or '-'\n-   start with an alphanumeric character\n-   end with an alphanumeric character"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/GraphQL/","title":"GraphQL"},"frontmatter":{"draft":false},"rawBody":"# GraphQL\n\nCreated: 2018-06-19 18:21:26 +0500\n\nModified: 2021-12-31 13:16:53 +0500\n\n---\n\nGraphQL will do to REST what JSON did to XML.\n\n**A Data Query Language**\nGraphQL is strongly typed, self-documenting, and embraces declarative data fetching, which allows clients to query for the exact data they need as well making it possible to build powerful and in-depth developer tools.\nGraphQL server - A server which implements the GraphQL Specifications and exposes an API endpoint for clients to query.\nGraphQL is typically used in scenarios where related data needs to be queried or in environments where multiple round trips to the server can negatively impact the customer experience.\n**Problems that GraphQL solves**\n\n1.  The need to do multiple round trips to fetch data required by a view\n\n2.  Clients dependency on servers\n\n3.  The bad front-end developer experience\n\n4.  No Versioning of APIs\n\n5.  Get only what you asked\n**Problems with GraphQL**\n\n1.  Resource Exhaustion Attacks (DOS) using overly complex queries\n\n2.  N+1 queries\n\nSolution - <https://github.com/facebook/dataloader>\n\n<https://xuorig.medium.com/the-graphql-dataloader-pattern-visualized-3064a00f319f>\n\n## GraphQL Core Concepts**\n-   Schema Definition Language (Used to define the schema of an API)\n-   Defining Types (Type system allows us to check if query is valid or not before resolving the query)\n-   Sending Queries and Mutations\n    -   Queries with Arguments (extra information sent with query and mutation)\n-   Mutations\n    -   Creating new data\n    -   Updating existing data\n    -   Deleting existing data\n-   Subscriptions\n-   Resolvers\n-   Variables\n-   Inspection\n    -   This allows us to know the schema using introspection and see documentation of all the endpoints with autocomplete in clients like Graphiql\n**Security**\n-   Timeout\n-   Maximum Query Depth\n-   Query Complexity\n-   Throttling\n**How to GraphQL**\n\nIn GraphQL, aTypeis an object that may contain multiplefields. Each field is calculated throughresolvers, that returns a value. A collection of types is called aschema. Every schema has a special type calledqueryfor getting data from the server andmutationfor sending data to the server.\n-   Type\n-   Query\n-   Mutation\n-   Resolver\nTutorial\n-   Queries\n-   Mutations\n-   Authentications\n-   Links and Voting\n-   Error Handling\n-   Filtering\n-   Pagination\n\n<https://hasura.io/blog/cursor-offset-pagination-with-hasura-graphql>\n-   Relay\n**Queries**\n\nmutation {\n\ncreateLink (\n\nurl: \"<https://www.google.com>\",\n\ndescription: \"Google\"\n\n) {\n\nid\n\nurl\n\ndescription\n\n}\n\n}\nquery {\n\nlinks {\n\nid\n\nurl\n\ndescription\n\n}\n\n}\nmutation {\n\ntokenAuth(username: \"deepak\", password: \"deepaksood\") {\n\ntoken\n\n}\n\n}\n! - bang at the end of a type represents required\n\nIn graphene-python it is written as required=True\n**Client**\n\n1.  Apollo\n\n2.  Relay\n\n3.  Graphiql\n\n4.  Insomnia\n\n5.  Postman\n**Schema stitching**\n\nSchema stitching is the process of creating a single GraphQL schema from multiple underlying GraphQL APIs.\nOne of the main benefits of GraphQL is that we can query all of our data as part of one schema, and get everything we need in one request. But as the schema grows, it might become cumbersome to manage it all as one codebase, and it starts to make sense to split it into different modules. We may also want to decompose your schema into separate microservices, which can be developed and deployed independently.\nIn both cases, we usemergeSchemasto combine multiple GraphQL schemas together and produce a merged schema that knows how to delegate parts of the query to the relevant subschemas. These subschemas can be either local to the server, or running on a remote server. They can even be services offered by 3rd parties, allowing us to connect to external data and create mashups.\n<https://www.apollographql.com/docs/graphql-tools/schema-stitching>\n\n## Apollo Federation**\n\nApollo Federation is an architecture for composing multiple GraphQL services into a single graph that addresses this need. Unlike other approaches such as schema stitching, it is based on a declarative composition programming model that allowsproper separation of concerns. This design allows teams to implement an enterprise-scale shared data graph as a set of loosely coupled, separately maintained GraphQL services.\n<https://principledgraphql.com/integrity#1-one-graph>\n\n<https://www.apollographql.com/docs/apollo-server/federation/introduction>\n\n<https://blog.apollographql.com/apollo-federation-f260cf525d21>\n\n<https://github.com/apollographql/federation-demo>\n\n## Tools**\n\n**Hasura - <https://hasura.io>**\n\n<https://github.com/hasura/graphql-engine>\n\nQuiver - <https://medium.com/@syrusakbary/quiver-graphql-on-steroids-13612ea1ea77>\n\n**GraphQL Voyager** - <https://github.com/APIs-guru/graphql-voyager>\n\n**GraphQL Mesh**\n\n<https://the-guild.dev/blog/graphql-mesh>\n\n<https://www.youtube.com/watch?v=T0zpPO7Ub_s&ab_channel=GOTOConferences>\n\n## Monitoring GraphQL APIs**\n\n<https://www.moesif.com/blog/technical/monitoring/How-to-Best-Monitor-GraphQL-APIs>\n\n<https://xuorig.medium.com/why-graphql-performance-monitoring-is-hard-41381bc7c44d>\n\n## Persisted Queries -** <https://www.apollographql.com/blog/apollo-client/persisted-graphql-queries>\n\n## References**\n\n<https://medium.freecodecamp.org/rest-apis-are-rest-in-peace-apis-long-live-graphql-d412e559d8e4>\n\n<https://www.quora.com/Is-GraphQL-a-REST-killer>\n\n<http://docs.graphene-python.org/projects/django/en/latest>\n\n<https://www.howtographql.com>\n\n<https://building.buildkite.com/tutorial-getting-started-with-graphql-queries-and-mutations-11211dfe5d64>\n\n<https://www.toptal.com/graphql/creating-your-first-graphql-api>\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/HTTP---HTTPS/","title":"HTTP / HTTPS"},"frontmatter":{"draft":false},"rawBody":"# HTTP / HTTPS\n\nCreated: 2017-11-18 19:26:42 +0500\n\nModified: 2022-06-09 17:11:27 +0500\n\n---\n\nURL - Uniform Resource Locator\n\nURI - Uniform Resource Indicator\nStanding for Hypertext Transfer Protocol, HTTP is an application layer protocol that enables communication between two entities in a network.\nHTTP is considered the foundation of the modern web and it works on top of TCP/IP communication protocol. While other ports may be used, the reserved HTTP port is 80.\nCommunication process is based on a **request/response or request/reply** flow:\n-   The client initiates the communication by sending an HTTP request message to the host, over an already existing TCP connection.\n-   After the request has been made, the client waits for a response.\n-   The host processes the request\n-   The host sends back the appropriate response.\nOne fundamental characteristic that made HTTP a powerful protocol is that both parties are aware of each other during the current request only and will forget about each other afterwards. This is the reason why HTTP is considered to be stateless.\nHTTP is a request/response architecture, where client sends a request and server sends the request back. Server cannot initiate the request to the client. So if user A has some data to send to user B, then user A will send data to server, but server cannot send data to user B, because server cannot initiate request to the client.\nThe 4 Parts of a HTTP request\n-   Method (GET)\n-   Path (/cat.png)\n-   Headers\n-   Body\nThe 3 parts of a HTTP response\n-   Response Code (like 200)\n-   Headers\n-   Body\n**Versions**\n\n1.  HTTP/0.9 (1991)\n\n2.  HTTP/1.0\n\na new TCP connection is created for each request/response exchange between clients and servers, meaning that all requests incur a latency penalty as the TCP and TLS handshakes are completed before each request.\nWorse still, rather than sending all outstanding data as fast as possible once the connection is established, TCP enforces a warm-up period called \"slow start\", which allows the TCP congestion control algorithm to determine the amount of data that can be in flight at any given moment before congestion on the network path occurs, and avoid flooding the network with packets it can't handle. But because new connections have to go through the slow start process, they can't use all of the network bandwidth available immediately.\n3.  HTTP/1.1 (1997)\n\nIntroduced the concept of \"keep-alive\" connections, that allow clients to reuse TCP connections, and thus amortize the cost of the initial connection establishment and slow start across multiple requests. But this was no silver bullet: while multiple requests could share the same connection, they still had to be serialized one after the other, so a client and server could only execute a single request/response exchange at any given time for each connection.\n4.  HTTP/2 (2015)\n\nIntroduced the concept of HTTP \"streams\": an abstraction that allows HTTP implementations to concurrently multiplex different HTTP exchanges onto the same TCP connection, allowing browsers to more efficiently reuse TCP connections.\n5.  HTTP/3 and QUIC\n**HTTP/2 Characteristics -**\n-   **Multiplexing:** Can use a single connection for multiple requests, (pipelining of requests)\n-   **Bidirectional Streaming:** No more polling, sockets, or clunky SSE (Server Side Events)\n-   **Flow Control:** Control your congestion\n-   Is binary, instead of textual,\n-   Uses **header compression** to reduce overhead-   HTTP/2 is a binary protocol where HTTP 1.x is textual. Binary protocols are more efficient to parse because there is only one code path where HTTP 1.x defines 4 different ways to parse a message. Binary protocols are also more space efficient on the wire. In return, it is more difficult for humans to deal with them, as they are not human-readable. A tradeoff.\n-   HTTP/2 is multiplexed to tackle a known limitation in networking known as head-of-line blocking (HOL Blocking). This problem can occur with HTTP 1.1 when multiple requests are issued on a single TCP connection (aka HTTP pipelining). As the entire connection is ordered and blocking (FIFO), a slow request can hold up the connection, slowing down all subsequent requests. Multiplexing definitively solve this problem by allowing several request and response to fly on the wire at the same time.\n-   HTTP/2 uses header compression to reduce overhead. Typical header sizes of 1KB are common mainly because of the cookies that we all have to accept for a smooth user experience. Transferring 1KB can take several network round trips just to exchange headers, and those headers are being re-sent every time because of the stateless nature of HTTP 1.x. The TCP Slow-start makes the problem even worse by limiting the number of packets that can be sent during the first round trips until TCP effectively finishes to probe the network to figure out the available capacity and properly adapt its congestion window. In this context, compressing headers significantly limits the number of required round trips.\n-   HTTP/2 Server Push allows servers to proactively send responses into client caches. In a typical HTTP 1.x workflow, the browser requests a page, the server sends the HTML in the response, and then needs to wait for the browser to parse the response and issue additional requests to fetch the additional embedded assets (JavaScript, CSS, etc.). Server push allows the server to speculatively start sending resources to the client. Here, the browser does not have to parse the HTML page and find out which other resources to load; instead the server can start sending them immediately.\n**Ports**\n-   HTTP - 80\n-   HTTPS - 443\n**HTTP Polling**\n-   HTTP Short Polling\n-   HTTP Long Polling\n-   HTTP Periodic Polling\n-   HTTP Streaming\n-   SSE (Server Sent Events / EventSource)\n-   HTTP/2 Server Push\n<https://medium.com/platform-engineer/web-api-design-35df8167460>\n\n## HTTPS**\n\nPrivacy, Integrity and Identification\n\n<https://howhttps.works/why-do-we-need-https>\n\n<https://howhttps.works/the-handshake>\n\n## Https vs TLS (Transport Layer Security) vs SSL (Secure Sockets Layer)**\n\nHTTP is the protocol used by your browser and web servers to communicate and exchange information. When that exchange of data is encrypted with SSL/TLS, then we call it HTTPS. The \"S\" stands for \"Secure\". SSL is the predecessor to TLS.\n<https://howhttps.works/https-ssl-tls-differences>\n\n## TLS (Tranport Layer Security)**\n\nTransport Layer Security(TLS), and its now-deprecated predecessor,Secure Sockets Layer(SSL),are[cryptographic protocols](https://en.wikipedia.org/wiki/Cryptographic_protocol)designed to provide[communications security](https://en.wikipedia.org/wiki/Communications_security)over a[computer network](https://en.wikipedia.org/wiki/Computer_network).Several versions of the protocols find widespread use in applications such as[web browsing](https://en.wikipedia.org/wiki/Web_navigation),[email](https://en.wikipedia.org/wiki/Email),[instant messaging](https://en.wikipedia.org/wiki/Instant_messaging), and[voice over IP](https://en.wikipedia.org/wiki/Voice_over_IP)(VoIP).[Websites](https://en.wikipedia.org/wiki/Website)can use TLS to secure all communications between their[servers](https://en.wikipedia.org/wiki/Server_(computing))and[web browsers](https://en.wikipedia.org/wiki/Web_browser).\nThe TLS protocol aims primarily to provide[privacy](https://en.wikipedia.org/wiki/Privacy)and[data integrity](https://en.wikipedia.org/wiki/Data_integrity)between two or more communicating computer applications.When secured by TLS, connections between a client (e.g., a web browser) and a server (e.g., wikipedia.org) should have one or more of the following properties:\n-   The connection isprivate(orsecure) because[symmetric cryptography](https://en.wikipedia.org/wiki/Symmetric-key_algorithm)is used to [encrypt](https://en.wikipedia.org/wiki/Encryption)the data transmitted. The[keys](https://en.wikipedia.org/wiki/Key_(cryptography))for this symmetric encryption are generated uniquely for each connection and are based on a[shared secret](https://en.wikipedia.org/wiki/Shared_secret)that was negotiated at the start of the[session](https://en.wikipedia.org/wiki/Session_(computer_science)). The server and client negotiate the details of which encryption algorithm and cryptographic keys to use before the first[byte](https://en.wikipedia.org/wiki/Byte)of data is transmitted. The negotiation of a shared secret is both secure (the negotiated secret is unavailable to[eavesdroppers](https://en.wikipedia.org/wiki/Eavesdropping)and cannot be obtained, even by an attacker who places themselves in the middle of the connection) and reliable (no attacker can modify the communications during the negotiation without being detected).\n-   The identity of the communicating parties can beauthenticatedusing[public-key cryptography](https://en.wikipedia.org/wiki/Public-key_cryptography). This authentication can be made optional, but is generally required for at least one of the parties (typically the server).\n-   The connection isreliablebecause each message transmitted includes a message integrity check using a[message authentication code](https://en.wikipedia.org/wiki/Message_authentication_code)to prevent undetected loss or alteration of the data during[transmission](https://en.wikipedia.org/wiki/Data_transmission).\n<https://en.wikipedia.org/wiki/Transport_Layer_Security>\n\n## ALPN (Application Layer Protocol Negotiation)**\n\n[Application-Layer](https://en.wikipedia.org/wiki/Application_layer)Protocol Negotiation(ALPN) is a[Transport Layer Security](https://en.wikipedia.org/wiki/Transport_Layer_Security)(TLS) extension that allows the application layer to negotiate which[protocol](https://en.wikipedia.org/wiki/Communications_protocol)should be performed over a secure connection in a manner that avoids additional round trips and which is independent of the application-layer protocols. It is needed by secure[HTTP/2](https://en.wikipedia.org/wiki/HTTP/2)connections, which improves the compression of web pages and reduces their latency compared to HTTP/1.x. The ALPN and HTTP/2 standards emerged from development work done by Google on the now withdrawn[SPDY](https://en.wikipedia.org/wiki/SPDY)protocol.\n\n![Client Server ClientHello (ALPN Extension + List of Protocols) ServerHello (ALPN Extension + Selected Protocol) ChangeCipherSpec Finished ChangeCipherSpec Finished ](media/HTTP---HTTPS-image1.png)\n\n<https://en.wikipedia.org/wiki/Application-Layer_Protocol_Negotiation>\n\n## SOP - Same Origin Policy**\n\nThesame-origin policyis a critical security mechanism that restricts how a document or script loaded from one[origin](https://developer.mozilla.org/en-US/docs/Glossary/origin)can interact with a resource from another origin.It helps isolate potentially malicious documents, reducing possible attack vectors.\n<https://developer.mozilla.org/en-US/docs/Web/Security/Same-origin_policy>\n\n**CORS - Cross-Origin Resource Sharing**\n\nCross-Origin Resource Sharing (CORS) is a mechanism that uses additional HTTP headers to tell a browser to let a web application running at one origin (domain) have permission to access selected resources from a server at a different origin. A web application makes a cross-origin HTTP request when it requests a resource that has a different origin (domain, protocol, and port) than its own origin.\nA mechanism that allows restricted resources (e.g. fonts) on a web page to be requested from another domain outside the domain from which the first resource was served.\n<https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS>\n\n[Cross Origin Resource Sharing (Explained by Example)](https://www.youtube.com/watch?v=Ka8vG5miErk)\n**Cookies**\n\nAnHTTP cookie(also calledweb cookie,Internet cookie,browser cookie, or simplycookie) is a small piece of data sent from a[website](https://en.wikipedia.org/wiki/Website)and stored on the user's computer by the user's[web browser](https://en.wikipedia.org/wiki/Web_browser)while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember [stateful](https://en.wikipedia.org/wiki/Program_state) information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons,[logging in](https://en.wikipedia.org/wiki/Access_control), or recording which pages were visited in the past). They can also be used to remember pieces of information that the user previously entered into form fields, such as names, addresses, passwords, and credit-card numbers.\n<https://en.wikipedia.org/wiki/HTTP_cookie>\n\n[HTTP Cookies Crash Course](https://www.youtube.com/watch?v=sovAIX4doOE)\n**Zombie Cookie**\n\n[How Un-deletable Zombie Cookies work (with implementation example)](https://www.youtube.com/watch?v=lq6ZimHh-j4)\n**XHR** (XMLHttpRequest)\n\nXMLHttpRequest(XHR) is an[API](https://en.wikipedia.org/wiki/Application_programming_interface)in the form of an[object](https://en.wikipedia.org/wiki/Object-oriented_programming)whose[methods](https://en.wikipedia.org/wiki/Method_(computer_programming))transfer data between a[web browser](https://en.wikipedia.org/wiki/Web_browser)and a[web server](https://en.wikipedia.org/wiki/Web_server). The object is provided by the browser's[JavaScript](https://en.wikipedia.org/wiki/JavaScript)environment. Particularly, retrieval of data from XHR for the purpose of continually modifying a loaded[web page](https://en.wikipedia.org/wiki/Web_page)is the underlying concept of[Ajax](https://en.wikipedia.org/wiki/Ajax_(programming))design. Despite the name, XHR can be used with protocols other than[HTTP](https://en.wikipedia.org/wiki/HTTP)and data can be in the form of not only[XML](https://en.wikipedia.org/wiki/XML),but also[JSON](https://en.wikipedia.org/wiki/JSON),[HTML](https://en.wikipedia.org/wiki/HTML)or[plain text](https://en.wikipedia.org/wiki/Plain_text).\n<https://en.wikipedia.org/wiki/XMLHttpRequest>\nWhich part of the HTTP response determines whether the browser should redirect to another page?\n-   The response code - If the response code is 301 or 302, your browser will redirect to the URL in the **Location** header.\nWhat part of the HTTP request does your browser use to keep you logged into a website?\n-   Headers - The browser will send **Cookie** headers with whatever the website set your cookies to when you logged in. That's what keeps you logged in.\nWhen you submit a form, what request method does your browser use?\n-   The default is POST, but you can use < form method = \"GET\" > to send a GET request\n-   POST puts the form data in the request body\n-   GET puts the data in the URL.\nHTTP2.0 Demo - <http://www.http2demo.io>\n\n<https://www.code-maze.com/http-series-part-1>\n\n[The Journey of an HTTP request to the Backend | Backend Engineering Show](https://youtu.be/K2qV6VpfR7I)\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/HTTP-3-QUIC/","title":"HTTP/3 QUIC"},"frontmatter":{"draft":false},"rawBody":"# HTTP/3 QUIC\n\nCreated: 2019-10-06 13:58:23 +0500\n\nModified: 2019-11-06 00:21:35 +0500\n\n---\n\nQUIC - Quick UDP Internet Connections\nHTTP/3 don't use TCP as the transport layer for the session, it uses[QUIC, a new Internet transport protocol](https://blog.cloudflare.com/the-road-to-quic/), which, among other things, introduces streams as first-class citizens at the transport layer. QUIC streams share the same QUIC connection, so no additional handshakes and slow starts are required to create new ones, but QUIC streams are delivered independently such that in most cases packet loss affecting one stream doesn't affect others. This is possible because QUIC packets are encapsulated on top of UDP datagrams.\nUsing UDP allows much more flexibility compared to TCP, and enables QUIC implementations to live fully in user-space --- updates to the protocol's implementations are not tied to operating systems updates as is the case with TCP. With QUIC, HTTP-level streams can be simply mapped on top of QUIC streams to get all the benefits of HTTP/2 without the head-of-line blocking.\nQUIC also combines the typical 3-way TCP handshake with[TLS 1.3](https://blog.cloudflare.com/rfc-8446-aka-tls-1-3/)'s handshake. Combining these steps means that encryption and authentication are provided by default, and also enables faster connection establishment. In other words, even when a new QUIC connection is required for the initial request in an HTTP session, the latency incurred before data starts flowing is lower than that of TCP with TLS.\n\n![](media/HTTP-3-QUIC-image1.png)\n\nBut why not just use HTTP/2 on top of QUIC, instead of creating a whole new HTTP revision? After all, HTTP/2 also offers the stream multiplexing feature. As it turns out, it's somewhat more complicated than that.\nWhile it's true that some of the HTTP/2 features can be mapped on top of QUIC very easily, that's not true for all of them. One in particular,[HTTP/2's header compression scheme called HPACK](https://blog.cloudflare.com/hpack-the-silent-killer-feature-of-http-2/), heavily depends on the order in which different HTTP requests and responses are delivered to the endpoints. QUIC enforces delivery order of bytes within single streams, but does not guarantee ordering among different streams.\nThis behavior required the creation of a new HTTP header compression scheme, called QPACK, which fixes the problem but requires changes to the HTTP mapping. In addition, some of the features offered by HTTP/2 (like per-stream flow control) are already offered by QUIC itself, so they were dropped from HTTP/3 in order to remove unnecessary complexity from the protocol.\nQUIC Features\n-   All QUIC connections are fully encrypted. (End to end encryption)\n-   forward-error correction (FEC)\n\nWhen NASA's Deep Space Network talks to the Voyager 2 spacecraft (which recently left our solar system) it transmits messages that become garbled crossing 17.6 billion km of space (that's about 11 billion miles). Voyager 2 can't send back the equivalent of \"Say again?\" when it receives a garbled message so the messages sent to Voyager 2 contain error-correcting codes that allow it to reconstruct the message from the mess.\n-   QUIC also solves the HTTP/2 HoL (Head of Line Blocking) problem\n-   Finally, one of the slower parts of a standard HTTP/2 over TCP connection is the very beginning. When the app or browser makes a connection there's an initial handshake at the TCP level followed by a handshake to establish encryption. Over a high latency connection (say on a mobile phone on 3G) that creates a noticeable delay. Since QUIC controls all aspects of the connect it merges together connection and encryption into a single handshake.\nOther features -\n\n1.  connection migration\n\n2.  zero round trip time resumption\n<https://blog.cloudflare.com/http3-the-past-present-and-future>\n\n<https://blog.cloudflare.com/the-road-to-quic>\n<https://www.mnot.net/blog/2018/11/27/header_compression>\n\n<https://www.mnot.net/blog/2019/10/13/h2_api_multiplexing>\nHTTP Version 2 - <https://httpwg.org/specs/rfc7540.html>\n\nHPACK: Header Compression for HTTP/2 - <https://httpwg.org/specs/rfc7541.html>\n<https://www.toptal.com/web/performance-working-with-http-3>\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/Http-Status-Code/","title":"Http Status Code"},"frontmatter":{"draft":false},"rawBody":"# Http Status Code\n\nCreated: 2017-10-04 19:16:27 +0500\n\nModified: 2020-11-20 21:12:35 +0500\n\n---\n\n1xx - Informational\n\n2xx - Success\n\n3xx - Redirection\n\n4xx - Client Error\n\n5xx - Server Error-   **1Ã—Ã— Informational**\n    -   [100Continue](https://httpstatuses.com/100)\n    -   [101Switching Protocols](https://httpstatuses.com/101)\n    -   [102Processing](https://httpstatuses.com/102)-   **2Ã—Ã— Success**\n    -   [200OK](https://httpstatuses.com/200)\n    -   [201Created](https://httpstatuses.com/201)\n    -   [202Accepted](https://httpstatuses.com/202)\n    -   [203Non-authoritative Information](https://httpstatuses.com/203)\n    -   [204No Content](https://httpstatuses.com/204)\n    -   [205Reset Content](https://httpstatuses.com/205)\n    -   [206Partial Content](https://httpstatuses.com/206)\n    -   [207Multi-Status](https://httpstatuses.com/207)\n    -   [208Already Reported](https://httpstatuses.com/208)\n    -   [226IM Used](https://httpstatuses.com/226)-   **3Ã—Ã— Redirection**\n    -   [300Multiple Choices](https://httpstatuses.com/300)\n    -   [301Moved Permanently](https://httpstatuses.com/301)\n    -   [302Found](https://httpstatuses.com/302)\n    -   [303See Other](https://httpstatuses.com/303)\n    -   [304Not Modified](https://httpstatuses.com/304)\n    -   [305Use Proxy](https://httpstatuses.com/305)\n    -   [307Temporary Redirect](https://httpstatuses.com/307)\n    -   [308Permanent Redirect](https://httpstatuses.com/308)-   **4Ã—Ã— Client Error**\n    -   [400Bad Request](https://httpstatuses.com/400)\n    -   [401Unauthorized](https://httpstatuses.com/401)\n    -   [402Payment Required](https://httpstatuses.com/402)\n    -   [403Forbidden](https://httpstatuses.com/403)\n    -   [404Not Found](https://httpstatuses.com/404)\n    -   [405Method Not Allowed](https://httpstatuses.com/405)\n    -   [406Not Acceptable](https://httpstatuses.com/406)\n    -   [407Proxy Authentication Required](https://httpstatuses.com/407)\n    -   [408Request Timeout](https://httpstatuses.com/408)\n    -   [409Conflict](https://httpstatuses.com/409)\n    -   [410Gone](https://httpstatuses.com/410)\n    -   [411Length Required](https://httpstatuses.com/411)\n    -   [412Precondition Failed](https://httpstatuses.com/412)\n    -   [413Payload Too Large](https://httpstatuses.com/413)\n    -   [414Request-URI Too Long](https://httpstatuses.com/414)\n\nCan happen when GET requests length is long\n\nCan be solved by increasing the size of URI that server can accept\n-   [415Unsupported Media Type](https://httpstatuses.com/415)\n-   [416Requested Range Not Satisfiable](https://httpstatuses.com/416)\n-   [417Expectation Failed](https://httpstatuses.com/417)\n-   [418I'm a teapot](https://httpstatuses.com/418)\n-   [421Misdirected Request](https://httpstatuses.com/421)\n-   [422Unprocessable Entity](https://httpstatuses.com/422)\n-   [423Locked](https://httpstatuses.com/423)\n-   [424Failed Dependency](https://httpstatuses.com/424)\n-   [426Upgrade Required](https://httpstatuses.com/426)\n-   [428Precondition Required](https://httpstatuses.com/428)\n-   [429Too Many Requests](https://httpstatuses.com/429)\n-   [431Request Header Fields Too Large](https://httpstatuses.com/431)\n-   [444Connection Closed Without Response](https://httpstatuses.com/444)\n-   [451Unavailable For Legal Reasons](https://httpstatuses.com/451)\n-   [499Client Closed Request](https://httpstatuses.com/499)\n\nHTTP 499 in Nginx means that the**client closed the connection**before the server answered the request.-   **5Ã—Ã— Server Error**\n    -   [500Internal Server Error](https://httpstatuses.com/500)\n    -   [501Not Implemented](https://httpstatuses.com/501)\n    -   [502Bad Gateway](https://httpstatuses.com/502)\n    -   [503Service Unavailable](https://httpstatuses.com/503)\n    -   [504Gateway Timeout](https://httpstatuses.com/504)\n    -   [505HTTP Version Not Supported](https://httpstatuses.com/505)\n    -   [506Variant Also Negotiates](https://httpstatuses.com/506)\n    -   [507Insufficient Storage](https://httpstatuses.com/507)\n    -   [508Loop Detected](https://httpstatuses.com/508)\n    -   [510Not Extended](https://httpstatuses.com/510)\n    -   [511Network Authentication Required](https://httpstatuses.com/511)\n    -   [599Network Connect Timeout Error](https://httpstatuses.com/599)\n<https://httpstatuses.com>\n\n<https://dev.to/khaosdoctor/the-complete-guide-to-status-codes-for-meaningful-rest-apis-conclusion-2pf8>\n\n## Most Important Error Codes**\n\n| **Error Code** | **Meaning**                                                                                 |\n|-----------|-------------------------------------------------------------|\n| 400            | Bad Request -- Your request is invalid.                                                    |\n| 401            | Unauthorized -- Your API key is wrong.                                                     |\n| 403            | Forbidden -- The kitten requested is hidden for administrators only.                       |\n| 404            | Not Found -- The specified kitten could not be found.                                      |\n| 405            | Method Not Allowed -- You tried to access a kitten with an invalid method.                 |\n| 406            | Not Acceptable -- You requested a format that isn't json.                                 |\n| 410            | Gone -- The kitten requested has been removed from our servers.                            |\n| 418            | I'm a teapot.                                                                              |\n| 429            | Too Many Requests -- You're requesting too many kittens! Slow down!                       |\n| 500            | Internal Server Error -- We had a problem with our server. Try again later.                |\n| 503            | Service Unavailable -- We're temporarily offline for maintenance. Please try again later. |"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/Messaging/","title":"Messaging"},"frontmatter":{"draft":false},"rawBody":"# Messaging\n\nCreated: 2018-03-26 21:35:41 +0500\n\nModified: 2019-02-18 16:28:36 +0500\n\n---\n\nFor Messaging consider it like email, not between people but between different apps on different machines. A message is typically some container-like format with some extra metadata naming the sender and the recipent(s), maybe timestamps and serial numbers. All you can do in a messaging system is basically to send a message to a particular address. Whether or when the resident at that address responds is not possible to determine -- just like email in that sense. For a large scale example of a messaging system we have the internet itself. The very much hyped REST interactions of online services is also an example where messaging is starting to show success.\n1.  **Messaging Systems**\n\n    a.  HTTP and Email\n\n    b.  REST(ful) web services\n\n    c.  Dbus\n\n    d.  Protocol Buffers\n\n    e.  GraphQL\n**AMQP vs MQTT**\n\nEssentially both AMQP (speaking for the 0.9.1 version) and MQTT implement a publish/subscribe messaging pattern. Both protocols rely on intermediate queuing/store-and-forward techniques to reduce or eliminate message loss in case a client loses the connection to the broker. The main conceptual difference is that in MQTT one client (identified by a 'unique' client id), has one TCP connection to the broker and only a single \"queue\". As a consequence, even if a client has multiple subscriptions all messages end up in the same queue. In contrast, with AMQP a queue is a resource on the broker and is decoupled from the client, multiple clients can consume the same queue e.g. for load balancing purposes. So a client can create many queues and decide if and when to consume messages. Moreover, a AMQP connection, which is just a TCP connection, is multiplexed via logical channels, enabling the development of highly performant consumers and publishers.\n**See also**\n-   Message Oriented Architecture (MOM)\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/OpenAPI/","title":"OpenAPI"},"frontmatter":{"draft":false},"rawBody":"# OpenAPI\n\nCreated: 2020-04-08 21:37:47 +0500\n\nModified: 2022-02-05 00:21:12 +0500\n\n---\n\n1.  [What Is OpenAPI?](https://swagger.io/docs/specification/about/)\n\nAn**open API**(often referred to as a public API) is a publicly available[application programming interface](https://en.wikipedia.org/wiki/Application_programming_interface)that provides developers with programmatic access to a proprietary software application or web service.-   Writing API Specification\n-   API specifications can be written in YAML or JSON\nSwaggeris a set of open-source tools built around the OpenAPI Specification that can help you design, build, document and consume REST APIs. The major Swagger tools include:\n-   [Swagger Editor](http://editor.swagger.io/)-- browser-based editor where you can write OpenAPI specs.\n-   [Swagger UI](https://swagger.io/swagger-ui/)-- renders OpenAPI specs as interactive API documentation.\n-   [Swagger Codegen](https://github.com/swagger-api/swagger-codegen)-- generates server stubs and client libraries from an OpenAPI spec.\n2.  [Basic Structure](https://swagger.io/docs/specification/basic-structure/)\n\n3.  [API Server and Base Path](https://swagger.io/docs/specification/api-host-and-base-path/)\n\n4.  [Media Types](https://swagger.io/docs/specification/media-types/)\n\n5.  [Paths and Operations](https://swagger.io/docs/specification/paths-and-operations/)\n\n6.  [Describing Parameters](https://swagger.io/docs/specification/describing-parameters/)\n\n7.  [Parameter Serialization](https://swagger.io/docs/specification/serialization/)\n\n8.  [Describing Request Body](https://swagger.io/docs/specification/describing-request-body/)\n\n9.  [Describing Responses](https://swagger.io/docs/specification/describing-responses/)\n\n10. [Data Models (Schemas)](https://swagger.io/docs/specification/data-models/)\n\n11. [Adding Examples](https://swagger.io/docs/specification/adding-examples/)\n\n12. [Authentication](https://swagger.io/docs/specification/authentication/)\n\n    a.  Basic\n\n    b.  Bearer\n\n    c.  other HTTP schemes as defined by[RFC 7235](https://tools.ietf.org/html/rfc7235)and[HTTP Authentication Scheme Registry](https://www.iana.org/assignments/http-authschemes/http-authschemes.xhtml)\n\n    d.  [API keys](https://swagger.io/docs/specification/authentication/api-keys/)in headers, query string or cookies\n\n    e.  Cookie Authentication\n\n    f.  OAuth 2.0\n\n    g.  OpenID Connect Discovery\n\n13. [Links](https://swagger.io/docs/specification/links/)\n\n14. [Callbacks](https://swagger.io/docs/specification/callbacks/)\n\n15. [Components Section](https://swagger.io/docs/specification/components/)\n\n16. [Using $ref](https://swagger.io/docs/specification/using-ref/)\n\n17. [API General Info](https://swagger.io/docs/specification/api-general-info/)\n\n18. [Grouping Operations With Tags](https://swagger.io/docs/specification/grouping-operations-with-tags/)\n\n19. [OpenAPI Extensions](https://swagger.io/docs/specification/openapi-extensions/)\nOpenAPI has a way to define multiple security \"schemes\"\n-   **apiKey:** an application specific key that can come from:\n    -   A query parameter\n    -   A header\n    -   A cookie\n-   **http:** standard HTTP authentication systems, including:\n    -   bearer: a headerAuthorizationwith a value ofBearerplus a token. This is inherited from OAuth2\n    -   HTTP Basic authentication\n    -   HTTP Digest, etc\n-   **oauth2:** all the OAuth2 ways to handle security (called \"flows\")\n    -   Several of these flows are appropriate for building an OAuth 2.0 authentication provider (like Google, Facebook, Twitter, GitHub, etc):\n        -   implicit\n        -   clientCredentials\n        -   authorizationCode\n    -   But there is one specific \"flow\" that can be perfectly used for handling authentication in the same application directly:\n        -   password: some next chapters will cover examples of this\n-   **openIdConnect:** has a way to define how to discover OAuth2 authentication data automatically\n    -   This automatic discovery is what is defined in the OpenID Connect specification\n**Other Tools**\n-   Apiary\n-   OpenAPI\n-   API Blueprint\n-   Spotlight Studio\n    -   <https://stoplight.io>\n-   Spectral - api validation\n-   <https://github.com/apideck-libraries/portman>\n<https://swagger.io/docs/specification/about>\n\n<https://www.openapis.org>\n\n<https://github.com/OAI/OpenAPI-Specification>\n\n[GOTO 2019 â€¢ Introduction to OpenAPI â€¢ Lorna Jane Mitchell](https://www.youtube.com/watch?v=s9u3mXQZbXI)\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/OpenThread/","title":"OpenThread"},"frontmatter":{"draft":false},"rawBody":"# OpenThread\n\nCreated: 2019-03-09 11:36:33 +0500\n\nModified: 2019-03-16 15:41:00 +0500\n\n---\n\n**Thread**\n\nThread is designed to address the unique interoperability, security, power, and architecture challenges of the IoT.\n-   Thread is a low-power wireless mesh networking protocol,based on the universally-supported Internet Protocol (IP), and built using open and proven standards.\n-   Thread enables device-to-device and device-to-cloud communicationsand reliably connects hundreds (or thousands) of products and includes mandatory security features.\n-   Thread networks have no single point of failure,can self-heal and reconfigure when a device is added or removed, and are simple to setup and use.\n-   Thread is based on the broadly supported IEEE 802.15.4 radio standard,which is designed from the ground up for extremely low power consumption and low latency.\n[Thread](http://threadgroup.org/)is an IPv6-based networking protocol designed for low-power Internet of Things devices in an IEEE 802.15.4-2006 wireless mesh network, commonly called a Wireless Personal Area Network (WPAN). Thread is independent of other 802.15.4 mesh networking protocols, such a ZigBee, Z-Wave, and Bluetooth LE.\n\nThread's primary features include:\n-   Simplicity --- Simple installation, start up, and operation\n-   Security --- All devices in a Thread network are authenticated and all communications are encrypted\n-   Reliability --- Self-healing mesh networking, with no single point of failure, and spread-spectrum techniques to provide immunity to interference\n-   Efficiency --- Low-power Thread devices can sleep and operate on battery power for years\n-   Scalability --- Thread networks can scale up to hundreds of devices\n**Features**\n\nOpenThread implements all Thread networking layers (IPv6, 6LoWPAN, IEEE 802.15.4 with MAC security, Mesh Link Establishment, Mesh Routing) and device roles, as well as Border Router support.\n[APPLICATION SERVICES](https://openthread.io/reference/)\n-   IPv6 configuration and raw data interface\n-   UDP sockets\n-   CoAP client and server\n-   DHCPv6 client and server\n-   DNSv6 client\n\n[ENHANCED FEATURES](https://openthread.io/guides/build)\n-   Child Supervision\n-   Inform Previous Parent on Reattach\n-   Jam Detection\n-   Periodic Parent Search\n\n[NCP SUPPORT](https://openthread.io/guides/ncp/)\n-   [Spinel](https://github.com/openthread/openthread/tree/master/src/ncp), a general purpose NCP protocol\n-   [wpantund](https://github.com/openthread/wpantund), a user-space NCP network interface driver/daemon\n-   Sniffer support via NCP Spinel nodes\n\n[BORDER ROUTER](https://openthread.io/guides/border-router)\n-   Web UI for configuration and management\n-   Thread Border Agent to support an External Commissioner\n-   NAT64 for connecting to IPv4 networks\n-   Thread interface driver using wpantund\n**Node Roles and Types**\n\nIn a Thread network, nodes are split into two forwarding roles:\n\n**Router**\n\nA Router is a node that:\n-   forwards packets for network devices\n-   provides secure commissioning services for devices trying to join the network\n-   keeps its transceiver enabled at all times\n\n**End Device**\n\nAn End Device (ED) is a node that:\n-   communicates primarily with a single Router\n-   does not forward packets for other network devices\n-   can disable its transceiver to reduce power\n![OT Node Roles](media/OpenThread-image1.png)\n**Device Types**\n\n**Full Thread Device**\n\nA Full Thread Device (FTD) always has its radio on, subscribes to the all-routers multicast address, and maintains IPv6 address mappings. There are three types of FTDs:\n-   Router\n-   Router Eligible End Device (REED) --- can be promoted to a Router\n-   Full End Device (FED) --- cannot be promoted to a Router\n\nAn FTD can operate as a Router (Parent) or an End Device (Child).\n\n**Minimal Thread Device**\n\nA Minimal Thread Device does not subscribe to multicast traffic and forwards all messages to its Parent. There are two types of MTDs:\n-   Minimal End Device (MED) --- transceiver always on, does not need to poll for messages from its parent\n-   Sleepy End Device (SED) --- normally disabled, wakes on occasion to poll for messages from its parent\n\nAn MTD can only operate as an End Device (Child).\n![OT Device Taxonomy](media/OpenThread-image2.png)**References**\n\n<https://openthread.io>\n\n<https://openthread.io/guides/thread-primer>\n\n<https://openthread.io/guides/thread-primer/node-roles-and-types>\n\n<https://openthread.io/guides/ncp>"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2018-08-13 16:02:43 +0500\n\nModified: 2021-10-11 00:01:09 +0500\n\n---\n\n**LWM2M - Light Weight Machine-to-Machine**\n\nIt is an application layer protocol built over CoAP\n**HSTS -** HTTPS Strict Transport Security\n\nThis is a list of websites that have requested to be contacted via HTTPS only.\n**Fibre Channel**\n\nFibre Channel(FC) is a high-speed data transfer protocol (commonly running at 1, 2, 4, 8, 16, 32, and 128[gigabit](https://en.wikipedia.org/wiki/Gigabit)per second rates) providing in-order, losslessdelivery of raw block data, primarily used to connect[computer data storage](https://en.wikipedia.org/wiki/Computer_data_storage)to[servers](https://en.wikipedia.org/wiki/Server_(computing)).Fibre Channel is mainly used in[storage area networks](https://en.wikipedia.org/wiki/Storage_area_network)(SAN) in commercial[data centers](https://en.wikipedia.org/wiki/Data_center). Fibre Channel networks form a[switched fabric](https://en.wikipedia.org/wiki/Switched_fabric)because they operate in unison as one big switch. Fibre Channel typically runs on[optical fiber](https://en.wikipedia.org/wiki/Optical_fiber)cables within and between data centers, but can also run on copper cabling.\nMost[block storage](https://en.wikipedia.org/wiki/Block_(data_storage))runs over Fibre Channel Fabrics and supports many upper level protocols.[Fibre Channel Protocol](https://en.wikipedia.org/wiki/Fibre_Channel_Protocol)(FCP) is a transport protocol that predominantly transports[SCSI](https://en.wikipedia.org/wiki/Small_Computer_System_Interface)commands over Fibre Channel networks.[Mainframe computers](https://en.wikipedia.org/wiki/Mainframe_computer)run the[FICON](https://en.wikipedia.org/wiki/FICON)command set over Fibre Channel because of its high reliability and throughput. Fibre Channel can be used to transport data from storage systems that use solid-state[flash memory](https://en.wikipedia.org/wiki/Flash_memory)storage medium by transporting[NVMe](https://en.wikipedia.org/wiki/NVM_Express)protocol commands.\nThe goal of Fibre Channel is to create a[storage area network](https://en.wikipedia.org/wiki/Storage_area_network)(SAN) to connect servers to storage.\n<https://en.wikipedia.org/wiki/Fibre_Channel>\n\n## Storage Area Network (SAN)**\n\nThe SAN is a dedicated network that enables multiple servers to access data from one or more storage devices.[Enterprise storage](https://en.wikipedia.org/wiki/Enterprise_storage)uses the SAN to backup to secondary storage devices including disk arrays, tape libraries, and other backup while the storage is still accessible to the server. Servers may access storage from multiple storage devices over the network as well.\nSANs are often designed with dual fabrics to increase fault tolerance. Two completely separate fabrics are operational and if the primary fabric fails, then the second fabric becomes the primary.\nA**storage area network(SAN) orstorage network**is a[Computer network](https://en.wikipedia.org/wiki/Computer_network)which provides access to consolidated,[block-level data storage](https://en.wikipedia.org/wiki/Block_device). SANs are primarily used to enhance accessibility of storage devices, such as[disk arrays](https://en.wikipedia.org/wiki/Disk_array)and[tape libraries](https://en.wikipedia.org/wiki/Tape_library), to[servers](https://en.wikipedia.org/wiki/Server_(computing))so that the devices appear to the[operating system](https://en.wikipedia.org/wiki/Operating_system)as[locally-attached devices](https://en.wikipedia.org/wiki/Direct-attached_storage). A SAN typically is a dedicated network of storage devices not accessible through the[local area network](https://en.wikipedia.org/wiki/Local_area_network)(LAN) by other devices, thereby preventing interference of LAN traffic in data transfer.\n<https://en.wikipedia.org/wiki/Storage_area_network>\n\n## DHCP**\n\nThe**Dynamic Host Configuration Protocol(DHCP)** is a[network management protocol](https://en.wikipedia.org/wiki/Network_protocol)used on[UDP/IP](https://en.wikipedia.org/wiki/UDP/IP)networks whereby a DHCP server dynamically assigns an[IP address](https://en.wikipedia.org/wiki/IP_address)and other network configuration parameters to each device on a network so they can communicate with other IP networks.A DHCP server enables computers to request IP addresses and networking parameters automatically from the[Internet service provider](https://en.wikipedia.org/wiki/Internet_service_provider)(ISP), reducing the need for a[network administrator](https://en.wikipedia.org/wiki/Network_administrator)or a user to manually assign IP addresses to all network devices.In the absence of a DHCP server, a computer or other device on the network needs to be manually assigned an IP address, or to assign itself an[APIPA](https://en.wikipedia.org/wiki/APIPA)address, which will not enable it to communicate outside its local subnet.\n\nDHCP can be implemented on networks ranging in size from[home networks](https://en.wikipedia.org/wiki/Home_network)to large[campus networks](https://en.wikipedia.org/wiki/Campus_network)and regional[Internet service provider](https://en.wikipedia.org/wiki/Internet_service_provider)networks.A[router](https://en.wikipedia.org/wiki/Router_(computing))or a[residential gateway](https://en.wikipedia.org/wiki/Residential_gateway)can be enabled to act as a DHCP server. Most residential network routers receive a globally unique IP address within the ISP network. Within a local network, a DHCP server assigns a local IP address to each device connected to the network.\n**Operation**\n\nThe DHCP employs a[connectionless](https://en.wikipedia.org/wiki/Connectionless_communication)service model, using the[User Datagram Protocol](https://en.wikipedia.org/wiki/User_Datagram_Protocol)(UDP). It is implemented with two UDP port numbers for its operations which are the same as for the bootstrap protocol ([BOOTP](https://en.wikipedia.org/wiki/BOOTP)). UDP port number 67 is the destination port of a server, and UDP port number 68 is used by the client.\n<https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol>\n\n## Server Message Block (SMB)**\n\nIn[computer networking](https://en.wikipedia.org/wiki/Computer_network),**Server Message Block(SMB**), one version of which was also known as**Common Internet File System(CIFS**)operates as an[application-layer](https://en.wikipedia.org/wiki/Application_layer)or[presentation-layer](https://en.wikipedia.org/wiki/Presentation_layer)[network protocol](https://en.wikipedia.org/wiki/Communication_protocol)[[3]](https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-3)mainly used for providing[shared access](https://en.wikipedia.org/wiki/Shared_access)to[files](https://en.wikipedia.org/wiki/Computer_file),[printers](https://en.wikipedia.org/wiki/Computer_printer), and[serial ports](https://en.wikipedia.org/wiki/Serial_port)and miscellaneous communications between nodes on a network. It also provides an authenticated[inter-process communication](https://en.wikipedia.org/wiki/Inter-process_communication)mechanism. Most usage of SMB involves computers running[Microsoft Windows](https://en.wikipedia.org/wiki/Microsoft_Windows), where it was known as \"Microsoft Windows Network\" before the introduction of[Active Directory](https://en.wikipedia.org/wiki/Active_Directory). Corresponding[Windows services](https://en.wikipedia.org/wiki/Windows_service)are LAN Manager Server (for the server component) and LAN Manager Workstation (for the client component).\n<https://en.wikipedia.org/wiki/Server_Message_Block>\n\n## Reactive Streams**\n\nReactive Streams is an initiative to provide a standard for asynchronous stream processing with non-blocking back pressure. This encompasses efforts aimed at runtime environments (JVM and JavaScript) as well as network protocols.\n[https://www.reactive-streams.org](https://www.reactive-streams.org/)\n**Web Proxy Auto-Discovery (WPAD) Protocol**\n\nTheWeb Proxy Auto-Discovery (WPAD) Protocolis a method used by clients to locate the URL of a configuration file using[DHCP](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol)and/or[DNS](https://en.wikipedia.org/wiki/Domain_Name_System)discovery methods. Once detection and download of the configuration file is complete, it can be executed to determine the proxy for a specified URL.\n<https://en.wikipedia.org/wiki/Web_Proxy_Auto-Discovery_Protocol>\n\n## Contrained Application Protocol (CoAP)**\n\n**Constrained Application Protocol(CoAP)** is a specialized Internet Application Protocol for constrained devices, as defined in[RFC 7252](https://tools.ietf.org/html/rfc7252). It enables those constrained devices called \"nodes\" to communicate with the wider Internet using similar protocols. CoAP is designed for use between devices on the same constrained network (e.g., low-power, lossy networks), between devices and general nodes on the Internet, and between devices on different constrained networks both joined by an internet. CoAP is also being used via other mechanisms, such as SMS on mobile communication networks.\nCoAP is a[service layer](https://en.wikipedia.org/wiki/Service_layer)protocol that is intended for use in resource-constrained internet devices, such as[wireless sensor network](https://en.wikipedia.org/wiki/Wireless_sensor_network)nodes. CoAP is designed to easily translate to[HTTP](https://en.wikipedia.org/wiki/HTTP)for simplified integration with the web, while also meeting specialized requirements such as[multicast](https://en.wikipedia.org/wiki/Multicast)support, very low overhead, and simplicity.Multicast, low overhead, and simplicity are extremely important for[Internet of Things](https://en.wikipedia.org/wiki/Internet_of_Things)(IoT) and[Machine-to-Machine](https://en.wikipedia.org/wiki/Machine-to-Machine)(M2M) devices, which tend to be deeply[embedded](https://en.wikipedia.org/wiki/Embedded_system)and have much less memory and power supply than traditional internet devices have. Therefore, efficiency is very important. CoAP can run on most devices that support[UDP](https://en.wikipedia.org/wiki/User_Datagram_Protocol)or a UDP analogue.\n<https://www.eclipse.org/californium>\n\n## Lightweight Directory Access Protocol (LDAP)**\n\nLDAP is an open, vendor-neutral, industry standard[application protocol](https://en.wikipedia.org/wiki/Application_protocol)for accessing and maintaining distributed[directory information services](https://en.wikipedia.org/wiki/Directory_service)over an[Internet Protocol](https://en.wikipedia.org/wiki/Internet_Protocol)(IP) network.[Directory services](https://en.wikipedia.org/wiki/Directory_service)play an important role in developing[intranet](https://en.wikipedia.org/wiki/Intranet)and Internet applications by allowing the sharing of information about users, systems, networks, services, and applications throughout the network.As examples, directory services may provide any organized set of records, often with a hierarchical structure, such as a corporate[email](https://en.wikipedia.org/wiki/Email) directory. Similarly, a[telephone directory](https://en.wikipedia.org/wiki/Telephone_directory)is a list of subscribers with an address and a phone number.\n<https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol>\n\n## ModBus**\n-   Serial Communication Protocol\nModbus has become a[*de facto*](https://en.wikipedia.org/wiki/De_facto)[standard](https://en.wikipedia.org/wiki/Standardization)communication protocol and is now a commonly available means of connecting industrial[electronic](https://en.wikipedia.org/wiki/Electronics)devices.The main reasons for the use of Modbus in the industrial environment are:\n-   developed with industrial applications in mind,\n-   openly published and royalty-free,\n-   easy to deploy and maintain,\n-   moves raw bits or words without placing many restrictions on vendors.\nModbus enables communication among many devices connected to the same network, for example, a system that measures temperature and humidity and communicates the results to a[computer](https://en.wikipedia.org/wiki/Computer). Modbus is often used to connect a supervisory computer with a[remote terminal unit](https://en.wikipedia.org/wiki/Remote_terminal_unit)(RTU) in supervisory control and data acquisition ([SCADA](https://en.wikipedia.org/wiki/SCADA)) systems. Many of the data types are named from its use in driving relays: a single-bit physical output is called a*coil*, and a single-bit physical input is called a*discrete input*or a*contact*.\n**See also**\n\nBACnet Protocol (Building Automation and Control networks)\n**Libraries**\n\n<https://github.com/ljean/modbus-tk>\n\n## Zenoh**\n\n**Zero Overhead Pub/sub, Store/Query and Compute.**\n\nzenoh unifies data in motion, data in-use, data at rest and computations. It carefully blends traditional pub/sub with geo-distributed storages, queries and computations, while retaining a level of time and space efficiency that is well beyond any of the mainstream stacks.\n<http://zenoh.io>\n\n## OPC Unified Architecture(OPC UA)**\n\nOPC Unified Architecture(OPC UA) is a[machine to machine](https://en.wikipedia.org/wiki/Machine_to_machine)[communication protocol](https://en.wikipedia.org/wiki/Communication_protocol)for[industrial automation](https://en.wikipedia.org/wiki/Industrial_automation)developed by the[OPC Foundation](https://en.wikipedia.org/wiki/OPC_Foundation). Distinguishing characteristics are:\n-   Focus on communicating with industrial equipment and systems for data collection and control\n-   [Open](https://en.wikipedia.org/wiki/Open_standard)- freely available and implementable under GPL 2.0 license\n-   [Cross-platform](https://en.wikipedia.org/wiki/Cross-platform)- not tied to one operating system or programming language\n-   [Service-oriented architecture](https://en.wikipedia.org/wiki/Service-oriented_architecture)(SOA)\n-   Inherent complexity - the specification consists of 1250 pages in 14 documents\n-   Offers[security](https://en.wikipedia.org/wiki/Information_security)functionality for authentication, authorization, integrity and confidentiality\n-   Integral[information model](https://en.wikipedia.org/wiki/Information_model), which is the foundation of the infrastructure necessary for information integration where vendors and organizations can model their complex data into an OPC UA namespace to take advantage of the rich service-oriented architecture of OPC UA. There are over 35 collaborations with the OPC Foundation currently. Key industries include[pharmaceutical](https://en.wikipedia.org/wiki/Pharmaceutical_industry),[oil and gas](https://en.wikipedia.org/wiki/Oil_and_gas_industry),[building automation](https://en.wikipedia.org/wiki/Building_automation),[industrial robotics](https://en.wikipedia.org/wiki/Industrial_robotics), security, manufacturing and[process control](https://en.wikipedia.org/wiki/Process_control).\n**Specification**\n\nThe OPC UA specification is a multi-part specification and consists of the following parts:\n\n1.  Concepts\n\n2.  Security Model\n\n3.  Address Space Model\n\n4.  Services\n\n5.  Information Model\n\n6.  Mappings\n\n7.  Profiles\n\n8.  Data Access\n\n9.  Alarms and Conditions\n\n10. Programs\n\n11. Historical Access\n\n12. Discovery and Global Services\n\n13. Aggregates\n\n14. PubSub\n<https://en.wikipedia.org/wiki/OPC_Unified_Architecture>\n\n<https://opcfoundation.org/about/opc-technologies/opc-ua>\n\n## Dedicated short-range communications (DSRC)**\n\nDedicated short-range communications(DSRC) are one-way or two-way short-range to medium-range[wireless](https://en.wikipedia.org/wiki/Wireless)communication channels specifically designed for automotive use[[1]](https://en.wikipedia.org/wiki/Dedicated_short-range_communications#cite_note-1)and a corresponding set of protocols and standards.\n<https://en.wikipedia.org/wiki/Dedicated_short-range_communications>\n<https://web.dev/webtransport>\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/Protocols-Intro/","title":"Protocols Intro"},"frontmatter":{"draft":false},"rawBody":"# Protocols Intro\n\nCreated: 2019-04-25 11:20:23 +0500\n\nModified: 2021-06-06 16:47:25 +0500\n\n---\n\n**Media Access Control (MAC Address)**\n\nMedia access control is a communications protocol that is used to distinguish specific devices. Each device is supposed to get a unique MAC address during the manufacturing process that differentiates it from every other device on the internet.\nAddressing hardware by the MAC address allows you to reference a device by a unique value even when the software on top may change the name for that specific device during operation.\nMedia access control is one of the only protocols from the link layer that you are likely to interact with on a regular basis.\n<https://en.wikipedia.org/wiki/MAC_address>\n\n## IP**\n\nThe IP protocol is one of the fundamental protocols that allow the internet to work. IP addresses are unique on each network and they allow machines to address each other across a network. It is implemented on the internet layer in the IP/TCP model.\nNetworks can be linked together, but traffic must be routed when crossing network boundaries. This protocol assumes an unreliable network and multiple paths to the same destination that it can dynamically change between.\nThere are a number of different implementations of the protocol. The most common implementation today is IPv4, although IPv6 is growing in popularity as an alternative due to the scarcity of IPv4 addresses available and improvements in the protocols capabilities.\n**ICMP**\n\nICMP stands for **internet control message protocol**. It is used to send messages between devices to indicate the availability or error conditions. These packets are used in a variety of network diagnostic tools, such as ping and traceroute.\nUsually ICMP packets are transmitted when a packet of a different kind meets some kind of a problem. Basically, they are used as a feedback mechanism for network communications.\n**UDP (Connectionless Protocol)**\n\nUDP stands for user datagram protocol. It is a popular companion protocol to TCP and is also implemented in the transport layer.\nThe fundamental difference between UDP and TCP is that UDP offers unreliable data transfer. It does not verify that data has been received on the other end of the connection. This might sound like a bad thing, and for many purposes, it is. However, it is also extremely important for some functions.\nBecause it is not required to wait for confirmation that the data was received and forced to resend data, UDP is much faster than TCP. It does not establish a connection with the remote host, it simply fires off the data to that host and doesn't care if it is accepted or not.\nBecause it is a simple transaction, it is useful for simple communications like querying for network resources. It also doesn't maintain a state, which makes it great for transmitting data from one machine to many real-time clients. This makes it ideal for VOIP, games, and other applications that cannot afford delays.\n**FTP**\n\nFTP stands for file transfer protocol. It is also in the application layer and provides a way of transferring complete files from one host to another.\nIt is inherently insecure, so it is not recommended for any externally facing network unless it is implemented as a public, download-only resource.\n**SSH**\n\nSSH stands for secure shell. It is an encrypted protocol implemented in the application layer that can be used to communicate with a remote server in a secure way. Many additional technologies are built around this protocol because of its end-to-end encryption and ubiquity.\n**RTSP (Real Time Streaming Protocol)**\n\nTheReal Time Streaming Protocol(RTSP) is a network control[protocol](https://en.wikipedia.org/wiki/Communications_protocol)designed for use in entertainment and communications systems to control[streaming media](https://en.wikipedia.org/wiki/Streaming_media)[servers](https://en.wikipedia.org/wiki/Web_server). The protocol is used for establishing and controlling media sessions between end points. Clients of media servers issue[VHS](https://en.wikipedia.org/wiki/VHS)-style commands, such asplay,recordandpause, to facilitate real-time control of the media streaming from the server to a client (Video On Demand) or from a client to the server (Voice Recording).\nThe transmission of streaming data itself is not a task of RTSP. Most RTSP servers use the[Real-time Transport Protocol](https://en.wikipedia.org/wiki/Real-time_Transport_Protocol)(RTP) in conjunction with[Real-time Control Protocol](https://en.wikipedia.org/wiki/RTCP)(RTCP) for media stream delivery.\n<https://en.wikipedia.org/wiki/Real_Time_Streaming_Protocol>\n\n## What is RTP (Real-time Transport Protocol)?**\n\nThe Real-time Transport Protocol is a network protocol used to deliver streaming audio and video media over the internet, thereby enabling the Voice Over Internet Protocol (VoIP).\nRTP is generally used with a signaling protocol, such as SIP, which sets up connections across the network. RTP applications can use the Transmission Control Protocol (TCP), but most use the User Datagram protocol (UDP) instead because UDP allows for faster delivery of data.\n**What is RTCP (Real-time Transport Control Protocol)?**\n\nWhile RTP allows for real-time data transfer, RTCP provides out-of-band statistics and control information for any given RTP session. It doesn't actually transport any media data, but rather helps with quality control.\n**SSRC and CSRC: How do they work with RTP?**\n\nSSRC (Synchronization Source) values are randomly assigned in order to keep track of synchronization sources within a given RTP session. No two sources within the same session will have the same SSRC identifiers; users can spot and trace looping audio paths if overlaps do occur.\nCSRC (Contributing Source) values make up the full array of up to 15 contributing sources for a given packet payload within an RTP session. For example, if multiple audio sources are mixing together on a conference call, CSRC can help differentiate between those sources.\n<https://www.extrahop.com/resources/protocols/rtp>\n\n## OCSP Stapling**\n\nThe**Online Certificate Status Protocol (OCSP) stapling**, formally known as the**TLS Certificate Status Request**extension, is a standard for checking the revocation status of[X.509](https://en.wikipedia.org/wiki/X.509)[digital certificates](https://en.wikipedia.org/wiki/Digital_certificate).It allows the presenter of a certificate to bear the resource cost involved in providing[Online Certificate Status Protocol](https://en.wikipedia.org/wiki/Online_Certificate_Status_Protocol)(OCSP) responses by appending (\"stapling\") a[time-stamped](https://en.wikipedia.org/wiki/Timestamp)OCSP response[signed](https://en.wikipedia.org/wiki/Cryptographic_signature)by the CA to the initial[TLS handshake](https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_handshake), eliminating the need for clients to contact the CA, with the aim of improving both security and performance.\n<https://en.wikipedia.org/wiki/OCSP_stapling>\n\n## ACME Protocol**\n\nThe**Automatic Certificate Management Environment(ACME)** protocol is a[communications protocol](https://en.wikipedia.org/wiki/Communications_protocol)for automating interactions between[certificate authorities](https://en.wikipedia.org/wiki/Certificate_authority)and their users' web servers, allowing the automated deployment of[public key infrastructure](https://en.wikipedia.org/wiki/Public_key_infrastructure)at very low cost.It was designed by the[Internet Security Research Group](https://en.wikipedia.org/wiki/Internet_Security_Research_Group)(ISRG) for their[Let's Encrypt](https://en.wikipedia.org/wiki/Let%27s_Encrypt)service.\nThe protocol, based on passing[JSON](https://en.wikipedia.org/wiki/JSON)-formatted messages over[HTTPS](https://en.wikipedia.org/wiki/HTTPS),has been published as an[Internet Standard](https://en.wikipedia.org/wiki/Internet_Standard)in[RFC 8555](https://tools.ietf.org/html/rfc8555)by its own chartered[IETF](https://en.wikipedia.org/wiki/Internet_Engineering_Task_Force)working group.\n<https://en.wikipedia.org/wiki/Automated_Certificate_Management_Environment>\n\n## Address Resolution Protocol (ARP)**\n\nTheAddress Resolution Protocol(ARP) is a[communication protocol](https://en.wikipedia.org/wiki/Communication_protocol)used for discovering the[link layer](https://en.wikipedia.org/wiki/Link_layer)address, such as a[MAC address](https://en.wikipedia.org/wiki/MAC_address), associated with a given[internet layer](https://en.wikipedia.org/wiki/Internet_layer)address, typically an[IPv4 address](https://en.wikipedia.org/wiki/IPv4_address). This mapping is a critical function in the[Internet protocol suite](https://en.wikipedia.org/wiki/Internet_protocol_suite). ARP was defined in 1982 by[RFC](https://en.wikipedia.org/wiki/Request_for_Comments)[826](https://tools.ietf.org/html/rfc826),which is[Internet Standard](https://en.wikipedia.org/wiki/Internet_Standard)STD 37.\nARP has been implemented with many combinations of network and data link layer technologies, such as[IPv4](https://en.wikipedia.org/wiki/IPv4),[Chaosnet](https://en.wikipedia.org/wiki/Chaosnet),[DECnet](https://en.wikipedia.org/wiki/DECnet)and Xerox[PARC Universal Packet](https://en.wikipedia.org/wiki/PARC_Universal_Packet)(PUP) using[IEEE 802](https://en.wikipedia.org/wiki/IEEE_802)standards,[FDDI](https://en.wikipedia.org/wiki/FDDI),[X.25](https://en.wikipedia.org/wiki/X.25),[Frame Relay](https://en.wikipedia.org/wiki/Frame_Relay)and[Asynchronous Transfer Mode](https://en.wikipedia.org/wiki/Asynchronous_Transfer_Mode)(ATM). IPv4 over[IEEE 802.3](https://en.wikipedia.org/wiki/IEEE_802.3)and[IEEE 802.11](https://en.wikipedia.org/wiki/IEEE_802.11)is the most common usage.\nIn[Internet Protocol Version 6](https://en.wikipedia.org/wiki/IPv6)(IPv6) networks, the functionality of ARP is provided by the[Neighbor Discovery Protocol](https://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol)(NDP).\nThe MAC address is how machines on a subnet communicate. When machine A sends packets to another machine on its subnet, it sends it using the MAC address. When sending a packet to a machine on the public Internet, the packet is sent to the MAC address of the router interface that is the default gateway. IP addresses are used to figure out the MAC address to send to using ARP.\n**ARP Basics**\n\nARP stands for Address Resolution Protocol. When you try to ping an IP address on your local network, say 192.168.1.1, your system has to turn the IP address 192.168.1.1 into a MAC address. This involves using ARP to resolve the address, hence its name.\nSystems keep an ARP look-up table where they store information about what IP addresses are associated with what MAC addresses. When trying to send a packet to an IP address, the system will first consult this table to see if it already knows the MAC address. If there is a value cached, ARP is not used.\nIf the IP address is not found in the ARP table, the system will then send a broadcast packet to the network using the ARP protocol to ask \"who has 192.168.1.1\". Because it is a broadcast packet, it is sent to a special MAC address that causes all machines on the network to receive it. Any machine with the requested IP address will reply with an ARP packet that says \"I am 192.168.1.1\", and this includes the MAC address which can receive packets for that IP.\n<https://www.tummy.com/articles/networking-basics-how-arp-works>\n\n<https://www.geeksforgeeks.org/how-address-resolution-protocol-arp-works>\n\n<https://en.wikipedia.org/wiki/Address_Resolution_Protocol>\n\n<https://networkengineering.stackexchange.com/questions/36605/when-exactly-is-arp-protocol-is-used>\n\n## Neighbor Discovery Protocol (NDP)**\n\nTheNeighbor Discovery Protocol(NDP,ND)is a protocol in the[Internet protocol suite](https://en.wikipedia.org/wiki/Internet_protocol_suite)used with[Internet Protocol Version 6](https://en.wikipedia.org/wiki/IPv6)(IPv6). It operates at the[link layer](https://en.wikipedia.org/wiki/Link_layer)of the Internet model ([RFC 1122](https://tools.ietf.org/html/rfc1122)), and is responsible for gathering various information required for internet communication, including the configuration of local connections and the[domain name servers](https://en.wikipedia.org/wiki/Domain_Name_System)and gateways used to communicate with more distant systems.\nThe protocol defines five different ICMPv6 packet types to perform functions for IPv6 similar to the[Address Resolution Protocol](https://en.wikipedia.org/wiki/Address_Resolution_Protocol)(ARP) and[Internet Control Message Protocol](https://en.wikipedia.org/wiki/Internet_Control_Message_Protocol)(ICMP)[Router Discovery](https://en.wikipedia.org/wiki/ICMP_Router_Discovery_Protocol)and[Router Redirect](https://en.wikipedia.org/wiki/ICMP_Redirect_Message)protocols for[IPv4](https://en.wikipedia.org/wiki/IPv4). However, it provides many improvements over its IPv4 counterparts ([RFC 4861](https://tools.ietf.org/html/rfc4861), section 3.1). For example, it includes Neighbor Unreachability Detection (NUD), thus improving robustness of packet delivery in the presence of failing routers or links, or mobile nodes.\nThe Inverse Neighbor Discovery (IND) protocol extension ([RFC 3122](https://tools.ietf.org/html/rfc3122)) allows nodes to determine and advertise an IPv6 address corresponding tools a given link-layer address, similar to[Reverse ARP](https://en.wikipedia.org/wiki/Reverse_Address_Resolution_Protocol)for IPv4. The[Secure Neighbor Discovery Protocol](https://en.wikipedia.org/wiki/Secure_Neighbor_Discovery_Protocol)(SEND), a security extension of NDP, uses[Cryptographically Generated Addresses](https://en.wikipedia.org/wiki/Cryptographically_Generated_Addresses)(CGA) and the[Resource Public Key Infrastructure](https://en.wikipedia.org/wiki/Resource_Public_Key_Infrastructure)(RPKI) to provide an alternative mechanism for securing NDP with a cryptographic method that is independent of[IPsec](https://en.wikipedia.org/wiki/IPsec). Neighbor Discovery Proxy (ND Proxy) ([RFC 4389](https://tools.ietf.org/html/rfc4389)) provides a service similar to IPv4[Proxy ARP](https://en.wikipedia.org/wiki/Proxy_ARP)and allows bridging multiple network segments within a single subnet prefix when bridging cannot be done at the link layer.\n**Functions (ICMPv6 packets)**\n-   Router Solicitation\n-   Router Advertisement\n-   Neighbor Solicitation\n-   Neighbor Advertisement\n-   Redirect\n<https://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol>\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/Protocols/","title":"Protocols"},"frontmatter":{"draft":false},"rawBody":"# Protocols\n\nCreated: 2019-04-25 10:48:54 +0500\n\nModified: 2021-06-04 10:28:47 +0500\n\n---\n\n1.  Application Layer\n    -   BGP\n    -   DHCP\n    -   DNS\n    -   FTP\n    -   HTTP / HTTPS\n    -   IMAP\n    -   LDAP\n    -   MGCP\n    -   MQTT\n    -   NNTP\n    -   NTP\n    -   POP\n    -   ONC/RPC\n    -   RTP\n    -   RTSP\n    -   RIP\n    -   SIP\n    -   SMTP\n    -   SNMP\n    -   SSH\n    -   Telnet\n    -   TLS/SSL\n    -   XMPP\n    -   LWM2M\n\n2.  Transport Layer\n    -   TCP\n    -   UDP\n    -   DCCP\n    -   SCTP\n    -   RSVP\n\n3.  Internet Layer\n    -   IP (IPv4 / IPv6)\n    -   ICMP / ICMPv6\n    -   ECN\n    -   IGMP\n    -   IPsec\n\n4.  Link Layer\n    -   ARP\n    -   NDP\n    -   OSPF\n    -   Tunnels (L2TP)\n    -   PPP\n    -   MAC (Ethernet / DSL / ISDN / FDDI)\n        -   Carrier-sense multiple access with collision detection(CSMA/CD)\n        -   Carrier-sense multiple access with collision avoidance(CSMA/CA)\n<https://en.wikipedia.org/wiki/Internet_protocol_suite>\n\n## Networking Protocols**\n-   **http2 -** multiple requests in one connection-   **BOSH - Bidirectional stream Over Synchronous HTTP,** it works over HTTP (User will request a connection to the server and than server will hold onto that request until some time, and return to him if any data needs to be transfered, than user will again initiate the request)\n-   **Long Polling over HTTP -** Client will request information to the server, and then server will respond with that information or tell the client that there is no new information. After that client will wait some time before again sending request.\n**STOMP (Simple/Streaming Text Oriented Message Protocol)**\n\nIs the only one to be text-based, making it more analogous to HTTP.\n\nLike AMQP, STOMP provides a message (or frame) header with properties, and a frame body.\n\nDo not have queues and topics - it uses a SEND semantic with a \"destination\" string.\nIt uses a set of commands like CONNECT, SEND, or SUBSCRIBE to manage the conversation. STOMP clients, written in any language, can talk with any message broker supporting the protocol.\n**TLS/SSL**\n\nTransport Layer Security(TLS), and its now-deprecated predecessor,Secure Sockets Layer(SSL),are[cryptographic protocols](https://en.wikipedia.org/wiki/Cryptographic_protocol)designed to provide[communications security](https://en.wikipedia.org/wiki/Communications_security)over a[computer network](https://en.wikipedia.org/wiki/Computer_network).Several versions of the protocols find widespread use in applications such as[web browsing](https://en.wikipedia.org/wiki/Web_navigation),[email](https://en.wikipedia.org/wiki/Email),[instant messaging](https://en.wikipedia.org/wiki/Instant_messaging), and[voice over IP](https://en.wikipedia.org/wiki/Voice_over_IP)(VoIP).[Websites](https://en.wikipedia.org/wiki/Website)can use TLS to secure all communications between their[servers](https://en.wikipedia.org/wiki/Server_(computing))and[web browsers](https://en.wikipedia.org/wiki/Web_browser).\nThe TLS protocol aims primarily to provide[privacy](https://en.wikipedia.org/wiki/Privacy)and[data integrity](https://en.wikipedia.org/wiki/Data_integrity)between two or more communicating computer applications.When secured by TLS, connections between a client (e.g., a web browser) and a server (e.g., wikipedia.org) should have one or more of the following properties:\n-   The connection isprivate(orsecure) because[symmetric cryptography](https://en.wikipedia.org/wiki/Symmetric-key_algorithm)is used to[encrypt](https://en.wikipedia.org/wiki/Encryption)the data transmitted. The[keys](https://en.wikipedia.org/wiki/Key_(cryptography))for this symmetric encryption are generated uniquely for each connection and are based on a[shared secret](https://en.wikipedia.org/wiki/Shared_secret)that was negotiated at the start of the[session](https://en.wikipedia.org/wiki/Session_(computer_science)). The server and client negotiate the details of which encryption algorithm and cryptographic keys to use before the first[byte](https://en.wikipedia.org/wiki/Byte)of data is transmitted. The negotiation of a shared secret is both secure (the negotiated secret is unavailable to[eavesdroppers](https://en.wikipedia.org/wiki/Eavesdropping)and cannot be obtained, even by an attacker who places themselves in the middle of the connection) and reliable (no attacker can modify the communications during the negotiation without being detected).\n-   The identity of the communicating parties can beauthenticatedusing[public-key cryptography](https://en.wikipedia.org/wiki/Public-key_cryptography). This authentication can be made optional, but is generally required for at least one of the parties (typically the server).\n-   The connection isreliablebecause each message transmitted includes a message integrity check using a[message authentication code](https://en.wikipedia.org/wiki/Message_authentication_code)to prevent undetected loss or alteration of the data during[transmission](https://en.wikipedia.org/wiki/Data_transmission).\nThe TLS protocol comprises two layers: the[TLS record](https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_record)and the[TLS handshake](https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_handshake)protocols.\nTLS can be used on top of a transport-layer security protocol like[TCP](https://www.cloudflare.com/learning/ddos/glossary/tcp-ip/). There are three main components to TLS: Encryption, Authentication, and Integrity.\n-   **Encryption**:hides the data being transferred from third parties.\n-   **Authentication**:ensures that the parties exchanging information are who they claim to be.\n-   **Integrity**:verifies that the data has not been forged or tampered with.\n[HTTPS](https://www.cloudflare.com/learning/ssl/what-is-https/)is an implementation of TLS encryption on top of the[HTTP](https://www.cloudflare.com/learning/ddos/glossary/hypertext-transfer-protocol-http/)protocol, which is used by all websites as well as some other web services.\nTLS_ECDHE_RSA_with_AES_128_GCM_SHA256\n**TLSv1.3**\n\nThe updated protocol added a function called \"0-RTT resumption\" that enables the client and server to remember if they have communicated before. If prior communications exist, the previous keys can be used, security checks skipped and the client and server can begin communicating immediately. It is believed that some of the bigger tech companies pushed for 0-RTT because they benefit from the faster connections, but there is some concern from security professionals.\n**TLS Handshake**\n-   The client computer sends aClientHellomessage to the server with its Transport Layer Security (TLS) version, list of cipher algorithms and compression methods available.\n-   The server replies with aServerHellomessage to the client with the TLS version, selected cipher, selected compression methods and the server's public certificate signed by a CA (Certificate Authority). The certificate contains a public key that will be used by the client to encrypt the rest of the handshake until a symmetric key can be agreed upon.\n-   The client verifies the server digital certificate against its list of trusted CAs. If trust can be established based on the CA, the client generates a string of pseudo-random bytes and encrypts this with the server's public key. These random bytes can be used to determine the symmetric key.\n-   The server decrypts the random bytes using its private key and uses these bytes to generate its own copy of the symmetric master key.\n-   The client sends aFinishedmessage to the server, encrypting a hash of the transmission up to this point with the symmetric key.\n-   The server generates its own hash, and then decrypts the client-sent hash to verify that it matches. If it does, it sends its ownFinishedmessage to the client, also encrypted with the symmetric key.\n-   From now on the TLS session transmits the application (HTTP) data encrypted with the agreed symmetric key\n<https://en.wikipedia.org/wiki/Transport_Layer_Security>\n\n<https://www.cloudflare.com/learning/ssl/transport-layer-security-tls>\n\n<https://security.stackexchange.com/questions/93333/what-layer-is-tls>\n\n## Mutual Authentication**\n\nMutual authenticationortwo-way authenticationrefers to two parties[authenticating](https://en.wikipedia.org/wiki/Authenticating)each other at the same time, being a default mode of[authentication](https://en.wikipedia.org/wiki/Authentication_protocol)in some protocols ([IKE](https://en.wikipedia.org/wiki/Internet_Key_Exchange),[SSH](https://en.wikipedia.org/wiki/Secure_Shell)) and optional in others ([TLS](https://en.wikipedia.org/wiki/Transport_Layer_Security)).\nBy default the TLS protocol only proves the identity of the server to the client using[X.509 certificate](https://en.wikipedia.org/wiki/X.509_certificate)and the authentication of the client to the server is left to the application layer. TLS also offers client-to-server authentication using client-side X.509 authentication.As it requires provisioning of the certificates to the clients and involves less user-friendly experience, it's rarely used in end-user applications.\nMutual TLS authentication (mTLS) is much more widespread in[business-to-business](https://en.wikipedia.org/wiki/Business-to-business)(B2B) applications, where a limited number of programmatic and homogeneous clients are connecting to specific web services, the operational burden is limited, and security requirements are usually much higher as compared to consumer environments.\nMost Mutual authentication is machine-to-machine, leaving it up to chance whether or not users will notice (or care) when the remote authentication fails (e.g. a red address bar browser padlock, or a wrong domain name). Non-technical mutual-authentication also exists to mitigate this problem, requiring users to complete a challenge, effectively forcing them to notice, and blocking them from authenticating with false endpoints.\nMutual authentication is of two types:\n\ni.  Certificate based\n\nii. User name-password based\n<https://en.wikipedia.org/wiki/Mutual_authentication>-   We use TLS/SSL for two main reasons\n\n    1.  Encryption\n\n    2.  Authentication\n-   After you've established a TLS connection, what algorithm is used to encrypt the data?\n    -   Symmetric cipher like AES\n    -   RSA and ECDSA are slow, So instead the client and server pick a secret (and faster to use) symmetric key and use that to encrypt everything\n-   Does every TLS server certificate have a hostname on it?\n    -   Yes, the point of a certificate is to prove that a server is the \"real\" server for a hostname, so every certificate has a hostname on it\n-   How does a browser check that your certificate is signed by someone it trusts?\n    -   it has a hardcoded list\n-   Are TLS certificates secret?\n    -   A TLS certificate contains\n        -   A public key\n        -   The hostname(s) it's valid for\n        -   An expiration date\n        -   Signature(s) for a CA\n    -   The private key for the cert is Super Secret though\n**Two types of APIs**\n\n1.  Pull based API: HTTP, gRPC\n\n2.  Push based API: AMQP, MQTT, Kafka\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/REST-Representational-State-Transfer---RESTFul/","title":"REST Representational State Transfer / RESTFul"},"frontmatter":{"draft":false},"rawBody":"# REST Representational State Transfer / RESTFul\n\nCreated: 2018-02-10 12:21:12 +0500\n\nModified: 2022-02-05 00:20:48 +0500\n\n---\n\nREST + JSON over HTTP\nREST, or REpresentational State Transfer, is an architectural style for providing standards between computer systems on the web, making it easier for systems to communicate with each other. REST-compliant systems, often called RESTful systems, are characterized by how they are stateless and separate the concerns of client and server.\n**SEPARATION OF CLIENT AND SERVER**\n\nIn the REST architectural style, the implementation of the client and the implementation of the server can be done independently without each knowing about the other.\nBy using a REST interface, different clients hit the same REST endpoints, perform the same actions, and receive the same responses.\n**STATELESSNESS**\n\nSystems that follow the REST paradigm are stateless, meaning that the server does not need to know anything about what state the client is in and vice versa. In this way, both the server and the client can understand any message received, even without seeing previous messages.\n**MAKING REQUESTS**\n\nREST requires that a client make a request to the server in order to retrieve or modify data on the server. A request generally consists of:\n-   an HTTP verb, which defines what kind of operation to perform\n-   aheader, which allows the client to pass along information about the request\n-   a path to a resource\n-   an optional message body containing data\n**Versioning**\n\n/api/v1/article/1234\n\n/api/v2/article/1234\n**HTTP Request Methods / Verbs**\n\n1.  [GET](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/GET)\n\nTheGETmethod requests a representation of the specified resource. Requests usingGET should only retrieve data.\n\n2.  [HEAD](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/HEAD)\n\nTheHEADmethod asks for a response identical to that of aGETrequest, but without the response body.\n\n3.  [POST](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/POST)\n\nThePOSTmethod is used to submit an entity to the specified resource, often causing a change in state or side effects on the server.\n\n4.  [PUT](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/PUT)\n\nThePUTmethod replaces all current representations of the target resource with the request payload.\n\n5.  [DELETE](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/DELETE)\n\nTheDELETEmethod deletes the specified resource.\n\n6.  [CONNECT](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/CONNECT)\n\nTheCONNECTmethod establishes a tunnel to the server identified by the target resource.\n\n7.  [OPTIONS](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/OPTIONS)\n\nTheOPTIONSmethod is used to describe the communication options for the target resource.\n\n8.  [TRACE](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/TRACE)\n\nTheTRACEmethod performs a message loop-back test along the path to the target resource.\n\n9.  [PATCH](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/PATCH)\n\nThePATCHmethod is used to apply partial modifications to a resource.\n**HEADERS AND ACCEPT PARAMETERS**\n\n1.  Accept: type of content that client is able to receive, it ensures that the server does not send data that cannot be understood or processed by the client. Options are MIME Types\n\n2.  Paths\n**Request Headers**\n\n**Referer header -**â€Štells the URL from where the request has originated.\n\n**User-Agent header -**â€Šadditional information about the browser being used to generate the request.\n\n**Host header -**â€Šuniquely identifies a host name; it is necessary when multiple web pages are hosted on the same server.\n\n**Cookie header -**â€Šsubmits additional parameters to the client.\n**Response Headers**\n\n**Server header -**â€Šinformation about which web server software is being used.\n\n**Set-Cookie header -**â€Šissues the cookie to the browser.\n\n**Message body -**â€Šit is common for an HTTP response to hold a message body.\n\n**Content-Length header -**â€Štells the size of the message body in bytes.\nEX - If we wanted to view all customers, the request would look like this:\n\nGET <http://fashionboutique.com/customers>\nAccept: application/json\nA possible response header would look like:\n\nStatus Code: 200 (OK)\nContent-type: application/json\n\nfollowed by thecustomersdata requested inapplication/json format.\n**Idempotent -** An idempotent HTTP method is a HTTP method that can be called many times without different outcomes.\n\n**Safe -** Safe methods are HTTP methods that do not modify resources.\n\n| **HTTP Method** | **Idempotent** | **Safe** |\n|-----------------|----------------|----------|\n| OPTIONS         | Yes            | Yes      |\n| GET             | Yes            | Yes      |\n| HEAD            | Yes            | Yes      |\n| PUT             | Yes            | No       |\n| POST            | No             | No       |\n| DELETE          | Yes            | No       |\n| PATCH           | No             | No       |\n<https://www.codecademy.com/articles/what-is-rest>\n\n## Form content types**\n\n1.  **application/x-www-form-urlencoded**\n\nThis is the default content type. Forms submitted with this content type must be encoded as follows:\n\n1.  Control names and values are escaped. Space characters are replaced by`+', and then reserved characters are escaped as described in[[RFC1738]](https://www.w3.org/TR/html401/references.html#ref-RFC1738), section 2.2: Non-alphanumeric characters are replaced by`%HH', a percent sign and two hexadecimal digits representing the ASCII code of the character. Line breaks are represented as \"CR LF\" pairs (i.e.,`%0D%0A').\n\n2.  The control names/values are listed in the order they appear in the document. The name is separated from the value by`='and name/value pairs are separated from each other by`&'.2.  **multipart/form-data**\n\nThe content type \"application/x-www-form-urlencoded\" is inefficient for sending large quantities of binary data or text containing non-ASCII characters. The content type \"multipart/form-data\" should be used for submitting forms that contain files, non-ASCII data, and binary data.-   If you want to send simple text/ ASCII data, thenx-www-form-urlencodedwill work. This is the default.\n-   You can useRawif you want to send plain text or JSON or any other kind of string. Like the name suggests, Postman sends your raw string data as it is without modifications. The type of data that you are sending can be set by using the content-type header from the drop down.\n-   Binarycan be used when you want to attach non-textual data to the request, e.g. a video/audio file, images, or any other binary data file.-   **Request body**\n\nWhile constructing requests, you would be dealing with the request body editor a lot. Postman lets you send almost any kind of HTTP request. The body editor is divided into 4 areas and has different controls depending on the body type.-   **form-data**\n\nmultipart/form-data is the default encoding a web form uses to transfer data.This simulates filling a form on a website, and submitting it. The form-data editor lets you set key/value pairs (using the key-value editor) for your data. You can attach files to a key as well. Do note that due to restrictions of the HTML5 spec, files are not stored in history or collections. You would have to select the file again at the time of sending a request.-   **urlencoded**\n\nThis encoding is the same as the one used in URL parameters. You just need to enter key/value pairs and Postman will encode the keys and values properly. Note that you can not upload files through this encoding mode. There might be some confusion between form-data and urlencoded so make sure to check with your API first.-   **raw**\n\nA raw request can contain anything. Postman doesn't touch the string entered in the raw editor except replacing environment variables. Whatever you put in the text area gets sent with the request. The raw editor lets you set the formatting type along with the correct header that you should send with the raw body. You can set the Content-Type header manually as well. Normally, you would be sending XML or JSON data here.-   **binary**\n\nbinary data allows you to send things which you can not enter in Postman. For example, image, audio or video files. You can send text files as well. As mentioned earlier in the form-data section, you would have to reattach a file if you are loading a request through the history or the collection.\n<http://restcookbook.com>\n\n## Six Contraints**\n-   Uniform Interface\n-   Stateless\n-   Client-server\n-   Cacheable\n-   Layered system\n-   Code on demand\n**Resource Based**\n-   Things vs actions\n-   Nouns vs verbs\n-   Versus SOAP-RPC\n-   Identified by URIs\n    -   Multiple URIs may refer to same resource\n-   Separate from their representation(s)\n**Representations**\n-   How resources get manipulated\n-   Part of the resource state\n    -   Transferred between client and server\n-   Typically JSON or XML\n-   Example:\n    -   Resource: person (Todd)\n    -   Service: contact information (GET)\n    -   Representation:\n        -   name, address, phone number\n        -   JSON or XML format\n**Uniform Interface Contraint**\n-   Defines the interface between client and server\n-   Simplifies and decouples the architecture\n-   Fundamental to RESTful design\n    -   HTTP verbs (GET, PUT, POST, DELETE)\n    -   URIs (resource name)\n    -   HTTP response (status, body)\n**Stateless**\n-   Server contains no client state\n-   Each request contains enough context to process the message\n    -   Self-descriptive messages\n-   Any session state is held on the client\n**Client-Server**\n-   Assume a disconnected system\n-   Separation of concerns\n-   Uniform interface is the link between the two\n**Cacheable**\n-   Server responses (representations) are cacheable\n    -   Implicitly\n    -   Explicitly\n    -   Negotiated\n**Layered System**\n-   Client can't assume direct connection to server\n-   Software or hardware intermediaries between client and server\n-   Improves scalability\n**Code on Demand**\n-   Server can temporily extend client\n-   Transfer logic to client\n-   Client executes logic\n-   For examples:\n    -   Java applets\n    -   JavaScript\n-   The only optional constraint\n**Summary**\n-   Violating any constraint other than Code on Demand means service is not strictly RESTful\n    -   Example: Three-legged OAUTH2\n-   Compliance with REST constraints allow:\n    -   Scalability\n    -   Simplicity\n    -   Modifiability\n    -   Visibility\n    -   Portability\n    -   Reliability\n<https://www.restapitutorial.com/lessons/whatisrest.html>\n\n## Best Practices**\n\nProducing a great API is 80% art and 20% science. Creating a URL hierarchy representing sensible resources is the art part. Having sensible resource names (which are just URL paths, such as /customers/12345/orders) improves the clarity of what a given request does.\nAppropriate resource names provide context for a service request, increasing understandability of the API. Resources are viewed hierarchically via their URI names, offering consumers a friendly, easily-understood hierarchy of resources to leverage in their applications.\nHere are some quick-hit rules for URL path (resource name) design:\n-   Use identifiers in your URLs instead of in the query-string. Using URL query-string parameters is fantastic for filtering, but not for resource names.\n    -   **Good:**/users/12345\n    -   **Poor:**/api?type=user&id=23\n-   Leverage the hierarchical nature of the URL to imply structure.\n-   Design for your clients, not for your data.\n-   Resource names should be nouns. Avoid verbs as resource names, to improve clarity. Use the HTTP methods to specify the verb portion of the request.\n-   Use plurals in URL segments to keep your API URIs consistent across all HTTP methods, using the collection metaphor.\n    -   **Recommended:**/customers/33245/orders/8769/lineitems/1\n    -   **Not:**/customer/33245/order/8769/lineitem/1\n-   Avoid using collection verbiage in URLs. For example 'customer_list' as a resource. Use pluralization to indicate the collection metaphor (e.g. customers vs. customer_list).\n-   Use lower-case in URL segments, separating words with underscores ('_') or hyphens ('-'). Some servers ignore case so it's best to be clear.\n-   Keep URLs as short as possible, with as few segments as makes sense.\n<https://www.restapitutorial.com/lessons/restquicktips.html>\n\n<https://www.restapitutorial.com/lessons/restfulresourcenaming.html>\n\n## API Authentication**\n\n<https://medium.com/data-rebels/fastapi-authentication-revisited-enabling-api-key-authentication-122dc5975680>\n\n## Others**\n\nHATEOAS (Hypermedia As The Engine Of Application State, is a constraint of the REST application architecture that distinguishes it from most other network application architectures. The principle is that a client interacts with a network application entirely through hypermedia provided dynamically by application servers)\n<http://restcookbook.com/Basics/hateoas>\n\n## References**\n\n<https://medium.com/@liran.tal/a-comprehensive-guide-to-contract-testing-apis-in-a-service-oriented-architecture-5695ccf9ac5a>\n<https://blog.feathersjs.com/design-patterns-for-modern-web-apis-1f046635215>\n-   **Service layer:** A protocol independent interface to our application logic\n-   **REST:** An architectural design principle for creating web APIs\n-   **RESTful services:** A service layer that follows the REST architecture and HTTP protocol methods\n-   **Middleware:** Reusable functions that can control the flow of data and trigger additional functionality when interacting with REST services\n-   **Real-time:** A set of events that can be sent automatically when following the REST architecture\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/Rsocket/","title":"Rsocket"},"frontmatter":{"draft":false},"rawBody":"# Rsocket\n\nCreated: 2019-07-03 12:46:30 +0500\n\nModified: 2019-07-03 12:50:32 +0500\n\n---\n\nRSocket is a binary protocol for use on byte stream transports such as TCP, WebSockets, and Aeron.\nIt enables the following symmetric interaction models via async message passing over a single connection:\n-   request/response (stream of 1)\n-   request/stream (finite stream of many)\n-   fire-and-forget (no response)\n-   channel (bi-directional streams)\nIt supports session resumption, to allow resuming long-lived streams across different transport connections. This is particularly useful for mobileâ¬„server communication when network connections drop, switch, and reconnect frequently.\nRSocketis an application protocol initially developed by[Netflix](https://en.wikipedia.org/wiki/Netflix),that supports[Reactive Streams](https://en.wikipedia.org/wiki/Reactive_Streams). The motivation behind its development was to replace hypertext transfer protocol[(HTTP](https://en.wikipedia.org/wiki/HTTP)), which is inefficient for many tasks such as[microservices](https://en.wikipedia.org/wiki/Microservice)communication, with a protocol that has less overhead.\nAlternative - grpc\n<http://rsocket.io>\n\n<https://en.wikipedia.org/wiki/Rsocket>\n\n<https://github.com/rsocket/rsocket>\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/TCP-(Connection-Oriented-Protocol)/","title":"TCP (Connection Oriented Protocol)"},"frontmatter":{"draft":false},"rawBody":"# TCP (Connection Oriented Protocol)\n\nCreated: 2019-06-19 16:46:08 +0500\n\nModified: 2020-07-14 23:58:54 +0500\n\n---\n\nTCP stands for transmission control protocol. It is implemented in the transport layer of the IP/TCP model and is used to establish reliable connections.\nTCP is one of the protocols that encapsulates data into packets. It then transfers these to the remote end of the connection using the methods available on the lower layers. On the other end, it can check for errors, request certain pieces to be resent, and reassemble the information into one logical piece to send to the application layer.\nThe protocol builds up a connection prior to data transfer using a system called a **three-way handshake**. This is a way for the two ends of the communication to acknowledge the request and agree upon a method of ensuring data reliability.\nAfter the data has been sent, the connection is torn down using a similar **four-way handshake**.\nTCP is the protocol of choice for many of the most popular uses for the internet, including WWW, FTP, SSH, and email. It is safe to say that the internet we know today would not be here without TCP.\n**TCP Congestion Control**\n\n**Common Algorithms**\n-   **Reno**\n-   **BIC**\n-   **CUBIC**\n**General Strategy**\n-   Increase sending rate if ACK\n-   Decrease if ACK missed\n![ACKs being received, X loss, so decrease rate so increase rate ](media/TCP-(Connection-Oriented-Protocol)-image1.png)\ncat /proc/sys/net/ipv4/tcp_congestion_control\n**TCP BBR**\n\nIt's a TCP Congestion Control Algorithm\n\nNew Backoff algorithm for handling network congestion. Great for clients working in low bandwidth connection like 2G.\n\nOnly needed to set a flag on server, Client doesn't need to be updated\n<https://medium.com/google-cloud/tcp-bbr-magic-dust-for-network-performance-57a5f1ccf437>\n\n## Additive increase/multiplicative decrease (AIMD)**\n\nTheadditive-increase/multiplicative-decrease(AIMD) algorithm is a feedback control algorithm best known for its use in[TCP congestion control](https://en.wikipedia.org/wiki/TCP_congestion_control). AIMD combines linear growth of the congestion window with an exponential reduction when congestion is detected. Multiple flows using AIMD congestion control will eventually converge to use equal amounts of a shared link.The related schemes of multiplicative-increase/multiplicative-decrease (MIMD) and additive-increase/additive-decrease (AIAD) do not reach[stability](https://en.wikipedia.org/wiki/Stability_theory).\n<https://en.wikipedia.org/wiki/Additive_increase/multiplicative_decrease>\nTCP is a reliable stream delivery service which guarantees that all bytes received will be identical and in the same order as those sent. Since packet transfer by many networks is not reliable, TCP achieves this using a technique known as*positive acknowledgement with re-transmission*. This requires the receiver to respond with an acknowledgement message as it receives the data. The sender keeps a record of each packet it sends and maintains a timer from when the packet was sent. The sender re-transmits a packet if the timer expires before receiving the acknowledgement. The timer is needed in case a packet gets lost or corrupted.\nWhile IP handles actual delivery of the data, TCP keeps track of*segments*- the individual units of data transmission that a message is divided into for efficient routing through the network. For example, when an HTML file is sent from a web server, the TCP software layer of that server divides the file into segments and forwards them individually to the[internet layer](https://en.wikipedia.org/wiki/Internet_layer)in the[network stack](https://en.wikipedia.org/wiki/Network_stack). The internet layer software encapsulates each TCP segment into an IP packet by adding a header that includes (among other data) the destination[IP address](https://en.wikipedia.org/wiki/IP_address). When the client program on the destination computer receives them, the TCP software in the transport layer re-assembles the segments and ensures they are correctly ordered and error-free as it streams the file contents to the receiving application.\n**Concepts**\n\nThe absolute limitation onTCPpacketsizeis 64K (65535 bytes), but in practicality this is far larger than thesizeof any packet you will see, because the lower layers (e.g. ethernet) have lower packetsizes. The MTU (MaximumTransmission Unit) for Ethernet, for instance, is 1500 bytes\n**MTU (Maximum Transmission Unit)**\n\nMaximum transmission unit is the maximum size of a packet or frame that can flow across the network, without being fragmented. For Ethernet networks, the maximum MTU value is 1500 bytes.\nPath MTU Discovery(PMTUD) is a standardized technique in[computer networking](https://en.wikipedia.org/wiki/Computer_networking) for determining the[maximum transmission unit (MTU)](https://en.wikipedia.org/wiki/Maximum_transmission_unit)size on the network path between two Internet Protocol (IP) hosts, usually with the goal of avoiding[IP fragmentation](https://en.wikipedia.org/wiki/IP_fragmentation). PMTUD was originally intended for routers in[Internet Protocol Version 4](https://en.wikipedia.org/wiki/IPv4)(IPv4).However, all modern operating systems use it on endpoints. In[IPv6](https://en.wikipedia.org/wiki/IPv6), this function has been explicitly delegated to the end points of a communications session.\n<https://en.wikipedia.org/wiki/Path_MTU_Discovery>\n\n## MSS (Maximum Segment Size)**\n\nMaximum segment size is the maximum TCP datagram size. It represents the maximum payload size an endpoint is willing to accept within a single packet. Maximum MSS value is 1460 bytes. The MSS, IP header and TCP header, together make up the MTU value. That is, 1500 MTU = 1460 byte MSS + 20 byte IP header + 20 byte TCP header. Said another way, MSS = MTU --- 40.\n![20 bytes IP header 20 bytes TCP header MTU 1460 bytes TCP Payload MSS ](media/TCP-(Connection-Oriented-Protocol)-image2.png)\n\nDo note that MSS is only announced during the TCP handshake in the SYN segment, it is not a negotiated parameter. Meaning, client and server can announce their own individual and different MSS values[[rfc879]](https://tools.ietf.org/html/rfc879). The actual MSS is selected based on the endpoint's buffer and outgoing interface MTU. This can be represented visually by considering a communication between client A and server B[[cisco-ipfrag]](https://www.cisco.com/c/en/us/support/docs/ip/generic-routing-encapsulation-gre/25885-pmtud-ipfrag.html).**TCP Segment Structure**\n\nTransmission Control Protocol accepts data from a data stream, divides it into chunks, and adds a TCP header creating a TCP segment. The TCP segment is then[encapsulated](https://en.wikipedia.org/wiki/Encapsulation_(networking))into an Internet Protocol (IP) datagram, and exchanged with peers.\nThe termTCP packetappears in both informal and formal usage, whereas in more precise terminology segment refers to the TCP[protocol data unit](https://en.wikipedia.org/wiki/Protocol_data_unit)(PDU),datagramto the IP PDU, andframeto the data link layer PDU:\n\nProcesses transmit data by calling on the TCP and passing buffers of data as arguments. The TCP packages the data from these buffers into segments and calls on the internet module [e.g. IP] to transmit each segment to the destination TCP.\nA TCP segment consists of **a segmentheaderand adatasection**. The TCP header contains 10 mandatory fields, and an optional extension field (Options, pink background in table).\nThe data section follows the header. Its contents are the payload data carried for the application. The length of the data section is not specified in the TCP segment header. It can be calculated by subtracting the combined length of the TCP header and the encapsulating IP header from the total IP datagram length (specified in the IP header).\n![Offsets Octet 7 6 5 4 3 2 1 TCP Header 1 4 3 2 3 Octet o 4 8 12 16 20 Bit o 32 64 96 128 160 o 7 Source port 2 1 7 6 5 210765 Destination port Window Size Urgent pointer (if URG set) 2 1 Sequence number Acknowledgment number (if ACK set) Data offset Reserved 000 s C w R E R G c p s R T N N Checksum Options (if data offset > 5. Padded at the end with \"O\" bytes if necessary.) ](media/TCP-(Connection-Oriented-Protocol)-image3.png)\n**Sequence number (32 bits)**\n\nTCP uses sequence number field to keep track of the amount of data sent in a communication stream. TCP uses a random 32-bit number to identify the beginning of a conversation. This is known as the initial sequence number (ISN). Rather than starting all TCP conversations with 1, a random ISN helps to identify and keep traffic separate for each flow\nHas a dual role:\n-   If the SYN flag is set (1), then this is the initial sequence number. The sequence number of the actual first data byte and the acknowledged number in the corresponding ACK are then this sequence number plus 1.\n-   If the SYN flag is clear (0), then this is the accumulated sequence number of the first data byte of this segment for the current session.\n**Acknowledgement number**\n\nUsed by the receiving host to acknowledge successful receipt of a TCP segment. An ACK message is replied to the sending host, which includes the received sequence number incremented by 1. This number also informs the sending host, the sequence number of the next segment expected by the receiving host[[www.firewall.cx]](http://www.firewall.cx/networking-topics/protocols/tcp/134-tcp-seq-ack-numbers.html).\n**Relative sequence number**\n\nWireshark uses numbers relative to each TCP stream to keep track of each session. This essentially means that the 'Sequence' and 'Acknowledgement numbers' will always begin with a 0 for each new session. Using a small number makes it easier to read the packet captures, as opposed to looking at a large number (since 32-bit ISN can be anything from 0 to 4.2 billion). To verify if Wireshark is using this option, go to Wireshark->preferences->protocols->TCP-> check 'Relative sequence numbers'.\n**Next sequence number**\n\nThis is the length of the TCP payload + the current sequence number. It indicates the sequence number of the next segment that will sent by the client.\n**Selective ACK (SACK)**\n\nThis TCP option is used to identify a block of data that was received by a host. The sender does not re-transmit data identified by the left edge and right edge of SACK. This option can be used only if supported by both the parties and is negotiated during the TCP handshake[[packetlife-sack]](http://packetlife.net/blog/2010/jun/17/tcp-selective-acknowledgments-sack/).\n\n![Options: (12 bytes), Maxirnurn segment size, No-operation (NOP), TCP Option --- Maximum segment size: 146e bytes TCP Option --- No---operation (NOP) TCP Option --- Window scale: 8 (multiply by 256) TCP Option --- No---operation (NOP) TCP Option --- No---operation (NOP) TCP Option --- SACK permitted Kind: SACK Permitted (4) Length: 2 Window scale, No-operation (NOP), No-operation (NOP), SACK permitted ](media/TCP-(Connection-Oriented-Protocol)-image4.png)\n**Duplicate ACK**\n\nAs part of the TCP fast re-transmit mechanism, duplicate ACKs are used to inform sender of either segments received out-of-order or lost segments. Re-transmission of missing segments is performed immediately[[rfc2001-sec3]](https://tools.ietf.org/html/rfc2001).\n**Flags (9 bits) (aka Control bits)**\n\nContains 9 1-bit flags\n-   NS (1 bit): ECN-nonce - concealment protection (experimental: see[RFC 3540](https://tools.ietf.org/html/rfc3540)).\n-   CWR (1 bit): Congestion Window Reduced (CWR) flag is set by the sending host to indicate that it received a TCP segment with the ECE flag set and had responded in congestion control mechanism (added to header by[RFC 3168](https://tools.ietf.org/html/rfc3168)).\n-   ECE (1 bit): ECN-Echo has a dual role, depending on the value of the SYN flag. It indicates:\n    -   If the SYN flag is set (1), that the TCP peer is[ECN](https://en.wikipedia.org/wiki/Explicit_Congestion_Notification)capable.\n    -   If the SYN flag is clear (0), that a packet with Congestion Experienced flag set (ECN=11) in the IP header was received during normal transmission (added to header by[RFC 3168](https://tools.ietf.org/html/rfc3168)). This serves as an indication of network congestion (or impending congestion) to the TCP sender.\n-   URG (1 bit): indicates that the Urgent pointer field is significant\n-   ACK (1 bit): indicates that the Acknowledgment field is significant. All packets after the initial SYN packet sent by the client should have this flag set.\n-   PSH (1 bit): Push function. Asks to push the buffered data to the receiving application.\n-   RST (1 bit): Reset the connection\n-   SYN (1 bit): Synchronize sequence numbers. Only the first packet sent from each end should have this flag set. Some other flags and fields change meaning based on this flag, and some are only valid when it is set, and others when it is clear.\n-   FIN (1 bit): Last packet from sender.\n<https://en.wikipedia.org/wiki/Transmission_Control_Protocol>\n\n<https://medium.com/walmartlabs/how-tcp-segment-size-can-affect-application-traffic-flow-7bbceed5816e>\n\n## TCP Handshake (3-way handshake) (Positive Acknowledgement with Re-transmission / PAR)**\n\nA three-way handshake is a method used in a TCP/IP network to create a connection between a local host/client and server. It is a three-step method that requires both the client and server to exchange SYN and ACK (acknowledgment) packets before actual data communication begins.\nNow a device using PAR resend the data unit until it receives an acknowledgement. If the data unit received at the receiver's end is damaged(It checks the data with checksum functionality of the transport layer that is used for Error Detection), then receiver discards the segment. So the sender has to resend the data unit for which positive acknowledgement is not received.\n![HOST p send SYN (seq-x) receive SYN (seq y, ACK-x+1) send ACK (ack=y+l) HOST Q receive SYN (seq x) send SYN, (seq-y, ACK=x+1 receive ACK (ack=y+l) ](media/TCP-(Connection-Oriented-Protocol)-image5.png)\n-   **Step 1 (SYN):**In the first step, client wants to establish a connection with server, so it sends a segment with SYN(Synchronize Sequence Number) which informs server that client is likely to start communication and with what sequence number it starts segments with\n-   **Step 2 (SYN + ACK):**Server responds to the client request with SYN-ACK signal bits set. Acknowledgement(ACK) signifies the response of segment it received and SYN signifies with what sequence number it is likely to start the segments with\n-   **Step 3 (ACK):**In the final part client acknowledges the response of server and they both establish a reliable connection with which they will start the actual data transfer\n\nThe steps 1, 2 establish the connection parameter (sequence number) for one direction and it is acknowledged. The steps 2, 3 establish the connection parameter (sequence number) for the other direction and it is acknowledged. With these, a full-duplex communication is established.\nInitial sequence numbers are randomly selected while establishing connections between client and server.\n<https://www.geeksforgeeks.org/tcp-3-way-handshake-process>\n\n## TCP Connection Termination**\n\n![11](media/TCP-(Connection-Oriented-Protocol)-image6.png)\n\n1.  **Step 1 (FIN From Client) --**Suppose that the client application decides it wants to close the connection. (Note that the server could also choose to close the connection). This causes the client send a TCP segment with the**FIN**bit set to**1**to server and to enter the**FIN_WAIT_1**state. While in the**FIN_WAIT_1**state, the client waits for a TCP segment from the server with an acknowledgment (ACK).\n\n2.  **Step 2 (ACK From Server) --**When Server received FIN bit segment from Sender (Client), Server Immediately send acknowledgement (ACK) segment to the Sender (Client).\n\n3.  **Step 3 (Client waiting) --**While in the**FIN_WAIT_1**state, the client waits for a TCP segment from the server with an acknowledgment. When it receives this segment, the client enters the**FIN_WAIT_2**state. While in the**FIN_WAIT_2**state, the client waits for another segment from the server with the FIN bit set to 1.\n\n4.  **Step 4 (FIN from Server) --**Server sends FIN bit segment to the Sender(Client) after some time when Server send the ACK segment (because of some closing process in the Server).\n\n5.  **Step 5 (ACK from Client) --**When Client receive FIN bit segment from the Server, the client acknowledges the server's segment and enters the**TIME_WAIT**state. The**TIME_WAIT**state lets the client resend the final acknowledgment in case the**ACK**is lost.The time spent by client in the**TIME_WAIT**state is depend on their implementation, but their typical values are 30 seconds, 1 minute, and 2 minutes. After the wait, the connection formally closes and all resources on the client side (including port numbers and buffer data) are released.\nTCP states visited by ClientSide --\n\n![Closed Wait 30 seconds Receive FIN, Send ACK IN WAIT Receive ACK, send nothin Client application initiates a TCP connection Send SYN sta Receive SYN & ACK, send ACK Send FIN Client application initiates close connection Fig TCP states visited by a client TCP ](media/TCP-(Connection-Oriented-Protocol)-image7.png)\n\nTCP states visited by ServerSide --\n\n![Sewer application creates a listen socket Closed RCV ACK, Send nothin AST A Receive SYN & send send FIN SYN & ACK Receive ACK, send nothin Established Receive FIN, send ACK Fig TCP states visited by a client TCP ](media/TCP-(Connection-Oriented-Protocol)-image8.png)\n<https://www.geeksforgeeks.org/tcp-connection-termination>\n\n## Problems**\n-   **Head of line blocking**\n\nThe role of TCP is to deliver the entire stream of bytes, in the correct order, from one endpoint to the other. When a TCP packet carrying some of those bytes is lost on the network path, it creates a gap in the stream and TCP needs to fill it by resending the affected packet when the loss is detected. While doing so, none of the successfully delivered bytes that follow the lost ones can be delivered to the application, even if they were not themselves lost and belong to a completely independent HTTP request. So they end up getting unnecessarily delayed as TCP cannot know whether the application would be able to process them without the missing bits. This problem is known as \"head-of-line blocking\".\n**Peformance-enhancing proxy**\n\n**Performance-enhancing proxies**(**PEPs**) are network agents designed to improve the end-to-end performance of some[communication protocols](https://en.wikipedia.org/wiki/Communication_protocols).\n**Classification**\n\nAvailable PEP implementations use different methods to enhance performance.\n-   **Proxy type:**A PEP can either 'split' a connection or 'snoop' into it. In the first case, the proxy pretends to be the opposite endpoint of the connection in each direction, literally splitting the connection into two. In the latter case, the proxy controls the transmissions of the TCP segments in both directions, by ack filtering and reconstruction in the existing connection (see[protocol spoofing](https://en.wikipedia.org/wiki/Protocol_spoofing)). This is based on the OSI level of implementation of the PEP.[^[1]^](https://en.wikipedia.org/wiki/Performance-enhancing_proxy#cite_note-1)\n-   **Distribution:**PEPs can be either integrated or distributed. Integrated PEP will run on a single box, while distributed PEP will require to be installed on both sides of the link that cause the performance degradation. This is quite common in commercial PEP devices, which act as a[black box](https://en.wikipedia.org/wiki/Black_box), using more or less open protocols to communicate between them in the place of TCP.\n-   **Symmetry:**A PEP implementation may be symmetric or asymmetric. Symmetric PEPs use identical behavior in both directions; the actions taken by the PEP occur independent from which interface a packet is received. Asymmetric PEPs operate differently in each direction, which can cause, for example, only one link direction performance to be enhanced.\n**Types**\n-   Split-TCP\n-   Ack filtering/decimation\n-   Snoop\n-   D-proxy\n<https://en.wikipedia.org/wiki/Performance-enhancing_proxy>\n\n## TCP Split**\n\nTCP splitting uses a performance enhancing proxy access node that divides the end-to-end TCP connection between the client and the server into a multi-overlay-hop path where each overlay hop is an independent TCP connection, such that the RTT of each overlay hop is lower than the direct RTT between A and B. Each hop's throughput is governed by that hop's RTT and is individually higher than the direct throughput between A and B.\nBut performance issues can arise due to the interaction among path segments and different layers for any particular solution. For example, in TCP Splitting, the intermediate node (the spoofer) sends back a spoofing ACK packet to the TCP sender immediately upon receiving a TCP data packet instead of waiting for the ACK from the final TCP destination.\nOne of the well known problems of TCP splitting is that by breaking the end-to-end connection, a split TCP connection is no longer reliable or secure, and a server failure may cause the client to believe that data has been successfully received when it has not. From theApplication Server TCP Splittingpane, you can access the TCP Splitting for the Application Server.\n<https://webhelp.radware.com/AppDirector/v214/214Advanced%20Capabilities.07.04.htm>\n\n[split tcp protocol | Adhoc N/W | lec-34 | Bhanu Priya](https://www.youtube.com/watch?v=U1ryk2zIAjc)\n\n![é˜¿ ã• ã€ ~ ç‚º 4 ã„ è¦ & å‹Ÿ ã ã€ 1 ( ã‚¤ ã€ é˜œ ](media/TCP-(Connection-Oriented-Protocol)-image9.jpg)\n**Tools**\n\n**netstat**\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/UDP/","title":"UDP"},"frontmatter":{"draft":false},"rawBody":"# UDP\n\nCreated: 2019-09-28 13:50:40 +0500\n\nModified: 2019-09-28 14:20:46 +0500\n\n---\n\nIn[computer networking](https://en.wikipedia.org/wiki/Computer_network), theUser Datagram Protocol(UDP) is one of the core members of the[Internet protocol suite](https://en.wikipedia.org/wiki/Internet_protocol_suite). The protocol was designed by[David P. Reed](https://en.wikipedia.org/wiki/David_P._Reed)in 1980 and formally defined in[RFC](https://en.wikipedia.org/wiki/Request_for_Comments_(identifier))[768](https://tools.ietf.org/html/rfc768). With UDP, computer applications can send messages, in this case referred to as[datagrams](https://en.wikipedia.org/wiki/Datagram), to other hosts on an[Internet Protocol](https://en.wikipedia.org/wiki/Internet_Protocol)(IP) network. Prior communications are not required in order to set up[communication channels](https://en.wikipedia.org/wiki/Communication_channel)or data paths.\nUDP uses a simple[connectionless communication](https://en.wikipedia.org/wiki/Connectionless_communication)model with a minimum of protocol mechanisms. UDP provides[checksums](https://en.wikipedia.org/wiki/Checksum)for data integrity, and[port numbers](https://en.wikipedia.org/wiki/Port_numbers)for addressing different functions at the source and destination of the datagram. It has no [handshaking](https://en.wikipedia.org/wiki/Handshaking) dialogues, and thus exposes the user's program to any[unreliability](https://en.wikipedia.org/wiki/Reliability_(computer_networking))of the underlying network; there is no guarantee of delivery, ordering, or duplicate protection. If error-correction facilities are needed at the network interface level, an application may use[Transmission Control Protocol](https://en.wikipedia.org/wiki/Transmission_Control_Protocol)(TCP) or[Stream Control Transmission Protocol](https://en.wikipedia.org/wiki/Stream_Control_Transmission_Protocol)(SCTP) which are designed for this purpose.\nUDP is suitable for purposes where error checking and correction are either not necessary or are performed in the application; UDP avoids the overhead of such processing in the[protocol stack](https://en.wikipedia.org/wiki/Protocol_stack). Time-sensitive applications often use UDP because dropping packets is preferable to waiting for packets delayed due to[retransmission](https://en.wikipedia.org/wiki/Retransmission_(data_networks)), which may not be an option in a[real-time system](https://en.wikipedia.org/wiki/Real-time_system).\n**Attributes**\n\nUDP is a simple message-oriented[transport layer](https://en.wikipedia.org/wiki/Transport_layer)protocol that is documented in[RFC](https://en.wikipedia.org/wiki/Request_for_Comments_(identifier))[768](https://tools.ietf.org/html/rfc768). Although UDP provides integrity verification (via[checksum](https://en.wikipedia.org/wiki/Checksum)) of the header and payload,it provides no guarantees to the[upper layer protocol](https://en.wikipedia.org/wiki/Upper_layer_protocol)for message delivery and the UDP layer retains no state of UDP messages once sent. For this reason, UDP sometimes is referred to as[Unreliable](https://en.wikipedia.org/wiki/Reliability_(computer_networking))Datagram Protocol.If transmission reliability is desired, it must be implemented in the user's application.\nA number of UDP's attributes make it especially suited for certain applications.\n-   It istransaction-oriented, suitable for simple query-response protocols such as the[Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System)or the[Network Time Protocol](https://en.wikipedia.org/wiki/Network_Time_Protocol).\n-   It provides[datagrams](https://en.wikipedia.org/wiki/Datagram), suitable for modeling other protocols such as[IP tunneling](https://en.wikipedia.org/wiki/IP_tunneling)or[remote procedure call](https://en.wikipedia.org/wiki/Remote_procedure_call)and the[Network File System](https://en.wikipedia.org/wiki/Network_File_System).\n-   It issimple, suitable for[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping)or other purposes without a full[protocol stack](https://en.wikipedia.org/wiki/Protocol_stack), such as the[DHCP](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol)and[Trivial File Transfer Protocol](https://en.wikipedia.org/wiki/Trivial_File_Transfer_Protocol).\n-   It isstateless, suitable for very large numbers of clients, such as in[streaming media](https://en.wikipedia.org/wiki/Streaming_media)applications such as[IPTV](https://en.wikipedia.org/wiki/IPTV).\n-   Thelack of retransmission delaysmakes it suitable for real-time applications such as[Voice over IP](https://en.wikipedia.org/wiki/Voice_over_IP),[online games](https://en.wikipedia.org/wiki/Online_games), and many protocols using[Real Time Streaming Protocol](https://en.wikipedia.org/wiki/Real_Time_Streaming_Protocol).\n-   Because it supports[multicast](https://en.wikipedia.org/wiki/Multicast), it is suitable for broadcast information such as in many kinds of[service discovery](https://en.wikipedia.org/wiki/Service_discovery)and shared information such as[Precision Time Protocol](https://en.wikipedia.org/wiki/Precision_Time_Protocol)and[Routing Information Protocol](https://en.wikipedia.org/wiki/Routing_Information_Protocol).\n**Packet Structure**\n\n| ***Offsets***                                                | [**Octet**](https://en.wikipedia.org/wiki/Octet_(computing)) | **0**       | **1**     | **2**            | **3**     |\n|-----------|-----------|--------------|-----------|------------------|---------|\n| [**Octet**](https://en.wikipedia.org/wiki/Octet_(computing)) | [**Bit**](https://en.wikipedia.org/wiki/Bit)                 | **0-7**    | **8-15** | **16-23**        | **24-31** |\n| **0**                                                        | **0**                                                       | Source port |          | Destination port |          |\n| **4**                                                        | **32**                                                       | Length      |          | Checksum         |          |\n\nThe UDP header consists of 4 fields, each of which is 2 bytes (16 bits).The use of the checksum andsource portfields is optional in IPv4 (pink background in table). In IPv6 only thesource portfield is optional.\n\n**Source port number**\n\nThis field identifies the sender's port, when used, and should be assumed to be the port to reply to if needed. If not used, it should be zero. If the source host is the client, the port number is likely to be an ephemeral port number. If the source host is the server, the port number is likely to be a[well-known port](https://en.wikipedia.org/wiki/Well-known_port)number.\n**Destination port number**\n\nThis field identifies the receiver's port and is required. Similar to source port number, if the client is the destination host then the port number will likely be an ephemeral port number and if the destination host is the server then the port number will likely be a well-known port number.\n**Length**\n\nThis field specifies the length in bytes of the UDP header and UDP data. The minimum length is 8 bytes, the length of the header. The field size sets a theoretical limit of 65,535 bytes (8 byte header + 65,527 bytes of data) for a UDP datagram. However the actual limit for the data length, which is imposed by the underlying[IPv4](https://en.wikipedia.org/wiki/IPv4)protocol, is 65,507 bytes (65,535 âˆ’ 8 byte UDP header âˆ’ 20 byte[IP header](https://en.wikipedia.org/wiki/IPv4_header)).\nUsing IPv6[jumbograms](https://en.wikipedia.org/wiki/Jumbogram)it is possible to have UDP packets of size greater than 65,535 bytes.[[5]](https://en.wikipedia.org/wiki/User_Datagram_Protocol#cite_note-5)[RFC](https://en.wikipedia.org/wiki/Request_for_Comments_(identifier))[2675](https://tools.ietf.org/html/rfc2675)specifies that the length field is set to zero if the length of the UDP header plus UDP data is greater than 65,535.\n**Checksum**\n\nThe[checksum](https://en.wikipedia.org/wiki/Checksum)field may be used for error-checking of the header and data. This field is optional in IPv4, and mandatory in IPv6.The field carries all-zeros if unused.\n**Checksum computation**\n\nThe method used to compute the checksum is defined in[RFC](https://en.wikipedia.org/wiki/Request_for_Comments_(identifier))[768](https://tools.ietf.org/html/rfc768):\n\nChecksum is the 16-bit[one's complement](https://en.wikipedia.org/wiki/One%27s_complement)of the one's complement sum of a pseudo header of information from the IP header, the UDP header, and the data, padded with zero octets at the end (if necessary) to make a multiple of two octets.\nIn other words, all 16-bit words are summed using one's complement arithmetic. Add the 16-bit values up. On each addition, if a carry-out (17th bit) is produced, swing that 17th carry bit around and add it to the least significant bit of the running total.Finally, the sum is then one's complemented to yield the value of the UDP checksum field.\nIf the checksum calculation results in the value zero (all 16 bits 0) it should be sent as the one's complement (all 1s) as a zero-value checksum indicates no checksum has been calculated.\nThe difference between[IPv4](https://en.wikipedia.org/wiki/IPv4)and[IPv6](https://en.wikipedia.org/wiki/IPv6)is in the pseudo header used to compute the checksum and the checksum is not optional in IPv6.\n**Use Cases**\n\n1.  Streaming Media\n\nVoice and video traffic is generally transmitted using UDP. Real-time video and audio streaming protocols are designed to handle occasional lost packets, so only slight degradation in quality occurs, rather than large delays if lost packets were retransmitted.\n\n2.  Real-time multiplayer games\n\n3.  VoIP\n\n4.  Areas where latency and jitter are the primary concernsTCP port numbers aren't the same as UDP port numbers\nPorts\n\n| udp/53 | DNS Domain Name Services queries |\n|--------|----------------------------------|\nUse Case -\n\n1.  VoIP (Voice over IP)\n\nWe don't care if some packets are lost\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/Video---Live-Streaming/","title":"Video / Live Streaming"},"frontmatter":{"draft":false},"rawBody":"# Video / Live Streaming\n\nCreated: 2018-06-25 01:09:35 +0500\n\nModified: 2022-08-18 15:27:11 +0500\n\n---\n\n**Tools**\n-   **Zoom**\n-   <https://meet.jit.si>\n-   <https://jitsi.org>\n-   Google Meet\n-   Skype\n-   <https://goodmeetings.ai>\nRTMP - Real-Time Messaging Protocol\n\n<https://github.com/facebookincubator/rtmp-go-away>\nHLS - <https://www.toptal.com/apple/introduction-to-http-live-streaming-hls>\nMPEG-DASH - Dynamic Adaptive Streaming over HTTP\n**WebRTC**\n\n**Peer to peer live streaming protocol**\n\n**ICE - Interactivity Connection Establishment**\n-   A framework for connecting peers\n-   Tries to find the best path for each call\n-   Vast majority of calls can use STUN\n-   ICE Agent\n-   ICE Candidates\n\n**SDP - Session Description Protocol**\n-   What capabilities are there in a call (Audio/Video)\n-   What codecs can be used\n-   What bandwidth is available for the call\n\n**STUN - Session Traversals Utility for NAT**\n-   What is my public IP address (because of NAT)\n-   NAT Hole punching\n-   A server that is publicly available on internet and that server will respond with your public ip and port that you requested with.\n-   Google/Twilio hosts STUN server / CoTurn/ Xirsys (<https://xirsys.com>)\n\n**TURN - Traversal Using Relay around NAT**\n-   Provide a cloud fallback if peer-to-peer communication fails\n-   Data is sent through server, uses server environment\n-   Ensures the call works in almost all environments\n-   Relays packet from point A to point B\n-   All TURN servers are also STUN servers\n![Signaling App App SessionDescription WebRTC Browser Caller Media Signaling App SessionDescription Browser Callee ](media/Video---Live-Streaming-image1.png)\n![Signalling TURN server Media NAT peer STUN server webrtc Signalling TURN server NAT Peer STUN server ](media/Video---Live-Streaming-image2.png)\n[WebRTC Crash Course](https://youtu.be/FExZvpVvYxA)\n**FFMPEG Streaming**\n\nffmpeg (fast forward motion pictures expert group) for streaming videos over rtp to any number of different locations using multicasting\n**Commands**\n\nffmpeg -i demo.mp4 -v 0 -vcodec mpeg4 -f mpegts udp://192.168.1.119:1234\n\nffplay udp://192.168.1.119:1234\n\nffmpeg -i rtp://@239.35.10.4:10000 -map 0:0 -map 0:2 -vcodec copy -acodec\n\ncopy -t 10 -y test.mkv-   Wowza Streaming Server\n-   Red5 FOSS Streaming Server\nFFMPEG stitching multiple files into one\n\n<https://trac.ffmpeg.org/wiki/Concatenate>\n\n## References**\n\n<https://www.wowza.com/docs/tutorials>\n\n<http://red5.org>\n\n<https://www.wowza.com/docs/how-to-publish-a-video-file-as-a-live-stream>\n<https://s3-ap-southeast-1.amazonaws.com/shivamzenatix/Test-Deepak/demo.mp4>\n<https://aws.amazon.com/cloudfront/streaming>\n\n<https://aws.amazon.com/answers/media-entertainment/live-streaming>\n<https://trac.ffmpeg.org/wiki/Map>\n\n<https://trac.ffmpeg.org/wiki/StreamingGuide>\n\n<https://trac.ffmpeg.org/wiki/Capture/Webcam>\n[**https://go.bitmovin.com/ultimate-guide-container-formats**](https://go.bitmovin.com/ultimate-guide-container-formats)\n\n![Chapter 1 Welcome to the ultimate guide to Container Formats. This all---inclusive whitepaper covers the four most common container formats (as of 2019) and it matters to you, The first chapter defines key terminology and how containers function Within Video players. Container Basics & Terminology Definition Of Codecs Codecs are an internet protocol used to store media Signals in binary form. The goal Of most CO-docs 'o compress the raw media Signal in a IOSSY Way. meaning 'hot the compression is irreversible. Partial data is discarded from the media Signal and approximations ore made. The most cornrnon media signals are Video, audio and Specific examples Of Video COdÃ¸cs include: AVI, 4264. HEVC, and VP9: The most commonly used audio codecs AAC MP3, and Opus. There ore many different for each media Signal (as illustrated in fig l). Subtitles SRI SCC Video Audio AAC MP3 opus A Single media Signal is Often called an Elementary Stream or more Simply - Stream. The video development/broodcast industry will Often use terms Such Media, or H (or H interchangeably with â€¢ Video Streams: What is a media container? Media Containers (also known as Container Formats) are a metafile format specification describing how different multimedia data elements (streams) and metadata coexist in Some of the elements specified by a container format are; Stream Encapsulation Allowance of one or more media streams to exist in a single Timing/ Synchrmization --- The container adds dota on how the different streams in the file can be used together, Ex; Affixing correct timestamps to synchronize lip-movement in a video stream with sounds in the audio stream, Seeking --- The container provides additional time-oriented information that determines a specific point which a viewer can jump to in a video file/stream, Ex; A viewer wants to watch a movie from a specific scene or would like to Skip the Intro of their favorite series ](media/Video---Live-Streaming-image3.png)\n\n![afa - There many types Of and one con easily add them to a Video file using a container formot_ Ex: Language Of an audio - subtitles are Olso sometimes considered as metadata. The most cornrnon container formvats are: MPd, MPEG2-TS and Motroska, and con represent different Video and audio Each container forrnaf has ifs Strengths and weaknesses defined by o Video'S compatibility, streaming and Size overhead. Encoding Demuxing REVC H .264 Data Conversion The following terms are used to describe to most common transfo\"nations for media assets. A transformation reduces the Size Of the media 'o Odd compatibility or to enrich media data with additional data. like metadata or rnedia data. The Of converting a raw Signal to a binary file Of a codec. example encoding a series of raw images to the video codec H ,264_ Encoding can also refer to the of converting a very high quality raw He into fom-.at for Simpler Sharing transmission - Ex: taking On uncompressed RGg IE-bit frame , with a Size Of 12.4MB, for 50 seconds (measured at 24 frames/sec) totalling 17.96B - and compressing it into a-bit frames with a size of 3.11Mg per frame, which for the same video Of 60 at 24fps is 2AGB in total CompÃ¦ssing the siÃ¦ of the file down by 15GB! The opposite Of encoding; decoding is the process Of converting binary files back into raw medio Signols_ Ex: H .25' codec Streams into viewable images. TranÂ«oding: The process Of converting one codec to another (Or the some) codec. Both & encoding Ore necessary steps to achieving a successful trorwtode. gest described as: decoding the source codec Stream and then encoding it again to a new target codec stream. Although encoding is typically lossy, additional tochniquos like frame interpolation and upscaling increase the quality of tho conversion of a compressed video format. Muxing: The process of adding one or rmro codec streams into a container format. Dâ€¢muxing: Extracting a codec stream from a container format. Transmuxing: Extracting streams from one container format and putting them in a different (or the same) container format. Multiplexing: The process of interweaving audio and video into ono data stream. Ex: An elementary stream (audio & video) from the encoder are turned into Packotized Elementary Streams (PES) and then converted into Transport Streams CTS). Demultiplexing: The reverse operation of multiplexing. This moans extracting an elementary stream from a media container. E.g.: Extracting the mp3 audio dota from an mp4 music video. ](media/Video---Live-Streaming-image4.png)\n\n![](media/Video---Live-Streaming-image5.png)\n\n![seen in fig EMSG boxes Of fMPd segments), Where the player has to parse relevant data from the media container forma' file, keep track Of a timeline and process the dato Ot 'he Correct time Within the content (like displaying the right captions 'he right time). This requires the player implementation to have desired handling in place. For player logic Custom Handling CEA-608 DRY i n it doto (p \"h Segment EG. fMP4 Client-side Transmuxing In scenarios where a simple encode-deccnde cycle doesn't work, the process of transmuxing comes into play. This is most commonly seen within various brcmsers that often lack for certain container formats. A prime example Of this issue is that web Chrome, Firefox, Edge and Internet Explorer not (properly) supporting the MPEG-TS container format. The MPEG (Motion Picture Experts Group) Transport Stream format was specifically designed for Digital Video Broadcasting (OVB) applications. You can find more details on this format in chapter three (p. X). Since MPEG-TS is very commonly used container format. the only solution is to convert media from MPEG-TS to a container format that these browsers do support (ex: fMP4). This conversion step can take place at the client directly before the content is forwarded to the browser's media stack for demuxing and decoding. This process includes demultiplexing the MPEG-TS and then re---multiplexing the elementary streams to fMP4, This process is visualized below in fig 5: Transmuxing packets, Figwes ](media/Video---Live-Streaming-image6.png)\n\n![Chapter 2 With the basic terminology behind container formats covered, the following chapter will dive into specific container formats. namely, MP4 and CMAF. This includes additional terminology around as well as the use and application of fragmentation or chunking of files. MP4 --- Overview Of Standards Base Formats Terminology (Ffyp) - A four-letter code found Within the structure Of a Video file to identify the Of encoding being using, it'S â€¢compa'ibilityâ€¢, or its â€¢intended --- Contains either audio or Video track. - Contains the box header information the TrackBoxes. Movie BOX --- Contains header and information to Signal that the movie continues in Fragments usually only contain a fraction Of the Whole mv3vie. Movie BOX --- Contains movie fragment header data and track fragment data. MPEG-4 Part 14 MPEG-4 Part 14 (MP4) is one Of the most commonly used container formats Ond Often has a file ending. It is used for Dynamic Adaptive Streaming over H TIP (DASH) a red can also be used for Apple's BLS streaming. MP4 is based on the ISO Base Media File Form-at (MPEG-4 Part 12), Which is based on tho QuickTime File MPEG-4 Part 14 ISO Base Media File (MPEG-4 part 12) - MP4 File Format MP' extension MPEG stands for Moving Pictures Exports Group and is a cooperation of the International Organization for Standardization (ISO) and the International Eloctrotochnical Commission (IEC). MPEG was initially formed to creato and maintain a set of standards for audio and video compression and transmission. MPEG-4 ho.vovor, applies specifically towards tho standards of coding of audio-visual (AV) objects. The MP4 container format supports a wide range of codecs, most commonly: H.264 or HEVC for ](media/Video---Live-Streaming-image7.png)\n\n![Video Advanced Audio Coding (AAC) for audiO_ AAC wos designed os the Successor fo the for-no us MP3 codec. ISO Base Media File Format ISO Base Media File Format (ISOBMFF, MPEG-4 Part 12) is the of the MP4 container format. ISOBMFF is a standard that defines time-based multimedia Time---base multimedia usual* refers to audio and video, often delivered as a steady stream. It is designed to be flexible and easy to extend, enabling interchangeability, management, editing and presentability of nultimedia data. The component of ISOBMFF are boxes, atoms. These are defined using classes and an oriented approach - there are cu\"ently hundreds of different class types, please refer to the public* available standards lists by the International Organization of Standards (ISO) for relevant class information, using inheritance, all boxes extend a base class and can be made specific in their purpose by adding new class properties. An example Of this could be a class 'car' which then has a specific subclass which is 'SUV', the 'SUV' class inherits an the properties Of 'car' plus defines some new ones, In the same way a box in the context of MP4 is a general class which then a Movieaox is inheriting all the properties of ISOBMFF and then defining some of a The structure of an ISOBMFF is defined below in fig 7 â€¢ Base Class - Example A (ftyp) is used to identify the purpose and usage Of an ISOBMFF file and is most commonly applied in the beginning Of o file A box Can have Children and form a Of boxes (illustrated above). example: o MovieBox (me-ov) can have NWtiple TrackB-Oxes (track). A track in the Context Of ISOBMFF is o Single media Stream and typically references its binary CO-dec data. E.g. a Moviegox contains a track box for video and one track box for The binary data Con be Stored in o Media Data BOX (mdat)_ ](media/Video---Live-Streaming-image8.png)\n\n![](media/Video---Live-Streaming-image9.png)\n\n![](media/Video---Live-Streaming-image10.png)\n![from Encoder MOAT Still being encoded/rruxed Traditional fMP4 Segment to Decoder Chunked CMAF Segment \"DAT Can already be loaded and player Playback of segment can only start once fully downloaded. Playback can be started once first chunk is received. Player already plays on one end while encoder Still writes on the other. In traditional (f)MP4 the whole segment has to be fully downloaded to be played out. With chunked encoding, any completely loaded chunks of the segment can be decoded and played, while loading the rest of the segment. Hereby, the achievable live latency is no longer dependent on the segment duration as muxed chunks of an incomplete segment can be loaded and played at the client. With (f)MP4, MPEG-CMAF, and Chunked CMAF have been covered, you're ready for the complex container functions: MPEG-TS & Matroska (WebM) ](media/Video---Live-Streaming-image11.jpg)\n\n![Muxing multiple Elementary Streams A single elernentary stream represents either audio or video content. Most video elementary streams aro accompanied by ot least ono audio elementary stream. These correlated ESâ€¢s are muxed into the same transport stream with separate PI DS for each ES and it's packets. Illustrated in fig 12; Vide E Muxing multiple Programs The most complex variation Of muxing a stream is the process Of multiple programs. With MPEG-TS, a program is o set Of related elementary streams 'hat belong together, e.g. Video and the matching audio. A Single transport can Carry multiple programs, e.g. o different TV channel. This process Can be seen in Fig 13: program I Program 2 PES PES ](media/Video---Live-Streaming-image13.png)\n\n![Program Association From a Icnn-level perspective a transport stream is just a sequence of 188 byte Bng TS packets. As previously mentioned, there can be many programs with multiple elementary streams, but a client is usually only capable of presenting one program ot a time. It must therefore determine which packets to consume and which to discard upon receiving tho transport stream. For this purpose there aro two kinds of special pockets: program Association Tab l. (PAT) --- PAT packet. the PID O and contains the PID. for program Map tables of all programs within the transport stream. program Map Table (PMT) - PMT represent. a and PID. for The prcn:ess by which PATS and PMTS determine which packets to consume takes four steps, as defined in fig 14 and elaborated below: TS Packet PID 202 PID 102 TS Packet PID 200 PID 100 PID O 202 Inspect the TS Pocket With PID O, Which contains the PAT _ 2_ Find the PMT---PID Of the Program the player Should play back in the PAT (in this example: 200), 3. Get the TS Pocket with the relevant PMT- PID, which the PMT (200). _ The PMT contains the PID for all the media tracks, which are part Of the Program to play. A client receiving the transport Stream would first read the PAT packet it received and pick a program to be played depending on o user'S selection. The Client pulls the selected program's PMT (including the ESS and AIDS) from the PAL Then, the Client filters for the specified ADS, each representing a separate ES Of the chosen program, and Consumes deco-des and plays them for the user. OTT---specific aspects and Conclusion MPEG-TS is very broadcast---oriented, in OTT, however, there additional special considerations. OTT clients hove network connections that are unstable and do not have guaranteed bandwidth: ](media/Video---Live-Streaming-image14.png)\n\n![](media/Video---Live-Streaming-image15.png)\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/Weave/","title":"Weave"},"frontmatter":{"draft":false},"rawBody":"# Weave\n\nCreated: 2019-03-16 15:42:51 +0500\n\nModified: 2019-03-16 15:54:44 +0500\n\n---\n\nWeave is a set of application-level networking protocols built around a common addressing and naming architecture with low overhead serialization protocols and modern security.\nWeave protocols deliver device-to-device, device-to-mobile, and device-to-cloud communication for both control and data in the Internet of Things (IoT) space. While Weave is architected around IPv6, it can utilize any IP network or point-to-point communication technology such as BLE.\n\nWeave was designed with the following goals in mind:\n-   Low overhead --- Lightweight solution for low-power devices\n-   Pervasive --- Scalable and unified protocol everywhere\n-   Robust --- Leverages ThreadÂ® and is self-healing to the cloud\n-   Secure --- Interactions are secure, even when the network is not\n-   Easy to use --- Flexible setup and configuration\n-   Versatile --- Strongly-typed data for powerful interaction\n![Weave Components](media/Weave-image1.png)\n**References**\n\n<https://openweave.io>\n\n<https://openweave.io/guides/weave-primer>\n\n<https://codelabs.developers.google.com/codelabs/happy-weave-getting-started/#0>\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/WebSockets/","title":"WebSockets"},"frontmatter":{"draft":false},"rawBody":"# WebSockets\n\nCreated: 2019-12-09 15:49:15 +0500\n\nModified: 2022-01-07 15:11:53 +0500\n\n---\n\nWebSocketis a computer[communications protocol](https://en.wikipedia.org/wiki/Communications_protocol), providing[full-duplex](https://en.wikipedia.org/wiki/Full-duplex)communication channels over a single[TCP](https://en.wikipedia.org/wiki/Transmission_Control_Protocol)connection. The WebSocket protocol was standardized by the[IETF](https://en.wikipedia.org/wiki/Internet_Engineering_Task_Force)as[RFC](https://en.wikipedia.org/wiki/Request_for_Comments)6455 in 2011, and the WebSocket[API](https://en.wikipedia.org/wiki/Application_programming_interface)in[Web IDL](https://en.wikipedia.org/wiki/Web_IDL)is being standardized by the[W3C](https://en.wikipedia.org/wiki/World_Wide_Web_Consortium).\nWebSocket is distinct from HTTP. Both protocols are located at layer 7 in the[OSI model](https://en.wikipedia.org/wiki/OSI_model)and depend on TCP at layer 4. Although they are different,[RFC 6455](https://tools.ietf.org/html/rfc6455)states that WebSocket \"is designed to work over HTTP ports 80 and 443 as well as to support HTTP proxies and intermediaries,\" thus making it compatible with the HTTP protocol. To achieve compatibility, the **WebSocket[handshake](https://en.wikipedia.org/wiki/Handshaking)uses the[HTTP Upgrade header](https://en.wikipedia.org/wiki/HTTP/1.1_Upgrade_header)to change from the HTTP protocol to the WebSocket protocol.**\nThe WebSocket protocol enables interaction between a[web browser](https://en.wikipedia.org/wiki/Web_browser)(or other client application) and a[web server](https://en.wikipedia.org/wiki/Web_server)with lower overhead than half-duplex alternatives such as HTTP polling, facilitating real-time data transfer from and to the server. This is made possible by providing a standardized way for the server to send content to the client without being first requested by the client, and allowing messages to be passed back and forth while keeping the connection open. In this way, a two-way ongoing conversation can take place between the client and the server. The communications are done over TCP[port](https://en.wikipedia.org/wiki/Port_(computer_networking))number 80 (or 443 in the case of[TLS](https://en.wikipedia.org/wiki/Transport_Layer_Security)-encrypted connections), which is of benefit for those environments which block non-web Internet connections using a[firewall](https://en.wikipedia.org/wiki/Firewall_(computing)). Similar two-way browser-server communications have been achieved in non-standardized ways using stopgap technologies such as[Comet](https://en.wikipedia.org/wiki/Comet_(programming)).\n<https://en.wikipedia.org/wiki/WebSocket>\nWebsockets are full bidirectional connection between hosts (once user is connected to the server, both client and server can initiate the request) (Also have sticky session, i.e. if one request from user gets to a server than all the further messages will be transfered via same server)\nWebSockets are typically used to make web applications more interactive. They can be helpful when implementing social feeds, online chats, news updates, or location-based apps.\nThe WebSocket protocol is a rather low-level protocol. It defines how a stream of bytes is transformed into frames. A frame may contain a text or a binary message. Because the message itself does not provide any additional information on how to route or process it, It is difficult to implement more complex applications without writing additional code. Fortunately, the WebSocket specification allows using of sub-protocols that operate on a higher, application level. One of them, supported by the Spring Framework, is STOMP.\n[WebSockets Crash Course](https://www.youtube.com/watch?v=2Nt-ZrNP22A)\n\n[WebSockets in 100 Seconds & Beyond with Socket.io](https://www.youtube.com/watch?v=1BfCnjr_Vjg&ab_channel=Fireship)\n**Will WebSocket survive HTTP/2?**\n\n|               | HTTP/2                      | WebSocket         |\n|----------------|-----------------------------|-------------------|\n| Headers        | Compressed (HPACK)          | None              |\n| Binary         | Yes                         | Binary or Textual |\n| Multiplexing   | Yes                         | Yes               |\n| Prioritization | Yes                         | No                |\n| Compression    | Yes                         | Yes               |\n| Direction      | Client/Server + Server Push | Bidirectional     |\n| Full-duplex    | Yes                         | Yes               |\nAs we have seen above, HTTP/2 introduces[Server Push](https://en.wikipedia.org/wiki/Push_technology?oldformat=true)which enables the server to proactively send resources to the client cache. It does not, however, allow for pushing data down to the client application itself. Server pushes are only processed by the browser and do not pop up to the application code, meaning there is no API for the application to get notifications for those events.\nThis is where Server-Sent Events (SSE) becomes very useful. SSE is a mechanism that allows the server to asynchronously push the data to the client once the client-server connection is established. The server can then decide to send data whenever a new \"chunk\" of data is available. It can be considered as a one-way publish-subscribe model. It also offers a standard JavaScript client API named EventSource implemented in most modern browser as part of HTML5 standard by[W3C](https://www.w3.org/TR/eventsource/). Note that browsers that do not support[EventSource API](http://caniuse.com/#feat=eventsource)can be easily polyfilled.\nSince SSE is based on HTTP, it has a natural fit with HTTP/2 and can be combined to get the best of both: HTTP/2 handling an efficient transport layer based on multiplexed streams and SSE providing the API up to the applications to enable push.\nNow that we have understood what multiplexing is all about, we have to remember that SSE is HTTP based. It means that with HTTP/2, not only can several SSE streams be interleaved onto a single TCP connection, but also several SSE streams (server to client push) with several client requests (client to server). Thanks to HTTP/2 and SSE, we now have a pure HTTP bidirectional connection with a simple API to let application code register to server pushes. Lack of bidirectional capabilities have often been perceived as a major drawback when comparing SSE to WebSocket. Thanks to HTTP/2 is it no longer the case. This opens up the opportunity to skip WebSockets and stick to a HTTP based signaling.\nTo provide some answers to the initial question:Will WebSocket survive HTTP/2?\n\nIt certainly will, mainly because it is already well adopted and, in very specific use cases, it has an advantage as it has been built from the ground up for bidirectional capabilities with less overhead (headers). Let's say that you need to exchange a high throughput of messages from both ends, with almost as much data flow upstream than downstream (e.g Massively Multiplayer Online Game that needs to keep all their players in sync). WebSocket will probably remain a better choice.\nIf you consider a use case like displaying real-time market news, market data, chat application, etc, relying on HTTP/2 + SSE will provide you an efficient bidirectional communication channel and keep the huge advantage of staying in the HTTP world:\n-   WebSocket can often be a source of pain when considering compatibility with existing web infrastructure as it upgrades an HTTP connection to a completely different protocol that has nothing to do with HTTP.\n-   Scale and security: Web components (Firewalls, Intrusion Detection, Load Balancers) are built, maintained and configured with HTTP in mind, an environment that large/critical applications will prefer in terms of resiliency, security and scalability.\n**Takeaways**\n-   HTTP/2 is not a full replacement of HTTP.\n-   Hacks such as Domain sharding, resource inlining and image spriting will be counter-productive in an HTTP/2 world.\n-   HTTP/2 is not a replacement for push technologies such as WebSocket or SSE.\n-   HTTP/2 Push server can only be processed by browsers, not by applications.\n-   Combining HTTP/2 and SSE provides efficient HTTP-based bidirectional communication.\nWebSocket will probably remain used but SSE and its EventSource API combined with the power of HTTP/2 will provide the same result in most use cases, just simpler.\n<https://www.infoq.com/articles/websocket-and-http2-coexist>\n<https://sookocheff.com/post/networking/how-do-websockets-work>\n\n<https://ably.com/topic/websockets>\n\n## socketio / socket.io**\n\nBidirectional and low-latency communication for every platform\n\n<https://github.com/socketio/socket.io>\n\n<https://socket.io>\n\n## Websocket vs socketio**\n\nSocketio advantages are that it simplifies the usage of WebSockets, and probably more importantly it provides fail-overs to other protocols in the event that WebSockets are not supported on the browser or server.\n<https://stackoverflow.com/questions/10112178/differences-between-socket-io-and-websockets>\n\n<https://davidwalsh.name/websocket>\n\n## >> Django Channels**\n"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/ZeroMQ--Distributed-Messaging/","title":"ZeroMQ: Distributed Messaging"},"frontmatter":{"draft":false},"rawBody":"# ZeroMQ: Distributed Messaging\n\nCreated: 2018-09-28 15:10:51 +0500\n\nModified: 2018-12-20 22:17:27 +0500\n\n---\n\nZeroMQ (also known as Ã˜MQ, 0MQ, or zmq) looks like an embeddable networking library but acts like a concurrency framework. It gives you sockets that carry atomic messages across various transports like in-process, inter-process, TCP, and multicast. You can connect sockets N-to-N with patterns like fan-out, pub-sub, task distribution, and request-reply. It's fast enough to be the fabric for clustered products. Its asynchronous I/O model gives you scalable multicore applications, built as asynchronous message-processing tasks. It has a score of language APIs and runs on most operating systems.-   Carries messages across inproc, IPC, TCP, TIPC, multicast.\n-   Smart patterns like pub-sub, push-pull, and router-dealer.\n-   High-speed asynchronous I/O engines, in a tiny library.\n**References**\n\n<http://zguide.zeromq.org/page:all>\n\n<https://www.pythonforthelab.com/blog/using-pyzmq-for-inter-process-communication-part-1>\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Software-Coding---Development-Engineering/Code-Smell/","title":"Code Smell"},"frontmatter":{"draft":false},"rawBody":"# Code Smell\n\nCreated: 2020-10-25 23:52:00 +0500\n\nModified: 2020-10-25 23:53:18 +0500\n\n---\n\nIn[computer programming](https://www.wikiwand.com/en/Computer_programming), acode smellis any characteristic in the[source code](https://www.wikiwand.com/en/Source_code)of a[program](https://www.wikiwand.com/en/Computer_program)that possibly indicates a deeper problem.Determining what is and is not a code smell is subjective, and varies by language, developer, and development methodology.\nOne way to look at smells is with respect to principles and quality: \"Smells are certain structures in the code that indicate violation of fundamental design principles and negatively impact design quality\".Code smells are usually not[bugs](https://www.wikiwand.com/en/Software_bug); they are not technically incorrect and do not prevent the program from functioning. Instead, they indicate weaknesses in design that may slow down development or increase the risk of bugs or failures in the future. Bad code smells can be an indicator of factors that contribute to[technical debt](https://www.wikiwand.com/en/Technical_debt).[Robert C. Martin](https://www.wikiwand.com/en/Robert_C._Martin)calls a list of code smells a \"value system\" for software craftsmanship.\n**Common code smells**\n\n**Application-level smells**\n-   [Duplicated code](https://www.wikiwand.com/en/Duplicate_code): identical or very similar code exists in more than one location.\n-   Contrived complexity: forced usage of overcomplicated[design patterns](https://www.wikiwand.com/en/Design_pattern_(computer_science))where simpler design would suffice.\n-   [Shotgun surgery](https://www.wikiwand.com/en/Shotgun_surgery): a single change needs to be applied to multiple classes at the same time.\n-   Uncontrolled side effects: very easy to cause runtime exceptions and unit tests can't capture it.\n-   Variable mutations: very hard to refactor code since the actual value is unpredictable and hard to reason about.\n-   Boolean blindness: easy to assert on the opposite value and still type checks.\n**Class-level smells**\n-   Large class: a[class](https://www.wikiwand.com/en/Class_(computer_science))that has grown too large. See[God object](https://www.wikiwand.com/en/God_object).\n-   Feature envy: a class that uses methods of another class excessively.\n-   Inappropriate intimacy: a class that has dependencies on implementation details of another class. See[Object orgy](https://www.wikiwand.com/en/Object_orgy).\n-   Refused bequest: a class that[overrides](https://www.wikiwand.com/en/Method_overriding_(programming))a method of a base class in such a way that the[contract](https://www.wikiwand.com/en/Contract_(software))of the[base class](https://www.wikiwand.com/en/Base_class)is not honored by the[derived class](https://www.wikiwand.com/en/Derived_class). See[Liskov substitution principle](https://www.wikiwand.com/en/Liskov_substitution_principle).\n-   Lazy class / freeloader: a class that does too little.\n-   Excessive use of literals: these should be coded as named constants, to improve readability and to avoid programming errors. Additionally,[literals](https://www.wikiwand.com/en/Literal_(computer_programming))can and should be externalized into resource files/scripts, or other data stores such as databases where possible, to facilitate localization of software if it is intended to be deployed in different regions.\n-   [Cyclomatic complexity](https://www.wikiwand.com/en/Cyclomatic_complexity): too many branches or loops; this may indicate a function needs to be broken up into smaller functions, or that it has potential for simplification.\n-   [Downcasting](https://www.wikiwand.com/en/Downcasting): a type cast which breaks the abstraction model; the abstraction may have to be refactored or eliminated.[[8]](https://www.wikiwand.com/en/Code_smell#citenote8)\n-   Orphan variable or constant class: a[class](https://www.wikiwand.com/en/Class_(computer_science))that typically has a collection of constants which belong elsewhere where those constants should be owned by one of the other member classes.\n-   [Data clump](https://www.wikiwand.com/en/Data_Clump_(Code_Smell)): Occurs when a group of variables are passed around together in various parts of the program. In general, this suggests that it would be more appropriate to formally group the different variables together into a single object, and pass around only this object instead.[[9]](https://www.wikiwand.com/en/Code_smell#citenote9)[[10]](https://www.wikiwand.com/en/Code_smell#citenote10)\n**Method-level smells**\n-   Too many parameters: a long list of parameters is hard to read, and makes calling and testing the function complicated. It may indicate that the purpose of the function is ill-conceived and that the code should be refactored so responsibility is assigned in a more clean-cut way.\n-   Long method: a[method](https://www.wikiwand.com/en/Method_(computer_science)), function, or procedure that has grown too large.\n-   Excessively long identifiers: in particular, the use of[naming conventions](https://www.wikiwand.com/en/Naming_convention_(programming))to provide disambiguation that should be implicit in the[software architecture](https://www.wikiwand.com/en/Software_architecture).\n-   Excessively short identifiers: the name of a variable should reflect its function unless the function is obvious.\n-   Excessive return of data: a function or method that returns more than what each of its callers needs.\n-   Excessively long line of code(or God Line): A line of code which is too long, making the code difficult to read, understand, debug, refactor, or even identify possibilities of software reuse. Example:\n\nnew XYZ(s).doSomething(buildParam1(x), buildParam2(x), buildParam3(x), a + Math.sin(x)*Math.tan(x*y + z)).doAnythingElse().build().sendRequest();\n<https://www.wikiwand.com/en/Code_smell>\n"},{"fields":{"slug":"/Computer-Science/Programming-Concepts/Software-Coding---Development-Engineering/Static-Code-Analysis/","title":"Static Code Analysis"},"frontmatter":{"draft":false},"rawBody":"# Static Code Analysis\n\nCreated: 2020-08-23 01:10:34 +0500\n\nModified: 2022-02-05 00:27:10 +0500\n\n---\n\nStatic code analysis looks at the code without executing it. It is usually extremely fast to execute, requires little effort to add to your workflow, and can uncover common mistakes. The only downside is that it is not tailored towards your code.\n**Code Complexity**\n\nOne way to measure code complexity is the[cyclomatic complexity](https://en.wikipedia.org/wiki/Cyclomatic_complexity), also called McCabe complexity as defined in[A Complexity Measure](https://books.google.de/books?id=vtNWAAAAMAAJ&pg=PA3&redir_esc=y):\n\nCC = E - N + 2*P\n\nwhere N is the number of nodes in the control flow graph, E is the number of edges and P is the number of condition-nodes (if-statements, while/for loops).\nCyclomatic complexityis a[software metric](https://en.wikipedia.org/wiki/Software_metric)used to indicate the[complexity of a program](https://en.wikipedia.org/wiki/Programming_complexity). It is a quantitative measure of the number of linearly independent paths through a program's[source code](https://en.wikipedia.org/wiki/Source_code). It was developed by[Thomas J. McCabe, Sr.](https://en.wikipedia.org/w/index.php?title=Thomas_J._McCabe,_Sr.&action=edit&redlink=1)in 1976.\nCyclomatic complexity is computed using the[control flow graph](https://en.wikipedia.org/wiki/Control_flow_graph)of the program: the nodes of the[graph](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics))correspond to indivisible groups of commands of a program, and a[directed](https://en.wikipedia.org/wiki/Directed_graph)edge connects two nodes if the second command might be executed immediately after the first command. Cyclomatic complexity may also be applied to individual[functions](https://en.wikipedia.org/wiki/Function_(computer_science)),[modules](https://en.wikipedia.org/wiki/Modular_programming),[methods](https://en.wikipedia.org/wiki/Method_(computer_science))or[classes](https://en.wikipedia.org/wiki/Class_(computer_science))within a program.\nOne[testing](https://en.wikipedia.org/wiki/Software_testing)strategy, called[basis path testing](https://en.wikipedia.org/wiki/Basis_path_testing)by McCabe who first proposed it, is to test each linearly independent path through the program; in this case, the number of test cases will equal the cyclomatic complexity of the program.\n<https://en.wikipedia.org/wiki/Cyclomatic_complexity>\n\n## Test & Code Coverage**\n\nIn[computer science](https://en.wikipedia.org/wiki/Computer_science),test coverageis a measure used to describe the degree to which the[source code](https://en.wikipedia.org/wiki/Source_code)of a[program](https://en.wikipedia.org/wiki/Computer_program)is executed when a particular[test suite](https://en.wikipedia.org/wiki/Test_suite)runs. A program with high test coverage, measured as a percentage, has had more of its source code executed during testing, which suggests it has a lower chance of containing undetected[software bugs](https://en.wikipedia.org/wiki/Software_bug)compared to a program with low test coverage.\nMany different metrics can be used to calculate test coverage; some of the most basic are the percentage of program[subroutines](https://en.wikipedia.org/wiki/Subroutine)and the percentage of program[statements](https://en.wikipedia.org/wiki/Statement_(computer_science))called during execution of the test suite.\nTest coverage was among the first methods invented for systematic[software testing](https://en.wikipedia.org/wiki/Software_testing).\nCode Coverage is a measurement of how many lines/blocks/arcs of your code are executed while the automated tests are running.\nCode coverage is collected by using a specialized tool to instrument the binaries to add tracing calls and run a full set of automated tests against the instrumented product. A good tool will give you not only the percentage of the code that is executed, but also will allow you to drill into the data and see exactly which lines of code were executed during particular test.\n\nCode Coverage = (Number of lines of code exercised)/(Total Number of lines of code) * 100%\n**Following are the types of code coverage Analysis:**\n-   **Statement coverage and Block coverage (Line coverage)**\n-   Function coverage\n-   Function call coverage\n-   **Branch coverage**\n-   Modified condition/decision coverage\nKnowing the percentage of code that is covered by tests, can help developers assess the quality of their test cases and help them add missing tests and thereby find and remove software faults\n<https://codecov.io>\n\n## Lint / Linting / Linter**\n\nLint (In computer programming, lint is a Unix utility that flags some suspicious and non-portable constructs (likely to be bugs) in C language source code; generically, lint or a linter is any tool that flags suspicious usage in software written in any computer language.)\n[Code linting](https://en.wikipedia.org/wiki/Lint_(software))is the act of finding bugs, stylistic errors, and suspicious constructs from static code analysis.\n**Used**\n-   Flagging bugs in your code from syntax errors\n-   Giving you warnings when code may not be intuitive\n-   Providing suggestions for common best practices\n-   Keeping track of TODO's and FIXME's\n-   Keeping a consistent code style\n<https://www.freecodecamp.org/news/dont-just-lint-your-code-fix-it-with-prettier>\nCode Validator / Linter / Analysis - [https://deepsource.io](https://deepsource.io/)\n**Application Inspector**\n\nMicrosoft Application Inspector is a software source code analysis tool that helps identify and surface well-known features and other interesting characteristics of source code to aid in determiningwhat the software isorwhat it does. It has received attention on[ZDNet](https://www.zdnet.com/article/microsoft-application-inspector-is-now-open-source-so-use-it-to-test-code-security/),[SecurityWeek](https://www.securityweek.com/microsoft-introduces-free-source-code-analyzer),[CSOOnline](https://www.csoonline.com/article/3514732/microsoft-s-offers-application-inspector-to-probe-untrusted-open-source-code.html),[Linux.com/news](https://www.linux.com/news/microsoft-application-inspector-is-now-open-source-so-use-it-to-test-code-security/),[HelpNetSecurity](https://www.helpnetsecurity.com/2020/01/17/microsoft-application-inspector/), Twitter and more and was first featured on[Microsoft.com](https://www.microsoft.com/security/blog/2020/01/16/introducing-microsoft-application-inspector/).\nApplication Inspector is different from traditional static analysis tools in that it doesn't attempt to identify \"good\" or \"bad\" patterns; it simply reports what it finds against a set of over 400 rule patterns for feature detection including features that impact security such as the use of cryptography and more. This can be extremely helpful in reducing the time needed to determine what Open Source or other components do by examining the source directly rather than trusting to limited documentation or recommendations.\nThe tool supports scanning various programming languages including C, C++, C#, Java, JavaScript, HTML, Python, Objective-C, Go, Ruby, PowerShell and[more](https://github.com/microsoft/ApplicationInspector/wiki/2.1-Field:-applies_to-(languages-support))and can scan projects with mixed langauge files. It also includes HTML, JSON and text output formats with the default being an HTML report similar to the one shown here.\n<https://github.com/Microsoft/ApplicationInspector>\n\n## SonarQube (Continuous Code Quality Inspector)**\n\nSonarQube(formerlySonar)is an[open-source](https://en.wikipedia.org/wiki/Open-source_software)platform developed by[SonarSource](https://en.wikipedia.org/wiki/SonarSource)for continuous inspection of[code quality](https://en.wikipedia.org/wiki/Software_quality)to perform automatic reviews with static[analysis of code](https://en.wikipedia.org/wiki/Static_program_analysis)to detect[bugs](https://en.wikipedia.org/wiki/Software_bug),[code smells](https://en.wikipedia.org/wiki/Code_smell), and security vulnerabilities on 20+[programming languages](https://en.wikipedia.org/wiki/Programming_language). SonarQube offers reports on[duplicated code](https://en.wikipedia.org/wiki/Duplicate_code),[coding standards](https://en.wikipedia.org/wiki/Programming_style),[unit tests](https://en.wikipedia.org/wiki/Unit_testing),[code coverage](https://en.wikipedia.org/wiki/Code_coverage),[code complexity](https://en.wikipedia.org/wiki/Cyclomatic_complexity),[comments](https://en.wikipedia.org/wiki/Comment_(computer_programming)),[bugs](https://en.wikipedia.org/wiki/Defensive_programming), and security vulnerabilities.\nSonarQube can record metrics history and provides evolution graphs. SonarQube provides fully automated analysis and integration with[Maven](https://en.wikipedia.org/wiki/Apache_Maven),[Ant](https://en.wikipedia.org/wiki/Apache_Ant),[Gradle](https://en.wikipedia.org/wiki/Gradle),[MSBuild](https://en.wikipedia.org/wiki/MSBuild)and[continuous integration](https://en.wikipedia.org/wiki/Continuous_integration)tools ([Atlassian Bamboo](https://en.wikipedia.org/wiki/Bamboo_(software)),[Jenkins](https://en.wikipedia.org/wiki/Jenkins_(software)),[Hudson](https://en.wikipedia.org/wiki/Hudson_(software)), etc.).\n<https://en.wikipedia.org/wiki/SonarQube>\n\n<https://www.sonarqube.org>\n\n## AI Autocomplete & Assistant**\n-   Kite\n-   <https://www.tabnine.com>\n\n## Other Tools**\n-   CodeScene\n-   <https://github.com/adamtornhill/code-maat>\n-   CodeClimate\n\n<https://github.com/codeclimate/codeclimate>\n\n<https://codeclimate.com>\n-   Sourcegraph\n\n<https://about.sourcegraph.com>\n-   <https://www.jetbrains.com/qodana>\n"},{"fields":{"slug":"/Computer-Science/Security/Authentication/Certificates/","title":"Certificates"},"frontmatter":{"draft":false},"rawBody":"# Certificates\n\nCreated: 2020-01-22 18:09:33 +0500\n\nModified: 2022-03-12 17:48:50 +0500\n\n---\n\n**Types of certificates**\n-   Domain-validated certificates\n-   Extended validation (EV) certificates\n-   High-assurance certificates\n-   Wildcard certificates (*.example.com)\n-   Subject alternative name (SAN) certificates (example.comandexample.net)\n**X.509**\n\nIn[cryptography](https://en.wikipedia.org/wiki/Cryptography),X.509is a standard defining the format of[public key certificates](https://en.wikipedia.org/wiki/Public_key_certificate).[[1]](https://en.wikipedia.org/wiki/X.509#cite_note-1)X.509 certificates are used in many Internet protocols, including[TLS/SSL](https://en.wikipedia.org/wiki/Transport_Layer_Security), which is the basis for HTTPS[[2]](https://en.wikipedia.org/wiki/X.509#cite_note-:0-2), the secure protocol for browsing the[web](https://en.wikipedia.org/wiki/World_Wide_Web). They are also used in offline applications, like[electronic signatures](https://en.wikipedia.org/wiki/Electronic_signature). An X.509 certificate contains a public key and an identity (a hostname, or an organization, or an individual), and is either signed by a[certificate authority](https://en.wikipedia.org/wiki/Certificate_authority)or self-signed. When a certificate is signed by a trusted certificate authority, or validated by other means, someone holding that certificate can rely on the public key it contains to establish secure communications with another party, or validate documents[digitally signed](https://en.wikipedia.org/wiki/Digital_signature)by the corresponding[private key](https://en.wikipedia.org/wiki/Private_key).\nX.509 also defines[certificate revocation lists](https://en.wikipedia.org/wiki/Certificate_revocation_list), which are a means to distribute information about certificates that have been deemed invalid by a signing authority, as well as a[certification path validation algorithm](https://en.wikipedia.org/wiki/Certification_path_validation_algorithm), which allows for certificates to be signed by intermediate CA certificates, which are, in turn, signed by other certificates, eventually reaching a[trust anchor](https://en.wikipedia.org/wiki/Trust_anchor).\nX.509 is defined by the[International Telecommunications Union's](https://en.wikipedia.org/wiki/International_Telecommunication_Union)Standardization sector ([ITU-T](https://en.wikipedia.org/wiki/ITU-T)), and is based on[ASN.1](https://en.wikipedia.org/wiki/Abstract_Syntax_Notation_One), another ITU-T standard.\n<https://en.wikipedia.org/wiki/X.509>\n"},{"fields":{"slug":"/Computer-Science/Security/Authentication/HTTP-Authentication/","title":"HTTP Authentication"},"frontmatter":{"draft":false},"rawBody":"# HTTP Authentication\n\nCreated: 2018-12-03 18:23:29 +0500\n\nModified: 2021-10-16 00:45:16 +0500\n\n---\n\n[RFC 7235](https://tools.ietf.org/html/rfc7235)defines the HTTP authentication framework which can be used by a server to[challenge](https://developer.mozilla.org/en-US/docs/Glossary/challenge)a client request and by a client to provide authentication information.\nThe general HTTP authentication framework is used by several authentication schemes. Schemes can differ in security strength and in their availability in client or server software.\n\nThe most common authentication scheme is the \"Basic\" authentication scheme which is introduced in more details below. IANA maintains a[list of authentication schemes](https://www.iana.org/assignments/http-authschemes/http-authschemes.xhtml), but there are other schemes offered by host services, such as Amazon AWS. Common authentication schemes include:\n-   Basic(see[RFC 7617](https://tools.ietf.org/html/rfc7617), base64-encoded credentials. See below for more information.),\n-   Bearer(see[RFC 6750](https://tools.ietf.org/html/rfc6750), bearer tokens to access OAuth 2.0-protected resources),\n-   Digest(see[RFC 7616](https://tools.ietf.org/html/rfc7616), only md5 hashing is supported in Firefox, see[bug472823](https://bugzilla.mozilla.org/show_bug.cgi?id=472823)for SHA encryption support),\n-   HOBA(see[RFC 7486](https://tools.ietf.org/html/rfc7486)(draft),HTTPOrigin-BoundAuthentication, digital-signature-based),\n-   Mutual(see[draft-ietf-httpauth-mutual](https://tools.ietf.org/html/draft-ietf-httpauth-mutual-11)),\n-   AWS4-HMAC-SHA256(see[AWS docs](http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-auth-using-authorization-header.html)).\n**Phases**\n\nBasic and Digest authentication use a four step process to authenticate users.\n\n![](media/Authentication_HTTP-Authentication-image1.jpg)\n\nFirst HTTP client makes a request to the web server. Request method doesn't has to be GET it can be any method. If web server sees that the requested resource need authentication to access then it sends backs 401 Unauthorized status code along with WWW-Authenticate header. And then client displays a dialog box to take username and password as input. Once the credentials has been enter the client sends it using the Authorization header. If the credentials are correct then server responds with 200 status code and Authentication-Info header.\n\nIf client sends wrong credentials in the Authorization request then server again responds with 401 status code. The client is allowed to try again and again.\n\nThis is the basic process followed by Basic and Digest models. Values assigned to the authentication headers is different for both models, this is why they differ.\n**Basic Authentication**\n\nHTTP Basic authentication is a method for the client to provide a username and a password when making a request.\n\nThis is the simplest possible way to enforce access control as it doesn't require cookies, sessions or anything else. To use this, the client has to send theAuthorizationheader along with every request it makes. The username and password are not encrypted, but constructed this way:\n-   username and password are concatenated into a single string:username:password\n-   this string is encoded with Base64\n-   theBasickeyword is put before this encoded value\nExample for a user namedjohnwith passwordsecret:\n\ncurl --header \"Authorization: Basic am9objpzZWNyZXQ=\" my-website.com\n\n>>> **from** requests.auth **import** HTTPBasicAuth\n>>> requests.get**(**'https://api.github.com/user'**,** auth=HTTPBasicAuth**(**'user'**,** 'pass'**))**\nResponse[200]\n\n**Cons**\n-   the username and password are sent with every request, potentially exposing them - even if sent via a secure connection\n-   connected to SSL/TLS, if a website uses weak encryption, or an attacker can break it, the usernames and passwords will be exposed immediately\n-   there is no way to log out the user using Basic auth\n-   expiration of credentials is not trivial - you have to ask the user to change password to do so\n**Digest**\n\nIn a digest authentication flow, the client sends a request to a server, which sends back nonce and realm values for the client to authenticate. The client sends back a hashed username and password with the nonce and realm. The server then sends back the requested data.\n**References**\n\n<https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication>\n\n<http://docs.python-requests.org/en/master/user/authentication>\n\n<http://qnimate.com/understanding-http-authentication-in-depth/#Basic_Authentication>\n<https://nullsweep.com/http-security-headers-a-complete-guide>\n\n<https://stackoverflow.blog/2021/10/06/best-practices-for-authentication-and-authorization-for-rest-apis>\n\n"},{"fields":{"slug":"/Computer-Science/Security/Authentication/JWT/","title":"JWT"},"frontmatter":{"draft":false},"rawBody":"# JWT\n\nCreated: 2021-04-07 23:46:43 +0500\n\nModified: 2021-10-08 00:53:18 +0500\n\n---\n\n**JSON Web Token (JWT)** is an open standard ([RFC 7519](https://tools.ietf.org/html/rfc7519)) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. This information can be verified and trusted because it is digitally signed. JWTs can be signed using a secret (with theHMACalgorithm) or a public/private key pair usingRSAorECDSA.\nJWT is a very modern, simple and secure approach which extends for Json Web Tokens. Json Web Tokens are a stateless solution for authentication. So there is no need to store any session state on the server, which of course is perfect for restful APIs. Restful APIs should always be stateless, and the most widely used alternative to authentication with JWTs is to just store the user's log-in state on the server using sessions. But then of course does not follow the principle that says that restful APIs should be stateless and that's why solutions like JWT became popular and effective.\nAlthough JWTs can be encrypted to also provide secrecy between parties, we will focus onsignedtokens. Signed tokens can verify theintegrityof the claims contained within it, while encrypted tokenshidethose claims from other parties. When tokens are signed using public/private key pairs, the signature also certifies that only the party holding the private key is the one that signed it.\n**When should you use JSON Web Tokens?**\n-   **Authorization:** This is the most common scenario for using JWT. Once the user is logged in, each subsequent request will include the JWT, allowing the user to access routes, services, and resources that are permitted with that token. Single Sign On is a feature that widely uses JWT nowadays, because of its small overhead and its ability to be easily used across different domains.\n-   **Information Exchange:** JSON Web Tokens are a good way of securely transmitting information between parties. Because JWTs can be signed---for example, using public/private key pairs---you can be sure the senders are who they say they are. Additionally, as the signature is calculated using the header and the payload, you can also verify that the content hasn't been tampered with.\nJWT consists of three parts (seperated by \".\"):\n-   **Header**, containing the type of the token and the hashing algorithm\n\nTheheadertypically consists of two parts: the token's type, and the hashing algorithm that is being used.\n\n{\n\n\"alg\": \"HS256\",\n\n\"typ\": \"JWT\"\n\n}-   **Payload**, containing the claims\n\n{\n\n\"sub\": \"65165751325\",\n\n\"name\": \"Rajat S\",\n\n\"admin\": true\n\n}-   **Signature**, which can be calculated as follows if you chose HMAC SHA256:HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), secret)\n\nThesignatureis used to verify that the message was not altered before reaching its destination. This is usually done by using private keys.\nThese three parts are usually encoded into three Base64-URI strings that are separated by a.in between them.\n![Browser POST (login with username + password Return JWT to the browser Send JWT in Authorization header Send response to client Server Create JWT with secret Check JWT signature with secret. Get user info from JWT ](media/Authentication_JWT-image1.png)\n\n**Payload**\n\nThe second part of the token is the payload, which contains the claims. Claims are statements about an entity (typically, the user) and additional data. There are three types of claims:registered,public, andprivateclaims.\n-   [Registered claims](https://tools.ietf.org/html/rfc7519#section-4.1): These are a set of predefined claims which are not mandatory but recommended, to provide a set of useful, interoperable claims. Some of them are:iss(issuer),exp(expiration time),sub(subject),aud(audience), and[others](https://tools.ietf.org/html/rfc7519#section-4.1).\n\nNotice that the claim names are only three characters long as JWT is meant to be compact.\nThe JWT specification defines some registered claim names and defines how they should be used. PyJWT supports these registered claim names:\n-   \"exp\" (Expiration Time) Claim\n-   \"nbf\" (Not Before Time) Claim\n-   \"iss\" (Issuer) Claim\n-   \"aud\" (Audience) Claim\n-   \"iat\" (Issued At) Claim\n\n-   [Public claims](https://tools.ietf.org/html/rfc7519#section-4.2): These can be defined at will by those using JWTs. But to avoid collisions they should be defined in the[IANA JSON Web Token Registry](https://www.iana.org/assignments/jwt/jwt.xhtml)or be defined as a URI that contains a collision resistant namespace.-   [Private claims](https://tools.ietf.org/html/rfc7519#section-4.3): These are the custom claims created to share information between parties that agree on using them and are neitherregisteredorpublicclaims.\nAn example payload could be:\n\n{\n\"sub\": \"1234567890\",\n\"name\": \"John Doe\",\n\"admin\": true\n}\n\nThe payload is thenBase64Urlencoded to form the second part of the JSON Web Token.\nDo note that for signed tokens this information, though protected against tampering, is readable by anyone. Do not put secret information in the payload or header elements of a JWT unless it is encrypted.\n**RFCs**\n-   JWT - JSON Web Token\n-   JWS - JSON Web Signature\n-   JWE - JSON Web Encryption\n-   JWK - JSON Web Key\n-   JWA - JSON Web Algorithms\n<https://jwt.io>\n\n<https://jwt.io/introduction>\n\n<https://medium.com/@rahulgolwalkar/pros-and-cons-in-using-jwt-json-web-tokens-196ac6d41fb4>\n\n<https://gist.github.com/deepaksood619/4cc5656a42158927ca6006a1ec7d5eea>\n\n<https://www.pingidentity.com/en/company/blog/posts/2019/jwt-security-nobody-talks-about.html>\n\n<https://auth0.com/learn/json-web-tokens>\n\n[OAuth Vs JWT | What is the difference? | Tech Primers](https://www.youtube.com/watch?v=a9R3Gq1BKxI)\n\n[What Is JWT and Why Should You Use JWT](https://www.youtube.com/watch?v=7Q17ubqLfaM)\n\n[100% Stateless with JWT (JSON Web Token) by Hubert SablonniÃ¨re](https://www.youtube.com/watch?v=67mezK3NzpU&t=24s&ab_channel=Devoxx)\n**If you can decode JWT, how are they secure?**\n\nJWTs can be either signed, encrypted or both. If a token is signed, but not encrypted, everyone can read its contents, but when you don't know the private key, you can't change it. Otherwise, the receiver will notice that the signature won't match anymore.\nThe short answer is that JWT doesn't concern itself with encryption. It cares about validation. That is to say, it can always get the answer for \"Have the contents of this token been manipulated\"? This means user manipulation of the JWT token is futile because the server will know and disregard the token. The server adds a signature based on the payload when issuing a token to the client. Later on it verifies the payload and matching signature.\nThe logical question is what is the motivation for not concerning itself with encrypted contents?\n\n1.  The simplest reason is because it assumes this is a solved problem for the most part. If dealing with a client like the web browser for example, you can store the JWT tokens in a cookie that issecure(is not transmitted via HTTP, only via HTTPS) andhttpOnly(can't be read by Javascript) and talks to the server over an encrypted channel (HTTPS). Once you know you have a secure channel between the server and client you can securely exchange JWT or whatever else you want.\n\n2.  This keeps thing simple. A simple implementation makes adoption easier but it also lets each layer do what it does best (let HTTPS handle encryption).\n\n3.  JWT isn't meant to store sensitive data. Once the server receives the JWT token and validates it, it is free to lookup the user ID in its own database for additional information for that user (like permissions, postal address, etc). This keeps JWT small in size and avoids inadvertent information leakage because everyone knows not to keep sensitive data in JWT.\nIt's not too different from how cookies themselves work. Cookies often contain unencrypted payloads. If you are using HTTPS then everything is good. If you aren't then it's advisable to encrypt sensitive cookies themselves. Not doing so will mean that a man-in-the-middle attack is possible--a proxy server or ISP reads the cookies and then replays them later on pretending to be you. For similar reasons, JWT should always be exchanged over a secure layer like HTTPS.\n**How it works**\n\nThe token is created using asecret stringthat isstored on a server. Next, the server then sends that JWT back to the client which will store it either in a cookie or in local storage.\nSo the server does in fact not know which user is actually logged in, but of course, the user knows that he's logged in because he has a valid Json Web Token which is a bit like a passport to access protected parts of the application.\nAll this communication must happen over https, so secure encrypted Http in order to prevent that anyone can get access to passwords or Json Web Tokens. Only then we have a really secure system.\nThe signing algorithm takes the header, the payload, and the secret to create a unique signature. So only this data plus the secret can create this signature, all right? Then together with the header and the payload, these signature forms the JWT, which then gets sent to the client.\nStore your JWT in cookies for web applications, because of the additional security they provide, and the simplicity of protecting against CSRF with modern web frameworks. HTML5 Web Storage is vulnerable to XSS, has a larger attack surface area, and can impact all application users on a successful attack.\n<https://stormpath.com/blog/where-to-store-your-jwts-cookies-vs-html5-web-storage>\n\n<https://stackoverflow.com/questions/27301557/if-you-can-decode-jwt-how-are-they-secure>\n\n## pyjwt**\n\nPyJWTis a Python library which allows you to encode and decode JSON Web Tokens (JWT). JWT is an open, industry-standard ([RFC 7519](https://tools.ietf.org/html/rfc7519)) for representing claims securely between two parties.\n$ pip install pyjwt\n>>> encoded_jwt = jwt.encode({\"some\": \"payload\"}, \"secret\", algorithm=\"HS256\")\n>>> print(encoded_jwt)\neyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzb21lIjoicGF5bG9hZCJ9.Joh1R2dYzkRvDkqv3sygm5YyK8Gi4ShZqbhK2gxcs2U\n>>> jwt.decode(encoded_jwt, \"secret\", algorithms=[\"HS256\"])\n{'some': 'payload'}\n\n**Example**\n```\nfrom datetime import datetime, timedelta\nimport jwt\n\nJWT_SECRET = 'secret'\nJWT_ALGORITHM = 'HS256'\nJWT_EXP_DELTA_SECONDS = 20\n\nasync def login(request):\npost_data = await request.post()\n\ntry:\nuser = User.objects.get(email=post_data['email'])\nuser.match_password(post_data['password'])\nexcept (User.DoesNotExist, User.PasswordDoesNotMatch):\nreturn json_response({'message': 'Wrong credentials'}, status=400)\n\npayload = {\n'user_id': user.id,\n'exp': datetime.utcnow() + timedelta(seconds=JWT_EXP_DELTA_SECONDS)\n}\njwt_token = jwt.encode(payload, JWT_SECRET, JWT_ALGORITHM)\nreturn json_response({'token': jwt_token.decode('utf-8')})\n\napp = web.Application()\napp.router.add_route('POST', '/login', login)**# Auth Middleware**\n\nasync def get_user(request):\nreturn json_response({'user': str(request.user)})\n\nasync def auth_middleware(app, handler):\nasync def middleware(request):\nrequest.user = None\njwt_token = request.headers.get('authorization', None)\nif jwt_token:\ntry:\npayload = jwt.decode(jwt_token, JWT_SECRET,\nalgorithms=[JWT_ALGORITHM])\nexcept (jwt.DecodeError, jwt.ExpiredSignatureError):\nreturn json_response({'message': 'Token is invalid'}, status=400)\n\nrequest.user = User.objects.get(id=payload['user_id'])\nreturn await handler(request)\nreturn middleware\n\napp = web.Application(middlewares=[auth_middleware])\napp.router.add_route('GET', '/get-user', get_user)\n```\n\n<https://steelkiwi.com/blog/jwt-authorization-python-part-1-practise>\n\n"},{"fields":{"slug":"/Computer-Science/Security/Authentication/OAuth/","title":"OAuth"},"frontmatter":{"draft":false},"rawBody":"# OAuth\n\nCreated: 2020-06-20 00:08:39 +0500\n\nModified: 2021-07-08 01:08:01 +0500\n\n---\n-   OAuth is an open standard for access delegation\n-   OAuth provides to clients a \"secure delegated access\" to server resources on behalf of a resource owner\n-   OAuth2 is more of a framework than a defined protocol\n[OAuth 2.0](https://oauth.net/2/)is a protocol that allows a user to grant limited access to their resources on one site, to another site, without having to expose their credentials.\nAccording to[OAuth's website](http://oauth.net/about/)the protocol is not unlike a valet key.\n\nMany luxury cars today come with a valet key. It is a special key you give the parking attendant and unlike your regular key, will not allow the car to drive more than a mile or two. Some valet keys will not open the trunk, while others will block access to your onboard cell phone address book. Regardless of what restrictions the valet key imposes, the idea is very clever. You give someone limited access to your car with a special key, while using your regular key to unlock everything.\n![OAuth 2.0 Roles User Application API ](media/Authentication_OAuth-image1.png)\n**Workflow of OAuth 2.0**\n\n![MyBucks Application InterSystems Authorization Request Authorization Grant Authorization Grant Access Token Access Token Protected Resource Sara Memorial Bank Authorization Server Memorial Bank Resource Server ](media/Authentication_OAuth-image2.png)\nThroughout this workflow OAuth 2.0 serves as an authorization framework, the actual authentication of user occurs through OpenID Connect, through the use of ID tokens that are passed along with the access token.\n**4 Grant Types**\n\nDefine how an application can retrieve tokens from your OAuth server and are used in different use cases.\n**Authorization Code Grant**\n\nThe Authorization Code flow is the most powerful and most secure by default. When the application redirects the user to the Identity Provider to authenticate, the IdP passes back a short-lived, one-time use authorization code. The application uses the authorization code to retrieve the Access Token.\nThe important part is twofold: First, by the time the user sees the authorization code, it's already been consumed and therefore can't be used again. Second, the Access Token is kept by the application in the backend. Assuming the application is built securely, a malicious user has to find another way to attack it.\nUnfortunately, this doesn't work for client side applications such as many Javascript apps or most mobile apps as the application itself can be attacked or decompiled for sensitive information. Therefore, we need a different approach.\n**Implicit Grant**\n\nThe Implicit flow is designed specifically for mobile apps or client side Javascript apps where embedded credentials could be compromised. The mechanics are simple in that the application redirects the user to the Identity Provider to authenticate, the IdP passes back token(s), and the application uses it according to the scopes it has.\nSince it's quite likely that the user could interact with the token(s), it's important that our use cases reflect that. If we have a banking app, allowing the send_wire_transfers_to_russia scope may be a bad idea unless we have additional factors baked into our authentication process to validate that the right user is using it. The next time you lose your phone, you'll appreciate that.\nAs a result, this is often used for OpenID Connect scenarios where a user wants to provide trusted profile information to a third party but not necessarily access or permissions to other systems. Since the underlying concepts are the same and the implementation looks very similar, it's most of the benefit for the same effort.\n**Resource Owner Password (Password Grant)**\n\nCompared to the previous grant types, Resource Owner Password makes me nervous. With both the Authorization Code and Implicit flows, the application redirects the user to the Identity Provider to submit their username and password. As a result, the application never sees their credentials. With the Resource Owner Password flow, the application itself accepts the credentials and submits them on behalf of the user.\nIf the application is malicious or even just poorly developed, it could store those credentials and compromise the user's information. Therefore, you should only use this if you're building applications for your users to interact with your legacy systems. For example, a bank may implement this for an internal employee portal.\nBut remember: Fundamentally, you're training users to put their credentials into applications they may not trust which is a bad habit at best and a security risk at all times.\n**Client Credential**\n\nThe Client Credential grant type is designed exclusively for backend server to server operations. Think of it as a server's username and password. Conceptually, it's not far from how your application connects to other backend systems such as your database or Twilio. The benefit is that your OAuth provider can return configuration information or other details within the token itself.\nFinally, since there's not a user involved, it doesn't support OpenID Connect.\n<https://auth0.com/docs/protocols/oauth2>\n\nYoutube - [OAuth 2.0: An Overview](https://www.youtube.com/watch?v=CPbvxxslDTU)\n\nYoutube - [What is OAuth2? How does OAuth2 work? | Tech Primers](https://www.youtube.com/watch?v=bzGKgC3N7SY)\n**OAuth**\n-   **Authorization Code**\n    -   Only valid for one-time use, since its only usage is to exchange it for an access token\n    -   Expires very quickly (according to this[article](https://www.oauth.com/oauth2-servers/authorization/the-authorization-response/), the OAuth protocol's recommended maximum is 10 minutes, and many services' authorization codes expire even earlier)-   **Access Token**\n    -   Can be obtained using the authorization code\n    -   Put in the headers of any API requests to Google on behalf of the user\n    -   Expires after one hour (the expiration time may vary if you're using something besides Google)\n    -   carry the necessary information to access a resource directly. In other words, when a client passes an access token to a server managing a resource, that server can use the information contained in the token to decide whether the client is authorized or not. Access tokens usually have an expiration date and are short-lived.-   **Refresh Token**\n    -   allows you to get new access tokens\n    -   carry the information necessary to get a new access token. In other words, whenever an access token is required to access a specific resource, a client may use a refresh token to get a new access token issued by the authentication server. Common use cases include getting new access tokens after old ones have expired, or getting access to a new resource for the first time. Refresh tokens can also expire but are rather long-lived. Refresh tokens are usually subject to strict storage requirements to ensure they are not leaked. They can also be **blacklisted** by the authorization server.\n**Sliding-sessions**\n\nSliding-sessions are sessions that expire after aperiod of inactivity. As you can imagine, this is easily implemented using access tokens and refresh tokens. When a user performs an action, a new access token is issued. If the user uses an expired access token, the session is considered inactive and a new access token is required. Whether this token can be obtained with a refresh token or a new authentication round is required is defined by the requirements of the development team.\n**OAuth vs OpenID Connect (OIDC)**\n\nThe[OAuth 2.0 Framework](https://www.oauth.com/oauth2-servers/map-oauth-2-0-specs/)describes overarching patterns for granting authorization but does not define how to actually perform authentication. The application using OAuth constructs a specific request for permissions to a third party system - usually called an Identity Provider (IdP) - which handles the authentication process and returns an Access Token representing success. The IdP may require additional factors such as SMS or email but that is entirely outside the scope of OAuth. Finally, the contents and structure of that Access Token are undefined by default. This ambiguity guarantees that Identity Providers will build incompatible systems.\nLuckily,[OpenID Connect](https://openid.net/connect/)or OIDC brings some sanity to the madness. It is an OAuth extension which adds and strictly defines an ID Token for returning user information. Now when we log in with our Identity Provider, it can return specific fields that our applications can expect and handle. The important thing to remember is that OIDC is just a special, simplified case of OAuth, not a replacement. It uses the same terminology and concepts.\n<https://blog.runscope.com/posts/understanding-oauth-2-and-openid-connect>\n\n## Map of OAuth 2.0 Specs**\n\n<https://www.oauth.com/oauth2-servers/map-oauth-2-0-specs>\n\n<https://tools.ietf.org/html/rfc6749>\n![](media/Authentication_OAuth-image3.png)\n**PKCE - Proof-Key for Code Exchange (pronounced - pixie)**\n**OAuth 2.1**\n-   Authorization Code + PKCE\n-   Client Credentials\n-   Tokens in HTTP Header\n-   Tokens in POST Form Body-   RFC6749 - OAuth 2.0 Core\n-   RFC6750 - Bearer Token Usage\n-   RFC7636 - PKCE\n-   Native app and brower-based app BCPs (Best Current Practices)\n-   Security BCP\n    -   Must support PKCE for all client types\n    -   No password grant\n    -   No implicit flow\n    -   Exact string matching for redirect URIs\n    -   No access tokens in query strings\n    -   Refresh tokens must be sender-contrained or one-time use\n<https://www.youtube.com/watch?v=g_aVPdwBTfw>\n\n## Others**\n\n<https://developer.okta.com/docs/concepts/api-access-management>\n\n<https://www.youtube.com/watch?v=996OiexHze0>\n"},{"fields":{"slug":"/Computer-Science/Security/Authentication/OpenID/","title":"OpenID"},"frontmatter":{"draft":false},"rawBody":"# OpenID\n\nCreated: 2020-06-20 00:07:35 +0500\n\nModified: 2020-06-20 15:32:03 +0500\n\n---\n\nOpenIdis HTTP based protocol that uses identity provider to validate a user. The user password is secured with one identity provider, this allows other service providers a way to achieve Single SignOn(SSO) without requiring password from user. There are many OpenId enabled account on the internet and organizations such as Google, Facebook, Wordpress, Yahoo, PayPal etc., uses OpenId to authenticate users. The latest version of OpenId is OpenId Connect (OIDC), which provides OpenId(authentication) on top of OAuth 2.0 (authorization) for complete security solution.\n**OIDC**\n\nIt is an OAuth extension which adds and strictly defines an ID Token for returning user information. Now when we log in with our Identity Provider, it can return specific fields that our applications can expect and handle. The important thing to remember is that OIDC is just a special, simplified case of OAuth, not a replacement. It uses the same terminology and concepts.\n[OpenID Connect](https://openid.net/connect/)is a flavor of OAuth2 supported by some OAuth2 providers, notably Azure Active Directory, Salesforce, and Google. The protocol's main extension of OAuth2 is an additional field returned with the access token called an[ID Token](https://openid.net/specs/openid-connect-core-1_0.html#IDToken). This token is a JSON Web Token (JWT) with well known fields, such as a user's email, signed by the server.\nTo identify the user, the authenticator uses theid_token(not theaccess_token) from the OAuth2[token response](https://openid.net/specs/openid-connect-core-1_0.html#TokenResponse)as a bearer token. See[above](https://kubernetes.io/docs/reference/access-authn-authz/authentication/#putting-a-bearer-token-in-a-request)for how the token is included in a request.\n![](media/Authentication_OpenID-image1.png)\n\n<https://openid.net/specs/openid-connect-core-1_0.html>\n![](media/Authentication_OpenID-image2.png)\nThe normal process of generating these tokens is much the same as it is in[OAuth 2.0](https://oauth.net/2/):\n\n1.  User hits the sign in button on the website,\n\n2.  The website redirects them to the Identity Provider,\n\n3.  Browser loads the Identity Provider login screen,\n\n4.  User logs in using their username and password,\n\n5.  Identity Provider redirects them back to the website with an authentication code in the query string,\n\n6.  Browser loads website with the authentication code in query string,\n\n7.  Website server exchanges the code for the ID token.\nOnce the server has this token, it can either use it to authenticate the user itself or it can provide it back to the user such that they can provide it to other services that trust the identity provider.\nKubernetes itself does not provide any sort of login website for OIDC authentication. It only consumes the tokens once you have retrieved them from some other means. This may leave you wondering where the trust relationship is formed between Kubernetes and the Identity Provider.\nAs mentioned earlier, the third part of the token is a signature, every ID token generated by an OIDC provider comes signed with a cryptographic key (usually RS256, generated and rotated periodically by the provider). By providing Kubernetes with the URL of the OIDC provider, Kubernetes can retrieve the public half of this key and verify that the token was indeed signed by the OIDC provider. At this point, Kubernetes will accept the token and trust the token's claim as to who the user is.\n**Limitations**\n\nThe id-token once generated cannot be revoked. Much like issuing certificates for auth, the id-token has an expiry time and it will authenticate the user until that time comes. This means the tokens are often only issued for 1 hour, but some providers do support requests for refresh tokens. Refresh tokens can be used (often indefinitely) to grant a new id-token and continue usage of the service.\n[An Illustrated Guide to OAuth and OpenID Connect](https://www.youtube.com/watch?v=t18YB3xDfXI)\n![](media/Authentication_OpenID-image3.jpg)\n"},{"fields":{"slug":"/Computer-Science/Security/Cryptography-Intro/Cryptographic-Algorithms/","title":"Cryptographic Algorithms"},"frontmatter":{"draft":false},"rawBody":"# Cryptographic Algorithms\n\nCreated: 2018-04-26 23:57:41 +0500\n\nModified: 2022-02-16 18:25:11 +0500\n\n---\n\n**Stream Ciphers (OTP)**\n-   RC4\n-   A5/1, A5/2\n-   Salsa\n**Block Ciphers**\n\nMessage (M) divided into multiple blocks\n**Symmetric Encryption**\n\nSymmetric encryption uses the same keyfor encryption and decryption.\n\nSymmetricencryption is preferred when you are encryptingonly for yourself.\n**Asymmetric Encryption**\n\nAsymmetric encryption uses different keys.\n\nAsymmetricencryption is preferred when you want someone to be able to send you encrypted data, butyou don't want to give them your private key.\n![Symmetric Encryption ABCDEF ABCDEF ABCDEF Message Shared Key â€¢ Encrypt Asymmetric Encryption ABCDEF ABCDEF ABCDEF Message Public Key Encrypt Ciphertext Ciphertext Shared Key Decrypt Private Key Decrypt 2 ABCDEF ABCDEF ABCDEF Message ABCDEF ABCDEF ABCDEF Message ](media/Cryptography-Intro_Cryptographic-Algorithms-image1.png)\n![Symmetric vs. Asymmetric Key Systems Attributes Key Exchange Speed Number of Keys Use Security Service Pro vided One key is shared betwÃ¦n two or more entities Out-of-band Algorithm is less complex and faster Grows exponentially as users grow Bulk encryption, which mÃ¦ns encrypting files and communication paths Confidentiality Asymmetric entity has a public key, and the other entity has a private key Symmetric key is encrypted and sent with message; thus, the key is distributed by in-bound means Algorithm is more complex and slower Grows linearly as users grow Key encryption and distributing keys Confi&ntiality, authentication, and non-repudiation ](media/Cryptography-Intro_Cryptographic-Algorithms-image2.jpg)\n**Ceaser Cipher / Substitution Cipher / Shift Cipher**\n\nShift characters\n**Vigenere Cipher (Polyalphabetic substitution)**\n**Vernam Cipher (One Time Pads)**\n**DES**\n-   16 round Feistel Network\n**3DES - Triple Data Encryption Algorithm**\n\n3DES is a symmetric key block cipher which applies DES algorithm three times to each data block.\n3DES uses 3 keys for all 3 rounds of DES. The 3 rounds consists of encryption, decryption and then again encryption.\n**AES (Advanced Encryption Standard)**\n\nIs a specification that has selected the Rijndael cipher as its symmetric key ciphering algorithm.-   Subs-Perm network\n-   AES - 128\n-   randomized enryption\n-   nonce-based enryption\n-   CBC (Cipher Block Chain) with random IV (Intialization Vector)\n<https://hackernoon.com/very-basic-intro-to-aes-256-cipher-qxr32yk>\n\n[**https://www.devglan.com/online-tools/aes-encryption-decryption**](https://www.devglan.com/online-tools/aes-encryption-decryption)\n**Fernet (symmetric encryption)**\n\nFernet guarantees that a message encrypted using it cannot be manipulated or read without the key.[Fernet](https://github.com/fernet/spec/)is an implementation of symmetric (also known as \"secret key\") authenticated cryptography. Fernet also has support for implementing key rotation via[MultiFernet](https://cryptography.io/en/latest/fernet/#cryptography.fernet.MultiFernet).\nFernet is a symmetric encryption method which makes sure that the message encrypted cannot be manipulated/read without the key. It uses URL safe encoding for the keys. Fernet also uses 128-bit AES in CBC mode and PKCS7 padding, with HMAC using SHA256 for authentication. The IV is created from os.random().\n\n<https://medium.com/coinmonks/if-youre-struggling-picking-a-crypto-suite-fernet-may-be-the-answer-95196c0fec4b>\n```\npython -c \"from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())\"\n```\n<https://cryptography.io/en/latest/fernet>\n\n### Galois/Counter Mode (GCM)\n\nIn [cryptography](https://en.wikipedia.org/wiki/Cryptography), Galois/Counter Mode(GCM) is a[mode of operation](https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation) for symmetric-key cryptographic[block ciphers](https://en.wikipedia.org/wiki/Block_cipher)which is widely adopted for its performance. GCM throughput rates for state-of-the-art, high-speed communication channels can be achieved with inexpensive hardware resources.The operation is an[authenticated encryption](https://en.wikipedia.org/wiki/Authenticated_encryption)algorithm designed to provide both data authenticity (integrity) and confidentiality. GCM is defined for block ciphers with a block size of 128 bits.Galois Message Authentication Code(GMAC) is an authentication-only variant of the GCM which can form an incremental message authentication code. Both GCM and GMAC can accept initialization vectors of arbitrary length.\nDifferent block cipher modes of operation can have significantly different performance and efficiency characteristics, even when used with the same block cipher. GCM can take full advantage of parallel processing and implementing GCM can make efficient use of an[instruction pipeline](https://en.wikipedia.org/wiki/Instruction_pipeline)or a hardware pipeline. By contrast, the[cipher block chaining](https://en.wikipedia.org/wiki/Cipher_block_chaining)(CBC) mode of operation incurs[pipeline stalls](https://en.wikipedia.org/wiki/Pipeline_stall)that hamper its efficiency and performance.\n\n<https://en.wikipedia.org/wiki/Galois/Counter_Mode>\nAES-GCM-SIVis a[mode of operation](https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation)for the[Advanced Encryption Standard](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard)which provides similar performance to[Galois/Counter Mode](https://en.wikipedia.org/wiki/Galois/Counter_Mode)as well as misuse resistance in the event of the reuse of a[cryptographic nonce](https://en.wikipedia.org/wiki/Cryptographic_nonce). The construction is defined in RFC 8452.\n\n<https://en.wikipedia.org/wiki/AES-GCM-SIV>\n\n## Kerberos\n-   Uses symmetric keys\n-   Used to authenticate users and allow nodes to communicate with one another over a non-secure medium. Confidentiality and forward secrecy assured.\n-   Designed for client-server environment wherein a user might want to use a service over an insecure medium (e.g. LAN)\n\n## Entities\n1.  Server - resource that could be accessed by the client and TGS\n2.  Client - wants to access the resource\n3.  Ticket - a temporal access privilege assurance to the resource\n4.  Ticket Granting Server (TGS) - the server that issues the ticket\n5.  Key Distribution Server (KDC) - knows about the private keys of all entities.\n\n## Drawbacks\n-   Availability of KDC + centralized point of failures and vulnerabilities\n-   Time synchronization and strict deadlines\n-   No standardized implementations\n-   Scalability - Every user and server needs its own private keys\n\n## Elliptic-curve Cryptography\n\n**Elliptic-curve cryptography**(**ECC**) is an approach to[public-key cryptography](https://en.wikipedia.org/wiki/Public-key_cryptography)based on the[algebraic structure](https://en.wikipedia.org/wiki/Algebraic_structure)of[elliptic curves](https://en.wikipedia.org/wiki/Elliptic_curve)over[finite fields](https://en.wikipedia.org/wiki/Finite_field). ECC requires smaller keys compared to non-EC cryptography (based on plain[Galois fields](https://en.wikipedia.org/wiki/Galois_field)) to provide equivalent security.\nElliptic curves are applicable for[key agreement](https://en.wikipedia.org/wiki/Key_agreement),[digital signatures](https://en.wikipedia.org/wiki/Digital_signature),[pseudo-random generators](https://en.wikipedia.org/wiki/CPRNG)and other tasks. Indirectly, they can be used for[encryption](https://en.wikipedia.org/wiki/Encryption)by combining the key agreement with a symmetric encryption scheme. They are also used in several[integer factorization](https://en.wikipedia.org/wiki/Integer_factorization)[algorithms](https://en.wikipedia.org/wiki/Algorithm)based on elliptic curves that have applications in cryptography, such as[Lenstra elliptic-curve factorization](https://en.wikipedia.org/wiki/Lenstra_elliptic-curve_factorization).\n<https://en.wikipedia.org/wiki/Elliptic-curve_cryptography>\n\n## ECDSA (Elliptic Curve Digital Signature Algorithm)\n\nIn[cryptography](https://en.wikipedia.org/wiki/Cryptography), theECDSA offers a variant of the[Digital Signature Algorithm](https://en.wikipedia.org/wiki/Digital_Signature_Algorithm)(DSA) which uses[elliptic curve cryptography](https://en.wikipedia.org/wiki/Elliptic_curve_cryptography).\nAs with elliptic-curve cryptography in general, the bit[size](https://en.wikipedia.org/wiki/Key_size)of the[public key](https://en.wikipedia.org/wiki/Public_key)believed to be needed for ECDSA is about twice the size of the[security level](https://en.wikipedia.org/wiki/Security_level), in bits. For example, at a security level of 80 bits (meaning an attacker requires a maximum of about280operations to find the private key) the size of an ECDSA public key would be 160 bits, whereas the size of a DSA public key is at least 1024 bits. On the other hand, the signature size is the same for both DSA and ECDSA: approximately4tbits, wheretis the security level measured in bits, that is, about 320 bits for a security level of 80 bits.\n<https://en.wikipedia.org/wiki/Elliptic_Curve_Digital_Signature_Algorithm>\n\n## RSA\n\n**RSA**(**Rivest--Shamir--Adleman**) is one of the first[public-key cryptosystems](https://en.wikipedia.org/wiki/Public-key_cryptography)and is widely used for secure data transmission. In such a[cryptosystem](https://en.wikipedia.org/wiki/Cryptosystem), the[encryption key](https://en.wikipedia.org/wiki/Encryption_key)is public and it is different from the[decryption key](https://en.wikipedia.org/wiki/Decryption_key)which is kept secret (private). In RSA, this asymmetry is based on the practical difficulty of the[factorization](https://en.wikipedia.org/wiki/Factorization)of the product of two large[prime numbers](https://en.wikipedia.org/wiki/Prime_number), the \"[factoring problem](https://en.wikipedia.org/wiki/Factoring_problem)\". The[acronym](https://en.wikipedia.org/wiki/Acronym)RSA is made of the initial letters of the surnames of[Ron Rivest](https://en.wikipedia.org/wiki/Ron_Rivest),[Adi Shamir](https://en.wikipedia.org/wiki/Adi_Shamir), and[Leonard Adleman](https://en.wikipedia.org/wiki/Leonard_Adleman), who first publicly described the algorithm in 1978.[Clifford Cocks](https://en.wikipedia.org/wiki/Clifford_Cocks), an English mathematician working for the British intelligence agency[Government Communications Headquarters](https://en.wikipedia.org/wiki/Government_Communications_Headquarters)(GCHQ), had developed an equivalent system in 1973, but this was not[declassified](https://en.wikipedia.org/wiki/Classified_information)until 1997\nA user of RSA creates and then publishes a public key based on two large[prime numbers](https://en.wikipedia.org/wiki/Prime_number), along with an auxiliary value. The prime numbers must be kept secret. Anyone can use the public key to encrypt a message, but with currently published methods, and if the public key is large enough, only someone with knowledge of the prime numbers can decode the message feasibly.Breaking RSA[encryption](https://en.wikipedia.org/wiki/Encryption)is known as the[RSA problem](https://en.wikipedia.org/wiki/RSA_problem). Whether it is as difficult as the factoring problem remains an open question.\nRSA is a relatively slow algorithm, and because of this, it is less commonly used to directly encrypt user data. More often, RSA passes encrypted shared keys for[symmetric key](https://en.wikipedia.org/wiki/Symmetric-key_algorithm)cryptography which in turn can perform bulk encryption-decryption operations at much higher speed.\n<https://en.wikipedia.org/wiki/RSA_(cryptosystem)>\n\n<https://en.wikipedia.org/wiki/RSA_(cryptosystem)#Operation>\n\n## Shamir's Secret Sharing\n\nShamir's Secret Sharingis an[algorithm](https://en.wikipedia.org/wiki/Algorithm)in[cryptography](https://en.wikipedia.org/wiki/Cryptography)created by[Adi Shamir](https://en.wikipedia.org/wiki/Adi_Shamir). It is a form of[secret sharing](https://en.wikipedia.org/wiki/Secret_sharing), where a secret is divided into parts, giving each participant its own unique part.\nTo reconstruct the original secret, a minimum number of parts is required. In the threshold scheme this number is less than the total number of parts. Otherwise all participants are needed to reconstruct the original secret.\n<https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing>"},{"fields":{"slug":"/Computer-Science/Security/Cryptography-Intro/Cryptographic-Hash-Functions/","title":"Cryptographic Hash Functions"},"frontmatter":{"draft":false},"rawBody":"# Cryptographic Hash Functions\n\nCreated: 2018-10-14 09:09:05 +0500\n\nModified: 2021-05-29 21:57:47 +0500\n\n---\n\nAcryptographic hash functionis a special class of[hash function](https://en.wikipedia.org/wiki/Hash_function)that has certain properties which make it suitable for use in[cryptography](https://en.wikipedia.org/wiki/Cryptography). It is a mathematical[algorithm](https://en.wikipedia.org/wiki/Algorithm)that[maps](https://en.wikipedia.org/wiki/Map_(mathematics))data of arbitrary size to a[bit string](https://en.wikipedia.org/wiki/Bit_string)of a fixed size (a hash) and is designed to be a[one-way function](https://en.wikipedia.org/wiki/One-way_function), that is, a function which is[infeasible](https://en.wikipedia.org/wiki/Computational_complexity_theory#Intractability)to invert. The only way to recreate the input data from an ideal cryptographic hash function's output is to attempt a[brute-force search](https://en.wikipedia.org/wiki/Brute-force_search)of possible inputs to see if they produce a match, or use a[rainbow table](https://en.wikipedia.org/wiki/Rainbow_table)of matched hashes.[Bruce Schneier](https://en.wikipedia.org/wiki/Bruce_Schneier)has called one-way hash functions \"the workhorses of modern cryptography\".The input data is often called themessage, and the output (thehash valueorhash) is often called themessage digestor simply thedigest.\nThe ideal cryptographic hash function has five main properties:\n-   it is[deterministic](https://en.wikipedia.org/wiki/Deterministic_algorithm)so the same message always results in the same hash\n-   it is quick to compute the hash value for any given message\n-   it is[infeasible](https://en.wikipedia.org/wiki/Computational_complexity_theory#Intractability)to generate a message from its hash value except by trying all possible messages\n-   a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value\n-   it is[infeasible](https://en.wikipedia.org/wiki/Computational_complexity_theory#Intractability)to find two different messages with the same hash value\nCryptographic hash functions have many[information-security](https://en.wikipedia.org/wiki/Information_security)applications, notably in[digital signatures](https://en.wikipedia.org/wiki/Digital_signature),[message authentication codes](https://en.wikipedia.org/wiki/Message_authentication_codes)(MACs), and other forms of[authentication](https://en.wikipedia.org/wiki/Authentication). They can also be used as ordinary[hash functions](https://en.wikipedia.org/wiki/Hash_function), to index data in[hash tables](https://en.wikipedia.org/wiki/Hash_table), for[fingerprinting](https://en.wikipedia.org/wiki/Fingerprint_(computing)), to detect duplicate data or uniquely identify files, and as[checksums](https://en.wikipedia.org/wiki/Checksum)to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital)fingerprints,checksums, or justhash values, even though all these terms stand for more general functions with rather different properties and purposes.\n**Hash Function Design**\n\n**Merkle--DamgÃ¥rd construction**\n![](media/Cryptography-Intro_Cryptographic-Hash-Functions-image1.png)\nA hash function must be able to process an arbitrary-length message into a fixed-length output. This can be achieved by breaking the input up into a series of equal-sized blocks, and operating on them in sequence using a[one-way compression function](https://en.wikipedia.org/wiki/One-way_compression_function). The compression function can either be specially designed for hashing or be built from a block cipher. A hash function built with the Merkle--DamgÃ¥rd construction is as resistant to collisions as is its compression function; any collision for the full hash function can be traced back to a collision in the compression function.\nThe last block processed should also be unambiguously[length padded](https://en.wikipedia.org/wiki/Padding_(cryptography)); this is crucial to the security of this construction. This construction is called the[Merkle--DamgÃ¥rd construction](https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction). Most common classical hash functions, including[SHA-1](https://en.wikipedia.org/wiki/SHA-1)and[MD5](https://en.wikipedia.org/wiki/MD5), take this form.\n**Wide pipe vs narrow pipe**\n\nA straightforward application of the Merkle--DamgÃ¥rd construction, where the size of hash output is equal to the internal state size (between each compression step), results in anarrow-pipehash design. This design causes many inherent flaws, including[length-extension](https://en.wikipedia.org/wiki/Length_extension_attack), multicollisions,long message attacks,generate-and-paste attacks,[[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)]and also cannot be parallelized. As a result, modern hash functions are built onwide-pipeconstructions that have a larger internal state size --- which range from tweaks of the Merkle--DamgÃ¥rd constructionto new constructions such as the[sponge construction](https://en.wikipedia.org/wiki/Sponge_construction)and[HAIFA construction](https://en.wikipedia.org/wiki/HAIFA_construction).None of the entrants in the[NIST hash function competition](https://en.wikipedia.org/wiki/NIST_hash_function_competition)use a classical Merkle--DamgÃ¥rd construction.\nMeanwhile, truncating the output of a longer hash, such as used in SHA-512/256, also defeats many of these attacks.\n**Salt**\n\nA \"salt\" is a random piece of data that is often added to the data you want to hash before you actually hash it. Adding a salt to your data before hashing it will make the output of the hash function different than it would be if you had only hashed the data.\nWhen a user sets their password (often on signing up), a random salt should be generated and used to compute the password hash. The salt should then be stored with the password hash. When the user tries to log in, combine the salt with the supplied password, hash the combination of the two, and compare it to the hash in the database.\n**Why should you use a salt?**\n\nWithout going into too much detail, hackers commonly use[rainbow table attacks](https://www.geeksforgeeks.org/understanding-rainbow-table-attack/),[dictionary attacks](https://en.wikipedia.org/wiki/Dictionary_attack), and[brute-force attacks](http://www.tenminutetutor.com/data-formats/cryptography/attacks-on-hash-algorithms/)to try and crack password hashes. While hackers can't compute the original password given only a hash, they can take a long list of possible passwords and compute hashes for them to try and match them with the passwords in the database. This is effectively how these types of attacks work, although each of the above works somewhat differently.\nA salt makes it much more difficult for hackers to perform these types of attacks. Depending on the hash function, salted hashes take nearly exponentially more time to crack than unsalted ones. They also make rainbow table attacks nearly impossible. It's therefore important to always use salts in your hashes.\n<https://dev.to/kmistele/how-to-securely-hash-and-store-passwords-in-your-next-application-4e2f>\n\n## Key derivation function**\n\nIn[cryptography](https://en.wikipedia.org/wiki/Cryptography), akey derivation function(KDF) is a[cryptographic hash function](https://en.wikipedia.org/wiki/Cryptographic_hash_function)that derives one or more[secret keys](https://en.wikipedia.org/wiki/Key_(cryptography))from a secret value such as a main key, a[password](https://en.wikipedia.org/wiki/Password), or a[passphrase](https://en.wikipedia.org/wiki/Passphrase)using a [pseudorandom function](https://en.wikipedia.org/wiki/Pseudorandom_function). KDFs can be used to stretch keys into longer keys or to obtain keys of a required format, such as converting a group element that is the result of a[Diffie--Hellman key exchange](https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange)into a symmetric key for use with[AES](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard).[Keyed cryptographic hash functions](https://en.wikipedia.org/wiki/HMAC)are popular examples of pseudorandom functions used for key derivation.\nThere are many forms of KDF's, and not all functions used as KDF are explicitly named as KDF's. For instance, the KDF of TLS is simply called \"the PRF\" for Pseudo-Random-Functions, which is a much more generic term.\nKey derivation functions derive bytes suitable for cryptographic operations from passwords or other data sources using a pseudo-random function (PRF). Different KDFs are suitable for different tasks such as:\n-   **Cryptographic key derivation**\n\nDeriving a key suitable for use as input to an encryption algorithm. Typically this means taking a password and running it through an algorithm such as[PBKDF2HMAC](https://cryptography.io/en/latest/hazmat/primitives/key-derivation-functions/#cryptography.hazmat.primitives.kdf.pbkdf2.PBKDF2HMAC)or[HKDF](https://cryptography.io/en/latest/hazmat/primitives/key-derivation-functions/#cryptography.hazmat.primitives.kdf.hkdf.HKDF). This process is typically known as[key stretching](https://en.wikipedia.org/wiki/Key_stretching).\n-   **Password storage**\n\nWhen storing passwords you want to use an algorithm that is computationally intensive. Legitimate users will only need to compute it once (for example, taking the user's password, running it through the KDF, then comparing it to the stored value), while attackers will need to do it billions of times. Ideal password storage KDFs will be demanding on both computational and memory resources.\n**Variable Cost Algorithm**\n-   **PBKDF2**\n\n[PBKDF2](https://en.wikipedia.org/wiki/PBKDF2)(Password Based Key Derivation Function 2) is typically used for deriving a cryptographic key from a password. It may also be used for key storage, but an alternate key storage KDF such as[Scrypt](https://cryptography.io/en/latest/hazmat/primitives/key-derivation-functions/#cryptography.hazmat.primitives.kdf.scrypt.Scrypt)is generally considered a better solution.\nIn[cryptography](https://en.wikipedia.org/wiki/Cryptography),**PBKDF1andPBKDF2(Password-Based Key Derivation Function 1and2)** are[key derivation functions](https://en.wikipedia.org/wiki/Key_derivation_function)with a sliding computational cost, used to reduce vulnerabilities of[brute-force attacks](https://en.wikipedia.org/wiki/Brute-force_attack).\nPBKDF2 applies a[pseudorandom function](https://en.wikipedia.org/wiki/Pseudorandom_function), such as[hash-based message authentication code](https://en.wikipedia.org/wiki/Hash-based_message_authentication_code) (HMAC), to the input[password](https://en.wikipedia.org/wiki/Password)or[passphrase](https://en.wikipedia.org/wiki/Passphrase)along with a[salt](https://en.wikipedia.org/wiki/Salt_(cryptography))value and repeats the process many times to produce aderived key, which can then be used as a[cryptographic key](https://en.wikipedia.org/wiki/Key_(cryptography))in subsequent operations. The added computational work makes[password cracking](https://en.wikipedia.org/wiki/Password_cracking)much more difficult, and is known as**[key stretching](https://en.wikipedia.org/wiki/Key_stretching).**\n<https://en.wikipedia.org/wiki/PBKDF2>-   Scrypt\n\nScrypt is a KDF designed for password storage by Colin Percival to be resistant against hardware-assisted attackers by having a tunable memory cost. It is described in[RFC 7914](https://tools.ietf.org/html/rfc7914.html).\n**Fixed Cost Algorithm**\n-   ConcatKDF\n\nConcatKDFHash (Concatenation Key Derivation Function) is defined by the NIST Special Publication[NIST SP 800-56Ar2](https://csrc.nist.gov/publications/detail/sp/800-56a/rev-2/final)document, to be used to derive keys for use after a Key Exchange negotiation operation.\nWarning - ConcatKDFHash should not be used for password storage.-   HKDF\n\n[HKDF](https://en.wikipedia.org/wiki/HKDF)(HMAC-based Extract-and-Expand Key Derivation Function) is suitable for deriving keys of a fixed size used for other cryptographic operations.\nWarning - ConcatKDFHash should not be used for password storage.-   KBKDF\n-   X963KDF\n<https://en.wikipedia.org/wiki/Key_derivation_function>\n\n<https://cryptography.io/en/latest/hazmat/primitives/key-derivation-functions>\n\n## Cryptographic Hash Functions**\n-   MD5\n\nWas commonly used for password hashing, but now considered insecure for cryptographic purposes due to some vulnerabilities that were discovered in it\n-   SHA-1 (Standard Hash Algorithm)\n\nOriginally designed by the NSA for various purposes, now considered deprecated and insecure\n-   RIPEMD-160 (RACE Integrity Primitives Evaluation Message Digest)\n-   bcrypt\n\nA slow hash function that is resistant to brute-force cracks. Commonly used in some Linux distributions. Considered very secure.\n```\nimport bcrypt\n\nfrom models import db, User\ndef insert_user_into_db(username, password):\n\npassword_hash = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt(12))\n\nuser = User(password=password_hash, username=username)\n\ndb.session.add(user)\n\ndb.session.commit()\n```\n<https://auth0.com/blog/hashing-in-action-understanding-bcrypt>\n-   Whirlpool\n-   SHA-2\n-   SHA-3\n\nBetter than SHA-1, considered both safe and flexible\n-   BLAKE2\n-   NTLM\n\nCommonly used in Windows active directory, but easy to crack. Use NTLMv2 instead.\n-   Argon2\n\nA complicated but extremely secure hash function, resistant to brute force attacks. Can be difficult to implement.\nArgon2is a[key derivation function](https://en.wikipedia.org/wiki/Key_derivation_function)that was selected as the winner of the[Password Hashing Competition](https://en.wikipedia.org/wiki/Password_Hashing_Competition)in July 2015.It was designed by[Alex Biryukov](https://en.wikipedia.org/wiki/Alex_Biryukov), Daniel Dinu, and[Dmitry Khovratovich](https://en.wikipedia.org/wiki/Dmitry_Khovratovich)from the[University of Luxembourg](https://en.wikipedia.org/wiki/University_of_Luxembourg).The reference implementation of Argon2 is released under a Creative Commons[CC0](https://en.wikipedia.org/wiki/CC0)license (i.e.[public domain](https://en.wikipedia.org/wiki/Public_domain)) or the[Apache License 2.0](https://en.wikipedia.org/wiki/Apache_License), and provides three related versions:\n-   Argon2d maximizes resistance to GPU cracking attacks. It accesses the memory array in a password dependent order, which reduces the possibility of[time--memory trade-off](https://en.wikipedia.org/wiki/Time%E2%80%93memory_trade-off)(TMTO) attacks, but introduces possible[side-channel attacks](https://en.wikipedia.org/wiki/Side-channel_attack).\n-   Argon2i is optimized to resist side-channel attacks. It accesses the memory array in a password independent order.\n-   Argon2id is a hybrid version. It follows the Argon2i approach for the first half pass over memory and the Argon2d approach for subsequent passes. The Internet draftrecommends using Argon2id except when there are reasons to prefer one of the other two modes.\nAll three modes allow specification by three parameters that control:\n-   execution time\n-   memory required\n-   degree of parallelism\n<https://en.wikipedia.org/wiki/Argon2>\nPerformance-wise, a SHA-256 hash is about 20-30% slower to calculate than either MD5 or SHA-1 hashes.\n**References**\n\n<https://en.wikipedia.org/wiki/Cryptographic_hash_function>\n\n<https://medium.com/analytics-vidhya/password-hashing-pbkdf2-scrypt-bcrypt-and-argon2-e25aaf41598e>\n\n"},{"fields":{"slug":"/Computer-Science/Security/Cryptography-Intro/Cryptography-Terms/","title":"Cryptography Terms"},"frontmatter":{"draft":false},"rawBody":"# Cryptography Terms\n\nCreated: 2021-05-28 23:58:12 +0500\n\nModified: 2021-10-25 23:02:01 +0500\n\n---\n\n**Cryptographic Techniques**\n\n1.  Substitution - is one in which the letters of the plaintext are replace by other letters\n\n    a.  Mono Alphabetic Substitution\n\n    b.  Poly Alphabetic Substitution\n\n2.  Transposition (Permutation) - Method of disguising text or alphabet by shuffling or exchanging their position\n**CSPRNGs** (Cryptographically Secure Pseudo-Random Number Generators), also known as Deterministic Random Bit Generators (**DRBGs**).\n\n1.  PRNG (Psuedo-random)\n\n    a.  Generates a random number with an even distribution\n\n    b.  Can be seeded to \"re-roll\" the same random values\n\n    c.  Random.new and rand()\n\n2.  Gaussian/Normal distribution\n\n    a.  Generates random numbers but distributes them closed to the \"center\" mark of zero\n\n3.  Perlin Noise\n\n    a.  Takes an (x), (x,y), (x,y,z) coordinate and returns a value from Float32:Max to Float32:Min\n\n    b.  Attempts to normally distribute, at least from my understanding of the rubygem I ported\n**Semantic Security**\n\nIn[cryptography](https://en.wikipedia.org/wiki/Cryptography), asemantically secure[cryptosystem](https://en.wikipedia.org/wiki/Cryptosystem)is one where only negligible information about the[plaintext](https://en.wikipedia.org/wiki/Plaintext)can be feasibly extracted from the[ciphertext](https://en.wikipedia.org/wiki/Ciphertext). Specifically, any[probabilistic, polynomial-time algorithm](https://en.wikipedia.org/wiki/PP_(complexity))(PPTA) that is given the ciphertext of a certain messagem (taken from any distribution of messages), and the message's length, cannot determine any partial information on the message with probability[non-negligibly](https://en.wikipedia.org/wiki/Negligible_function)higher than all other PPTA's that only have access to the message length (and not the ciphertext).This concept is the computational complexity analogue to[Shannon's](https://en.wikipedia.org/wiki/Claude_Shannon)concept of[perfect secrecy](https://en.wikipedia.org/wiki/Perfect_secrecy). Perfect secrecy means that the ciphertext reveals no information at all about the plaintext, whereas semantic security implies that any information revealed cannot be feasibly extracted.\n<https://en.wikipedia.org/wiki/Semantic_security>\n\n## Forward Secrecy**\n\nIn cryptography,**forward secrecy(FS), also known asperfect forward secrecy(PFS)**, is a feature of specific key agreement protocols that gives assurances that session keys will not be compromised even if the private key of the server is compromised.Forward secrecy protects past sessions against future compromises of secret keys or passwords.By generating a unique session key for every session a user initiates, the compromise of a single session key will not affect any data other than that exchanged in the specific session protected by that particular key.\nForward secrecy further protects data on the transport layer of a network that uses common SSL/TLS protocols, including[OpenSSL](https://en.wikipedia.org/wiki/OpenSSL), which had previously been affected by the [Heartbleed](https://en.wikipedia.org/wiki/Heartbleed) security bug. If forward secrecy is used, encrypted communications and sessions recorded in the past cannot be retrieved and decrypted should long-term secret keys or passwords be compromised in the future, even if the adversary actively interfered, for example via a[man-in-the-middle attack](https://en.wikipedia.org/wiki/Man-in-the-middle_attack).\nThe value of forward secrecy depends on the assumed capabilities of an adversary. Forward secrecy has value if an adversary is assumed to be able to obtain secret keys from a device (READ access) but not modify the way keys are generated in a device (WRITE access). In some cases an adversary who can read keys from a device may also be able to modify the functioning of the session key generator. In these cases forward secrecy has no value.\n<https://en.wikipedia.org/wiki/Forward_secrecy>\n\n## Envelope Encryption**\n\nEnvelope encryption is the practice of encrypting data with a data encryption key (DEK) and then encrypting the DEK with a root key that you can fully manage.\nEnvelope Encryption is an approach/process used within many applications to encrypt data. Using this approach your data is protected two-fold.\n**How does this work ?**\n-   Typically there are manylong term keys or master keysthat isheld in a key management system (KMS).\n-   When you need to encrypt some message :\n    -   A request is sent to the KMS to generate a data keybased on one of the master keys.\n    -   KMS returns a data key, which usually contains both the plain text version and the encrypted version of the data key.\n    -   The message is encrypted using the plain text key.\n    -   Then both the encrypted message and the encrypted data key are packaged into a structure (sometimes called envelope) and written.\n    -   The plain text key is immediately removed from memory.\n-   When it comes time to decrypt the message:\n    -   The encrypted data key is extracted from the envelope.\n    -   KMS is requested to decrypt the data key using the same master key as that was used to generate it.\n    -   Once the plain text version of the data key is obtained then the encrypted message itself is decrypted.\nUsing this approach if one wants to decrypt data, they need be authenticated with the KMS, since the master keys are only held there and never exported, and only the KMS can decrypt the data keys.\n![image](media/Cryptography-Intro_Cryptography-Terms-image1.png)\n<https://devender.me/2016/07/13/envelope-encryption>\n![Ğ·. Ğ Ğ¿Ğ¼Ğ°Ğ›-Ğ° Ğ¥ ĞšĞ•Ğ ' (5?JJte 4. Ğ›Â¯Ñ‹-Ğ•ÑÑÑ‚Ğ¾Ğº â€¢ Ğ Ğ³Ğ½Ğ¿â‚¬Ğµ â€¢ Ğ Ğ³ÑˆĞ°}Ñ â€¢ %qaqenhle ](media/Cryptography-Intro_Cryptography-Terms-image2.png)\n\n<https://www.druva.com/blog/druva-tech-moments-digital-envelope-encryption><https://crypto.stackexchange.com/questions/3965/what-is-the-main-difference-between-a-key-an-iv-and-a-nonce>\n\n**A[key](https://en.wikipedia.org/wiki/Key_(cryptography)),** in the context of[symmetric cryptography](https://en.wikipedia.org/wiki/Symmetric-key_algorithm), is something you keep secret. Anyone who knows your key (or can guess it) can decrypt any data you've encrypted with it (or forge any authentication codes you've calculated with it, etc.).\n(There's also \"asymmetric\" or[public key cryptography](https://en.wikipedia.org/wiki/Public-key_encryption), where the key effectively has two parts: the private key, which allows decryption and/or signing, and a public key (derived from the corresponding private key) which allows encryption and/or signature verification.)\n**An[IV](https://en.wikipedia.org/wiki/Initialization_vector)or initialization vector** is, in its broadest sense, just the initial value used to start some iterated process. The term is used in a couple of different contexts and implies different security requirements in each of them. For example,[cryptographic hash functions](https://en.wikipedia.org/wiki/Cryptographic_hash_function)typically have a fixed IV, which is just an arbitrary constant which is included in the hash function specification and is used as the initial hash value before any data is fed in:\n\n![Diagram of a Merkle-DamgÃ¥rd hash function from Wikipedia](media/Cryptography-Intro_Cryptography-Terms-image3.png)\n\nConversely, most[block cipher modes of operation](https://en.wikipedia.org/wiki/Block_cipher_modes_of_operation)require an IV which is random and unpredictable, or at least unique for each message encrypted with a given key. (Of course,ifeach key is only ever used to encrypt a single message, one can get away with using a fixed IV.) This random IV ensures that each message encrypts differently, such that seeing multiple messages encrypted with the same key doesn't give the attacker any more information than just seeing a single long message. In particular, it ensures that encrypting thesamemessage twice yields two completely different ciphertexts, which is necessary in order for the encryption scheme to be[semantically secure](https://en.wikipedia.org/wiki/Semantic_security).\nIn any case, the IV never needs to be kept secret --- if it did, it would be a key, not an IV. Indeed, in most cases, keeping the IV secret would not be practical even if you wanted to since the recipient needs to know it in order to decrypt the data (or verify the hash, etc.).\n**A[nonce](https://en.wikipedia.org/wiki/Cryptographic_nonce),** in the broad sense, is just \"a number used only once\". The only thing generally demanded of a nonce is that it should never be used twice (within the relevant scope, such as encryption with a particular key). The unique IVs used for block cipher encryption qualify as nonces, but various other cryptographic schemes make use of nonces as well.\nThere's some variation about which of the terms \"IV\" and \"nonce\" is used for different block cipher modes of operation: some authors use exclusively one or the other, while some make a distinction between them. For CTR mode, in particular, some authors reserve the term \"IV\" for the full cipher input block formed by the concatenation of the nonce and the initial counter value (usually a block of all zero bits), while others prefer not to use the term \"IV\" for CTR mode at all. This is all complicated by the fact that there are several variations on how the nonce/IV sent with the message in CTR mode is actually mapped into the initial block cipher input.\nConversely, for modes other than CTR (or related modes such as EAX or GCM), the term \"IV\" is almost universally preferred over \"nonce\". This is particularly true for CBC mode since it has requirements on its IV (specifically, that they are unpredictable) which go beyond the usual requirement of uniqueness expected of nonces.\n**Initialization vector**\n\nIn[cryptography](https://en.wikipedia.org/wiki/Cryptography), aninitialization vector(IV) orstarting variable(SV)is an input to a[cryptographic primitive](https://en.wikipedia.org/wiki/Cryptographic_primitive)being used to provide the initial state. The IV is typically required to be [random](https://en.wikipedia.org/wiki/Random) or [pseudorandom](https://en.wikipedia.org/wiki/Pseudorandom), but sometimes an IV only needs to be unpredictable or unique.[Randomization](https://en.wikipedia.org/wiki/Randomization)is crucial for some[encryption](https://en.wikipedia.org/wiki/Encryption)schemes to achieve[semantic security](https://en.wikipedia.org/wiki/Semantic_security), a property whereby repeated usage of the scheme under the same[key](https://en.wikipedia.org/wiki/Cryptographic_key)does not allow an attacker to infer relationships between (potentially similar) segments of the encrypted message. For[block ciphers](https://en.wikipedia.org/wiki/Block_cipher), the use of an IV is described by the[modes of operation](https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation).\nSome cryptographic primitives require the IV only to be non-repeating, and the required randomness is derived internally. In this case, the IV is commonly called a[nonce](https://en.wikipedia.org/wiki/Cryptographic_nonce)(number used once), and the primitives (e.g.[CBC](https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#CBC)) are consideredstatefulrather thanrandomized. This is because an IV need not be explicitly forwarded to a recipient but may be derived from a common state updated at both sender and receiver side. (In practice, a short nonce is still transmitted along with the message to consider message loss.) An example of stateful encryption schemes is the[counter mode](https://en.wikipedia.org/wiki/Counter_mode)of operation, which has a[sequence number](https://en.wikipedia.org/wiki/Sequence_number)for a nonce.\nThe IV size depends on the cryptographic primitive used; for block ciphers it is generally the cipher's block-size. In encryption schemes, the unpredictable part of the IV has at best the same size as the key to compensate for[time/memory/data tradeoff attacks](https://en.wikipedia.org/wiki/Time/memory/data_tradeoff_attack). When the IV is chosen at random, the probability of collisions due to the[birthday problem](https://en.wikipedia.org/wiki/Birthday_problem)must be taken into account. Traditional stream ciphers such as[RC4](https://en.wikipedia.org/wiki/RC4)do not support an explicit IV as input, and a custom solution for incorporating an IV into the cipher's key or internal state is needed. Some designs realized in practice are known to be insecure; the[WEP](https://en.wikipedia.org/wiki/Wired_Equivalent_Privacy)protocol is a notable example, and is prone to related-IV attacks.\n**Two of the same messages encrypted with the same key, but different IVs, will result in different ciphertext. This makes an attacker's job more difficult.**\n**A random IV is not a secret.** It is no more sensitive than the ciphertext itself. You can transmit it along with the ciphertext without concern.\nThe only secret in a properly designed crypto system is the key (and obviously the plaintext). Everything else (IVs, salts, algorithms, padding, everything) is assumed be be known by attackers.\n\n<https://stackoverflow.com/questions/38059749/handling-transfer-of-iv-initialization-vectors>\n<https://en.wikipedia.org/wiki/Initialization_vector>\n\n## MAC (Message Authentication Code)**\n\nIn[cryptography](https://en.wikipedia.org/wiki/Cryptography), a**message authentication code**(**MAC**), sometimes known as a*tag*, is a short piece of information used to[authenticate a message](https://en.wikipedia.org/wiki/Message_authentication)---in other words, to confirm that the message came from the stated sender (its authenticity) and has not been changed. The MAC value protects both a message's[data integrity](https://en.wikipedia.org/wiki/Data_integrity)as well as its[authenticity](https://en.wikipedia.org/wiki/Message_authentication), by allowing verifiers (who also possess the secret key) to detect any changes to the message content.\nInformally, a message authentication code consists of three algorithms:\n-   A key generation algorithm selects a key from the key space uniformly at random.\n-   A signing algorithm efficiently returns a tag given the key and the message.\n-   A verifying algorithm efficiently verifies the authenticity of the message given the key and the tag. That is, return*accepted*when the message and tag are not tampered with or forged, and otherwise return*rejected*\n<https://en.wikipedia.org/wiki/Message_authentication_code>\n\n## Message Integrity Codes (MIC) / MACs (Message Authentication Code)**\n-   Attacks\n    -   Existential Forgery\n    -   Selective Forgery\n    -   Key recovery\n-   Merkle-Damgard Iterated Construction\n-   HMAC (Hash-MAC)\n-   Encrypted CBC-MAC (ECBC-MAC)s\n-   NMAC (nested MAC)\n**HMAC (Hash MAC)**\n\nIn[cryptography](https://en.wikipedia.org/wiki/Cryptography), anHMAC(sometimes expanded as eitherkeyed-hash message authentication codeorhash-based message authentication code) is a specific type of[message authentication code](https://en.wikipedia.org/wiki/Message_authentication_code)(MAC) involving a[cryptographic hash function](https://en.wikipedia.org/wiki/Cryptographic_hash_function)and a secret[cryptographic key](https://en.wikipedia.org/wiki/Cryptographic_key). It may be used to simultaneously verify both **the[data integrity](https://en.wikipedia.org/wiki/Data_integrity)and the[authentication](https://en.wikipedia.org/wiki/Authentication) of a[message](https://en.wikipedia.org/wiki/Cleartext),** as with any MAC. Any cryptographic hash function, such as[SHA-256](https://en.wikipedia.org/wiki/SHA-256)or[SHA-3](https://en.wikipedia.org/wiki/SHA-3), may be used in the calculation of an HMAC; the resulting MAC algorithm is termed HMAC-X, where X is the hash function used (e.g. HMAC-SHA256 or HMAC-SHA3). The cryptographic strength of the HMAC depends upon the[cryptographic strength](https://en.wikipedia.org/wiki/Cryptographic_strength)of the underlying hash function, the size of its hash output, and the size and quality of the key.\nHMAC uses two passes of hash computation. The secret key is first used to derive two keys -- inner and outer. The first pass of the algorithm produces an internal hash derived from the message and the inner key. The second pass produces the final HMAC code derived from the inner hash result and the outer key. Thus the algorithm provides better immunity against[length extension attacks](https://en.wikipedia.org/wiki/Length_extension_attack).\n<https://en.wikipedia.org/wiki/HMAC>"},{"fields":{"slug":"/Computer-Science/Security/Cryptography-Intro/Diffie-Hellman-Key-Exchange/","title":"Diffie-Hellman Key Exchange"},"frontmatter":{"draft":false},"rawBody":"# Diffie-Hellman Key Exchange\n\nCreated: 2018-04-24 10:45:28 +0500\n\nModified: 2018-04-27 00:29:01 +0500\n\n---\n\n**Diffie--Hellman key exchange**(**DH**)is a method of securely exchanging[cryptographic keys](https://en.wikipedia.org/wiki/Key_(cryptography))over a public channel and was one of the first[public-key protocols](https://en.wikipedia.org/wiki/Public-key_cryptography)as originally conceptualized by[Ralph Merkle](https://en.wikipedia.org/wiki/Ralph_Merkle)and named after[Whitfield Diffie](https://en.wikipedia.org/wiki/Whitfield_Diffie)and[Martin Hellman](https://en.wikipedia.org/wiki/Martin_Hellman).\nThe simplest and the original implementation of the protocol uses the[multiplicative group of integers modulo](https://en.wikipedia.org/wiki/Multiplicative_group_of_integers_modulo_n)*p*, where*p*is[prime](https://en.wikipedia.org/wiki/Prime_number), and*g*is a[primitive root](https://en.wikipedia.org/wiki/Primitive_root_modulo_n)[modulo](https://en.wikipedia.org/wiki/Modular_arithmetic)*p*. These two values are chosen in this way to ensure that the resulting shared secret can take on any value from 1 to*p*--1. Here is an example of the protocol, with non-secret values inblue, and secret values in**red**.\n\n1.  [Alice and Bob](https://en.wikipedia.org/wiki/Alice_and_Bob)agree to use a modulus*p*=23and base*g*= 5 (which is a[primitive root modulo](https://en.wikipedia.org/wiki/Primitive_root_modulo_n)23).2.  Alice chooses a secret integer***a***= 4, then sends Bob*A*=*g**^a^***mod*p*\n    -   *A*= 5**^4^**mod23=4\n\n3.  Bob chooses a secret integer***b***= 3, then sends Alice*B*=*g**^b^***mod*p*\n    -   *B*= 5**^3^**mod23=10\n\n4.  Alice computes***s***=*B**^a^***mod*p*\n    -   ***s***=10**^4^**mod23=18\n\n5.  Bob computes***s***=*A**^b^***mod*p*\n    -   ***s***=4**^3^**mod23=18\n\n6.  Alice and Bob now share a secret (the number 18).\nBoth Alice and Bob have arrived at the same value s, because, under mod p,\nMore specifically,\n\nNote that only*a*,*b*, and (*g^ab^*mod*p*=*g^ba^*mod*p*) are kept secret. All the other values --*p*,*g*,*g^a^*mod*p*, and*g^b^*mod*p*-- are sent in the clear. Once Alice and Bob compute the shared secret they can use it as an encryption key, known only to them, for sending messages across the same open communications channel."},{"fields":{"slug":"/Computer-Science/Security/Cryptography-Intro/Public-key-cryptography/","title":"Public-key cryptography"},"frontmatter":{"draft":false},"rawBody":"# Public-key cryptography\n\nCreated: 2018-04-24 10:56:15 +0500\n\nModified: 2020-06-20 21:27:47 +0500\n\n---\n\n**Public key cryptography, orasymmetrical cryptography**, is any cryptographic system that uses pairs of[keys](https://en.wikipedia.org/wiki/Cryptographic_key):public keyswhich may be disseminated widely, andprivate keyswhich are known only to the owner. This accomplishes two functions:[authentication](https://en.wikipedia.org/wiki/Authentication_protocol), where the public key verifies that a holder of the paired private key sent the message, and[encryption](https://en.wikipedia.org/wiki/Encryption), where only the paired private key holder can decrypt the message encrypted with the public key.\nIn a public key encryption system, any person can encrypt a message using the receiver's public key. That encrypted message can only be decrypted with the receiver's private key. To be practical, the generation of a public and private key-pair must be computationally economical. The strength of a public key cryptography system relies on the computational effort (work factorin cryptography) required to find the private key from its paired public key. Effective security only requires keeping the private key private; the public key can be openly distributed without compromising security\n**Public Key Infrastructure (PKI) -**\n-   Relies on a centralized authority, aka Certificate Authority (CA)\n-   Users generally have their own identities - public-private key pair which is attested by the CA and stored in the form of certificates\n-   X.509 certificate format\n![Submitted To Certificate Signing Request (CSR) Key Pair Public Key Protected Private Key Store Created By Private Key Public Subject Identifier Client To Client Certificate Authority (CA) Issues Certificate ](media/Cryptography-Intro_Public-key-cryptography-image1.png)\n![Computer \"Alice\" Yo can I have your public key? Neat! Hey let's use this to encrypt stuff: spspspspspsps pspspspspsspp pspsppspspsps pp 3 Sure! It's BEGIN Oh nice! Let's do that. Now I can tell you secrets! pspspspsp Computer \"Bob\" ](media/Cryptography-Intro_Public-key-cryptography-image2.png)\n![my certificate: By the way, can you send yours? Certificate Authoritie Hmm. Hey what do you think? Yep that's bob Yo Bob is that you? Computer \"Alice\" certificate: By the way let's talk secrets: pspspspsps Computer \"Bob\" Yo is this Alice? Certificate Authorities Yep Nice! spspsps ](media/Cryptography-Intro_Public-key-cryptography-image3.png)\n<https://medium.com/sitewards/the-magic-of-tls-x509-and-mutual-authentication-explained-b2162dec4401>\n![](media/Cryptography-Intro_Public-key-cryptography-image4.png)\n\n"},{"fields":{"slug":"/Computer-Science/System-Design/Architecture-Guide/N-Tier-Application-Architecture/","title":"N-Tier Application Architecture"},"frontmatter":{"draft":false},"rawBody":"# N-Tier Application Architecture\n\nCreated: 2020-08-17 19:59:42 +0500\n\nModified: 2020-08-31 01:27:07 +0500\n\n---\n\nAn N-tier architecture divides an application intological layersandphysical tiers.\n\n![WAF Client Web Tier Messaging Service Middle Tier 1 Cache Data Tier Middle Tier 2 ](media/Architecture-Guide_N-Tier-Application-Architecture-image1.png)\n\nLayers are a way to separate responsibilities and manage dependencies. Each layer has a specific responsibility. **A higher layer can use services in a lower layer, but not the other way around.**\nTiers are physically separated, running on separate machines. A tier can call to another tier directly, or use asynchronous messaging (message queue). Although each layer might be hosted in its own tier, that's not required. Several layers might be hosted on the same tier. Physically separating the tiers improves scalability and resiliency, but also adds latency from the additional network communication.\n**3-Tier Architecture**\n\nA traditional **three-tier application** has **a presentation tier, a middle tier, and a database tier**. The middle tier is optional. More complex applications can have more than three tiers. The diagram above shows an application with two middle tiers, encapsulating different areas of functionality.\nA three-tier architecture is a software architecture pattern where the application is broken down into three logical tiers: the presentation layer, the business logic layer and the data storage layer. This architecture is used in a client-server application such as a web application that has the frontend, the backend and the database. Each of these layers or tiers does a specific task and can be managed independently of each other. This a shift from the monolithic way of building an application where the frontend, the backend and the database are both sitting in one place.\n**Common layers**\n\nIn a logical multilayered architecture for an information system with an[object-oriented design](https://en.wikipedia.org/wiki/Object-oriented_design), the following four are the most common:\n-   Presentation layer(a.k.a. UI layer, view layer, presentation tier in multitier architecture)\n-   Application layer(a.k.a.[service layer](https://en.wikipedia.org/wiki/Service-oriented_architecture)or[GRASP](https://en.wikipedia.org/wiki/GRASP_(object-oriented_design))Controller Layer)\n-   [Business layer](https://en.wikipedia.org/wiki/Business_layer)(a.k.a.[business logic layer](https://en.wikipedia.org/wiki/Business_logic_layer)(BLL), domain layer)\n-   [Data access layer](https://en.wikipedia.org/wiki/Data_access_layer)(a.k.a.[persistence layer](https://en.wikipedia.org/wiki/Persistence_layer), logging, networking, and other services which are required to support a particular business layer)\n<https://en.wikipedia.org/wiki/Multitier_architecture>\nAn N-tier application can have aclosed layer architectureor anopen layer architecture:\n-   In a closed layer architecture, a layer can only call the next layer immediately down.\n-   In an open layer architecture, a layer can call any of the layers below it.\nA closed layer architecture limits the dependencies between layers. However, it might create unnecessary network traffic, if one layer simply passes requests along to the next layer.\n**When to use this architecture**\n\nN-tier architectures are typically implemented as infrastructure-as-service (IaaS) applications, with each tier running on a separate set of VMs. However, an N-tier application doesn't need to be pure IaaS. Often, it's advantageous to use managed services for some parts of the architecture, particularly caching, messaging, and data storage.\nConsider an N-tier architecture for:\n-   Simple web applications.\n-   Unified development of on-premises and cloud applications.\nN-tier architectures are very common in traditional on-premises applications, so it's a natural fit for migrating existing workloads to Azure.\n**Benefits**\n-   Portability between cloud and on-premises, and between cloud platforms.\n-   Less learning curve for most developers.\n-   Natural evolution from the traditional application model.\n**Challenges**\n-   It's easy to end up with a middle tier that just does CRUD operations on the database, adding extra latency without doing any useful work.\n-   Monolithic design prevents independent deployment of features.\n-   Managing an IaaS application is more work than an application that uses only managed services.\n-   It can be difficult to manage network security in a large system.\n**Best practices**\n-   Use autoscaling to handle changes in load. See[Autoscaling best practices](https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling).\n-   Use[asynchronous messaging](https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-async-messaging)to decouple tiers.\n-   Cache semistatic data. See[Caching best practices](https://docs.microsoft.com/en-us/azure/architecture/best-practices/caching).\n-   Configure the database tier for high availability, using a solution such as[SQL Server Always On availability groups](https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/always-on-availability-groups-sql-server).\n-   Place a web application firewall (WAF) between the front end and the Internet.\n-   Place each tier in its own subnet, and use subnets as a security boundary.\n-   Restrict access to the data tier, by allowing requests only from the middle tier(s).\n**N-tier architecture on virtual machines**\n\n![Physical diagram of an N-tier architecture](media/Architecture-Guide_N-Tier-Application-Architecture-image2.png)\nEach tier consists of two or more VMs, placed in an availability set or virtual machine scale set. Multiple VMs provide resiliency in case one VM fails. Load balancers are used to distribute requests across the VMs in a tier. A tier can be scaled horizontally by adding more VMs to the pool.\nEach tier is also placed inside its own subnet, meaning their internal IP addresses fall within the same address range. That makes it easy to apply network security group rules and route tables to individual tiers.\nThe web and business tiers are stateless. Any VM can handle any request for that tier. The data tier should consist of a replicated database. For Windows, we recommend SQL Server, using Always On availability groups for high availability. For Linux, choose a database that supports replication, such as Apache Cassandra.\nNetwork security groups restrict access to each tier. For example, the database tier only allows access from the business tier.\n**Additional considerations**\n-   N-tier architectures are not restricted to three tiers. For more complex applications, it is common to have more tiers. In that case, consider using layer-7 routing to route requests to a particular tier.\n-   Tiers are the boundary of scalability, reliability, and security. Consider having separate tiers for services with different requirements in those areas.\n-   Use virtual machine scale sets for autoscaling.\n-   Look for places in the architecture where you can use a managed service without significant refactoring. In particular, look at caching, messaging, storage, and databases.\n-   For higher security, place a network DMZ in front of the application. The DMZ includes network virtual appliances (NVAs) that implement security functionality such as firewalls and packet inspection. For more information, see[Network DMZ reference architecture](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/dmz/secure-vnet-dmz).\n-   For high availability, place two or more NVAs in an availability set, with an external load balancer to distribute Internet requests across the instances. For more information, see[Deploy highly available network virtual appliances](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/dmz/nva-ha).\n-   Do not allow direct RDP or SSH access to VMs that are running application code. Instead, operators should log into a jumpbox, also called a bastion host. This is a VM on the network that administrators use to connect to the other VMs. The jumpbox has a network security group that allows RDP or SSH only from approved public IP addresses.\n-   You can extend the Azure virtual network to your on-premises network using a site-to-site virtual private network (VPN) or Azure ExpressRoute. For more information, see[Hybrid network reference architecture](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/hybrid-networking/).\n-   If your organization uses Active Directory to manage identity, you may want to extend your Active Directory environment to the Azure VNet. For more information, see[Identity management reference architecture](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/identity/).\n-   If you need higher availability than the Azure SLA for VMs provides, replicate the application across two regions and use Azure Traffic Manager for failover. For more information, see[Run Windows VMs in multiple regions](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/n-tier/multi-region-sql-server)or[Run Linux VMs in multiple regions](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/n-tier/n-tier-cassandra).\n<https://docs.microsoft.com/en-us/azure/architecture/guide/architecture-styles/n-tier>"},{"fields":{"slug":"/Computer-Science/System-Design/Microservice-Architecture/Design-Patterns/","title":"Design Patterns"},"frontmatter":{"draft":false},"rawBody":"# Design Patterns\n\nCreated: 2020-07-23 16:30:51 +0500\n\nModified: 2021-12-28 21:45:09 +0500\n\n---\n\n**Design Patterns of Microservices**\n\n1.  [Aggregator](https://www.edureka.co/blog/microservices-design-patterns#Aggregator)2.  [**API Gateway**](https://www.edureka.co/blog/microservices-design-patterns#APIGateway)\n\n3.  [**Chained or Chain of Responsibility**](https://www.edureka.co/blog/microservices-design-patterns#Chained)\n\n4.  [**Asynchronous Messaging**](https://www.edureka.co/blog/microservices-design-patterns#AsynchronousMessaging)5.  [Database or Shared Data](https://www.edureka.co/blog/microservices-design-patterns#Database)\n\n6.  [Event Sourcing](https://www.edureka.co/blog/microservices-design-patterns#EventSourcing)\n\n7.  [Branch](https://www.edureka.co/blog/microservices-design-patterns#Branch)\n\n8.  [Command Query Responsibility Segregator](https://www.edureka.co/blog/microservices-design-patterns#CQRS)\n\n9.  [Circuit Breaker](https://www.edureka.co/blog/microservices-design-patterns#CircuitBreaker)\n\n10. [Decomposition](https://www.edureka.co/blog/microservices-design-patterns#Decomposition)\n<https://www.edureka.co/blog/microservices-design-patterns>\n![Motivating Pattern Solution A * - Solution Pattern - + Solution B Database per Service Core Single Service per Host Deployment Multiple Services per host Microservice architecture Server-side discovery Client-side discovery Discovery Cross-cutting concerns Microservice Chassis Extemalized configuration Access Token Security Circuit Breaker Testing Service Integration Contract Test Service Component Test Audit logging Application logging Health check Distributed tracing Application metrics Exception tracking Monolithic architecture API gateway Communication Messaging Remote Procedure Invocation Observability Decomposition Decompose by business capability Decompose by subdomain Server-side page fragment composition Client-side UI composition ](media/Microservice-Architecture_Design-Patterns-image1.jpg)\n-   Decomposition patterns\n    -   [Decompose by business capability](https://microservices.io/patterns/decomposition/decompose-by-business-capability.html)\n    -   [Decompose by subdomain](https://microservices.io/patterns/decomposition/decompose-by-subdomain.html)\n**Functional Decomposition**\n\nFunctional decomposition is a term that engineers use to describea set of steps in which they break down the overall function of a device, system, or process into its smaller parts. A function is simply a task that is performed by a device, system, or process. Decomposition is a process of breaking down.\n\n<https://en.wikipedia.org/wiki/Functional_decomposition>-   The[Database per Service pattern](https://microservices.io/patterns/data/database-per-service.html)describes how each service has its own database in order to ensure loose coupling.\n-   The[API Gateway pattern](https://microservices.io/patterns/apigateway.html)defines how clients access the services in a microservice architecture.\n-   The[Client-side Discovery](https://microservices.io/patterns/client-side-discovery.html)and[Server-side Discovery](https://microservices.io/patterns/server-side-discovery.html)patterns are used to route requests for a client to an available service instance in a microservice architecture.\n-   The Messaging and Remote Procedure Invocation patterns are two different ways that services can communicate.\n-   The[Single Service per Host](https://microservices.io/patterns/deployment/single-service-per-host.html)and[Multiple Services per Host](https://microservices.io/patterns/deployment/multiple-services-per-host.html)patterns are two different deployment strategies.\n-   Cross-cutting concerns patterns:[Microservice chassis pattern](https://microservices.io/patterns/microservice-chassis.html)and[Externalized configuration](https://microservices.io/patterns/externalized-configuration.html)\n-   Testing patterns:[Service Component Test](https://microservices.io/patterns/testing/service-component-test.html)and[Service Integration Contract Test](https://microservices.io/patterns/testing/service-integration-contract-test.html)\n-   [Circuit Breaker](https://microservices.io/patterns/reliability/circuit-breaker.html)\n-   [Access Token](https://microservices.io/patterns/security/access-token.html)\n-   Observability patterns:\n    -   [Log aggregation](https://microservices.io/patterns/observability/application-logging.html)\n    -   [Application metrics](https://microservices.io/patterns/observability/application-metrics.html)\n    -   [Audit logging](https://microservices.io/patterns/observability/audit-logging.html)\n    -   [Distributed tracing](https://microservices.io/patterns/observability/distributed-tracing.html)\n    -   [Exception tracking](https://microservices.io/patterns/observability/exception-tracking.html)\n    -   [Health check API](https://microservices.io/patterns/observability/health-check-api.html)\n    -   [Log deployments and changes](https://microservices.io/patterns/observability/log-deployments-and-changes.html)\n-   UI patterns:\n    -   [Server-side page fragment composition](https://microservices.io/patterns/ui/server-side-page-fragment-composition.html)\n    -   [Client-side UI composition](https://microservices.io/patterns/ui/client-side-ui-composition.html)\n<https://microservices.io/patterns/microservices.html>\n\n## Catalog of patterns**\n\n<table>\n<colgroup>\n<col style=\"width: 22%\" />\n<col style=\"width: 58%\" />\n<col style=\"width: 19%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th><strong>Pattern</strong></th>\n<th><strong>Summary</strong></th>\n<th><strong>Category</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/ambassador\">Ambassador</a></td>\n<td>Create helper services that send network requests on behalf of a consumer service or application.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/management-monitoring\">Management and Monitoring</a></p></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/anti-corruption-layer\">Anti-Corruption Layer</a></td>\n<td>Implement a faÃ§ade or adapter layer between a modern application and a legacy system.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/management-monitoring\">Management and Monitoring</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/async-request-reply\">Asynchronous Request-Reply</a></td>\n<td>Decouple backend processing from a frontend host, where backend processing needs to be asynchronous, but the frontend still needs a clear response.</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging\">Messaging</a></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends\"><strong>Backends for Frontends</strong></a></td>\n<td><p><strong>Create separate backend services to be consumed by specific frontend applications or interfaces.</strong></p>\n<p></p>\n<p><a href=\"https://samnewman.io/patterns/architectural/bff/\">https://samnewman.io/patterns/architectural/bff/</a></p></td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\"><strong>Design and Implementation</strong></a></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead\">Bulkhead</a></td>\n<td>Isolate elements of an application into pools so that if one fails, the others will continue to function.</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency\">Resiliency</a></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside\">Cache-Aside</a></td>\n<td>Load data on demand into a cache from a data store</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/data-management\">Data Management</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/choreography\">Choreography</a></td>\n<td>Let each service decide when and how a business operation is processed, instead of depending on a central orchestrator.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging\">Messaging</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker\">Circuit Breaker</a></td>\n<td>Handle faults that might take a variable amount of time to fix when connecting to a remote service or resource.</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency\">Resiliency</a></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/claim-check\">Claim Check</a></td>\n<td>Split a large message into a claim check and a payload to avoid overwhelming a message bus.</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging\">Messaging</a></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/compensating-transaction\">Compensating Transaction</a></td>\n<td>Undo the work performed by a series of steps, which together define an eventually consistent operation.</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency\">Resiliency</a></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/competing-consumers\">Competing Consumers</a></td>\n<td>Enable multiple concurrent consumers to process messages received on the same messaging channel.</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging\">Messaging</a></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/compute-resource-consolidation\">Compute Resource Consolidation</a></td>\n<td>Consolidate multiple tasks or operations into a single computational unit</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/cqrs\">CQRS</a></td>\n<td>Segregate operations that read data from operations that update data by using separate interfaces.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/data-management\">Data Management</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/deployment-stamp\">Deployment Stamps</a></td>\n<td>Deploy multiple independent copies of application components, including data stores.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/availability\">Availability</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing\">Event Sourcing</a></td>\n<td>Use an append-only store to record the full series of events that describe actions taken on data in a domain.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/data-management\">Data Management</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/external-configuration-store\">External Configuration Store</a></td>\n<td>Move configuration information out of the application deployment package to a centralized location.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/management-monitoring\">Management and Monitoring</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/federated-identity\">Federated Identity</a></td>\n<td>Delegate authentication to an external identity provider.</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/security\">Security</a></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/gatekeeper\">Gatekeeper</a></td>\n<td>Protect applications and services by using a dedicated host instance that acts as a broker between clients and the application or service, validates and sanitizes requests, and passes requests and data between them.</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/security\">Security</a></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/gateway-aggregation\">Gateway Aggregation</a></td>\n<td>Use a gateway to aggregate multiple individual requests into a single request.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/management-monitoring\">Management and Monitoring</a></p></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/gateway-offloading\">Gateway Offloading</a></td>\n<td>Offload shared or specialized service functionality to a gateway proxy.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/management-monitoring\">Management and Monitoring</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/gateway-routing\">Gateway Routing</a></td>\n<td>Route requests to multiple services using a single endpoint.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/management-monitoring\">Management and Monitoring</a></p></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/geodes\">Geodes</a></td>\n<td>Deploy backend services into a set of geographical nodes, each of which can service any client request in any region.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/availability\">Availability</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/health-endpoint-monitoring\">Health Endpoint Monitoring</a></td>\n<td>Implement functional checks in an application that external tools can access through exposed endpoints at regular intervals.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/availability\">Availability</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/management-monitoring\">Management and Monitoring</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency\">Resiliency</a></p></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/index-table\">Index Table</a></td>\n<td>Create indexes over the fields in data stores that are frequently referenced by queries.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/data-management\">Data Management</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/leader-election\">Leader Election</a></td>\n<td>Coordinate the actions performed by a collection of collaborating task instances in a distributed application by electing one instance as the leader that assumes responsibility for managing the other instances.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency\">Resiliency</a></p></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/materialized-view\">Materialized View</a></td>\n<td>Generate prepopulated views over the data in one or more data stores when the data isn't ideally formatted for required query operations.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/data-management\">Data Management</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/pipes-and-filters\">Pipes and Filters</a></td>\n<td>Break down a task that performs complex processing into a series of separate elements that can be reused.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging\">Messaging</a></p></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/priority-queue\">Priority Queue</a></td>\n<td>Prioritize requests sent to services so that requests with a higher priority are received and processed more quickly than those with a lower priority.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging\">Messaging</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber\">Publisher/Subscriber</a></td>\n<td>Enable an application to announce events to multiple interested consumers asynchronously, without coupling the senders to the receivers.</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging\">Messaging</a></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/queue-based-load-leveling\">Queue-Based Load Leveling</a></td>\n<td>Use a queue that acts as a buffer between a task and a service that it invokes in order to smooth intermittent heavy loads.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/availability\">Availability</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging\">Messaging</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency\">Resiliency</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/retry\">Retry</a></td>\n<td>Enable an application to handle anticipated, temporary failures when it tries to connect to a service or network resource by transparently retrying an operation that's previously failed.</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency\">Resiliency</a></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/scheduler-agent-supervisor\">Scheduler Agent Supervisor</a></td>\n<td>Coordinate a set of actions across a distributed set of services and other remote resources.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging\">Messaging</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency\">Resiliency</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/sequential-convoy\">Sequential Convoy</a></td>\n<td>Process a set of related messages in a defined order, without blocking processing of other groups of messages.</td>\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging\">Messaging</a></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/sharding\">Sharding</a></td>\n<td>Divide a data store into a set of horizontal partitions or shards.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/data-management\">Data Management</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/sidecar\">Sidecar</a></td>\n<td>Deploy components of an application into a separate process or container to provide isolation and encapsulation.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/management-monitoring\">Management and Monitoring</a></p></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/static-content-hosting\">Static Content Hosting</a></td>\n<td>Deploy static content to a cloud-based storage service that can deliver them directly to the client.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/data-management\">Data Management</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/strangler\">Strangler</a></td>\n<td>Incrementally migrate a legacy system by gradually replacing specific pieces of functionality with new applications and services.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation\">Design and Implementation</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/management-monitoring\">Management and Monitoring</a></p></td>\n</tr>\n<tr class=\"even\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/throttling\">Throttling</a></td>\n<td>Control the consumption of resources used by an instance of an application, an individual tenant, or an entire service.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/availability\">Availability</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability\">Performance and Scalability</a></p></td>\n</tr>\n<tr class=\"odd\">\n<td><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/valet-key\">Valet Key</a></td>\n<td>Use a token or key that provides clients with restricted direct access to a specific resource or service.</td>\n<td><p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/data-management\">Data Management</a>,</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/category/security\">Security</a></p></td>\n</tr>\n</tbody>\n</table>\n\n<https://docs.microsoft.com/en-us/azure/architecture/patterns>\n\n## Enterprise Application Architecture\n-   **Domain Logic Patterns:** Transaction Script, Domain Model, Table Module, Service Layer.\n-   **Data Source Architectural Patterns:** Table Data Gateway, Row Data Gateway, Active Record, Data Mapper.\n-   **Object-Relational Behavioral Patterns:** Unit of Work, Identity Map, Lazy Load\n-   **Object-Relational Structural Patterns:** Identity Field, Foreign Key Mapping, Association Table Mapping, Dependent Mapping, Embedded Value, Serialized LOB, Single Table Inheritance, Class Table Inheritance, Concrete Table Inheritance, Inheritance Mappers.\n-   **Object-Relational Metadata Mapping Patterns:** Metadata Mapping, Query Object, Repository.\n-   **Web Presentation Patterns:** Model View Controller, Page Controller, Front Controller, Template View, Transform View, Two-Step View, Application Controller.\n-   **Distribution Patterns:** Remote Facade, Data Transfer Object\n-   **Offline Concurrency Patterns:** Optimistic Offline Lock, Pessimistic Offline Lock, Coarse Grained Lock, Implicit Lock.\n-   **Session State Patterns:** Client Session State, Server Session State, Database Session State.\n-   **Base Patterns:** Gateway, Mapper, Layer Supertype, Separated Interface, Registry, Value Object, Money, Special Case, Plugin, Service Stub, Record Set\n<https://martinfowler.com/eaaCatalog/index.html>\n\n"},{"fields":{"slug":"/Computer-Science/System-Design/Microservice-Architecture/Domain-Driven-Design/","title":"Domain Driven Design"},"frontmatter":{"draft":false},"rawBody":"# Domain Driven Design\n\nCreated: 2020-11-01 12:13:11 +0500\n\nModified: 2021-11-29 10:28:05 +0500\n\n---\n\nDomain-driven design(DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the[business domain](https://www.wikiwand.com/en/Business_domain). For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\nDDD connects the[implementation](https://www.wikiwand.com/en/Implementation)to an evolving model.\nDomain-driven design is predicated on the following goals\n-   placing the project's primary focus on the core[domain](https://www.wikiwand.com/en/Domain_(software_engineering))and domain logic;\n-   basing complex designs on a model of the domain;\n-   initiating a creative collaboration between technical and[domain experts](https://www.wikiwand.com/en/Domain_expert)to iteratively refine a conceptual model that addresses particular domain problems.\nTwo synergic properties in software design\n\ni.  High cohesion\n\nii. Low coupling\nDDD is an approach to software development that tackles complex systems by mapping activities, tasks, events, and data from a business domain to software artifacts. One of the most important concepts of DDD is the *bounded context*, which is a cohesive and well-defined unit within the business model in which you define the boundaries of your software artifacts\n<https://www.wikiwand.com/en/Domain-driven_design>\n\n**Financial System**\n\n![](media/Microservice-Architecture_Domain-Driven-Design-image1.png)\n\n**Design Pattern: Bounded Context -> Domain-Driven Design**\n\nOur first challenge is to logically segment the business into micro-subdomains, so that each can be supported by a smallempowered autonomous team. Each subdomain's scope should be bound by its team's capacity to manage the lifecycle of its supporting microservice(s) --- from inception to post-production. This shift from working on transient-projects to autonomous domain-ownership incentivizes accountability for all aspects of microservice design and empowers agile decision-making --- which results inimproved time-to-market.\nThink of the prefix \"micro\" alluding to the size of the team needed to support the entire lifecycle of the microservice(s) within its bounded business subdomain.\nWithin the context of our mockup architecture, let's begin the organizational design process by starting with the payment-processing domain --- which includes fraud detection, payments, settlement, and more. Since this scope is likely too complicated for a small team to manage, let's choose to narrow their ownership boundary down to just the fraud-detection subdomain.\n\n![](media/Microservice-Architecture_Domain-Driven-Design-image2.png)\n\nThe diagram above shows that fraud-detection is composed of the workflow's first three microservices --- which include digital identities, statistical analysis, and AI-based transaction risk-scoring. Since their scope is likely still too broad for a small team to manage, let's split fraud detection further down into two subdomains --- which finally seems more manageable.\nAt a very high level, the process we just followed is called[Domain-Driven Design (DDD)](https://dddcommunity.org/learning-ddd/what_is_ddd/), which is supported by the recommended pattern to bind each microservice's scope and ownership claim to a business subdomain called[bounded context](https://martinfowler.com/bliki/BoundedContext.html).\nNotice that each microservice has its own dedicated database for**isolation**. The**empowered autonomous team**that owns the blue bounded context chose[RediSearch](https://redislabs.com/modules/redis-search/)to support their \"Authenticate Digital Identity\" microservice, and[RedisBloom](https://redislabs.com/modules/redis-bloom/)to support their \"Probabilistic Fraud Detection Checkpoint\" microservice. Meanwhile, a separate team that owns the purple bounded context chose[RedisAI](https://redislabs.com/modules/redis-ai/)to support \"Transaction Risk Scoring\" in real-time.\n![](media/Microservice-Architecture_Domain-Driven-Design-image3.png)\n\n<https://www.domainlanguage.com/ddd/reference/attachment/pattern-language-overview-med>\n\n<https://www.youtube.com/watch?v=hv-LiKQgN90>\n\n[Domain Driven Design Patterns in Python](https://youtu.be/v0wBmQq8qcA)\n"},{"fields":{"slug":"/Computer-Science/System-Design/Microservice-Architecture/Example/","title":"Example"},"frontmatter":{"draft":false},"rawBody":"# Example\n\nCreated: 2020-11-01 12:20:51 +0500\n\nModified: 2020-11-01 15:51:15 +0500\n\n---\n\n<https://thenewstack.io/how-redis-simplifies-microservices-design-patterns>\n\n## Financial System**\n\n![API Gateway 1 Authenticate Digital Identity RedisGearE Probabilistic Fraud Detection Checkpoint RedisGE3Ã† Transaction Risk Scoring RediEGeÃ¤Ã† Approve I Decline Payment RESiscx Legend Redis EnWprise Payment History Clearing & Settlement Update Risk Profile Message Broker n n n n GdiE i:re-m Synchronous Communication --- --- Asynchronous Communication Analytics Dashboard ](media/Microservice-Architecture_Example-image1.png)\n**Design Pattern: Bounded Context -> Domain-Driven Design**\n\nOur first challenge is to logically segment the business into micro-subdomains, so that each can be supported by a smallempowered autonomous team. Each subdomain's scope should be bound by its team's capacity to manage the lifecycle of its supporting microservice(s) --- from inception to post-production. This shift from working on transient-projects to autonomous domain-ownership incentivizes accountability for all aspects of microservice design and empowers agile decision-making --- which results inimproved time-to-market.\nThink of the prefix \"micro\" alluding to the size of the team needed to support the entire lifecycle of the microservice(s) within its bounded business subdomain.\nWithin the context of our mockup architecture, let's begin the organizational design process by starting with the payment-processing domain --- which includes fraud detection, payments, settlement, and more. Since this scope is likely too complicated for a small team to manage, let's choose to narrow their ownership boundary down to just the fraud-detection subdomain.\n\n![Authenticate Digital Identity Probabilistic Fraud Detection Checkpoint ESloom ded Transaction Risk Scoring RediEAl Approve I Decline Payment ](media/Microservice-Architecture_Example-image2.png)\n\nThe diagram above shows that fraud-detection is composed of the workflow's first three microservices --- which include digital identities, statistical analysis, and AI-based transaction risk-scoring. Since their scope is likely still too broad for a small team to manage, let's split fraud detection further down into two subdomains --- which finally seems more manageable.\nAt a very high level, the process we just followed is called[Domain-Driven Design (DDD)](https://dddcommunity.org/learning-ddd/what_is_ddd/), which is supported by the recommended pattern to bind each microservice's scope and ownership claim to a business subdomain called[bounded context](https://martinfowler.com/bliki/BoundedContext.html).\nNotice that each microservice has its own dedicated database for**isolation**. The**empowered autonomous team**that owns the blue bounded context chose[RediSearch](https://redislabs.com/modules/redis-search/)to support their \"Authenticate Digital Identity\" microservice, and[RedisBloom](https://redislabs.com/modules/redis-bloom/)to support their \"Probabilistic Fraud Detection Checkpoint\" microservice. Meanwhile, a separate team that owns the purple bounded context chose[RedisAI](https://redislabs.com/modules/redis-ai/)to support \"Transaction Risk Scoring\" in real-time.\n**Design Pattern: Asynchronous Messaging -> Interservice Communication**\n\nNow that we've identified a bounded context and optimal data model for each microservice, our next challenge is to enable communication between them without breaking compliance to**isolation**. This can be solved byembracing**eventual consistency**, which presumes the microservice on the receiving end of interservice communication will not be available during outbound transmission, however, can consume the message as soon as availability is restored.\n\n![Authenticate Digital Identity API Gateway Probabilistic Fraud Detection Checkpoint Message Broker ](media/Microservice-Architecture_Example-image3.png)\n\nThe recommended pattern for interservice communication is[asynchronous messaging](https://microservices.io/patterns/communication-style/messaging.html)using a publish-subscribe message broker as its event distribution hub. In this pattern, a producer can publish an event without requisite awareness of whether or not any consumer is listening, and --- in the same way --- consumers of that event can react to it at their convenience or ignore it altogether. This is typically the foundation of an event-driven architecture.\nSince we have already chosen Redis as the primary database for multiple microservices, we can simplify our architecture by also using it to implement this pattern with[Redis Streams](https://university.redislabs.com/courses/ru202/). Redis Streams is an immutable time-ordered log data structure that allows a producer to publish asynchronous messages to multiple subscribed consumers. This ensures the microservice that is publishing events will remain decoupled from the microservice(s) consuming them --- so there are no cross-dependencies on availability and release cycles. In addition, Redis Streams can be configured to handle different delivery guarantees, support consumer groups, and other nuances that are similar in nature to[Kafka](https://kafka.apache.org/)--- also a staple across microservice architectures.\n**Design Pattern: Choreography-Based Saga -> Distributed Transactions**\n\nNow that we've enabled interservice communication, our next challenge is to handle transactions that span across multiple bounded contexts without breaking compliance toisolation. In the past, this was trivial to implement, since all operations within the transactional scope were executed against a single RDBMS that provided row-locking, deadlock-detection, and roll-back features. Once data became distributed across multiple databases, the Two-Phase Commit protocol (2PC) became a standard for distributed transactions. However, while both approaches worked, they were not designed witheventual consistencyin mind.\nIf we presume a dependency will be unavailable during a distributed transaction, then we should also presume frequent rollbacks will cause sporadic unavailability across the system --- which is neither cloud native nor improves time-to-market.\nThis can be solved by relaxing strict requirements for ACID guarantees, which have propped up relational databases across most traditional architectures for decades. While relational databases have their place within microservice architectures, their relevance becomes much more situational. For example, if referential integrity is not a requirement then why wouldn't anempowered autonomous teamchoose to optimize their microservice with a NoSQL database that is purpose-built to handle their specific data access patterns and SLAs.\n\n![Probabilistic Fraud Detection Checkpoint Transaction Risk Scoring Message Broker Approve I Decline Payment ](media/Microservice-Architecture_Example-image4.png)\n\nRecall that our payment-processing workflow is composed of multiple microservices that are organized into separate bounded contexts and supported by Redis --- a NoSQL database. Within this context, the recommended pattern to handle distributed transactions is a[choreography-based saga](https://microservices.io/patterns/data/saga.html), which performs a sequence of isolated local transactions with published events facilitating the transition between workflow stages.\nEach microservice participating in the saga will listen only for its own workflow-related event, which will notify it to perform a local database transaction and subsequently publish its own event to the message broker. This event-driven choreography can include compensating microservices for rollback purposes and decision services for complex business processes.\nIt's worth noting that in a choreography-based saga there is no central orchestrator, which avoids coupling the release cycles of participating microservices. However, it is not always the right solution. There can be cases where strong consistency is an absolute requirement --- such as account transfers. Within that context, either an orchestration-based saga might be better suited, or relying on a 2PC between microservices within the same bounded context.\n**Design Pattern: Transactional Outbox and Message Relay -> Consistency**\n\nNow that we've choreographed transactions that span multiple bounded contexts, our next challenge is to mitigate the risks of inconsistency between a microservice's database and the message broker --- even if Redis is used for both. Recall that in the previous two design patterns, each microservice committed locally to its database and subsequently published an event. If this is implemented using some variation of the[dual writes](https://thorben-janssen.com/dual-writes/)pattern, communication could become lost and distributed transactions could become orphaned --- especially in a cloud environment.\nCode-complexity can be added to each microservice to handle various failure and inconsistency scenarios, however consider this effort multiplied across 100s of teams and the risks of incorrect implementations --- all adding no business value.\n\n![Authenticate Digital Identity RediSEirch geol on RedisGÃ¤iÃ† Message Broker ](media/Microservice-Architecture_Example-image5.png)\n\nTo avoid the risks and costs of various application-level implementations, the recommended patterns are[transactional outbox and message replay](https://microservices.io/patterns/data/transactional-outbox.html). Redis simplifies and supports the combined implementation of both patterns, known as[write-behind](https://github.com/RedisGears/rgsync), by using[Redis Streams](https://university.redislabs.com/courses/ru202/)as the transactional outbox and[RedisGears](https://redislabs.com/modules/redis-gears/)as the message relay. Within Redis a secondary thread can listen for changed-data events, durably store them in time order, and publish them to the message broker --- whenever it's available. This can be uniformly enabled or upgraded on each Redis database at the same time withinfrastructure automation.\n<https://microservices.io/patterns/data/transactional-outbox.html>\n\n## Design Pattern: Command Query Responsibility Segregation (CQRS) -> Performance**\n\nNotice that when we defined our fraud-related bounded contexts, we left out the final stage of the payment-processing workflow. This was because itsempowered autonomous teamchose a non-Redis database to support its microservice.\n\n![Authenticate Digital Identity Probabilistic Fraud Detection Checkpoint ESloom ded Transaction Risk Scoring RediEAl Approve I Decline Payment ](media/Microservice-Architecture_Example-image2.png)\nSo, let's now assume that the \"Approve | Decline Payment\" microservice is supported by a disk-based database which is not optimized for query-performance. Since it presumably has strong durability guarantees, it's a logical choice for record-keeping --- however, what if its bounded context also includes a microservice that requires this data for query. This means our next challenge is to optimize query-performance when Redis is not the system-of-record.\n\n![API Gateway Approve I DecÃ¼ne payment Payment History ](media/Microservice-Architecture_Example-image6.png)\n\nThe recommended pattern is[CQRS](https://martinfowler.com/bliki/CQRS.html), which segregates the responsibility for a dataset's writes --- Command --- and reads --- Query. Implementing CQRS by using separate databases optimizes the data structure, or data model, to the data access pattern on both sides of the segregation and their individual SLAs. Since our goal is to optimize performance, the direction of data replication will typically flow into Redis from a disk-based database --- i.e. MongoDB, Cassandra, RDBMS, etc. Easy, right?\nHere's the catch --- to implement this pattern we will need to solve for near-real-time continuous data replication, maintaineventual consistencybetween heterogeneous databases, and transform the data to avoid an impedance mismatch between Command and Query data-structures. This should sound familiar, since we did this when Redis was the source database --- recall the transactional outbox and message relay patterns. However, since in this case Redis is the target and most other databases don't support[write-behind](https://github.com/RedisGears/rgsync), we'll need an external implementation to replicate changed-data events.\nWithin this context, we can simplify the implementation of CQRS by using a Change Data Capture (CDC) framework that can integrate with both Command and Query databases. CDC frameworks typically use[transaction-log tailing](https://microservices.io/patterns/data/transaction-log-tailing.html)or[polling-publisher](https://microservices.io/patterns/data/polling-publisher.html)patterns to scan for changed-data events on the Command database and replicate them as a transformed payload to the Query database. Notice how this is different than using Redis in a[cache-aside](https://redislabs.com/solutions/use-cases/caching/)pattern, since it does not couple the databases at the microservice level --- maintainingisolation.\n**Design Pattern: Shared Data -> Reusability**\n\nNow that we've addressed optimizing performance when Redis is not the system-of-record, our next challenge is to handle shared data between microservices that are separated by different bounded contexts or a database outside of the microservice architecture.\nA real-world example of the latter, is a[strangler application](https://martinfowler.com/bliki/StranglerFigApplication.html)migration from a monolithic to microservice architecture. In this case, a microservice's database could be reliant on an external system-of-record for multiple years, or even indefinitely, as part of a Hybrid Cloud deployment. We actually solved this when we introduced the CQRS pattern, however let's extend the problem statement to include microservices that share the same data and data-access pattern.\nIn this case, here are some applicable patterns where Redis simplifies the implementation:\n\n[Read Replicas](https://docs.redislabs.com/5.2/rs/administering/intercluster-replication/replica-of/)--- replicate source data to multiple destinations\n-   [Shared Database](https://microservices.io/patterns/data/shared-database.html)--- allow cross-dependencies between separate bounded contexts\n-   [Domain-Driven Design](https://dddcommunity.org/learning-ddd/what_is_ddd/)--- include all microservices sharing data within a single bounded context\n\n![Wawa.uas BuveaD watuAed Aeraa1e9 IdV lu a tuAed aupaa I ano.ddv ](media/Microservice-Architecture_Example-image7.png)\n\nHere's the catch --- while these patterns address shared data between a few bounded contexts, none of them would scale at a global level.\n\n![user:profile behavioral :profile transaction:profile geo location trusted : devices Authenticate Digital Identity RediSearch API Gateway Probabilistic Fraud Detection Checkpoi Redi53100m user: sew on oauth:token Redis Enterprise Message Broker ](media/Microservice-Architecture_Example-image8.png)\n\nFor global data, the recommended pattern is anisolateddatabase for the API Gateway. Since this database could potentially be accessed by every transaction that flows through the architecture, we must consider business continuity, scalability, and performance as critical success criteria for its selection."},{"fields":{"slug":"/Computer-Science/Networking/Others/Falcor/","title":"Falcor"},"frontmatter":{"draft":false},"rawBody":"# Falcor\n\nCreated: 2018-12-04 00:44:49 +0500\n\nModified: 2018-12-04 00:48:23 +0500\n\n---\n\nA JavaScript library for efficient data fetching\n1.  One Model Everywhere\n\nFalcor lets you represent all your remote data sources as a single domain model via a virtual JSON graph. You code the same way no matter where the data is, whether in memory on the client or over the network on the server.\n\n2.  The Data is the API\n\nA JavaScript-like path syntax makes it easy to access as much or as little data as you want, when you want it. You retrieve your data using familiar JavaScript operations like get, set, and call. If you know your data, you know your API.\n\n3.  Bind to the Cloud\n\nFalcor automatically traverses references in your graph and makes requests as needed. Falcor transparently handles all network communications, opportunistically batching and de-duping requests.\n**References**\n\n<https://netflix.github.io/falcor>\n\n<https://netflix.github.io/falcor/starter/what-is-falcor.html>\n\n<https://github.com/netflix/falcor-express-demo>\n\n<https://blog.apollographql.com/graphql-vs-falcor-4f1e9cbf7504>\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/Data-formats/","title":"Data formats"},"frontmatter":{"draft":false},"rawBody":"# Data formats\n\nCreated: 2018-09-30 19:28:01 +0500\n\nModified: 2021-05-29 00:06:03 +0500\n\n---\n\n**Hierarchical Data Format**\n\n**Hierarchical Data Format**(**HDF**) is a set of file formats (**HDF4**,**HDF5**) designed to store and organize large amounts of data.\nHDF5 simplifies the file structure to include only two major types of object:\n-   Datasets, which are multidimensional arrays of a homogeneous type\n-   Groups, which are container structures which can hold datasets and other groups\nIn addition to these advances in the file format, HDF5 includes an improved type system, and dataspace objects which represent selections over dataset regions. The API is also object-oriented with respect to datasets, groups, attributes, types, dataspaces and property lists.\nBecause it uses[B-trees](https://en.wikipedia.org/wiki/B-trees)to index table objects, HDF5 works well for[time series](https://en.wikipedia.org/wiki/Time_series)data such as stock price series, network monitoring data, and 3D meteorological data. The bulk of the data goes into straightforward arrays (the table objects) that can be accessed much more quickly than the rows of an[SQL](https://en.wikipedia.org/wiki/SQL)database, but B-tree access is available for non-array data. The HDF5 data storage mechanism can be simpler and faster than an SQL[star schema](https://en.wikipedia.org/wiki/Star_schema).\n**References**\n\n<https://en.wikipedia.org/wiki/Hierarchical_Data_Format>\n\n## Cap'n Proto**\n\nCap'n Proto is an insanely fast data interchange format and capability-based RPC system. Think JSON, except binary. Or think[Protocol Buffers](http://protobuf.googlecode.com/), except faster.\nCap'n Proto gets a perfect score because*there is no encoding/decoding step*. The Cap'n Proto encoding is appropriate both as a data interchange format and an in-memory representation, so once your structure is built, you can simply write the bytes straight out to disk!\n![Encoding round-trip time ](media/Data-formats-image1.png)<https://capnproto.org>\n\n<https://github.com/capnproto/capnproto>\n\n## Apache Thrift**\n\n**Thrift**is an[interface definition language](https://en.wikipedia.org/wiki/Interface_definition_language)and[binary communication protocol](https://en.wikipedia.org/wiki/Binary_protocol)used for defining and creating[services](https://en.wikipedia.org/wiki/Service_(systems_architecture))for numerous languages. It forms a[remote procedure call](https://en.wikipedia.org/wiki/Remote_procedure_call)(RPC) framework and was developed at[Facebook](https://en.wikipedia.org/wiki/Facebook)for \"scalable cross-language services development\". It combines a software stack with a code generation engine to build[cross-platform](https://en.wikipedia.org/wiki/Cross-platform)services which can connect applications written in a variety of languages and frameworks, including [ActionScript](https://en.wikipedia.org/wiki/ActionScript), [C](https://en.wikipedia.org/wiki/C_(programming_language)), [C++](https://en.wikipedia.org/wiki/C%2B%2B), [C#](https://en.wikipedia.org/wiki/C_Sharp_(programming_language)), [Cappuccino](https://en.wikipedia.org/wiki/Cappuccino_(application_development_framework)), [Cocoa](https://en.wikipedia.org/wiki/Cocoa_(API)), [Delphi](https://en.wikipedia.org/wiki/Embarcadero_Delphi), [Erlang](https://en.wikipedia.org/wiki/Erlang_(programming_language)), [Go](https://en.wikipedia.org/wiki/Go_(programming_language)), [Haskell](https://en.wikipedia.org/wiki/Haskell_(programming_language)), [Java](https://en.wikipedia.org/wiki/Java_(programming_language)), [Node.js](https://en.wikipedia.org/wiki/Node.js), [Objective-C](https://en.wikipedia.org/wiki/Objective-C), [OCaml](https://en.wikipedia.org/wiki/OCaml), [Perl](https://en.wikipedia.org/wiki/Perl), [PHP](https://en.wikipedia.org/wiki/PHP), [Python](https://en.wikipedia.org/wiki/Python_(programming_language)), [Ruby](https://en.wikipedia.org/wiki/Ruby_(programming_language)) and [Smalltalk](https://en.wikipedia.org/wiki/Smalltalk).\n<https://en.wikipedia.org/wiki/Apache_Thrift>\n\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/Comparisions/","title":"Comparisions"},"frontmatter":{"draft":false},"rawBody":"# Comparisions\n\nCreated: 2020-05-03 00:58:23 +0500\n\nModified: 2020-05-06 10:17:55 +0500\n\n---\n\n**Avro vs Protobuf**\n\nWhat should you choose then? Avro, especially at the beginning, seems much easier to use. The cost of this is that you will need to provide both reader and writer schema to deserialize anything.\n![](media/Comparisions-image1.png)\n\nSometimes this might be quite problematic. That's why tools like[Schema Registry](https://www.confluent.io/confluent-schema-registry/)were developed.\nThe next problem you might face with Avro is the overall impact on your domain events. At some point, Avro can leak into your domain. Some constructions like e.g. a map with keys that are not strings are not supported in Avro model. When the serialization mechanism is forcing you to change something in your domain model --- it's not a good sign.\nWith Protocol Buffers, schema management is much simpler --- you just need schema artifact, which can be published as any other artifact to your local repository. Also, your domain can be perfectly separated from the serialization mechanism. The cost is the boilerplate code required for translation between domain and serialization layers.\nPersonally, I would use Avro for simple domains with mostly primitive types. For rich domains, with complex types and structures, I've been using Protocol Buffers for quite some time. Clean domain with no serialization influence is really worth paying the boilerplate code price.\n<https://blog.softwaremill.com/the-best-serialization-strategy-for-event-sourcing-9321c299632b>\n[(Big) Data Serialization with Avro and Protobuf](https://www.slideshare.net/gschmutz/big-data-serialization-with-avro-and-protobuf)\n![](media/Comparisions-image2.png)\n![](media/Comparisions-image3.png)\n\n![](media/Comparisions-image4.png)\n\n![](media/Comparisions-image5.png)\n\n![](media/Comparisions-image6.png)\n\n![](media/Comparisions-image7.png)\n\n![](media/Comparisions-image8.png)\n\n![](media/Comparisions-image9.png)\n\n![](media/Comparisions-image10.png)\n\n![](media/Comparisions-image11.png)\n\n![](media/Comparisions-image12.png)\n\n![](media/Comparisions-image13.png)\n\n![](media/Comparisions-image14.png)\n\n![](media/Comparisions-image15.png)\n\n![](media/Comparisions-image16.png)\n\n![](media/Comparisions-image17.png)\n\n![](media/Comparisions-image18.png)\n\n![](media/Comparisions-image19.png)\n\n![](media/Comparisions-image20.png)\n\n![](media/Comparisions-image21.png)\n\n![](media/Comparisions-image22.png)\n\n![](media/Comparisions-image23.png)\n\n![](media/Comparisions-image24.png)\n\n![](media/Comparisions-image25.png)\n\n![](media/Comparisions-image26.png)\n**Big Data and Fast Data**\n\n![](media/Comparisions-image27.png)\n\n![](media/Comparisions-image28.png)\n\n![](media/Comparisions-image29.png)\n\n![](media/Comparisions-image30.png)\n\n![](media/Comparisions-image31.png)\n![](media/Comparisions-image32.png)\n**Summary**\n\n![](media/Comparisions-image33.png)\n\n![](media/Comparisions-image34.png)\n**Performance benchmarks**\n\n<https://labs.criteo.com/2017/05/serialization>\n\n<https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8>\n\n<https://www.datanami.com/2018/05/16/big-data-file-formats-demystified>\n\n## Parquet vs Avro**\n\nOn their face, Avro and Parquet are similar they both write the schema of their enclosed data in a file header and deal well with schema drift (adding/removing columns). They're so similar in this respect that Parquet even natively supports[Avro schemas](https://github.com/apache/parquet-mr#avro), so you can migrate your Avro pipelines to Parquet storage in a pinch.\nThe big difference in the two formats is that Avro stores data BY ROW, and parquet stores data BY COLUMN..\n**BENEFITS OF PARQUET OVER AVRO**\n\nTo recap on my columnar file format guide, the advantage to Parquet (and columnar file formats in general) are primarily two fold:\n\ni.  Reduced Storage Costs (typically) vs Avro\n\nii. 10-100x improvement in reading data when you only need a few columns\nI cannot overstate the benefit of a 100x improvement in record throughput. It provides a truly massive and fundamental improvement to data processing pipelines that it is very hard to overlook.\n\nWhen simply counting rows, Parquet blows Avro away, thanks to the metadata parquet stores in the header of row groups.\n\n![Parquet Row count](media/Comparisions-image35.png)\n\nWhen running a group-by query, parquet is still almost 2x faster (although I'm unsure of the exact query used here).\n\n![Parquet Group by](media/Comparisions-image36.png)\n\nThe same case study also finds improvements in storage space, and even in full-table scans, likely due to Spark having to scan a smaller datasize.\n**BENEFITS OF AVRO OVER PARQUET**\n\nI have heard some folks argue in favor of Avro vs Parquet. Such arguments are typically based around two points:\n\ni.  When you are reading entire records at once, Avro wins in performance.\n\nii. Write-time is increased drastically for writing Parquet files vs Avro files\nSo the wider your dataset, the worse Parquet becomes for scanning entire records (which makes sense). This is an extreme example, most datasets are not 700 columns wide, for anything reasonable (< 100) Parquet read performance is close enough to Avro to not matter.\n**Others**\n\n<https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html>\n"},{"fields":{"slug":"/Databases/Concepts/Indexing/Database-Index/","title":"Database Index"},"frontmatter":{"draft":false},"rawBody":"# Database Index\n\nCreated: 2019-04-20 13:01:45 +0500\n\nModified: 2022-05-11 23:39:26 +0500\n\n---\n\nAdatabase indexis a[data structure](https://en.wikipedia.org/wiki/Data_structure)that improves the speed of data retrieval operations on a[database table](https://en.wikipedia.org/wiki/Table_(database))at the cost of additional writes and storage space to maintain the index data structure. Indexes are used to quickly locate data without having to search every row in a database table every time a database table is accessed. Indexes can be created using one or more[columns of a database table](https://en.wikipedia.org/wiki/Column_(database)), providing the basis for both rapid random [lookups](https://en.wikipedia.org/wiki/Lookup) and efficient access of ordered records.\nAn index is a copy of selected columns of data from a table that can be searched very efficiently that also includes a low-level disk block address or direct link to the complete row of data it was copied from. Some databases extend the power of indexing by letting developers create indexes on functions or[expressions](https://en.wikipedia.org/wiki/Expression_(programming)). For example, an index could be created on upper(last_name), which would only store the upper-case versions of the last_name field in the index. Another option sometimes supported is the use of[partial indices](https://en.wikipedia.org/wiki/Partial_index), where index entries are created only for those records that satisfy some conditional expression. A further aspect of flexibility is to permit indexing on[user-defined functions](https://en.wikipedia.org/wiki/User-defined_function), as well as expressions formed from an assortment of built-in functions.\n**Types of indexes**\n\n**Bitmap index**\n\nA bitmap index is a special kind of indexing that stores the bulk of its data as[bit arrays](https://en.wikipedia.org/wiki/Bit_array) (bitmaps) and answers most queries by performing[bitwise logical operations](https://en.wikipedia.org/wiki/Bitwise_operation)on these bitmaps. The most commonly used indexes, such as[B+ trees](https://en.wikipedia.org/wiki/B%2B_tree), are most efficient if the values they index do not repeat or repeat a small number of times. In contrast, the bitmap index is designed for cases where the values of a variable repeat very frequently. For example, the sex field in a customer database usually contains at most three distinct values: male, female or unknown (not recorded). For such variables, the bitmap index can have a significant performance advantage over the commonly used trees.\n**Dense index**\n\nA dense index in[databases](https://en.wikipedia.org/wiki/Database)is a[file](https://en.wikipedia.org/wiki/Computer_file)with pairs of keys and[pointers](https://en.wikipedia.org/wiki/Pointer_(computer_programming))for every[record](https://en.wikipedia.org/wiki/Record_(computer_science))in the data file. Every key in this file is associated with a particular pointer toa recordin the sorted data file. In clustered indices with duplicate keys, the dense index pointsto the first recordwith that key.\n**Sparse index**\n\nA sparse index in databases is a file with pairs of keys and pointers for every[block](https://en.wikipedia.org/wiki/Block_(data_storage))in the data file. Every key in this file is associated with a particular pointerto the blockin the sorted data file. In clustered indices with duplicate keys, the sparse index pointsto the lowest search keyin each block.\n**Reverse index**\n\nA reverse-key index reverses the key value before entering it in the index. E.g., the value 24538 becomes 83542 in the index. Reversing the key value is particularly useful for indexing data such as sequence numbers, where new key values monotonically increase.\nIt is an index of keywords that stores records of documents that contain keywords in the list.\n\nUse Case - Create a complete Tweet index.\n<https://en.wikipedia.org/wiki/Database_index>\n\n[Why this query is fast](https://youtu.be/HinCxBt6mNY)\n**Tips**\n-   B Tree / B+ Tree index is not good for columns with low cardinality (since it's create a full tree for the columns)\n-   BitMap index is good for low cardinality columns\n**Index Data Structures**\n\nSearch engine architectures vary in the way indexing is performed and in methods of index storage to meet the various design factors.\n**[Suffix tree](https://en.wikipedia.org/wiki/Suffix_tree)**\n\nFiguratively structured like a tree, supports linear time lookup. Built by storing the suffixes of words. The suffix tree is a type of[trie](https://en.wikipedia.org/wiki/Trie). Tries support[extendible hashing](https://en.wikipedia.org/wiki/Extendible_hashing), which is important for search engine indexing.Used for searching for patterns in[DNA](https://en.wikipedia.org/wiki/DNA)sequences and clustering. A major drawback is that storing a word in the tree may require space beyond that required to store the word itself.An alternate representation is a[suffix array](https://en.wikipedia.org/wiki/Suffix_array), which is considered to require less virtual memory and supports data compression such as the[BWT](https://en.wikipedia.org/wiki/Burrows-Wheeler_transform)algorithm.\n**[Inverted index](https://en.wikipedia.org/wiki/Inverted_index)**\n\nStores a list of occurrences of each atomic search criterion,typically in the form of a[hash table](https://en.wikipedia.org/wiki/Hash_table)or[binary tree](https://en.wikipedia.org/wiki/Binary_tree).\n[**Citation index**](https://en.wikipedia.org/wiki/Citation_index)\n\nStores citations or hyperlinks between documents to support citation analysis, a subject of[bibliometrics](https://en.wikipedia.org/wiki/Bibliometrics).\n**[Ngram index](https://en.wikipedia.org/wiki/N-gram)**\n\nStores sequences of length of data to support other types of retrieval or[text mining](https://en.wikipedia.org/wiki/Text_mining).\n**[Document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix)**\n\nUsed in latent semantic analysis, stores the occurrences of words in documents in a two-dimensional[sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix).\n**Star Tree Index**\n\nAllows us to maintain pre-aggregated values for some combination of dimensions example country and browser in below example. Incredible fast lookup.\n![Star-Tree Index Country Browser Root US CA Star Star Star ](media/Indexing_Database-Index-image1.jpeg)\n\n<https://engineering.linkedin.com/blog/2019/06/star-tree-index--powering-fast-aggregations-on-pinot>\n\n<https://docs.pinot.apache.org/basics/indexing/star-tree-index>\n<https://en.wikipedia.org/wiki/Search_engine_indexing>\n"},{"fields":{"slug":"/Databases/Concepts/Indexing/Inverted-Index/","title":"Inverted Index"},"frontmatter":{"draft":false},"rawBody":"# Inverted Index\n\nCreated: 2018-04-18 01:38:31 +0500\n\nModified: 2021-05-16 11:48:52 +0500\n\n---\n\nIn[computer science](https://en.wikipedia.org/wiki/Computer_science), aninverted index(also referred to aspostings fileorinverted file) is an[index data structure](https://en.wikipedia.org/wiki/Index_(database)) storing a mapping from content, such as words or numbers, to its locations in a[database file](https://en.wikipedia.org/wiki/Table_(database)), or in a document or a set of documents (named in contrast to a[forward index](https://en.wikipedia.org/wiki/Forward_index), which maps from documents to content). The purpose of an inverted index is to allow fast[full text searches](https://en.wikipedia.org/wiki/Full_text_search), at a cost of increased processing when a document is added to the database. The inverted file may be the database file itself, rather than its[index](https://en.wikipedia.org/wiki/Index_(database)). It is the most popular data structure used in[document retrieval](https://en.wikipedia.org/wiki/Document_retrieval)systems, used on a large scale for example in[search engines](https://en.wikipedia.org/wiki/Search_engine).\nThere are two main variants of inverted indexes: A**record-level inverted index**(orinverted file indexor justinverted file) contains a list of references to documents for each word. A**word-level inverted index**(orfull inverted indexorinverted list) additionally contains the positions of each word within a document. The latter form offers more functionality (like[phrase searches](https://en.wikipedia.org/wiki/Phrase_search)), but needs more processing power and space to be created.\n**Applications**\n\nThe inverted index[data structure](https://en.wikipedia.org/wiki/Data_structure)is a central component of a typical[search engine indexing algorithm](https://en.wikipedia.org/wiki/Index_(search_engine)). A goal of a search engine implementation is to optimize the speed of the query: find the documents where word X occurs. Once a[forward index](https://en.wikipedia.org/wiki/Search_engine_indexing#The_forward_index)is developed, which stores lists of words per document, it is next inverted to develop an inverted index. Querying the forward index would require sequential iteration through each document and to each word to verify a matching document. The time, memory, and processing resources to perform such a query are not always technically realistic. Instead of listing the words per document in the forward index, the inverted index data structure is developed which lists the documents per word.\n<https://en.wikipedia.org/wiki/Inverted_index>\n\n## Inverted Index in Elasticsearch**\n\nElasticsearch uses a structure calledaninverted index, which is designed to allow very fast full-text searches. An inverted index consists of a list of all the unique words that appear in any document, and for each word, a list of the documents in which it appears.\nFor example, let's say we have two documents, each with acontentfield containing the following:\n\na.  The quick brown fox jumped over the lazy dog\n\nb.  Quick brown foxes leap over lazy dogs in summer\nTo create an inverted index, we first split thecontentfield of each document into separatewords (which we callterms, ortokens), create a sorted list of all the unique terms, and then list in which document each term appears. The result looks something like this:\n\nTerm Doc_1 Doc_2\n-------------------------\nQuick | | X\nThe | X |\nbrown | X | X\ndog | X |\ndogs | | X\nfox | X |\nfoxes | | X\nin | | X\njumped | X |\nlazy | X | X\nleap | | X\nover | X | X\nquick | X |\nsummer | | X\nthe | X |\n------------------------\nNow, if we want to search forquick brown, we just need to find the documents in which each term appears:\n\nTerm Doc_1 Doc_2\n-------------------------\nbrown | X | X\nquick | X |\n------------------------\nTotal | 2 | 1\nBoth documents match, but the first document has more matches than the second. If we apply a naive **similarity algorithm** thatjust counts the number of matching terms, then we can say that the first document is a better match---ismore relevantto our query---than the second document.\nBut there are a few problems with our current inverted index:\n-   Quickandquickappear as separate terms, while the user probably thinks of them as the same word.\n-   foxandfoxesare pretty similar, as aredoganddogs; They share the same root word.\n-   jumpedandleap, while not from the same root word, are similar in meaning. They are synonyms.\nWith the preceding index, a search for+Quick +foxwouldn't match any documents. (Remember, a preceding+means that the word must be present.) Both the termQuickand the termfoxhave to be in the same document in order to satisfy the query, but the first doc containsquick foxand the second doc containsQuick foxes.\nOur user could reasonably expect both documents to match the query. We can do better.\nIf we normalize the terms into a standardformat, then we can find documents that contain terms that are not exactly the same as the user requested, but are similar enough to still be relevant. For instance:\n-   Quickcan be lowercased to becomequick.\n-   foxescan be**stemmed**--reduced to its root form---to becomefox. Similarly,dogscould be stemmed todog.\n-   jumpedandleapare synonyms and can be indexed as just the single termjump.\nNow the index looks like this:\n\nTerm Doc_1 Doc_2\n-------------------------\nbrown | X | X\ndog | X | X\nfox | X | X\nin | | X\njump | X | X\nlazy | X | X\nover | X | X\nquick | X | X\nsummer | | X\nthe | X | X\n------------------------\nBut we're not there yet. Our search for+Quick +foxwouldstillfail, because we no longer have the exact termQuickin our index. However, if we apply the same normalization rules that we used on the content field to our query string, it would become a query for+quick +fox, which would match both documents!\n*This is very important. You can find only terms that exist in your index, soboth the indexed text and the query string must be normalized into the same form.*\nThis process of **tokenization** and **normalization** is called**analysis**\n<https://www.elastic.co/guide/en/elasticsearch/guide/current/inverted-index.html>\n"},{"fields":{"slug":"/Databases/Concepts/Indexing/MySQL-Indexing/","title":"MySQL Indexing"},"frontmatter":{"draft":false},"rawBody":"# MySQL Indexing\n\nCreated: 2020-10-12 23:31:27 +0500\n\nModified: 2022-12-11 14:39:00 +0500\n\n---\n\n**Important Points**\n-   The primary key is already an index\n    -   We can't have a Primary Key column with a NULLvalue.\n-   Using composite indexes is vital if you're trying to speed up a particular query\n-   Index order is very VERY important\n-   Avoid performing file sorts\n-   In a B-Tree the index is sorted first by the leftmost column, then by the next column, and so on\n-   <https://dev.mysql.com/doc/refman/5.7/en/table-scan-avoidance.html>\n-   <https://dev.mysql.com/doc/refman/5.7/en/where-optimization.html>\n-   use index hints\n**You should never make these mistakes again:**\n-   Avoid using functions in predicates\n-   Avoid using a wildcard (%) at the beginning of a predicate (The predicate LIKE**'%abc'**causes a full table scan.)\n-   Avoid unnecessary columns in SELECT clause\n-   Use inner join, instead of outer join if possible\n\nUse outer join only when it is necessary. Using it needlessly not only limits database performance but also limits MySQL query optimization options, resulting in slower execution of SQL statements.\n-   Use DISTINCT and UNION only if it is necessary\n\nUsing UNION and DISTINCT operators without any major purpose causes unwanted sorting and slowing down of SQL execution. Instead of UNION, using UNION ALL brings more efficiency in the process and improves MySQL performance more precisely.\n-   The ORDER BY clause is mandatory in SQL if you expect to get a sorted result\n-   Not taking advantage of indexed top-N queries\n-   Choosing a poor column order in multi-column indexes\n-   Inefficient use oflikefilters\n-   Not using index-only scans\n-   FullText index is not always used\n[**https://use-the-index-luke.com/3-minute-test/mysql**](https://use-the-index-luke.com/3-minute-test/mysql)\nFurthermore: typicallyMySQL will only use one index, per table, per query, so things are going to be much more performant if we design a single index to satisfy our lookup!\n\nA multiple column (composite) index to the rescue!:\nTo speed this up, we can sort our list by last name and then by first name.\nALTER TABLE `phonebook` ADD INDEX (`last_name`, `first_name`);\nSELECT phone_number FROM phonebook where first_name = 'john' AND last_name = 'south';\n\n![Image for post](media/Indexing_MySQL-Indexing-image1.png)\nAs an example: the index we've just discussed(last_name, first_name)would not be beneficial for a query such as:\n\nSELECT * FROM phonebook where first_name = 'Donald';\nThis is because we haven't specified any criteria for the last_name. The index we built doesn't sort first names in any logical distribution that would allow us to quickly search through them, so instead, we must fall back to a full table scan. (refer back to the example pic above).\n**Examples:**\n\nINDEX (last, first)\n-   WHERE last = ...*** Good\n-   WHERE last = ... AND first = ...*** Good\n-   WHERE first = ... AND last = ...*** Good\n-   WHERE first = ...*** Index Useless\nINDEX (a, b) VS INDEX (b, a)\n-   WHERE a = 1 AND b = 3*** Both work well\n-   WHERE b = 2*** Second only\n-   WHERE a = 4*** First only\n**INDEX(a), INDEX(b) Is not the same as INDEX(a,b)**\nBut using the two indexes (a),(b) can benefit from:\n\n(SELECT ... WHERE A ...) UNION (SELECT ... WHERE B ...)\n**clustered index**\n\nTheInnoDBterm for aprimary keyindex.InnoDBtable storage is organized based on the values of the primary key columns, to speed up queries and sorts involving the primary key columns. For best performance, choose the primary key columns carefully based on the most performance-critical queries. Because modifying the columns of the clustered index is an expensive operation, choose primary columns that are rarely or never updated.\n\nIn the Oracle Database product, this type of table is known as anindex-organized table.\n**Relationship between Primary Key & Clustered Index:**\n\nYou can't create a clustered index manually using InnoDB in MySQL. MySQL chooses it for you. But how does it choose? The following excerpts are from MySQL documentation:\nWhen you define aPRIMARY KEYon your table,InnoDBuses it as the clustered index. Define a primary key for each table that you create. If there is no logical unique and non-null column or set of columns, add a new[auto-increment](https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_auto_increment)column, whose values are filled in automatically.\nIf you do not define aPRIMARY KEYfor your table, MySQL locates the firstUNIQUEindex where all the key columns areNOT NULLandInnoDBuses it as the clustered index.\nIf the table has noPRIMARY KEYor suitableUNIQUEindex,InnoDBinternally generates a hidden clustered index namedGEN_CLUST_INDEXon a synthetic column containing row ID values. The rows are ordered by the ID thatInnoDBassigns to the rows in such a table. The row ID is a 6-byte field that increases monotonically as new rows are inserted. Thus, the rows ordered by the row ID are physically in insertion order.\nIn short, the MySQL InnoDB engine actually manages the primary index as clustered index for improving performance, so the primary key & the actual record on disk are clustered together.\n**secondary index**\n\nA type ofInnoDBindexthat represents a subset of table columns. AnInnoDBtable can have zero, one, or many secondary indexes. (Contrast with theclustered index, which is required for eachInnoDBtable, and stores the data for all the table columns.)\nA secondary index can be used to satisfy queries that only require values from the indexed columns. For more complex queries, it can be used to identify the relevant rows in the table, which are then retrieved through lookups using the clustered index.\nCreating and dropping secondary indexes has traditionally involved significant overhead from copying all the data in theInnoDBtable. Thefast index creationfeature makes bothCREATE INDEXandDROP INDEXstatements much faster forInnoDBsecondary indexes.\n**search index**\n\nIn MySQL,full-text searchqueries use a special kind of index, theFULLTEXT index. In MySQL 5.6.4 and up, InnoDBandMyISAMtables both supportFULLTEXTindexes; formerly, these indexes were only available forMyISAMtables.\nBy default MySQL will ignore any word that is in 50% or more of the rows in the table as it considers it would be a 'noise' word.\n\nWith very few rows in a table, it is common to hit this 50% limit often (ie. if you have two rows, every word is in at least 50% of the rows!).\nThere are two modes for MySQL Fulltext searching: natural language mode and Boolean mode. A restriction of natural language mode is \" ... words that are present in 50% or more of the rows are considered common and do not match. Full-text searches are natural language searches if no modifier is given.\" And natural language is the default mode.\n<https://stackoverflow.com/questions/1125678/mysql-fulltext-not-working>\n<https://dev.mysql.com/doc/refman/5.7/en/innodb-index-types.html>\n\n## Primary Key Indexes - MySQL**\n-   Because the clustered index \"is\" the table in InnoDB, it's important that you choose a suitable primary key, as this key will be used often, and restructuring can be very expensive.\n-   Non-sequential primary keys could lead to fragmentation issues. Causing page splits and disk fragmentations that lead to overheads in I/O operations.\n-   You should strive to insert data in primary key order when using InnoDB, and you should try to use a clustering key that will give a monotonically increasing values for each new row. This will ensure that rows are inserted in sequential order and will offer better performance for joins using primary keys.\n**Disadvantages of Primary Index**\n\nSince the primary index contains a direct reference to the data block address through the virtual address space & disk blocks are physically organized in the order of the index key, every time the OS does some disk page split due toDMLoperations likeINSERT/UPDATE/DELETE, the primary index also needs to be updated. SoDMLoperations puts some pressure on the performance of the primary index.\n**Secondary Indexes**\n\nObviously there can only be one clustered index --- because you can't store the row data in two places at once; Therefore secondary indexes (any indexes we apply that aren't the primary) are not clustered, and are in fact separate structures to the table itself.\nThe leaf nodes for secondary indexes don't store row data as the Primary Key B-Tree did, instead they simply store Primary Key values which serve as \"pointers\" to the row data, as you can see below:\n\n![Image for post](media/Indexing_MySQL-Indexing-image2.png)\n\nThis typically means when utilizing a secondary index, InnoDB will first use the B-Tree of the secondary index to retrieve the Primary Key values of the applicable rows, and then after use these values in conjunction with the Primary Key B-tree to fetch the row data!\n**Disadvantages of a Secondary Index**\n\nWithDMLoperations likeDELETE/INSERT, the secondary index also needs to be updated so that the copy of the primary key column can be deleted / inserted. In such cases, the existence of lots of secondary indexes can create issues.\nAlso, if a primary key is very large like aURL, since secondary indexes contain a copy of the primary key column value, it can be inefficient in terms of storage. More secondary keys means a greater number of duplicate copies of the primary key column value, so more storage in case of a large primary key. Also the primary key itself stores the keys, so the combined effect on storage will be very high. Because the Primary Key is appended to every secondary index in innoDB, don't pick huge PK's. Ideally keep them short so that it doesn't take up too much memory, and remember all data will be clustered onto this primary key; Therefore a bulky Primary Key will lead to bulky secondary indexes.\nSlow, It should be quite obvious that this additional lookup to follow the Primary Key 'pointer' from the secondary index has some overhead, it's still quick because the primary key is indexed, but this is where the 'covering index' optimization comes into play\n**UNIQUE Key Index**\n\nLike primary keys, unique keys can also identify records uniquely with one difference --- the unique key column can containnullvalues.\nUnlike other database servers, in MySQL a unique key column can have as manynullvalues as possible. In SQL standard,nullmeans an undefined value. So if MySQL has to contain only one null value in a unique key column, it has to assume that all null values are the same.\nBut logically this is not correct sincenullmeans undefined --- and undefined values can't be compared with each other, it's the nature ofnull. As MySQL can't assert if allnulls mean the same, it allows multiplenullvalues in the column.\nThe following command shows how to create a unique key index in MySQL:\n\nCREATE UNIQUE INDEX unique_idx_1 ON index_demo (pan_no);\n**Composite Index**\n\nMySQL lets you define indices on multiple columns, up to 16 columns. This index is called a Multi-column / Composite / Compound index.\nLet's say we have an index defined on 4 columns ---col1,col2,col3,col4. With a composite index, we have search capability oncol1,(col1, col2),(col1, col2, col3),(col1, col2, col3, col4). So we can use any left side prefix of the indexed columns, but we can't omit a column from the middle & use that like ---(col1, col3)or(col1, col2, col4)orcol3orcol4etc. These are invalid combinations.\nThe following commands create 2 composite indexes in our table:\n\nCREATE INDEX composite_index_1 ON index_demo (phone_no, name, age);\n\nCREATE INDEX composite_index_2 ON index_demo (pan_no, name, age);\nIf you have queries containing aWHEREclause on multiple columns, write the clause in the order of the columns of the composite index. The index will benefit that query. In fact, while deciding the columns for a composite index, you can analyze different use cases of your system & try to come up with the order of columns that will benefit most of your use cases.\nComposite indices can help you inJOIN&SELECTqueries as well. Example: in the followingSELECT *query,composite_index_2is used.\nWhen several indexes are defined, the MySQL query optimizer chooses that index which eliminates the greatest number of rows or scans as few rows as possible for better efficiency.\n**Why do we use composite indices? Why not define multiple secondary indices on the columns we are interested in?**\n\nMySQL uses only one index per table per query except for UNION.(In a UNION, each logical query is run separately, and the results are merged.) So defining multiple indices on multiple columns does not guarantee those indices will be used even if they are part of the query.\nMySQL maintains something called index statistics which helps MySQL infer what the data looks like in the system. Index statistics is a generilization though, but based on this meta data, MySQL decides which index is appropriate for the current query.\n**How does composite index work?**\n\nThe columns used in composite indices are concatenated together, and those concatenated keys are stored in sorted order using a B+ Tree. When you perform a search, concatenation of your search keys is matched against those of the composite index. Then if there is any mismatch between the ordering of your search keys & ordering of the composite index columns, the index can't be used.\nIn our example, for the following record, a composite index key is formed by concatenatingpan_no,name,age---HJKXS9086Wkousik28.\n\n| name     | kousik     |\n|----------|------------|\n| age      | 28         |\n| pan_no   | HJKXS9086W |\n| phone_no | 9090909090 |\n**How to identify if you need a composite index**\n-   Analyze your queries first according to your use cases. If you see certain fields are appearing together in many queries, you may consider creating a composite index.\n-   If you are creating an index incol1& a composite index in (col1,col2), then only the composite index should be fine.col1alone can be served by the composite index itself since it's a left side prefix of the index.\n-   Consider cardinality. If columns used in the composite index end up having high cardinality together, they are good candidate for the composite index.\n**Covering Index**\n\nA covering index is a special kind of composite index where all the columns specified in the query somewhere exist in the index. So the query optimizer does not need to hit the database to get the data --- rather it gets the result from the index itself. Example: we have already defined a composite index on(pan_no, name, age), so now consider the following query:\n\nSELECT age FROM index_demo WHERE pan_no = 'HJKXS9086W' AND name = 'kousik'\nThe columns mentioned in theSELECT&WHEREclauses are part of the composite index. So in this case, we can actually get the value of theagecolumn from the composite index itself. Let's see what theEXPLAINcommand shows for this query:\n\nEXPLAIN FORMAT=JSON SELECT age FROM index_demo WHERE pan_no = 'HJKXS9086W' AND name = '111kousik1';\nIn the response, note that there is a key ---using_indexwhich is set totruewhich signifies that the covering index has been used to answer the query.\nIf your secondary index holds all the data needed to satisfy your query (it 'covers' it) then you don't need to follow the Primary Key values to fetch any additional data!\n**Partial Index**\n\nWe already know that Indices speed up our queries at the cost of space. The more indices you have, the more the storage requirement. We have already created an index calledsecondary_idx_1on the columnname. The columnnamecan contain large values of any length. Also in the index, the row locators' or row pointers' metadata have their own size. So overall, an index can have a high storage & memory load.\nIn MySQL, it's possible to create an index on the first few bytes of data as well. Example: the following command creates an index on the first 4 bytes of name. Though this method reduces memory overhead by a certain amount, the index can't eliminate many rows, since in this example the first 4 bytes may be common across many names. Usually this kind of prefix indexing is supported onCHAR,VARCHAR,BINARY,VARBINARYtype of columns.\n\nCREATE INDEX secondary_index_1 ON index_demo (name(4));\n**What happens under the hood when we define an index?**\n\nLet's run theSHOW EXTENDEDcommand again:\n\nSHOW EXTENDED INDEXES FROM index_demo;\nWe definedsecondary_index_1onname, but MySQL has created a composite index on (name,phone_no) wherephone_nois the primary key column. We createdsecondary_index_2onage& MySQL created a composite index on (age,phone_no). We createdcomposite_index_2on (pan_no,name,age) & MySQL has created a composite index on (pan_no,name,age,phone_no). The composite indexcomposite_index_1already hasphone_noas part of it.\nSo whatever index we create, MySQL in the background creates a backing composite index which in-turn points to the primary key. This means that the primary key is a first class citizen in the MySQL indexing world. It also proves that all the indexes are backed by a copy of the primary index ---but I am not sure whether a single copy of the primary index is shared or different copies are used for different indexes.\nThere are many other indices as well like Spatial index and Full Text Search index offered by MySQL. I have not yet experimented with those indices, so I'm not discussing them in this post.\n**How Indexes Affect `ORDER and GROUP BY`**\n\nAnother often overlooked factor is how indexes can be used during ordering and grouping!\nSay you've only got an index on(last_name)in your phone book, and you run the query:\n\nSELECT * FROM phone_book WHERE last_name = 'Simpson'ORDER BY first_name\nThis will use the last_name index as you'd hope to quickly narrow down the records with that last name, sadly you now need to perform a sort on these resulting records in order to get them sorted by first name. This is because the index didn't sort the results by first_name in any meaningful way.\nThis is known as a**File Sort:**a sort that occurs after the query; it requires fetching the data into a temporary buffer and sorting it before finally returning. This wouldn't have been needed if the data was already sorted by the index in the way you wanted!\nThis also applies even If you only wanted to read 5 rows. Say you ran the following:\n\nSELECT last_name FROM phonebook WHERE first_name='Homer' ORDER BY last_nameLIMIT 5;\nYou'll still be fetching thousands of records, sorting them, and only after this, returning the top 5 while discarding the rest of the records you spent time processing.\nThis also applies to GROUP BY statements. if we ran the following query with this composite index onlast_name and first_name:\n\nSELECT * FROM phonebook WHERE last_name = 'Burns' GROUP BY first_name\nThe records would already be sorted bylast_name, allowing us to quickly filter down the records with thelast_name'Burns'. After these results are returned they are also then sorted byfirst_namedue to the second part of the index, and so they are intrinsically already grouped for us! We wouldn't need to perform any additional sorting at the end which would add further overheads to our query.\n**INDEX (a,b)**\n-   ORDER BY a ASC, b ASC*** Good\n-   ORDER BY a DESC, B DESC*** Good\n-   ORDER BY a ASC, b DESC*** Cannot use index\n**Range Queries**\n\nSimilar to what we've just said with the prefix rule, the second you use a range query on a column in your index you reach as far as you can utilize this index. This should make sense, if you issued a range query such as:\n\nSELECT * FROM phonebook WHERE last_name LIKE 'f%' AND first_name ='Ned';ADD INDEX (last_name, first_name, phone_number)\nThis would utilize the first part (last_name) of our index, allowing us to quickly satisfy the range conditional and find all rows with thelast_namebeginning with 'f'; however after this, there isn't any way our B-Tree can be further utilized to quickly filter onfirst_name.\n***If you're utilizing an index for range queries, try make sure the column you're performing the range over is ordered last within the index.**\nSimilarly, you can't use an index fully to perform range queries on two columns for the points already mentioned.-   You may need to have indexes on the same columns in different orders depending on your queries.\n-   Try use as many columns as possible up to the first range of the query --- after a range no other index column can be used. So put the index that is likely to be ranged right at the end.\n**Index Condition Pushdown**\n\nIndex push down essentially allows us to push index conditions down to the database engine so that it doesn't have to return irrelevant rows that would only be filtered out later by MySQL.\nThis means in some cases we can still use an index past a range condition. It's best to test and see the results for yourself, but regardless of the situation, you'll be most effective if you're able to leave range columns towards the end of your index. ICP will be most useful for when you have no choice.\n**Prefix ranges?**\n\nSELECT ... WHERE last_name LIKE \"%mithers\"\nThis isn't a range, we can't use a B-Tree to traverse this obviously. Imagine yourself being at the root of a tree with such a query, do you go left or right? You don't know! This is exactly why we can use an index to support such a query.\n**Examples:**\n\n**INDEX (a, b) VS INDEX (b, a)**\n-   WHERE a = 1 AND b > 3*** First is better\n-   WHERE b = 5 AND b > 7*** Second is better\n-   WHERE a > 1 AND b > 3*** Each stops after 1st Column\n-   WHERE b = 2*** Second only\n-   WHERE b > 2*** Second only\n-   WHERE a = 4*** First only\n-   WHERE a > 4*** First only\n**INDEX (a, b, c)**\n-   WHERE a > 1 AND b =3 AND c = 4*** Uses only first part of index\n-   WHERE a = 1 AND b > 3 AND c = 4*** Uses first 2 parts of index\n-   WHERE a = 1 AND b = 3 AND c = 4*** Uses all of index\nIn general all of the below could leave an index unusable:\n-   !=\n-   less than equal to operator\n-   NOT LIKE, NOT IN...\n-   NOT EXISTS ( SELECT * ... ) --- essentially a LEFT JOIN, often efficient\n-   NOT (expression)\n**Don't use functions in your queries**\n\nMySQL generally can't use indexes on columns unless the columns are isolated in the query. So don't use functions or expressions in your queries.\nThe second you dofunc(column) on leftyou can't use the index and a full table scan will occur.\nExamples:\n-   WHERE id+3 = 4;*** BAD\n-   Bad: WHERE start_date + INTERVAL 1 YEAR > NOW()*** BAD\n-   WHERE YEAR(start_date) = 2015 AND MONTH(start_date) = 1*** BAD\n-   Where number +0 = 5;*** BAD\n-   WHERE func(number) = n;*** BAD\n-   WHERE number = 5+4;*** GOOD\n-   WHERE number = func(n);*** GOOD\n-   WHERE start_date > NOW() --- INTERVAL 1 YEAR*** GOOD\n-   WHERE start_date BETWEEN \"2015--01--01\" AND \"2015--01--31\"*** GOOD\n**Redundant indexes**\n\nOver indexing can hurt performance due to overheads\nThe drawback of having too many indexes is the maintenance cost.\nAdding new indexes might have a performance impact for INSERT, UPDATE, and DELETE operations, especially if a new index causes you to hit memory limits.\nEvery time you perform a write on a table, the indexes will need to be maintained. Furthermore when you run a query, each index must be considered by the MySQL Optimizer.\n-   If there is an index on (A, B), adding another index (A) would be redundant because it is a prefix of the first index. That is, the index on (A, B) can already be used as an index for column A alone.\n-   If there is an index on (A, PK_ID). The PK_ID column as you already know is already included if you're using InnoDB, so it's redundant, luckily it won't add it twice so you are safe to do it, you just don't need too.\nThe only time we want redundant indexes is when extending an existing index makes it much larger and thus reduces performance!\nYou can easily see which indexes are redundant, especially those that have never been used, by querying the INFORMATION_SCHEMA database.\nThat being said don't be afraid to add indexes that will actually be used! In a read heavy application the costs will be negligible. It's best to test whether an index is beneficial out for yourself!\n**Index Merge**\n\nWhen i said MySQL only uses one index, per query, per table, most of the times this is true! However, sometimes MySQL does in fact have the capabilities to use multiple single column indexes.\nFor example: it could use several indexes to fetch primary key values, and then perform a union or intersection depending on the query. These are useful in situations where you cannot form a suitable multicolumn index e.g. in the case of several 'OR' conditionals in your query.\nHowever it is rare that these are actually used, and if you are able to form a suitable multicolumn index then you should, because this will typically outperform a merge index\n**Index not used**\n-   Sometimes MySQL does not use an index, even if one is available. One circumstance under which this occurs is when the optimizer estimates that using the index would require MySQL to access a very large percentage of the rows in the table. (In this case, a table scan is likely to be much faster because it requires fewer seeks.) However, if such a query uses LIMIT to retrieve only some of the rows, MySQL uses an index anyway, because it can much more quickly find the few rows to return in the result.\n-   **MySQL will not use indexes if you query the wrong data type (if field is VARCHAR but you are passing integer), but works other way around i.e. index is used in an integer column if you pass string in where column, but you must always use the correct data type while querying**\n-   The OR operator in filtering conditions\n\nConsider this query:\n\n<table>\n<colgroup>\n<col style=\"width: 30%\" />\n<col style=\"width: 69%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th><p>1</p>\n<p>2</p>\n<p>3</p>\n<p>4</p>\n<p>5</p>\n<p>6</p></th>\n<th><p>SELECT</p>\n<p>a, b</p>\n<p>FROM</p>\n<p>tbl</p>\n<p>WHERE</p>\n<p>a = 3 OR b = 8;</p></th>\n</tr>\n</thead>\n<tbody>\n</tbody>\n</table>\n\nIn many cases, MySQL won't be able to use an index to apply an OR condition, and as a result, this query is not index-able.\nTherefore, we recommend to avoid such OR conditions and consider splitting the query to two parts, combined with a UNION DISTINCT (or even better, UNION ALL, in case you know there won't be any duplicate results)\n**General Indexing guidelines**\n-   Since indices consume extra memory, carefully decide how many & what type of index will suffice your need.\n-   WithDMLoperations, indices are updated, so write operations are quite costly with indexes. The more indices you have, the greater the cost. Indexes are used to make read operations faster. So if you have a system that is write heavy but not read heavy, think hard about whether you need an index or not.\n-   Cardinality is important --- cardinality means the number of distinct values in a column. If you create an index in a column that has low cardinality, that's not going to be beneficial since the index should reduce search space. Low cardinality does not significantly reduce search space.\n    Example: if you create an index on a boolean (int1or0only ) type column, the index will be very skewed since cardinality is less (cardinality is 2 here). But if this boolean field can be combined with other columns to produce high cardinality, go for that index when necessary.\n-   Indices might need some maintenance as well if old data still remains in the index. They need to be deleted otherwise memory will be hogged, so try to have a monitoring plan for your indices.\n# show cardinality of a table\n\nSHOW INDEXES FROM table_name;\n[**https://medium.com/@stormanning/mysql-indexing-101-660f3193dde1**](https://medium.com/@stormanning/mysql-indexing-101-660f3193dde1)\n\n<https://dev.mysql.com/doc/refman/8.0/en/mysql-indexes.html>\n\n<https://dev.mysql.com/doc/refman/8.0/en/optimization-indexes.html>\n\n<https://www.freecodecamp.org/news/database-indexing-at-a-glance-bb50809d48bd>\n\n[**https://dev.mysql.com/doc/refman/8.0/en/optimization-indexes.html**](https://dev.mysql.com/doc/refman/8.0/en/optimization-indexes.html)\n\n**Unused indexes**\n\nselect count(*) from sys.schema_unused_indexes;\n\n<https://www.eversql.com/how-to-find-unused-indexes-in-a-mysql-database>\n\n## Rebuilding or Repairing Tables or Indexes**\n\n**Methods for rebuilding a table include:**\n-   [Dump and Reload Method](https://dev.mysql.com/doc/refman/8.0/en/rebuilding-tables.html#rebuilding-tables-dump-reload)\n-   [ALTER TABLE Method](https://dev.mysql.com/doc/refman/8.0/en/rebuilding-tables.html#rebuilding-tables-alter-table)\n-   [REPAIR TABLE Method](https://dev.mysql.com/doc/refman/8.0/en/rebuilding-tables.html#rebuilding-tables-repair-table)\n<https://dev.mysql.com/doc/refman/8.0/en/rebuilding-tables.html>\n\n## MySQL 8.0 - Invisible Index**\n\nInvisible indexes make it possible to test the effect of removing an index on query performance, without making a destructive change that must be undone should the index turn out to be required. Dropping and re-adding an index can be expensive for a large table, whereas making it invisible and visible are fast, in-place operations.\n<https://dev.mysql.com/doc/refman/8.0/en/invisible-indexes.html>\n\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/AWS-DynamoDB/Cheetsheet/","title":"Cheetsheet"},"frontmatter":{"draft":false},"rawBody":"# Cheetsheet\n\nCreated: 2020-03-14 13:39:48 +0500\n\nModified: 2020-07-22 13:39:47 +0500\n\n---\n\n**The Basics of DynamoDB**\n\nDynamoDBis a fully managed NoSQL key/value and document database.\nDynamoDB is suited for workloads with any amount of data thatrequire predictable read and write performanceand automatic scaling from large to small and everywhere in between.\nDynamoDB scales up and down to support whateverread and write capacity you specifyper second in provisioned capacity mode. Or you can set it to On-Demand mode and there is little to no capacity planning.\n-   DynamoDB stores**3 copies** of dataon SSD drivesacross 3 AZsin a region.\n-   DynamoDB's most common datatypes areB(Binary),N(Number), andS(String)\n-   Tables consist ofItems(rows) and Items consist ofAttributes(columns)\n**Reads and Writes Consistency**\n\nDynamoDB can be set to supportEventually Consistent Reads(default) andStrongly Consistent Readson a per-call basis.\nEventually consistent readsdata is returned immediately but data can be inconsistent. Copies of data will be generally consistent in 1 second.\nStrongly Consistent Readswill always read from the leader partition since it always has an up-to-date copy. Data will never be inconsistent but latency may be higher. Copies of data will be consistent with a guarantee of 1 second.\n**Partitions**\n\nAPartitionis when DynamoDB slices your table up into smaller chunks of data. This speeds up reads for very large tables.\nDynamoDB automatically creates Partitions for:\n-   Every 10 GB of Data or\n-   When you exceed RCUs (3000) or WCUs (1000) limits for a single partition\n-   When DynamoDB sees a pattern of a hot partition, it will split that partition in an attempt to fix the issue.\nDynamoDB will try toevenly splitthe RCUs and WCUs across Partitions\n**Primary Key Design**\n\nPrimary keys definewhere and howyour data will be stored in partitions\nThe Key schema can be made up of two keys:\n-   Partition Key (PK) is also known asHASH\n-   TheSort Key (SK) is also known asRANGE\nWhen using the AWS DynamoDB API eg. CLI, SDK they refer to the PK and SK by their alternative names due to legacy reasons.\nPrimary key comes in two types:\n-   SimplePrimary Key (Using only a Partition Key)\n-   CompositePrimary Key (Using both a Partition and Sort Key)\nKey Uniqueness is as follows:\n-   When creating a Simple Primary Key the PKvalue may be unique\n-   When creating a Composite Primary Keythe combined PK and SK must be unique\nWhen using a Sort key, records on the partition are logically grouped together in Ascending order.\n**Secondary Indexes**\n\nDynamoDB has two types of Indexes:\n-   LSI- Local Secondary index\n-   GSI- Global Secondary Index\n**LSI - Local Secondary index**\n-   Supportsstronglyor eventual consistency reads\n-   Can only be created with initial table (cannot be modified or and cannot deleted unless also deleting the table)\n-   Only Composite\n-   10GB or less per partition\n-   Share capacity units with base table\n-   Must share Partition Key (PK) with base table.\n**GSI - Global Secondary Index**\n-   Only eventual consistencyreads (cannot provide strong consistency)\n-   Can create, modify, or delete at anytime\n-   Simple and Composite\n-   Can have whatever attributes as Primary Key (PK) or Secondary Key (SK)\n-   No size restriction per partition\n-   Has its own capacity settings (does not share with base table)\n**Scan**\n\nYour table(s) should be designed in such a way that your workload primary access patterns do not use Scans. Overall, scans should be needed sparingly, for example for an infrequent report.\n-   Scans through all items in a table and then returns one or more items through filters\n-   By default returns all attributes for every item (useProjectExpressionto limit)\n-   Scans are sequential, and you can speed up a scan through parallel scans usingSegmentsandTotal Segments\n-   Scans can be slow, especially with very large tables and can easily consume your provisioned throughput.\n-   Scans are one of the most expensive ways to access data in DynamoDB.\n**Query**\n-   Find items based on primary key values\n-   Table must have a composite key in order to be able to query\n-   By default queries are Eventually Consistent (useConsistentRead Trueto change Strongly Consistent)\n-   By default returns all attributes for each item found by a query (useProjectExpressionto limit)\n-   By default is sorted ascending (useScanIndexForwardto False to reverse order to descending)\n**Capacity Modes**\n\nDynamoDB has two capacity modes,ProvisionedandOn-Demand. You can switch between these modesonce every 24 hours.\n**Provisioned**\n\nProvisioned Throughput Capacityis the maximum amount of capacity your application is allowedto read or write per secondfrom a table or index\n-   Provisionedis suited for predictable or steady state workloads\n-   RCUsis Read Capacity Unit\n-   WCUsis Write Capacity Unit\nYou should enable Auto Scaling with Provisionedcapacity mode. In this mode, you set a floor and ceiling for the capacity you wish the table to support. DynamoDB will automatically add and remove capacity to between these values on your behalf and throttle calls that go above the ceiling for too long.\nIf you go beyond your provisioned capacity, you'll get an Exception: ProvisionedThroughputExceededException(throttling)\nThrottlingis whenrequests are blockeddue to read or write frequency higher than set thresholds. E.g. exceeding set provisioned capacity, partitions splitting, table/index capacity mismatch.\n**On-Demand**\n\nOn-Demand Capacityis pay per request. So you pay only for what you use.\n-   On-Demand is suited forneworunpredictableworkloads\n-   The throughput is only limited by the default upper limits for a table (40K RCUs and 40K WCUs)\n-   Throttling can occurif you exceed double your previous peak capacity (high water mark) within 30 minutes. For example, if you previously peaked to a maximum of 30,000 ops/sec, you could not peak immediately to 90,000 ops/sec, but you could to 60,000 ops/sec.\n-   Since there is no hard limit,On-Demand could become very expensivebased on emerging scenarios\n**Calculating Reads and Writes**\n\n**Calculating Reads (RCU)**\n\nA read capacity unitrepresents:\n-   one strongly consistent read per second,\n-   or two eventually consistent reads per second,\n-   for an item up to 4 KB in size.\n\nHow to calculate RCUs forstrong\n\ni.  Round data up to nearest 4.\n\nii. Divide data by 4\n\niii. Times by number of reads\n\nHere's an example:\n-   50 reads at 40KB per item. (40/4) x 50 = 500 RCUs\n-   10 reads at 6KB per item. (8/4) x 10 = 20 RCUs\n-   33 reads at 17KB per item. (20/4) x 33 = 132 RCUs\n\nHow to calculate RCUs foreventual\n\ni.  Round data up to nearest 4.\n\nii. Divide data by 4\n\niii. Times by number of reads\n\niv. Divide final number by 2\n\nv.  Round up to the nearest whole number\n\nHere's an example:\n-   50 reads at 40KB per item. (40/4) x 50 / 2 = 250 RCUs\n-   11 reads at 9KB per item. (12/4) x 11 / 2 = 17 RCUs\n-   14 reads at 24KB per item. (24/4) x 14 / 2 = 35 RCUs\n**Calculating Writes (Writes)**\n\nA write capacity unitrepresents:\n-   one write per second,\n-   for an item up to 1 KB\n\nHow to calculateWrites\n\ni.  Round data up to nearest 1.\n\nii. Times by number of writes\n\nHere's an example:\n-   50 writes at 40KB per item. 40 x 50 = 2000 WCUs\n-   11 writes at 1KB per item. 1 x 11 = 11 WCUs\n-   18 writes at 500 BYTES per item. 1 x 18 = 18 WCUs\n**DynamoDB Accelerator**\n\nDynamoDB Accelerator(DAX)is afully managed in-memory write through cachefor DynamoDB that runs in a cluster\n-   Reads are eventually consistent\n-   Incoming requests are evenly distributed across all of the nodes in the cluster.\n-   DAX can reduce read response times tomicroseconds\nDAX is ideal for:\n-   fastest response times possible\n-   apps that read a small number of items more frequently\n-   apps that areread intensive\nDAX is not ideal for:\n-   Apps that require strongly consistent reads\n-   Apps that do not require microsecond read response times\n-   Apps that arewrite intensive, or that do not perform much read activity\n-   If you don't need DAXconsider using ElastiCache\n**DynamoDB Transactions**\n\nDynamoDB supports transactions via theTransactWriteItemsandTransactGetItemsAPI calls.\n\nTransactionslet you query multiple tables at once and are an all-or-nothing approach (all API calls must succeed).\n**Global tables**\n\nDynamoDB Global tables provide a fully managed solution for deployingmulti-region, multi-master databases.\n**Streams**\n\nDynamoDB Streamsallows you to set up a Lambda function triggered every time data is modified in a table to react to changes.Streams do not consume RCUs.\n**DynamoDB API**\n\nDynamoDB API's most notable commands via CLI: aws dynamodb < command >\naws dynamodbget-itemreturns a set of attributes for the item with the given primary key. If no matching item, then it does not return any data and there will be no Item element in the response.\n\naws dynamodbput-itemCreates a new item, or replaces an old item with a new item. If an item that has the same primary key as the new item already exists in the specified table, the new item completely replaces the existing item.\naws dynamodbupdate-itemEdits an existing item's attributes, or adds a new item to the table if it does not already exist.\naws dynamodbbatch-get-itemreturns the attributes of one or more items from one or more tables. You identify requested items by primary key. A single operation can retrieve up to16 MB of data, which can contain as many as100 items.\naws dynamodbbatch-write-itemputs or deletes multiple items in one or more tables. Can write up to16 MB of data, which can comprise as many as25 put or delete requests. Individual items to be written can beas large as 400 KB.\naws dynamodbcreate-tableadds a new table to your account. Table names must be unique within each Region.\naws dynamodbupdate-tableModifies the provisioned throughput settings, global secondary indexes, or DynamoDB Streams settings for a given table.\naws dynamodbdelete-tableoperation deletes a table and all of its items.\naws dynamodbtransact-get-itemsis a synchronous operation that atomically retrieves multiple items from one or more tables (but not from indexes) in a single account and Region. Call can contain up to25 objects. The aggregate size of the items in the transactioncannot exceed 4 MB.\naws dynamodbtransact-write-itemsa synchronous write operation that groups up to25 action requests. These actions can target items in different tables, but not in different AWS accounts or Regions, and no two actions can target the same item.\naws dynamodbqueryfinds items based on primary key values. You can query table or secondary index that has a composite primary key.\naws dynamodbscanreturns one or more items and item attributes by accessing every item in a table or a secondary index.\n<https://www.freecodecamp.org/news/ultimate-dynamodb-2020-cheatsheet>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/AWS-DynamoDB/Core-components/","title":"Core components"},"frontmatter":{"draft":false},"rawBody":"# Core components\n\nCreated: 2020-03-12 17:07:40 +0500\n\nModified: 2021-09-23 15:09:16 +0500\n\n---\n\nIn DynamoDB, tables, items, and attributes are the core components that you work with. A**table**is a collection of**items**, and each item is a collection of**attributes**. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility. You can use DynamoDB Streams to capture data modification events in DynamoDB tables.\n**Tables, Items, and Attributes**\n-   **Tables**\n\nSimilar to other database systems, DynamoDB stores data in tables. Atableis a collection of data. For example, see the example table calledPeoplethat you could use to store personal contact information about friends, family, or anyone else of interest. You could also have aCarstable to store information about vehicles that people drive.-   **Items**\n\nEach table contains zero or more items. Anitemis a group of attributes that is uniquely identifiable among all of the other items. In aPeopletable, each item represents a person. For aCarstable, each item represents one vehicle. Items in DynamoDB are similar in many ways to rows, records, or tuples in other database systems. In DynamoDB, there is no limit to the number of items you can store in a table.-   **Attributes**\n\nEach item is composed of one or more attributes. Anattributeis a fundamental data element, something that does not need to be broken down any further. For example, an item in aPeopletable contains attributes calledPersonID,LastName,FirstName, and so on. For aDepartmenttable, an item might have attributes such asDepartmentID,Name,Manager, and so on. Attributes in DynamoDB are similar in many ways to fields or columns in other database systems.\n**Primary Key**\n\nWhen you create a table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key.\nDynamoDB supports two different kinds of primary keys:\n-   **Partition key**\n\nA simple primary key, composed of one attribute known as thepartition key.\nDynamoDB uses the partition key's value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored.\n\nIn a table that has only a partition key, no two items can have the same partition key value.\nThePeopletable described in[Tables, Items, and Attributes](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.TablesItemsAttributes)is an example of a table with a simple primary key (PersonID). You can access any item in thePeopletable directly by providing thePersonIdvalue for that item.-   **Partition key and sort key**\n\nReferred to as acomposite primary key, this type of key is composed of two attributes. The first attribute is thepartition key, and the second attribute is thesort key.\n\nDynamoDB uses the partition key value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored. All items with the same partition key value are stored together, in sorted order by sort key value.\n\nIn a table that has a partition key and a sort key, it's possible for two items to have the same partition key value. However, those two items must have different sort key values.\nEach primary key attribute must be a scalar (meaning that it can hold only a single value). The only data types allowed for primary key attributes are string, number, or binary. There are no such restrictions for other, non-key attributes.\n**Secondary Indexes**\n\nYou can create one or more secondary indexes on a table. Asecondary indexlets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn't require that you use indexes, but they give your applications more flexibility when querying your data. After you create a secondary index on a table, you can read data from the index in much the same way as you do from the table.\nDynamoDB supports two kinds of indexes:\n-   **Global secondary index**\n\nAn index with a partition key and sort key that can be different from those on the table.-   **Local secondary index**\n\nAn index that has the same partition key as the table, but a different sort key.\nEach table in DynamoDB has a limit of 20 global secondary indexes (default limit) and 5 local secondary indexes per table.\n<https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html>\n\n## DynamoDB Streams**\n\nDynamoDB Streams is an optional feature that captures data modification events in DynamoDB tables. The data about these events appear in the stream in near-real time, and in the order that the events occurred.\nEach event is represented by astream record. If you enable a stream on a table, DynamoDB Streams writes a stream record whenever one of the following events occurs:\n-   A new item is added to the table: The stream captures an image of the entire item, including all of its attributes.\n-   An item is updated: The stream captures the \"before\" and \"after\" image of any attributes that were modified in the item.\n-   An item is deleted from the table: The stream captures an image of the entire item before it was deleted.\nEach stream record also contains the name of the table, the event timestamp, and other metadata. Stream records have a lifetime of 24 hours; after that, they are automatically removed from the stream.\nYou can use DynamoDB Streams together with AWS Lambda to create atrigger---code that executes automatically whenever an event of interest appears in a stream. For example, consider aCustomerstable that contains customer information for a company. Suppose that you want to send a \"welcome\" email to each new customer. You could enable a stream on that table, and then associate the stream with a Lambda function. The Lambda function would execute whenever a new stream record appears, but only process new items added to theCustomerstable. For any item that has anEmailAddressattribute, the Lambda function would invoke Amazon Simple Email Service (Amazon SES) to send an email to that address.\n<https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns>\n<https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html>\n\n## Global Secondary Indexes Usage patterns**\n-   Querying and sorting data by multiple attributes\n-   Data filtering\n-   Reduce read capacity unit (RCU) consumption for large items\n-   Isolating read workloads\n<https://aws.amazon.com/blogs/database/how-to-use-dynamodb-global-secondary-indexes-to-improve-query-performance-and-reduce-costs>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/AWS-DynamoDB/Documentation/","title":"Documentation"},"frontmatter":{"draft":false},"rawBody":"# Documentation\n\nCreated: 2020-02-17 11:04:04 +0500\n\nModified: 2020-03-12 23:30:03 +0500\n\n---\n\n<https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html>\n\n## Partitions & Data Distribution**\n\n<https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html>\n\n## Best Practices > Partition key design**\n\nThe primary key that uniquely identifies each item in an Amazon DynamoDB table can be simple (a partition key only) or composite (a partition key combined with a sort key).\nGenerally speaking, you should design your application for uniform activity across all logical partition keys in the table and its secondary indexes. You can determine the access patterns that your application requires, and estimate the total read capacity units (RCU) and write capacity units (WCU) that each table and secondary index requires.\n<https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html>\n\n## Appendix**\n\nIntegrating with Amazon Data Pipeline\n\n<https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBPipeline.html>\n\n## Blogs**\n\nSimplify Amazon DynamoDB data extraction and analysis by using AWS Glue and Amazon Athena\n\n![Architecture diagram of the solution beginning from the Amazon DynamoDB table](media/AWS-DynamoDB_Documentation-image1.gif)\n\n<https://aws.amazon.com/blogs/database/simplify-amazon-dynamodb-data-extraction-and-analysis-by-using-aws-glue-and-amazon-athena>\n\n[**https://aws.amazon.com/blogs/big-data/how-to-export-an-amazon-dynamodb-table-to-amazon-s3-using-aws-step-functions-and-aws-glue/**](https://aws.amazon.com/blogs/big-data/how-to-export-an-amazon-dynamodb-table-to-amazon-s3-using-aws-step-functions-and-aws-glue/)\n\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/AWS-DynamoDB/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2020-02-17 13:05:14 +0500\n\nModified: 2020-12-22 15:19:44 +0500\n\n---\n\n**PartiQL**\n-   You now can use[PartiQL](https://aws.amazon.com/blogs/opensource/announcing-partiql-one-query-language-for-all-your-data/)with[NoSQL Workbench](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/workbench.html)to run SQL-compatible queries on your DynamoDB data. PartiQL makes it easier to interact with DynamoDB, and now you can use PartiQL to query, insert, update, and delete table data by using NoSQL Workbench.\n**Amazon DynamoDB Accelerator (DAX)**\n\n**Fully managed in-memory cache for DynamoDB**\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory[cache](https://aws.amazon.com/caching/)for[DynamoDB](https://aws.amazon.com/dynamodb/)that delivers up to a 10x performance improvement -- from milliseconds to microseconds -- even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. Now you can focus on building great applications for your customers without worrying about performance at scale. You do not need to modify application logic, since DAX is compatible with existing DynamoDB API calls.\n<https://aws.amazon.com/dynamodb/dax>\n\n<https://aws.amazon.com/blogs/aws/amazon-dynamodb-accelerator-dax-in-memory-caching-for-read-intensive-workloads>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/AWS-DynamoDB/Working/","title":"Working"},"frontmatter":{"draft":false},"rawBody":"# Working\n\nCreated: 2020-02-16 23:31:05 +0500\n\nModified: 2022-01-08 22:57:30 +0500\n\n---\n\n**DynamoDB Item Sizes**\n\nDynamoDB tables are schemaless, except for the primary key, so the items in a table can all have different attributes, sizes, and data types.\n\nThe total size of an item is the sum of the lengths of its attribute names and values. You can use the following guidelines to estimate attribute sizes:\n-   Strings are Unicode with UTF-8 binary encoding. The size of a string is(length of attribute name) + (number of UTF-8-encoded bytes).\n-   Numbers are variable length, with up to 38 significant digits. Leading and trailing zeroes are trimmed. The size of a number is approximately(length of attribute name) + (1 byte per two significant digits) + (1 byte).\n-   A binary value must be encoded in base64 format before it can be sent to DynamoDB, but the value's raw byte length is used for calculating size. The size of a binary attribute is(length of attribute name) + (number of raw bytes).\n-   The size of a null attribute or a Boolean attribute is(length of attribute name) + (1 byte).\n-   An attribute of typeListorMaprequires 3 bytes of overhead, regardless of its contents. The size of aListorMapis(length of attribute name) + sum (size of nested elements) + (3 bytes). The size of an emptyListorMapis(length of attribute name) + (3 bytes).\n<https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CapacityUnitCalculations.html>\n\n## Scan vs Query**\n\nA query operation searches only primary key attribute values and supports a subset of comparison operators on key attribute values to refine the search process.\n\nGetqueryyou are using a primary key inwherecondition, The computation complexity islog(n)as the most of key structure is binary tree.\nA scan operation scans the entire table. You can specify filters to apply to the results to refine the values returned to you, after the complete scan.\n\nwhilescanquery you have to scan whole table then apply filter on every singlerowto find the right result. The performance isO(n). Its much slower if your table is big.\nAlso, think about the global secondary index to support a different kind of queries on different keys to gain performance objective\n<https://stackoverflow.com/questions/43452219/what-is-the-difference-between-scan-and-query-in-dynamodb-when-use-scan-query>\n\n<https://medium.com/@amos.shahar/dynamodb-query-vs-scan-sql-syntax-and-join-tables-part-1-371288a7cb8f>\n\n## Working**\n\nRead capacity unit (RCU)\n\nWrite capacity unit (WCU)\n\nReplicated write capacity unit (rWCU)-   One**read capacity unit**represents one strongly consistent read per second, or two eventually consistent reads per second, for items up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of read capacity units required depends on the item size, and whether you want an eventually consistent or strongly consistent read.\n-   One**write capacity unit**represents one write per second for items up to 1 KB in size. If you need to write an item that is larger than 1 KB, DynamoDB will need to consume additional write capacity units. The total number of write capacity units required depends on the item size.\n![ATTRIBUTES PARTITION KEY Mandatory Key-value access pattern Determines data distribution TABLE SORT KEY Optional Model 1 relationships Enables rich query capabilities TABLE ITEMS All items for key \"begins with\" \"between\" \"contains\" \"in\" sorted results counts top/bottom N values ](media/AWS-DynamoDB_Working-image1.png)-   Secondary Indexes\n    -   Local secondary indexes\n    -   Global secondary indexes (asynchronous)\n<https://aws.amazon.com/dynamodb/pricing/provisioned>\n\n## NoSQL Data Modeling**\n-   Normalized vs De-normalized schema\n**Common NoSQL Design Patterns**\n-   Composite Keys\n-   Hierarchical Data\n-   Relational Data\n-   Query Filters\n-   Sparse Indexes\n**Vertical Partitioning**\n-   Large items\n-   Filters vs indexes\n-   M:N modeling - inbox and outbox\n**Modeling Real Applications**\n-   Partitions are 3 way replicated\n    -   Acknowledgement when 2 out of 3 replicas acknowledges\n-   Local Secondary Indexes\n    -   allows you to resort the data in the partition\n-   Global Secondary Indexes\n**Selecting a partition key**\n-   Large number of distinct values\n-   Items are uniformly requested and randomly distributed\n-   Examples\n    -   Bad: Status, Gender\n    -   Good: CustomerId, DeviceId\n**Selecting a sort key**\n-   Model 1:n and n:n relationships\n-   Efficient/selective patterns\n    -   Query multiple entities\n-   Leverage range queries\n-   Examples\n    -   Orders and OrderItems-   Composite Keys\n-   Multi-value Sorts and Filters\n-   DynamoDB Transactions API\n    -   Transact WriteItems\n        -   Synchronous update, put, delete, and check\n            -   Atomic\n            -   Automated Rollbacks\n        -   Up to 10 items within a transaction\n        -   Supports multiple tables\n        -   Complex conditional checks\n    -   Good use cases\n        -   Commit changes across items\n        -   Conditional batch inserts/updates\n    -   Bad use case\n        -   Maintaining normalized data\n**Advanced Data Modeling**\n-   How OLTP Apps use data\n    -   Mostly hierarchical structures\n    -   Entity driven workflows\n    -   Data spread across tables\n    -   Requires complex queries\n    -   Primary driver for ACID\n[AWS re:Invent 2018: Amazon DynamoDB Deep Dive: Advanced Design Patterns for DynamoDB (DAT401)](https://www.youtube.com/watch?v=HaEPXoXVf2k)\n**Write Sharding (Shard write-heavy partition keys)**\n-   If there are a lot of writes coming in, then create different partitions and randomly add data to partitions\n    -   Insert \"CandidateA_\" + rand(0, 10)\n-   Scatter gather pattern for shard aggregation\n-   Increase throughput with concurrency\n-   Consider RCU/WCU per key, item size and request rate\n-   **Important when:** your write workload is not horizontally scalable\n**Calculating partition counts (reads)**\n\n![100 K 0.2 KB 4 KB 10 3000 - -17 Average item size Requests per second Items per partition RCU Size Partition max RCU ](media/AWS-DynamoDB_Working-image2.png)\n**Time-based workflows**\n\nProcessing the entire table efficiently\n**Adjacency lists and materialized graphs**\n-   Partition table on node ID, add edges to define adjacency list\n-   Define a default edge for every node type to describe the node itself\n-   Use partitioned GSIs to query large nodes (dates, places, etc.)\n-   Use DynamoDB Streams/Lambda/EMR for graph query projections\n    -   Neighbor entity state\n    -   Subtree aggregations\n    -   Breadth first search\n    -   Node ranking\n[AWS re:Invent 2019: [REPEAT 1] Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)](https://www.youtube.com/watch?v=6yqfmXiZTlM)\n\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Cassandra/CQL-(Cassandra-Query-Language)/","title":"CQL (Cassandra Query Language)"},"frontmatter":{"draft":false},"rawBody":"# CQL (Cassandra Query Language)\n\nCreated: 2019-03-17 17:50:35 +0500\n\nModified: 2020-01-03 17:00:20 +0500\n\n---\n\n**CQL**\n-   CQL mapping to cassandra's internal data structure\n-   Use SQL like query language\n-   **CQL is a reintroduction of schema**\n-   **CQL creates a common language** so that details of the data model can be easily communicated\n-   **CQL is a best-practices Cassandra interface** and hides the messy details\n-   CQL let's you take advantage of the C* Data structure\n-   CQL protocol is binary and therefore interoperable with any language\n-   CQL is asynchronous and fast (Thrift transport layer is synchronous)\n-   CQL allows the possibility for prepared statements\n\n**Commands**\n```\nDESCRIBE KEYSPACES;\n\nDESCRIBE TABLES;\nCREATE KEYSPACE killrvideo WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\nUSE killrvideo;\nCREATE TABLE raw_data_by_day ( sensor text, day text, ts timeuuid, reading int, primary key((sensor, day), ts) ) WITH CLUSTERING ORDER BY (ts DESC) AND COMPACTION = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_unit': 'DAYS', 'compaction_window_size': 1};\nCREATE TABLE stuff ( a int, b int, myset set<int>, mylist list<int>, mymap map<int, int>, PRIMARY KEY (a,b));\n\nCREATE TABLE videos ( video_id timeuuid, added_date timestamp, Title text, PRIMARY KEY (video_id));\n\nCREATE TABLE videos_by_tag ( tag text, video_id timeuuid, added_date timestamp, Title text, PRIMARY KEY (tag, video_id));\nCREATE TABLE videos_by_tag ( tag text, video_id timeuuid, added_date timestamp, Title text, PRIMARY KEY (tag, added_date, video_id)) WITH CLUSTERING ORDER BY(added_date ASC, video_id ASC);\nINSERT INTO killrvideo.videos(video_id, added_date, Title) VALUES (1645ea59-14bd-11e5-a993-8138354b7e31, '2014-01-29', 'Cassandra History');\nBEGIN BATCH\n\nINSERT into users(username,name,age) VALUES('raziz12','Rashid Aziz',34);\n\nINSERT INTO users_by_cities(username,name,city, age) VALUES ('raziz12','Rashid Aziz','Karachi',30);\n\nAPPLY BATCH;UPDATE stuff SET myset = {1,2}, mylist = [3,4,5], mymap = {6:7, 8:9} WHERE a = 0 AND b = 1;\nSELECT * FROM stuff; (in cassandra-cli)\n\nSELECT * FROM killrvideo.videos;\n\nSELECT count(*) FROM killrvideo.videos;\n\nSELECT * FROM videos_by_tag WHERE tag='cassandra';\n\nSELECT * FROM videos_by_tag WHERE tag = 'cassandra' AND title='Cassandra Intro';\n\nSELECT * FROM videos_by_tag WHERE tag='cassandra' ORDER BY added_date DESC;\nLIST stuff;\nSELECT key_aliases, column_aliases FROM system.schema_columnfamilies WHERE keyspace_name = 'test' AND columnfamily_name = 'stuff'; (in cqlsh)\nTRUNCATE videos;\n\nDROP TABLE videos_by_tag;\nCOPY videos(video_id, added_date, title) FROM '/Users/deepaksood/Downloads/ds201-6.0-labwork/labwork/data-files/videos.csv' WITH HEADER=TRUE;\nCOPY videos_by_tag (tag, video_id, added_date, title) FROM '/Users/deepaksood/Downloads/ds201-6.0-labwork/labwork/data-files/videos-by-tag.csv' WITH HEADER=TRUE;\nQUIT\n**DESCRIBE TABLE videos;**\n\nSELECT token(video_id), video_id FROM videos;\n```\n\n\n![Example](media/Cassandra_CQL-(Cassandra-Query-Language)-image1.png)\n\n![Remember this](media/Cassandra_CQL-(Cassandra-Query-Language)-image3.png)\n\n![The CQL/Cassandra Mapping](media/Cassandra_CQL-(Cassandra-Query-Language)-image4.png)\n\n![The CQL/Cassandra Mapping](media/Cassandra_CQL-(Cassandra-Query-Language)-image5.png)\n\n![The CQL/Cassandra Mapping](media/Cassandra_CQL-(Cassandra-Query-Language)-image6.png)\n\n![CQL](media/Cassandra_CQL-(Cassandra-Query-Language)-image7.png)\n\n![Inserting](media/Cassandra_CQL-(Cassandra-Query-Language)-image8.png)\n\n![Updating](media/Cassandra_CQL-(Cassandra-Query-Language)-image9.png)\n\n![SETS](media/Cassandra_CQL-(Cassandra-Query-Language)-image10.png)\n\n![LISTS](media/Cassandra_CQL-(Cassandra-Query-Language)-image11.png)\n\n![MAPS](media/Cassandra_CQL-(Cassandra-Query-Language)-image12.png)\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Cassandra/Commands/","title":"Commands"},"frontmatter":{"draft":false},"rawBody":"# Commands\n\nCreated: 2019-12-29 16:32:42 +0500\n\nModified: 2020-01-04 11:51:35 +0500\n\n---\n\ndocker run --name cas1 -p 9042:9042 -e CASSANDRA_CLUSTER_NAME=MyCluster -e CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch -e CASSANDRA_DC=datacenter1 -d cassandra\ndocker run --name cas2 -e CASSANDRA_SEEDS=\"$(docker inspect --format='{{ .NetworkSettings.IPAddress }}' cas1)\" -e CASSANDRA_CLUSTER_NAME=MyCluster -e CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch -e CASSANDRA_DC=datacenter1 -d cassandra:latest\ndocker exec -it cas1 nodetool status\n\ndocker exec -it cas1 nodetool ring\n\ndocker exec -it cas1 nodetool getendpoints killrvideo videos '1645ea59-14bd-11e5-a993-8138354b7e31'\ndocker exec -it cas2 cqlsh\n**cqlsh - Cassandra cli tools**\n\nbrew install cassandra\n\n./bin/cqlsh\n**nodetool**\n-   assassinate Forcefully remove a dead node without re-replicating any data. Use as a last resort if you cannot removenode\n-   bootstrap Monitor/manage node's bootstrap process\n-   cleanup Triggers the immediate cleanup of keys no longer belonging to a node. By default, clean all keyspaces\n-   clearsnapshot Remove the snapshot with the given name from the given keyspaces. If no snapshotName is specified we will remove all snapshots\n-   compact Force a (major) compaction on one or more tables or user-defined compaction on given SSTables\n-   compactionhistory Print history of compaction\n-   compactionstats Print statistics on compactions\n-   decommission Decommission the *node I am connecting to*\n-   describecluster Print the name, snitch, partitioner and schema version of a cluster\n-   describering Shows the token ranges info of a given keyspace\n-   disableautocompaction Disable autocompaction for the given keyspace and table\n-   disablebackup Disable incremental backup\n-   disablebinary Disable native transport (binary protocol)\n-   disablegossip Disable gossip (effectively marking the node down)\n-   disablehandoff Disable storing hinted handoffs\n-   disablehintsfordc Disable hints for a data center\n-   disablethrift Disable thrift server\n-   drain Drain the node (stop accepting writes and flush all tables)\n-   enableautocompaction Enable autocompaction for the given keyspace and table\n-   enablebackup Enable incremental backup\n-   enablebinary Reenable native transport (binary protocol)\n-   enablegossip Reenable gossip\n-   enablehandoff Reenable future hints storing on the current node\n-   enablehintsfordc Enable hints for a data center that was previsouly disabled\n-   enablethrift Reenable thrift server\n-   failuredetector Shows the failure detector information for the cluster\n-   flush Flush one or more tables\n-   garbagecollect Remove deleted data from one or more tables\n-   gcstats Print GC Statistics\n-   getcompactionthreshold Print min and max compaction thresholds for a given table\n-   getcompactionthroughput Print the MB/s throughput cap for compaction in the system\n-   getconcurrentcompactors Get the number of concurrent compactors in the system.\n-   **getendpoints Print the end points that owns the key**\n\nreturns the IP addresses of the node(s)which store the partitions withthe respective partition key value\n-   getinterdcstreamthroughput Print the Mb/s throughput cap for inter-datacenter streaming in the system\n-   getlogginglevels Get the runtime logging levels\n-   getsstables Print the sstable filenames that own the key\n-   getstreamthroughput Print the Mb/s throughput cap for streaming in the system\n-   gettimeout Print the timeout of the given type in ms\n-   gettraceprobability Print the current trace probability value\n-   gossipinfo Shows the gossip information for the cluster\n-   help Display help information\n-   **info Print node information (uptime, load, ...)**\n-   invalidatecountercache Invalidate the counter cache\n-   invalidatekeycache Invalidate the key cache\n-   invalidaterowcache Invalidate the row cache\n-   join Join the ring\n-   listsnapshots Lists all the snapshots along with the size on disk and true size.\n-   move Move node on the token ring to a new token\n-   netstats Print network information on provided host (connecting node by default)\n-   pausehandoff Pause hints delivery process\n-   proxyhistograms Print statistic histograms for network operations\n-   rangekeysample Shows the sampled keys held across all keyspaces\n-   rebuild Rebuild data by streaming from other nodes (similarly to bootstrap)\n-   rebuild_index A full rebuild of native secondary indexes for a given table\n-   refresh Load newly placed SSTables to the system without restart\n-   refreshsizeestimates Refresh system.size_estimates\n-   reloadlocalschema Reload local node schema from system tables\n-   reloadtriggers Reload trigger classes\n-   relocatesstables Relocates sstables to the correct disk\n-   removenode Show status of current node removal, force completion of pending removal or remove provided ID\n-   repair Repair one or more tables\n-   replaybatchlog Kick off batchlog replay and wait for finish\n-   resetlocalschema Reset node's local schema and resync\n-   resumehandoff Resume hints delivery process\n-   **ring Print information about the token ring**\n-   scrub Scrub (rebuild sstables for) one or more tables\n-   setcachecapacity Set global key, row, and counter cache capacities (in MB units)\n-   setcachekeystosave Set number of keys saved by each cache for faster post-restart warmup. 0 to disable\n-   setcompactionthreshold Set min and max compaction thresholds for a given table\n-   setcompactionthroughput Set the MB/s throughput cap for compaction in the system, or 0 to disable throttling\n-   setconcurrentcompactors Set number of concurrent compactors in the system.\n-   sethintedhandoffthrottlekb Set hinted handoff throttle in kb per second, per delivery thread.\n-   setinterdcstreamthroughput Set the Mb/s throughput cap for inter-datacenter streaming in the system, or 0 to disable throttling\n-   setlogginglevel Set the log level threshold for a given class. If both class and level are empty/null, it will reset to the initial configuration\n-   setstreamthroughput Set the Mb/s throughput cap for streaming in the system, or 0 to disable throttling\n-   settimeout Set the specified timeout in ms, or 0 to disable timeout\n-   settraceprobability Sets the probability for tracing any given request to value. 0 disables, 1 enables for all requests, 0 is the default\n-   snapshot Take a snapshot of specified keyspaces or a snapshot of the specified table\n-   **status Print cluster information (state, load, IDs, ...)**\n-   statusbackup Status of incremental backup\n-   statusbinary Status of native transport (binary protocol)\n-   statusgossip Status of gossip\n-   statushandoff Status of storing future hints on the current node\n-   statusthrift Status of thrift server\n-   stop Stop compaction\n-   **stopdaemon Stop cassandra daemon**\n-   tablehistograms Print statistic histograms for a given table\n-   tablestats Print statistics on tables\n-   toppartitions Sample and print the most active partitions for a given column family\n-   tpstats Print usage statistics of thread pools\n-   truncatehints Truncate all hints on the local node, or truncate hints for the endpoint(s) specified.\n-   upgradesstables Rewrite sstables (for the requested tables) that are not on the current version (thus upgrading them to said current version)\n-   verify Verify (check data checksum for) one or more tables\n-   version Print cassandra version\n-   viewbuildstatus Show progress of a materialized view build\n\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Cassandra/Consistency/","title":"Consistency"},"frontmatter":{"draft":false},"rawBody":"# Consistency\n\nCreated: 2019-04-21 13:17:45 +0500\n\nModified: 2019-12-30 16:06:43 +0500\n\n---\n\n**ANY**\n-   any server (may not be replica)\n-   Fastest: coordinator caches write and replies quickly to client\n\n**ALL**\n-   all replicas\n-   Ensures strong consistency, but slowest\n\n**ONE**\n-   at least one replica\n-   Faster than ALL, but cannot tolerate a failure\n\n**QUORUM**\n-   quorum across all replicas in all datacenters\n-   Majority > 50%\n-   Quorums faster than ALL, but still ensure strong consistency\n-   Several key-value/NoSQL stores (e.g., Riak and Cassandra) use quorums\n\n![Quorums in Detail â€¢ Several key-value/NoSQL stores (e.g., Riak and Cassandra) use quorums. Reads â€¢ Client specifies value of R (s N = total number of replicas of that key). R = read consistency lev Coordinator waits fo R replicas t respond before sending result to clien . â€¢ In background, coordinator checks for consistency of remaining (N-R replicas, and initiates read repair if needed. Big Data Computing Design of Key-Value Stores ](media/Cassandra_Consistency-image1.png)\n\n![Quorums in Detail (Contd..) â€¢ Writes come in two flavors â€¢ Client specifies W N) W = write consistency level. Client writes new value to W replicas and returns. Two flavors: â€¢ Coordinator blocks until quorum is reached. â€¢ Asynchronous: Just write and return. Big Data Computing Design of Key-Value Stores ](media/Cassandra_Consistency-image2.png)\n\n![Quorums in Detail (Contd.) â€¢ R = read replica count, W = write replica count â€¢ Two necessary conditions: 1. W+R>N 2. W>N/2 â€¢ Select values based on applicatâ€¢on â€¢ (W=l, R=l): very few writes and reads â€¢ (W=N, R=l): great for read-heavy workloads â€¢ (W=N/2+1, R=N/2+1): great for write-heavy workloads â€¢ (W=l, R=N): great for write-heavy workloads with mostly one client writing per key Big Data Computing Design of Key-Value Stores ](media/Cassandra_Consistency-image3.png)\n\n![Cassandra Consistency Levels (Contd.) â€¢ Client is allowed to choose a consistency level for each operation (read/write) ANY: any server (may not be replica) â€¢ Fastest: coordinator may cache write and reply quickly to client ALL: all replicas â€¢ Slowest, but ensures strong consistency ONE: at least one replica â€¢ Faster than ALL, and ensures durability without failures QUORUM: quorum across all replicas in all datacenters (DCs) â€¢ Global consistency, but still fast â€¢ LOCAL QUORUM: quorum in coordinator's DC â€¢ Faster: only waits for quorum in first DC client contacts â€¢ EACH_QUORUM: quorum in every DC â€¢ Lets each DC do its own quorum: supports hierarchical replies Big Data Computing Design of Key-Value Stores ](media/Cassandra_Consistency-image4.png)\n\n![Types of Consistency â€¢ Cassandra offers Eventual Consistency â€¢ Are there other types of weak consistency models? Big Data Computing Design of Key-Value Stores ](media/Cassandra_Consistency-image5.png)\n\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Cassandra/Data-Model/","title":"Data Model"},"frontmatter":{"draft":false},"rawBody":"# Data Model\n\nCreated: 2019-12-27 19:18:38 +0500\n\nModified: 2020-01-07 22:00:50 +0500\n\n---\n-   **Partition Key**\n-   **Clustering Key**\n**Querying Clustering Columns**\n-   You must first provide a partition key\n-   Clustering columns can follow thereafter\n-   You can perform either equality (=) or range queries (<, >) on clustering columns\n-   All equality comparisions must come before inequality comparisions\n-   Since data is sorted on disk, range searches are a binary search followed by a linear read\n![C* Data Model Keyspace C lu n Famil Big Data Computing Column Famil ](media/Cassandra_Data-Model-image1.png)\n\n![C* Data Model (Contd.) Row K olumn Column Name Column Value (or Tombstone) Timestamp Time-to-live Big Data Computing Row Key, Column Name, Column Value types Column Name has comparator RowKey has partitioner Rows can have any number of columns - even in same column family Rows can have many columns Column Values can be omitted Time-to-live is useful! Tombstones ](media/Cassandra_Data-Model-image2.png)\n\n![C* Data Model: Writes- Mem CommitLog Row Bloom Filter Big Data Computing Insert into MemTable Dump to CommitLog o rea Very Fast! Blocks on CPU before 0/1! SSTable SSTable SSTable CQL ](media/Cassandra_Data-Model-image3.png)\n\n![C* Data Model: ReaÃŸs Mem CommitLog Row Bloom Filter Big Data Computing Get values from Memtable Get values from row cache if present Otherwise check bloom filtKr to find appropriate SSTables Check Key CachÃ© for fast SSTable Search Get values from SSTabIes Repopulate Row Cache Super Fast Col. retri al Fast row slicing SSIable STable STable ](media/Cassandra_Data-Model-image4.png)\n\n![C* Data Model: Reads (Contd.) Get values from Memtable Get values from row cache if present Otherwise check bloom filter to Mem find appropriate SSTables Table Check Key Cache for fast SSTable Search Get values from SSTabIes CommitLog Repopulate Row Cache Super Fast Col. retrieval Row Fast row slicing SSTable ey Bloom SSTable Filter SSTable Big Data Computing ](media/Cassandra_Data-Model-image5.png)\n**Bucketing**\n\nBucketing is a strategy that lets us control how much data is stored in each partition as well as spread writes out to the entire cluster.\nTo break up this big partition, we'll leverage our first form of bucketing. We'll break our partitions into smaller ones based on time window. The ideal size is going to keep partitions under 100MB. For example, one partition per sensor per day would be a good choice if we're storing 50-75MB of data per day. We could just as easily use week (starting from some epoch), or month and year as long as the partitions stay under 100MB. Whatever the choice, leaving a little headroom for growth is a good idea.\nA variation on this technique is to use a different table per time window. For instance, using a table per month means you'd have twelve tables per year\nThis strategy has a primary benefit of being useful for archiving and quickly dropping old data. For instance, at the beginning of each month, we could archive last month's data to HDFS or S3 in parquet format, taking advantage of cheap storage for analytics purposes. When we don't need the data in Cassandra anymore, we can simply drop the table. You can probably see there's a bit of extra maintenance around creating and removing tables, so this method is really only useful if archiving is a requirement. There are other methods to archive data as well, so this style of bucketing may be unnecessary.\nThe second technique uses multiple partitions at any given time to fan out inserts to the entire cluster. The nice part about this strategy is we can use a single partition for low volume, and many partitions for high volume.\n<https://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html>\n\n## Others**\n\n<https://shermandigital.com/blog/designing-a-cassandra-data-model>"},{"fields":{"slug":"/Databases/NoSQL-Databases/Cassandra/Design/","title":"Design"},"frontmatter":{"draft":false},"rawBody":"# Design\n\nCreated: 2019-04-21 13:14:47 +0500\n\nModified: 2020-01-04 11:38:21 +0500\n\n---\n\n**Partitioner**\n-   Token Value Distribution\n    -   +2^63^ - 1 to -2^63^\n**Membership**\n-   Any server in cluster could be the coordinator\n-   So every server needs to maintain a list of all the other servers that are currently in the server\n-   List needs to be updated automatically as servers join, leave, and fail\n-   Cassandra uses gossip-based cluster membership\n    -   Nodes periodically gossip their membership list\n    -   On receipt, the local membership list is updated\n    -   If any heartbeat older than Tfail, node is marked as failed\n**Joining the cluster**\n-   Nodes join the cluster by communicating with any node\n-   Cassandra finds these nodes list of possible nodes in cassandra.yaml\n-   Seed nodes communicate cluster topology to the joining node\n-   Once the new node joins the cluster, all nodes are peers\n-   States of nodes\n    -   Joining\n    -   Leaving\n    -   Up\n    -   Down-   Cassandra uses a Ring-based DHT but without finger tables or routing\n**Drivers**\n-   Drivers intelligently choose which node would best coordinate a request\n-   Per-query basis:\n\nResultSet results = session.execute(\"[query]\");-   TokenAwarePolicy - driver chooses node which contains the data\n-   RoundRobinPolicy - driver round robins the ring\n-   DCAwareRoundRobinPolicy - driver round robins the target data center\n**Data Placement Strategies**\n-   Replication Strategy\n    -   **SimpleStrategy**\n\nUses the partitioner, of which there are two kinds\n\n1.  **RandomPartitioner**: Chord-like hash partitioning\n\n2.  **ByteOrderedPartitioner**: Assigns ranges of keys to servers\n    -   Easier for range queries (e.g., Get me all twitter users starting with [a-b])\n-   **NetworkTopologyStrategy**: for multi-DC deployments\n    -   Two replicas per DC\n    -   Three replicas per DC\n    -   Per DC\n        -   First replica placed according to Partitioner\n        -   Then go clockwise around ring until you hit a different rack\n**Snitches**\n-   **Maps:** IPs to racks and DCs. Configured in cassandra.yaml config file\n-   Some options:\n    -   **SimpleSnitch:** Unaware of Topology (Rack-unaware)\n    -   **RackInferring:** Assumes topology of network by octet of server's IP address\n        -   101.102.103.104 = x.< DC octet >.< rack octet >.< node octet >\n    -   **PropertyFileSnitch:** uses a config file\n    -   **EC2Snitch:** uses EC@\n        -   EC2 Region = DC\n        -   Availability zone = rack\n    -   Other snitch options available\n**Suspicion mechanisms**\n-   Suspicion mechanisms to adaptively set the timeout based on underlying network and failure behavior\n-   **Accural detector:** Failure Detector outputs a value (PHI) representing suspicion\n-   Applications set an appropriate threshold\n-   PHI calculation for a member\n    -   Inter-arrival times for gossip messages\n    -   PHI(t) = -log(CDF or Probability(t_now - t_last))/log 10\n    -   PHI basically determines the detection timeout, but takes into account historical inter-arrival time variations for gossiped heatbeats\n-   In practice, PHI = 5 => 10-15 sec detection time\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Cassandra/Drivers---Clients/","title":"Drivers / Clients"},"frontmatter":{"draft":false},"rawBody":"# Drivers / Clients\n\nCreated: 2019-12-29 12:51:28 +0500\n\nModified: 2019-12-29 16:25:34 +0500\n\n---\n```\npip install cassandra-driver\nfrom cassandra.cluster import Cluster\n\ncluster = Cluster(protocol_version = 3)\n\nsession = cluster.connect('killrvideo')\nfor val in session.execute(\"select * from videos_by_tag\"):\n\nprint(val)\n```"},{"fields":{"slug":"/Databases/NoSQL-Databases/Cassandra/Questions/","title":"Questions"},"frontmatter":{"draft":false},"rawBody":"# Questions\n\nCreated: 2019-12-27 13:39:39 +0500\n\nModified: 2020-01-07 23:12:53 +0500\n\n---\n\n<https://www.edureka.co/blog/interview-questions/cassandra-interview-questions>\nWhich of the following is *not* a feature of Cassandra?\n-   No single point of failure\n-   High availability\n-   **ACID compliance**\n-   Linear scalability\nCassandra uses a master/slave architecture.\n\nFalse\nCassandra supports multiple datacenters out of the box.\n\nTrue\nWhich of the following statements about Cassandra is false?\n-   All nodes hold data and can answer queries\n-   **A master node determines who gets what data**\n-   Data is replicated on different nodes\n-   Data is partitioned around the ring\nWith the CAP theorum, Cassandra is:\n-   **Available and partition tolerant**\n-   Consistent and partition tolerant\n-   Available and consistent\n-   None of the above\nReplication Factor (RF) is how many copies of each piece of data should you have in your cluster.\n\nTrue\nWhere is Replication Factor (RF) set in Cassandra?\n-   **When creating a keyspace**\n-   When creating a table\n-   On a per-query basis\n-   In Cassandra configuration file\nHinted handoff is used when a node is down to replay all of the writes that occured.\n\nTrue\nConsistency level can be set for:\n-   Read requests\n-   Write requests\n-   **Both read and write requests**\n-   Is not configurable\nRead and write speed is *not* impacted by the consistency level chosen.\n\nFalse\nWhere is Consistency Level (CL) set in Cassandra?\n-   **On a per-query basis**\n-   When creating a table\n-   When creating a keyspace\n-   Is not configurable\nAny node in a cluster can service a write request.\n\nTrue\nWhat is the node that handles a request called?\n-   Leader node\n-   Master node\n-   Slave node\n-   **Coordinator node**\nThe order for the write path in Cassandra is:\n-   Commit log to SSTable to MemTable\n-   **Commit log to MemTable to SSTable**\n-   Commit log to MemTable to DiskTable\n-   None of the above\nCassandra does not do any writes or deletes in place.\n\nTrue\nSSTables in Cassandra are *not* immutable.\n\nFalse\nCompaction is the process of:\n-   **Taking small SSTables and merges them into bigger ones.**\n-   Taking Memtables and merging them into SSTables.\n-   Merging Memtables into larger Memtables.\n-   Merging Memtables into larger Memtables.\n\"Last write wins\" uses what to determine what data to return to client?\n-   Tombstones\n-   **Timestamps**\n-   Compaction\n-   Coordinator\nPartitions...\n-   format data for CQL queries.\n-   maintain a log of all executed queries.\n-   optimize query-time joins keeping frequently queried rows together on disk.\n-   **group rows physically together on disk based on the partition key.**\nWhat is the role of the partitioner?\n-   It organizes the data together physically on disk.\n-   **It hashes the partition key values to create a partition token.**\n-   It retrieves partition data for a CQL query.\n-   It physically separates rows into equal-sized partitions.\nPartition key columns are optional if you have clustering columns.\n\nFalse\nWhat benefits do Clustering columns provide?\n-   **Reading sorted data is a matter of seeking the disk head once.**\n-   They allow partitions to be spread over several drives.\n-   You can change the clustering criteria on a table at any time.\n-   The optimize writes by shuffling the data on disk at write time.\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Cassandra/Working/","title":"Working"},"frontmatter":{"draft":false},"rawBody":"# Working\n\nCreated: 2019-04-21 11:52:25 +0500\n\nModified: 2020-01-07 22:08:50 +0500\n\n---\n\n**Writes**\n-   Need to be lock-free and fast (no reads or disk seeks)\n-   Client sends write to one coordinator node in Cassandra cluster\n    -   Coordinator may be per-key, or per-client or per-query\n    -   Per-key coordinator ensures writes for the key are serialized\n-   Coordinator uses Partitioner to send query to all replica nodes responsible for key\n-   When X replicas respond, coordinator returns an acknowledgement to the client-   **Always writable: Hinted Handoff mechanism**\n    -   If any replica is down, the coordinator writes to all other replicas, and keeps the write locally until down replica comes back up\n    -   When all replicas are down, the Coordinator (front end) buffers write (for up to a few hours)-   **One ring per datacenter**\n    -   Per-DC coordinator elected to coordinate with other DCs\n    -   Election done via Zookeeper, which runs a Paxos (consensus) variant-   **Writes at a replica node**\n\n    1.  Log it in disk commit log (for failure recovery)\n\n    2.  Make changes to appropriate memtables\n        -   Memtable = In-memory representation of multiple key-value pairs\n        -   Typically append-only datastructure (fast)\n        -   Cache that can be searched by key\n        -   Write-back as opposed to write-through\n    -   Later When memtable is full or old, flush to disk\n        -   Data File: An SSTable (Sorted String Table) - list of key-value pairs, sorted by key\n        -   SSTables are immutable (once created, they don't change)\n        -   Index file: An SSTable of (key, position in data sstable) pairs\n        -   And a Bloom Filter (for efficient search)\n**The Write Path**\n-   Writes are written to any node in the cluster (coordinator)\n-   Writes are written to commit log, then to memtable\n-   Every write includes a timestamp\n-   Memtable flushed to disk periodically (sstable)\n-   New memtable is created in memory\n-   Deletes are a special write case, called a \"tombstone\"\n![client Write data Memory Disk 10 Cornnt log rrerntable INDEX Hush SSTable ](media/Cassandra_Working-image1.png)\n**What is an SSTable**\n-   Immutable data file for row storage\n-   Every write includes a timestamp of when it was written\n-   Partition is spread across multiple SSTables\n-   Same column can be in multiple SSTables\n-   Merged through compaction, only latest timestamp is kept\n-   Deletes are written as tombstones\n-   Easy backups\n**The Read Path**\n-   Any server may be queried, it acts as the coordinator\n-   Contacts nodes with the requested key\n-   On each node, data is pulled from SSTables and merged\n-   Consistency < All performs read repair in background (read_repair_chance -default 10% of reads)\n![](media/Cassandra_Working-image2.png)**Compaction**\n-   Data updates accumulate over time and SSTables and logs need to be compacted\n    -   The process of compaction merges SSTables, i.e., by merging updates for a key\n    -   Run periodically and locally at each server-   TimeWindowCompactionStrategy\n\n<https://thelastpickle.com/blog/2016/12/08/TWCS-part1.html>\n\n## Deletes**: don't delete item right away\n-   Add a tombstone to the log\n-   Eventually, when compaction encounters tombstone it will delete item\n**Reads**: Similar to writes, expect\n-   Coordinator can contact X replicas (e.g., in same rack)\n    -   Coordinator sends read to replicas that have responded quickest in past\n    -   When X replicas respond, coordinator returns the latest timestamped value from among those X\n-   Coordinator also fetches value from other replicas\n    -   Checks consistency in the background, initiating a read repair if any two values are different\n    -   This mechanism seeks to eventually bring all replicas up to date\n-   At a replica\n    -   A row may be split across SSTables => reads need to touch multiple SSTables => reads slower than writes (but still fast)"},{"fields":{"slug":"/Databases/NoSQL-Databases/Druid/Architecture/","title":"Architecture"},"frontmatter":{"draft":false},"rawBody":"# Architecture\n\nCreated: 2019-12-18 22:42:35 +0500\n\nModified: 2021-01-29 02:07:01 +0500\n\n---\n\nDruid has a multi-process, distributed architecture that is designed to be cloud-friendly and easy to operate. Each Druid process type can be configured and scaled independently, giving you maximum flexibility over your cluster. This design also provides enhanced fault tolerance: an outage of one component will not immediately affect other components.\nDruid's process types are:\n-   [**Historical**](http://druid.io/docs/latest/design/historical.html)processes are the workhorses that handle storage and querying on \"historical\" data (including any streaming data that has been in the system long enough to be committed). Historical processes download segments from deep storage and respond to queries about these segments. They don't accept writes.\n    -   Each Historical process serves up data that has been partitioned into segments. These segments are assigned to Historical by the Coordinator via ZooKeeper\n    -   When a Historical process is assigned a segment, it will copy the file from deep storage to its local storage\n    -   When a query is received from the Broker process, the Historical process returns the results-   [**MiddleManager**](http://druid.io/docs/latest/design/middlemanager.html)processes handle ingestion of new data into the cluster. They are responsible for reading from external data sources and publishing new Druid segments.\n    -   The MiddleManager process is a worker process that executes submitted tasks. Middle Managers forward tasks to Peons that run in separate JVMs. The reason we have separate JVMs for tasks is for resource and log isolation. Each[Peon](https://druid.apache.org/docs/latest/design/peons.html)is capable of running only one task at a time, however, a MiddleManager may have multiple Peons.\n    -   During real-time ingestion, the MiddleManager also serves queries on real-time data before it has been pushed to deep storage.\n    -   When a query is received from the Broker process, the MiddleManager process executes that query on real-time data and returns results.-   [**Broker**](http://druid.io/docs/latest/design/broker.html)processes receive queries from external clients and forward those queries to Historicals and MiddleManagers. When Brokers receive results from those subqueries, they merge those results and return them to the caller. End users typically query Brokers rather than querying Historicals or MiddleManagers directly.\n    -   Broker process is responsible for knowing the internal state of the cluster (from the ZooKeeper)\n    -   The broker finds out information from ZooKeeper about the Druid cluster\n        -   Which Historical processes are serving which segments\n        -   Which MiddleManager processes are serving which tasks' data\n        -   When a query is run, the Broker will figure out which process to contact-   [**Coordinator**](http://druid.io/docs/latest/design/coordinator.html)processes watch over the Historical processes. They are responsible for assigning segments to specific servers, and for ensuring segments are well-balanced across Historicals.\n    -   Segment management and distribution\n    -   It communicates with the Historical nodes to:\n        -   **Load -** Copy a segment from deep storage and start serving it\n        -   **Drop -** Delete a segment from its local copy and stop serving it-   [**Overlord**](http://druid.io/docs/latest/design/overlord.html)processes watch over the MiddleManager processes and are the controllers of data ingestion into Druid. They are responsible for assigning ingestion tasks to MiddleManagers and for coordinating segment publishing.\n    -   Accepting ingestion supervisors and tasks\n    -   Coordinating which servers run which tasks\n    -   Managing locks so tasks don't conflict with each other\n    -   Returning supervisor and task status to callers-   [**Router**](http://druid.io/docs/latest/development/router.html)processes areoptionalprocesses that provide a unified API gateway in front of Druid Brokers, Overlords, and Coordinators. They are optional since you can also simply contact the Druid Brokers, Overlords, and Coordinators directly.\n\n![Diagram](media/Druid_Architecture-image1.png)\n<https://docs.imply.io/cloud/design>\nDruid processes can be deployed individually (one per physical server, virtual server, or container) or can be colocated on shared servers. One common colocation plan is a three-type plan:\n\n1.  **\"Data\"** servers run Historical and MiddleManager processes.\n\n2.  **\"Query\"** servers run Broker and (optionally) Router processes.\n\n3.  **\"Master\"** servers run Coordinator and Overlord processes. They may run ZooKeeper as well.\nIn addition to these process types, Druid also has three external dependencies. These are intended to be able to leverage existing infrastructure, where present.\n-   **[Deep storage](http://druid.io/docs/latest/design/index.html#deep-storage),** shared file storage accessible by every Druid server. This is typically going to be a distributed object store like S3 or HDFS, cassandra, Google Cloud Storage or a network mounted filesystem. Druid uses this to store any data that has been ingested into the system.\n-   [**Metadata store**](http://druid.io/docs/latest/design/index.html#metadata-storage), shared metadata storage. This is typically going to be a traditional RDBMS like PostgreSQL or MySQL.\n-   [**ZooKeeper**](http://druid.io/docs/latest/design/index.html#zookeeper)is used for internal service discovery, coordination, and leader election.\nThe idea behind this architecture is to make a Druid cluster simple to operate in production at scale. For example, the separation of deep storage and the metadata store from the rest of the cluster means that Druid processes are radically fault tolerant: even if every single Druid server fails, you can still relaunch your cluster from data stored in deep storage and the metadata store.\nThe following diagram shows how queries and data flow through this architecture:\n\n![Coordinator Nodes Overlord Nodes Druid nodes External Dependencies Metadata Data/Segments Client Queries Streaming Data Nodes MiddleManager Nodes 1 1 1 Deep Storage Metadata Storage Zookeeper Historical Broker Nodes Batch ](media/Druid_Architecture-image2.png)"},{"fields":{"slug":"/Databases/NoSQL-Databases/Druid/Cheatsheet/","title":"Cheatsheet"},"frontmatter":{"draft":false},"rawBody":"# Cheatsheet\n\nCreated: 2019-11-20 00:28:15 +0500\n\nModified: 2019-11-20 00:28:52 +0500\n\n---\n\n| Common                                          |                                                                                                                        |\n|-----------------------|-------------------------------------------------|\n| /status                                         | Returns the Druid version, loaded extensÂ­ions, memory used, total memory and other useful informÂ­ation about the process. |\n| /statuÂ­s/hÂ­ealth                                  | Always returns a boolean \"Â­truÂ­e\" value with a 200 OK response, useful for automated health checks.                     |\n| /statuÂ­s/pÂ­ropÂ­erties                              | Returns the current configÂ­uration properties of the process.                                                            |\n| These endpoints are supported by all processes. |                                                                                                                        |\n\n| Master Server            |                               |                                                                  |\n|------------------|--------------------|----------------------------------|\n| CoordiÂ­nator - Leadership |                               |                                                                  |\n| GET                      | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/leader   | Returns the current leader CoordiÂ­nator of the cluster.            |\n| GET                      | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/isÂ­Leader | Returns a JSON object with field \"Â­leaÂ­derÂ­\", either true or false |\n\n| Master Server   |                                        |                                                                                      |\n|------------|----------------------|--------------------------------------|\n| Segment Loading |                                        |                                                                                      |\n| GET             | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/loÂ­adsÂ­tatus        | Returns the percentage of segments actually loaded in the cluster                     |\n| GET             | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/loÂ­adsÂ­tatÂ­us?Â­simple | Returns the number of segments left to load in each tier                              |\n| GET             | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/loÂ­adqueue         | Returns the ids of segments to load and drop for each Historical process.             |\n| GET             | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/loÂ­adqÂ­ueuÂ­e?sÂ­imple  | Returns the number of segments to load and drop                                       |\n| GET             | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/loÂ­adqÂ­ueuÂ­e?full    | Returns the serialized JSON of segments to load and drop for each Historical process. |\n\n| Master Server |                                                                                 |                                                                                                                         |\n|----------|----------------------------|----------------------------------|\n| Metadata info |                                                                                 |                                                                                                                         |\n| GET           | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/meÂ­tadÂ­ataÂ­/daÂ­tasÂ­ources                                       | Returns a list of the names of enabled datasoÂ­urces in the cluster.                                                       |\n| GET           | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/meÂ­tadÂ­ataÂ­/daÂ­tasÂ­ourÂ­cesÂ­?inÂ­cluÂ­deDÂ­isabled                       | Returns a list of the names of enabled and disabled datasoÂ­urces in the cluster.                                          |\n| GET           | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/meÂ­tadÂ­ataÂ­/daÂ­tasÂ­ourÂ­cesÂ­?full                                  | Returns a list of all enabled datasoÂ­urces with all metadata about those datasoÂ­urces as stored in the metadata store.     |\n| GET           | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/meÂ­tadÂ­ataÂ­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­Name}                      | Returns full metadata for a datasource as stored in the metadata store.                                                  |\n| GET           | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/meÂ­tadÂ­ataÂ­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­segÂ­ments             | Returns a list of all segments for a datasource as stored in the metadata store.                                         |\n| GET           | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/meÂ­tadÂ­ataÂ­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­segÂ­menÂ­ts?full        | Returns a list of all segments for a datasource with the full segment metadata as stored in the metadata store.          |\n| GET           | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/meÂ­tadÂ­ataÂ­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­segÂ­menÂ­ts/Â­{seÂ­gmeÂ­ntId} | Returns full segment metadata for a specific segment as stored in the metadata store.                                    |\n| POST          | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/meÂ­tadÂ­ataÂ­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­segÂ­ments             | Returns a list of all segments, overlaÂ­pping with any of given intervals                                                  |\n| POST          | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/meÂ­tadÂ­ataÂ­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­segÂ­menÂ­ts?full        | Returns a list of all segments, overlaÂ­pping with any of given intervals, for a datasource with the full segment metadata |\n\n| Master Server                                                                                                              |                                                                                   |                                                                                                                                                                           |\n|------------------------|-------------------|------------------------------|\n| DatasoÂ­urces                                                                                                                |                                                                                   |                                                                                                                                                                           |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ources                                                  | Returns a list of datasource names found in the cluster.                                                                                                                   |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­?simple                                           | Returns a list of JSON objects containing the name and properties of datasoÂ­urces found in the cluster.                                                                     |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­?full                                             | Returns a list of datasource names found in the cluster with all metadata about those datasoÂ­urces.                                                                         |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­Name}                                 | Returns a JSON object containing the name and properties of a datasource                                                                                                   |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}?full                            | Returns full metadata for a datasource .                                                                                                                                   |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­intÂ­ervals                       | Returns full metadata for a datasource .                                                                                                                                   |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­intÂ­ervals                       | Returns a set of segment intervals.                                                                                                                                        |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­intÂ­ervÂ­alsÂ­?simple                | Returns a map of an interval to a JSON object containing the total byte size of segments and number of segments for that interval.                                         |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­intÂ­ervÂ­alsÂ­?full                  | Returns a map of an interval to a map of segment metadata to a set of server names that contain the segment for that interval.                                             |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­intÂ­ervÂ­alsÂ­/{iÂ­nteÂ­rval}            | Returns a set of segment ids for an interval.                                                                                                                              |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­intÂ­ervÂ­alsÂ­/{iÂ­nteÂ­rvaÂ­l}?Â­simple     | Returns a map of segment intervals contained within the specified interval to a JSON object                                                                                |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­intÂ­ervÂ­alsÂ­/{iÂ­nteÂ­rvaÂ­l}?full       | Returns a map of segment intervals contained within the specified interval to a map of segment metadata to a set of server names that contain the segment for an interval. |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­intÂ­ervÂ­alsÂ­/{iÂ­nteÂ­rvaÂ­l}/Â­serÂ­verview | Returns a map of segment intervals contained within the specified interval to informÂ­ation about the servers that contain the segment for an interval.                      |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­segÂ­ments                        | Returns a list of all segments for a datasource in the cluster.                                                                                                            |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­segÂ­menÂ­ts?full                   | Returns a list of all segments for a datasource in the cluster with the full segment metadata.                                                                             |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­segÂ­menÂ­ts/Â­{seÂ­gmeÂ­ntId}            | Returns full segment metadata for a specific segment in the cluster.                                                                                                       |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­tiers                           | Return the tiers that a datasource exists in.                                                                                                                              |\n| POST                                                                                                                       | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­Name}                                 | Enables all segments of datasource which are not overshÂ­adowed by others.                                                                                                   |\n| POST                                                                                                                       | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­segÂ­menÂ­ts/Â­{seÂ­gmeÂ­ntId}            | Enables a segment of a datasoÂ­urce.                                                                                                                                         |\n| DELETE                                                                                                                     | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­Name}                                 | Disables a datasoÂ­urce.                                                                                                                                                     |\n| DELETE                                                                                                                     | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­intÂ­ervÂ­alsÂ­/{iÂ­nteÂ­rval}            | Runs a Kill task for a given interval and datasoÂ­urce.                                                                                                                      |\n| DELETE                                                                                                                     | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­segÂ­menÂ­ts/Â­{seÂ­gmeÂ­ntId}            | Disables a segment.                                                                                                                                                        |\n| Note that all interval URL parameters are ISO 8601 strings delimited by a _ instead of a / (e.g., 2016-0Â­6-2Â­7_2Â­016Â­-06Â­-28). |                                                                                   |                                                                                                                                                                           |\n\n| Master Server                                                                                                              |                                                                            |                                                                                                                                                          |\n|-------------------------|------------------|-----------------------------|\n| Retention Rules                                                                                                            |                                                                            |                                                                                                                                                          |\n| GET                                                                                                                        | Retention Rules                                                             | Returns all rules as JSON objects for all datasoÂ­urces in the cluster including the default datasoÂ­urce.                                                    |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/ruÂ­lesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­Name}                                | Returns all rules for a specified datasoÂ­urce.                                                                                                             |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/ruÂ­lesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}?full                           | Returns all rules for a specified datasource and includes default datasoÂ­urce.                                                                             |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/ruÂ­lesÂ­/hiÂ­stoÂ­ry?Â­intÂ­ervÂ­al=Â­                               | Returns audit history of rules for all datasoÂ­urces                                                                                                        |\n|                                                                                                                           |                                                                            | default value of interval can be specified by setting druid.aÂ­udÂ­it.mÂ­anÂ­ageÂ­r.aÂ­udiÂ­tHiÂ­stoÂ­ryMÂ­illis (1 week if not configÂ­ured) in CoordiÂ­nator runtimÂ­e.pÂ­ropÂ­erties |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/ruÂ­lesÂ­/hiÂ­stoÂ­ry?Â­couÂ­nt=Â­                                  | Returns last entries of audit history of rules for all datasoÂ­urces.                                                                                       |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/ruÂ­lesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­hisÂ­torÂ­y?iÂ­nteÂ­rvaÂ­l=[Â­intÂ­ervÂ­al] | Returns audit history of rules for a specified datasource                                                                                                 |\n|                                                                                                                           |                                                                            | default value of interval can be specified by setting druid.aÂ­udÂ­it.mÂ­anÂ­ageÂ­r.aÂ­udiÂ­tHiÂ­stoÂ­ryMÂ­illis (1 week if not configÂ­ured) in CoordiÂ­nator runtimÂ­e.pÂ­ropÂ­erties |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/ruÂ­lesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­hisÂ­torÂ­y?cÂ­ounÂ­t=[Â­n]           | Returns last entries of audit history of rules for a specified datasoÂ­urce.                                                                                |\n| POST                                                                                                                       | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/ruÂ­lesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­Name}                                | POST with a list of rules in JSON form to update rules.                                                                                                   |\n| Note that all interval URL parameters are ISO 8601 strings delimited by a _ instead of a / (e.g., 2016-0Â­6-2Â­7_2Â­016Â­-06Â­-28). |                                                                            |                                                                                                                                                          |\n\n| Master Server                                                                                                              |                                                  |                                                                                             |\n|-------------------------------|-----------------|------------------------|\n| Intervals                                                                                                                  |                                                  |                                                                                             |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/inÂ­tervals                   | Returns all intervals for all datasoÂ­urces with total size and count.                         |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/inÂ­terÂ­valÂ­s/{Â­intÂ­erval}        | Returns aggregated total size and count for all intervals that intersect given isointÂ­erval.  |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/inÂ­terÂ­valÂ­s/{Â­intÂ­ervÂ­al}Â­?simple | Returns total size and count for each interval within given isointÂ­erval.                     |\n| GET                                                                                                                        | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/inÂ­terÂ­valÂ­s/{Â­intÂ­ervÂ­al}Â­?full   | Returns total size and count for each datasource for each interval within given isointÂ­erval. |\n| Note that all interval URL parameters are ISO 8601 strings delimited by a _ instead of a / (e.g., 2016-0Â­6-2Â­7_2Â­016Â­-06Â­-28). |                                                  |                                                                                             |\n\n| Master Server                                           |                                                                                       |                                                                                                              |\n|------------------|--------------------------|-----------------------------|\n| Compaction ConfigÂ­uration                                |                                                                                       |                                                                                                              |\n| GET                                                     | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/coÂ­nfiÂ­g/cÂ­ompÂ­action                                                | Returns all compaction configs.                                                                               |\n| GET                                                     | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/coÂ­nfiÂ­g/cÂ­ompÂ­actÂ­ionÂ­/{dÂ­ataÂ­Source}                                   | Returns a compaction config of a dataSoÂ­urce.                                                                  |\n| POST                                                    | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/coÂ­nfiÂ­g/cÂ­ompÂ­actÂ­ionÂ­/taÂ­sksÂ­lotÂ­s?rÂ­atiÂ­o={Â­somÂ­eRaÂ­tioÂ­}&Â­maxÂ­={sÂ­omeÂ­MaxÂ­Slots} | Update the capacity for compaction tasks. ratio and max are used to limit the max number of compaction tasks. |\n| POST                                                    | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/coÂ­nfiÂ­g/cÂ­ompÂ­action                                                | Creates or updates the compaction config for a dataSoÂ­urce.                                                    |\n| DELETE                                                  | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/coÂ­nfiÂ­g/cÂ­ompÂ­actÂ­ionÂ­/{dÂ­ataÂ­Source}                                   | Removes the compaction config for a dataSoÂ­urce.                                                               |\n| See Compaction ConfigÂ­uration for configÂ­uration details. |                                                                                       |                                                                                                              |\n\n| Master Server      |                                     |                                                                                                                                                                                                                                                                  |\n|-----------|--------------|------------------------------------------------|\n| Server InformÂ­ation |                                     |                                                                                                                                                                                                                                                                  |\n| GET                | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/seÂ­rvers        | Returns a list of servers URLs using the format {hostnÂ­ameÂ­}:{Â­port}.                                                                                                                                                                                                |\n| GET                | /druidÂ­/coÂ­ordÂ­inaÂ­torÂ­/v1Â­/seÂ­rveÂ­rs?Â­simple | Returns a list of server data objects in which each object has the following keys: host: host URL include ({hostÂ­namÂ­e}:Â­{port}) type: process type (indexÂ­er-Â­exeÂ­cutor, historÂ­ical) currSize: storage size currently used maxSize: maximum storage size priority tier |\n\n| Overlord   |                           |                                                                       |\n|----------|--------------------|------------------------------------------|\n| Leadership |                           |                                                                       |\n| GET        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­leader   | Returns the current leader Overlord of the cluster.                    |\n| GET        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­isLÂ­eader | This returns a JSON object with field \"Â­leaÂ­derÂ­\", either true or false |\n\n| Overlord |                                                            |                                                                                                               |\n|---------|-------------------------|--------------------------------------|\n| Tasks    |                                                            |                                                                                                               |\n| GET      | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­tasks                                     | Retrieve list of tasks. Accepts query string parameters state, datasoÂ­urce, createÂ­dTiÂ­meIÂ­nteÂ­rval, max, and type. |\n| GET      | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­comÂ­pleÂ­teTasks                             | Retrieve list of complete tasks. Equivalent to /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­tasÂ­ks?Â­staÂ­te=Â­comÂ­plete.                         |\n| GET      | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­runÂ­ninÂ­gTasks                              | Retrieve list of running tasks. Equivalent to /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­tasÂ­ks?Â­staÂ­te=Â­runÂ­ning.                           |\n| GET      | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­waiÂ­tinÂ­gTasks                              | Retrieve list of waiting tasks. Equivalent to /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­tasÂ­ks?Â­staÂ­te=Â­waiÂ­ting.                           |\n| GET      | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­penÂ­dinÂ­gTasks                              | Retrieve list of pending tasks. Equivalent to /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­tasÂ­ks?Â­staÂ­te=Â­penÂ­ding.                           |\n| GET      | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­tasÂ­k/{Â­taskId}                             | Retrieve the 'payload' of a task.                                                                            |\n| GET      | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­tasÂ­k/{Â­tasÂ­kIdÂ­}/sÂ­tatus                      | Retrieve the status of a task.                                                                                 |\n| GET      | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­tasÂ­k/{Â­tasÂ­kIdÂ­}/sÂ­egments                    | Retrieve informÂ­ation about the segments of a task.                                                             |\n| GET      |                                                            | This API is deprecated and will be removed in future releases.                                                 |\n| GET      | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­tasÂ­k/{Â­tasÂ­kIdÂ­}/rÂ­eports                     | Retrieve a task completion report for a task. Only works for completed tasks.                                  |\n| POST     | /druidÂ­/inÂ­dexÂ­er/Â­v1/task                                      | Endpoint for submitting tasks and supervisor specs to the Overlord. Returns the taskId of the submitted task.  |\n| POST     | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­tasÂ­k/{Â­tasÂ­kIdÂ­}/sÂ­hutdown                    | Shuts down a task.                                                                                             |\n| POST     | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­datÂ­asoÂ­urcÂ­es/Â­{daÂ­taSÂ­ourÂ­ce}Â­/shÂ­utdÂ­ownÂ­AllÂ­Tasks | Shuts down all tasks for a dataSoÂ­urce.                                                                         |\n| POST     | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­tasÂ­kStatus                                | Retrieve list of task status objects for list of task id strings in request body.                              |\n| DELETE   | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­penÂ­dinÂ­gSeÂ­gmeÂ­ntsÂ­/{dÂ­ataÂ­Source}              | Manually clean up pending segments table in metadata storage for datasoÂ­urce. Returns a JSON object.            |\n\n| Overlord    |                                                         |                                                                                                 |\n|---------|-------------------------|---------------------------------------|\n| SupervÂ­isors |                                                         |                                                                                                 |\n| GET         | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervisor                             | Returns a list of strings of the currently active supervisor ids.                                |\n| GET         | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r?full                        | Returns a list of objects of the currently active supervÂ­isors.                                   |\n| GET         | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/[Â­supÂ­ervÂ­isoÂ­rId]           | Returns the current spec for the supervisor with the provided ID.                                |\n| GET         | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/[Â­supÂ­ervÂ­isoÂ­rId]/sÂ­tatus    | Returns the current status of the supervisor with the provided ID.                               |\n| GET         | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/hÂ­istory                     | Returns an audit history of specs for all supervÂ­isors (current and past).                        |\n| GET         | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/[Â­supÂ­ervÂ­isoÂ­rId]/hÂ­istory   | Returns an audit history of specs for the supervisor with the provided ID.                       |\n| POST        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervisor                             | Suspend the current running supervisor of the provided ID. Responds with updated SupervÂ­isoÂ­rSpec. |\n| POST        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/[Â­supÂ­ervÂ­isoÂ­rId]/sÂ­uspend   | Suspend the current running supervisor of the provided ID. Responds with updated SupervÂ­isoÂ­rSpec. |\n| POST        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/sÂ­uspÂ­endAll                  | Suspend all supervÂ­isors at once.                                                                 |\n| POST        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/[Â­supÂ­ervÂ­isoÂ­rId]/rÂ­esume    | Resume indexing tasks for a supervÂ­isor. Responds with updated SupervÂ­isoÂ­rSpec.                    |\n| POST        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/rÂ­esuÂ­meAll                   | Resume all supervÂ­isors at once.                                                                  |\n| POST        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/[Â­supÂ­ervÂ­isoÂ­rId]/reset     | Reset the specified supervÂ­isor.                                                                  |\n| POST        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/[Â­supÂ­ervÂ­isoÂ­rId]/tÂ­ermÂ­inate | Terminate a supervisor of the provided ID.                                                       |\n| POST        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/tÂ­ermÂ­inaÂ­teAll                | Terminate all supervÂ­isors at once.                                                               |\n| POST        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/[Â­supÂ­ervÂ­isoÂ­rId]/sÂ­hutdown  | Shutdown a supervÂ­isor.                                                                           |\n| POST        | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­supÂ­ervÂ­isoÂ­r/[Â­supÂ­ervÂ­isoÂ­rId]/sÂ­uspend   | Suspend the current running supervisor of the provided ID. Responds with updated SupervÂ­isoÂ­rSpec. |\n\n| Overlord              |                                                                     |                                                                                                                                                                                       |\n|------------|---------------------|----------------------------------------|\n| Dynamic ConfigÂ­uration |                                                                     |                                                                                                                                                                                       |\n| GET                   | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­worker                                             | Retreives current overlord dynamic configÂ­uraÂ­tion.                                                                                                                                      |\n| GET                   | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­worÂ­kerÂ­/hiÂ­stoÂ­ry?Â­intÂ­ervÂ­al=Â­{inÂ­terÂ­valÂ­}&Â­couÂ­nteÂ­r={Â­count} | Retrieves history of changes to overlord dynamic configÂ­uraÂ­tion. Accepts interval and count query string parameters to filter by interval and limit the number of results respecÂ­tively. |\n| GET                   | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­scaling                                            | Retrieves overlord scaling events if auto-sÂ­caling runners are in use.                                                                                                                  |\n| POST                  | /druidÂ­/inÂ­dexÂ­er/Â­v1/Â­worker                                             | Update overlord dynamic worker configÂ­uraÂ­tion.                                                                                                                                          |\n\n| Data Server   |                                        |                                                                                                     |\n|-----------|---------------------|-----------------------------------------|\n| MiddleÂ­Manager |                                        |                                                                                                     |\n| GET           | /druidÂ­/woÂ­rkeÂ­r/vÂ­1/eÂ­nabled                | Check whether a MiddleÂ­Manager is in an enabled or disabled state                                     |\n| GET           | /druidÂ­/woÂ­rkeÂ­r/vÂ­1/tasks                  | Retrieve a list of active tasks being run on MiddleÂ­ManÂ­ager.                                          |\n| GET           | /druidÂ­/woÂ­rkeÂ­r/vÂ­1/tÂ­askÂ­/{tÂ­askÂ­id}/log      | Retrieve task log output stream by task id                                                           |\n| POST          | /druidÂ­/woÂ­rkeÂ­r/vÂ­1/dÂ­isable                | 'Disable' a MiddleÂ­ManÂ­ager, causing it to stop accepting new tasks but complete all existing tasks. |\n| POST          | /druidÂ­/woÂ­rkeÂ­r/vÂ­1/eÂ­nable                 | 'Enable' a MiddleÂ­ManÂ­ager, allowing it to accept new tasks again if it was previously disabled.     |\n| POST          | /druidÂ­/woÂ­rkeÂ­r/vÂ­1/tÂ­askÂ­/{tÂ­askÂ­id}Â­/shÂ­utdown | Shutdown a running task by taskid.                                                                   |\n\n| Overlord |                                                 |                                                                                      |\n|---------|--------------------------|--------------------------------------|\n| Peon     |                                                 |                                                                                      |\n| GET      | /druidÂ­/woÂ­rkeÂ­r/vÂ­1/cÂ­hatÂ­/{tÂ­askÂ­Id}Â­/roÂ­wStats          | Retrieve a live row stats report from a Peon. See task reports for more details.      |\n| GET      | /druidÂ­/woÂ­rkeÂ­r/vÂ­1/cÂ­hatÂ­/{tÂ­askÂ­Id}Â­/unÂ­parÂ­seaÂ­bleÂ­Events | Retrieve an unparsÂ­eable events report from a Peon. See task reports for more details. |\n\n| Data Server |                                |                                                                                                                                                                                                        |\n|--------|-------------|---------------------------------------------------|\n| Historical  | Segment Loading                 |                                                                                                                                                                                                        |\n| GET         | /druidÂ­/hiÂ­stoÂ­ricÂ­al/Â­v1/Â­loaÂ­dstatus | Returns JSON of the form {\"caÂ­cheÂ­IniÂ­tiaÂ­lizÂ­ed\":Â­}, where value is either true or false indicating if all segments in the local cache have been loaded.                                                  |\n| GET         | /druidÂ­/hiÂ­stoÂ­ricÂ­al/Â­v1/Â­reaÂ­diness  | Similar to /druidÂ­/hiÂ­stoÂ­ricÂ­al/Â­v1/Â­loaÂ­dstÂ­atus, but instead of returning JSON with a flag, responses 200 OK if segments in the local cache have been loaded, and 503 SERVICE UNAVAIÂ­LABLE, if they haven't. |\n\n| Query Server           |                                                                                                                      |                                                                                                                                                                                                                                                   |\n|-----------|------------------------|-------------------------------------|\n| Broker                 |                                                                                                                      |                                                                                                                                                                                                                                                   |\n| Datasource InformÂ­ation |                                                                                                                      |                                                                                                                                                                                                                                                   |\n| GET                    | /druidÂ­/v2Â­/daÂ­tasÂ­ources                                                                                                 | Returns a list of queryable datasoÂ­urces.                                                                                                                                                                                                           |\n| GET                    | /druidÂ­/v2Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­Name}                                                                                | Returns the dimensions and metrics of the datasoÂ­urce.                                                                                                                                                                                              |\n|                       |                                                                                                                      | OptionÂ­ally, you can provide request parameter \"Â­fulÂ­l\" to get list of served intervals with dimensions and metrics being served for those intervals. You can also provide request param \"Â­intÂ­ervÂ­al\" explicitly to refer to a particular interval. |\n| GET                    | /druidÂ­/v2Â­/daÂ­tasÂ­ourÂ­cesÂ­/{dÂ­ataÂ­SouÂ­rceÂ­NamÂ­e}/Â­canÂ­didÂ­ateÂ­s?iÂ­nteÂ­rvaÂ­ls=Â­{coÂ­mmaÂ­-seÂ­parÂ­ateÂ­d-iÂ­nteÂ­rvaÂ­ls}Â­&nÂ­umCÂ­andÂ­idaÂ­tesÂ­={nÂ­umCÂ­andÂ­idates} | Returns segment informÂ­ation lists including server locations for the given datasource and intervals. If \"Â­numÂ­CanÂ­didÂ­ateÂ­s\" is not specified, it will return all servers for each interval.                                                          |\n\n| Query Server |                            |                                                                               |\n|----------|------------------|--------------------------------------------|\n| Load Status  |                            |                                                                               |\n| GET          | /druidÂ­/brÂ­okeÂ­r/vÂ­1/lÂ­oadÂ­status | Returns a flag indicating if the Broker knows about all segments in Zookeeper. |\n\n| Query Server |                      |                                                                                               |\n|-----------|----------------|----------------------------------------------|\n| Queries      |                      |                                                                                               |\n| POST         | /druid/v2/            | The endpoint for submitting queries. Accepts an option ?pretty that pretty prints the results. |\n| POST         | /druidÂ­/v2Â­/caÂ­ndiÂ­dates/ | Returns segment informÂ­ation lists including server locations for the given query..             |\n\nSQL Cheat Sheet\n\n| Metadata Commands           |\n|-----------------------------|\n| SELECT * FROM sys.seÂ­gments |\n| EXPLAIN PLAN FOR [Â­SQL]    |\n\n| INFORMÂ­ATIÂ­ON_Â­SCHEMA TABLES |\n|----------------------------|\n| SCHEMATA                   |\n| TABLES                     |\n| COLUMNS                    |\n\n| System Tables                                                                  |\n|------------------------------------------------------------------------|\n| sys.segments                                                                   |\n| sys.server_segments                                                            |\n| sys.tasks                                                                      |\n| The \"Â­sysÂ­\" schema provides visibility into Druid segments, servers and tasks. |\n\n| SQL Types |                   |\n|-----------|--------------------|\n| SQL Type  | DRUID RUNTIME TYPE |\n| CHAR      | STRING             |\n| VARCHAR   | STRING             |\n| DECIMAL   | DOUBLE             |\n| FLOAT     | FLOAT              |\n| REAL      | DOUBLE             |\n| DOUBLE    | DOUBLE             |\n| BOOLEAN   | LONG               |\n| TINYINT   | LONG               |\n| SMALLINT  | LONG               |\n| INTEGER   | LONG               |\n| BIGINT    | LONG               |\n| TIMESTAMP | LONG               |\n| DATE      | LONG               |\n| OTHER     | COMPLEX            |\n\n| JDBC CONNECTOR                                                    |\n|-------------------------------------------------------------------|\n| jdbc:aÂ­vatÂ­icaÂ­:reÂ­motÂ­e:uÂ­rl=Â­httÂ­p:/Â­/BRÂ­OKEÂ­R:8Â­082Â­/drÂ­uidÂ­/v2Â­/sqÂ­l/aÂ­vatÂ­ica/. |\n| You can make Druid SQL queries using the Avatica JDBC driver      |\n\n| AggregÂ­ation Functions |\n|-----------------------|\n| COUNT(*)             |\n| COUNT(Â­DISÂ­TINCT expr)  |\n| SUM(expr)             |\n| MIN(expr)             |\n| MAX(expr)             |\n| AVG(expr)             |\n\n| ApproxÂ­imate AggregÂ­ations                                                                                         |\n|------------------------------------------------------------------------|\n| APPROXÂ­_COÂ­UNTÂ­_DIÂ­STIÂ­NCTÂ­(expr)                                                                                    |\n| APPROXÂ­_COÂ­UNTÂ­_DIÂ­STIÂ­NCTÂ­_DSÂ­_HLÂ­L(expr, [lgK, tgtHllÂ­Type])                                                      |\n| APPROXÂ­_COÂ­UNTÂ­_DIÂ­STIÂ­NCTÂ­_DSÂ­_THÂ­ETAÂ­(expr, [size])                                                               |\n| APPROXÂ­_QUÂ­ANTÂ­ILEÂ­(expr, probabÂ­ility, [resolÂ­ution])                                                              |\n| APPROXÂ­_QUÂ­ANTÂ­ILEÂ­_DSÂ­(expr, probabÂ­ility, [k])                                                                   |\n| APPROXÂ­_QUÂ­ANTÂ­ILEÂ­_FIÂ­XEDÂ­_BUÂ­CKEÂ­TS(Â­expr, probabÂ­ility, numBucÂ­kets, lowerLÂ­imit, upperLÂ­imit, [outliÂ­erHÂ­andÂ­linÂ­gMode]) |\n\n| ApproxÂ­imate AggregÂ­ations                             |\n|------------------------------------------------------|\n| BLOOM_Â­FILÂ­TERÂ­(expr, numEntÂ­ries)                      |\n| BLOOM_Â­FILÂ­TERÂ­_TEÂ­ST(Â­< Â­exÂ­pr >, <Â­ seÂ­riaÂ­lizÂ­ed-Â­filÂ­ter Â­>) |\n\n### Comparison Operators\n```\nx = y \nx <> y \nx > y \nx >= y \nx < y \nx <= y \nx BETWEEN y AND z \nx NOT BETWEEN y AND z \nx LIKE pattern [ESCAPE esc]\nx NOT LIKE pattern [ESCAPE esc]\nx IS NULL \nx IS NOT NULL \nx IS TRUE \nx IS NOT TRUE \nx IS FALSE \nx IS NOT FALSE \nx IN (values)\nx NOT IN (values)\nx IN (subquery)\nx NOT IN (subquery)\nx AND y \nx OR y \nNOT x \n```\n\n### Other Functions\n```\nCAST(value AS TYPE)\nCASE expr WHEN value1 THEN result1 [ WHEN value2 THEN result2 ... ] [ ELSE resultN ] END\nCASE WHEN booleaÂ­n_expr1 THEN result1 [ WHEN booleaÂ­n_expr2 THEN result2 ... ] [ ELSE resultN ] END |\nNULLIFÂ­(vaÂ­lue1, value2)\nCOALESÂ­CE(Â­value1, value2, ...)\n```\n\n| Numeric Functions                                                                          |\n|------------------------------------------------------------------------|\n| ABS(expr)                                                                                  |\n| CEIL(expr)                                                                                 |\n| EXP(expr)                                                                                  |\n| FLOOR(Â­expr)                                                                                |\n| LN(expr)                                                                                   |\n| LOG10(Â­expr)                                                                                |\n| POWER(Â­expr, power)                                                                         |\n| SQRT(expr)                                                                                 |\n| TRUNCAÂ­TE(Â­expr[, digits])                                                                 |\n| TRUNC(Â­expr[, digits])                                                                    |\n| x + y                                                                                      |\n| x - y                                                                                      |\n| x * y                                                                                     |\n| x / y                                                                                      |\n| MOD(x, y)                                                                                  |\n| Numeric functions will return 64 bit integers or 64 bit floats, depending on their inputs. |\n\n| String Functions                                                                |\n|------------------------------------------------------------------------|\n| CONCATÂ­(expr, expr...)                                                          |\n| TEXTCAÂ­T(expr, expr)                                                             |\n| LENGTHÂ­(expr)                                                                    |\n| CHAR_LÂ­ENGÂ­TH(Â­expr)                                                               |\n| CHARACÂ­TERÂ­_LEÂ­NGTÂ­H(expr)                                                         |\n| STRLENÂ­(expr)                                                                    |\n| LOOKUPÂ­(expr, lookupÂ­Name)                                                        |\n| LOWER(Â­expr)                                                                     |\n| POSITIÂ­ON(Â­needle IN haystack [FROM fromInÂ­dex])                                 |\n| REGEXPÂ­_EXÂ­TRAÂ­CT(Â­expr, pattern, [index])                                       |\n| REPLACÂ­E(expr, pattern, replacÂ­ement)                                             |\n| STRPOSÂ­(haÂ­ystack, needle)                                                        |\n| SUBSTRÂ­INGÂ­(expr, index, [length])                                              |\n| SUBSTRÂ­(expr, index, [length])                                                 |\n| TRIM([BOTH | LEADING | TRAILING] [< Â­cÂ­harÂ­s > FROM] expr)                   |\n| BTRIM(Â­expr[, chars])                                                          |\n| LTRIM(Â­expr[, chars])                                                          |\n| UPPER(Â­expr)                                                                     |\n| String functions accept strings, and return a type appropÂ­riate to the function. |\n\n| Time Functions                                                                |\n|------------------------------------------------------------------------|\n| CURRENÂ­T_TÂ­IMEÂ­STAMP                                                             |\n| CURRENÂ­T_DATE                                                                  |\n| DATE_TÂ­RUNÂ­C                                      |\n| TIME_FÂ­LOOÂ­R |\n| TIME_SÂ­HIFÂ­T       |\n| TIME_EÂ­XTRÂ­ACT             |\n| TIME_PÂ­ARSÂ­E               |\n| TIME_FÂ­ORMÂ­AT            |\n| MILLISÂ­_TOÂ­_TIÂ­MESÂ­TAMÂ­P(mÂ­illÂ­is_Â­expr)                                           |\n| TIMESTÂ­AMPÂ­_TOÂ­_MIÂ­LLIÂ­S(tÂ­imeÂ­staÂ­mp_Â­expr)                                        |\n| EXTRACÂ­T(unit FROM timestÂ­ampÂ­_expr)                                        |\n| FLOOR(Â­timÂ­estÂ­ampÂ­_expr TO unit)                                            |\n| CEIL(tÂ­imeÂ­staÂ­mp_expr TO unit)                                              |\n| TIMESTÂ­AMPÂ­ADDÂ­(unit, Â­coÂ­untÂ­, tiÂ­mesÂ­tamÂ­p)                              |\n| timestÂ­ampÂ­_expr { + | - } inÂ­terÂ­valÂ­_exÂ­pr                                 |\n\n<https://imply.io/druid/cheat-sheet>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Druid/Commands/","title":"Commands"},"frontmatter":{"draft":false},"rawBody":"# Commands\n\nCreated: 2019-06-25 11:24:39 +0500\n\nModified: 2020-01-09 12:38:06 +0500\n\n---\n\n| **Service**             | **Port** |\n|-------------------------|----------|\n| **druid-zookeeper**     | 2181     |\n| **druid-coordinator**   | 8081     |\n| **druid-overlord**      | 8090     |\n| **druid-middlemanager** | 8091     |\n| **druid-historical**    | 8083     |\n| **druid-broker**        | 8082     |\n| **druid-router**        | 8888     |\n| **druid-postgresql**    | 5432     |\n**DRUID_ZOOKEEPER_IP**=**172.18.3.2**\n\n**DRUID_POSTGRESQL_IP**=**172.18.3.3**\n\n**DRUID_COORDINATOR_IP**=**172.18.3.4**\n\n**DRUID_HISTORICAL_IP**=**172.18.3.5**\n\n**DRUID_BROKER_IP**=**172.18.3.6**\n\n**DRUID_INIT_IP**=**172.18.3.6**\n\n**DRUID_OVERLORD_IP**=**172.18.3.7**\n\n**DRUID_MIDDLEMANAGER_IP**=**172.18.3.8**\n\n**DRUID_ROUTER_IP**=**172.18.3.15**\n**Hack**\n\ndocker exec -it druid-historical bash\n\nmkdir /var/druid/tmp\n**APIs**\n\nHistorical\n\n<http://localhost:8083/druid/historical/v1/readiness>\n\n[http://localhost:8083/druid/historical/v1/](http://localhost:8083/druid/historical/v1/readiness)loadstatus\n\n[http://localhost:8083/](http://localhost:8083/druid/historical/v1/readiness)status\n**Druid segment cleanup**\n\ndocker exec -it druid-historical bash\n**# first remove segment-cache and then segments**\n\ncd /var/druid/segment-cache and cd /var/druid/segments\nremove last 20 days of segments\n**Druid Commands**\n-   curl -X 'POST' -H 'Content-Type:application/json' -d @wikipedia-top-pages.json [http://localhost:8082/druid/v2?pretty](http://localhost:8082/druid/v2/?pretty) #query top pages from wikipedia dataset\n-   curl -XPOST -H'Content-Type: application/json' -d @wikipedia-kafka-supervisor.json <http://localhost:8090/druid/indexer/v1/supervisor> #submit supervisor spec to kafka-indexing-service\n-   curl -XPOST -H'Content-Type: application/json' -d @smap-kafka-supervisor-spec.json <http://localhost:8090/druid/indexer/v1/supervisor>\n\n## Dashboards**\n-   8081: coordinator (for seeing clusters and datasources\n-   8090: overlord (for managing supervisor spec and tasks)\n-   Others\n    -   8082: broker\n    -   8083: historical\n    -   8091: middlemanager\n    -   2181: zookeeper\n**Debugging**\n\nstop druid-historical\n\ncd /var/lib/docker/volumes/druid-volume/_data/segment-cache\n\nremove docker segment-cache\n\nrm -rf *\n\nstart druid-historical\n**SQL Commands**\n\n# find duplicate count\n\nSELECT controller_name, site_name, count(reading) as DuplicateCount FROM (SELECT controller_name, stream_path, site_name, reading, __time, count(reading) as readingCount FROM \"live-Samhi\"\n\nWHERE __time BETWEEN TIMESTAMP '2019-11-18 08:00:00' AND TIMESTAMP '2019-11-19 08:00:00'\n\nAND stream_path LIKE '/Samhi-41%'\n\nGROUP BY __time, stream_path, reading, site_name, controller_name\n\nHAVING readingCount > 1\n\nORDER BY __time DESC)\n\nGROUP BY site_name, controller_name\n# find duplicate values\n\nSELECT controller_name, stream_path, site_name, reading, __time, count(reading) as readingCount FROM \"live-Samhi\"\n\nWHERE __time BETWEEN TIMESTAMP '2019-11-18 08:00:00' AND TIMESTAMP '2019-11-19 09:00:00'\n\nAND stream_path LIKE '/Samhi-41%'\n\nGROUP BY __time, stream_path, reading, site_name, controller_name\n\nHAVING readingCount > 1\n\nORDER BY __time DESC\n**Dashboard**\n\n<http://10.9.1.21:8888/unified-console.html>\n\n## Ingestion Spec**\n\n{\n\n\"type\": \"kafka\",\n\n\"dataSchema\": {\n\n\"dataSource\": \"live-Samhi\",\n\n\"parser\": {\n\n\"type\": \"string\",\n\n\"parseSpec\": {\n\n\"format\": \"json\",\n\n\"timestampSpec\": {\n\n\"column\": \"time\",\n\n\"format\": \"auto\"\n\n},\n\n\"dimensionsSpec\": {\n\n\"spatialDimensions\": [\n\n{\n\n\"dimName\": \"coordinates\",\n\n\"dims\": [\n\n\"site_latitude\",\n\n\"site_longitude\"\n\n]\n\n}\n\n],\n\n\"dimensions\": [\n\n\"uuid\",\n\n\"path\",\n\n{\n\n\"name\": \"reading\",\n\n\"type\": \"float\"\n\n},\n\n\"stream_path\",\n\n\"stream_uuid\",\n\n\"device_tags\",\n\n\"device_is_virtual\",\n\n\"device_id\",\n\n\"device_display_name\",\n\n\"device_path\",\n\n\"device_type\",\n\n\"device_category\",\n\n\"physical_parameter_display\",\n\n\"physical_parameter_unit\",\n\n\"physical_parameter_type\",\n\n\"physical_parameter_name\",\n\n\"site_longitude\",\n\n\"site_latitude\",\n\n\"site_name\",\n\n\"customer_name\",\n\n\"client_name\",\n\n\"controller_name\",\n\n\"site_Size\",\n\n\"site_Floors\",\n\n\"site_site_code\",\n\n\"site_Hotel_Type\",\n\n\"site_Display_Name\",\n\n\"metric_tod_metadata\",\n\n\"metric_operational_metadata\",\n\n\"live_parent\",\n\n\"live_region\",\n\n\"live_purpose\",\n\n\"live_pie_breakup\",\n\n\"live_room_number\",\n\n\"live_room_direction\"\n\n]\n\n}\n\n}\n\n},\n\n\"metricsSpec\": [\n\n{\n\n\"type\": \"count\",\n\n\"name\": \"count\"\n\n}\n\n],\n\n\"granularitySpec\": {\n\n\"type\": \"uniform\",\n\n\"segmentGranularity\": \"HOUR\",\n\n\"rollup\": false\n\n}\n\n},\n\n\"tuningConfig\": {\n\n\"type\": \"kafka\",\n\n\"maxSavedParseExceptions\": 1000,\n\n\"forceExtendableShardSpecs\": true\n\n},\n\n\"ioConfig\": {\n\n\"topic\": \"druid_telemetry_data_Samhi\",\n\n\"taskCount\": 1,\n\n\"replicas\": 1,\n\n\"taskDuration\": \"PT120S\",\n\n\"completionTimeout\": \"PT5M\",\n\n\"useEarliestOffset\": true,\n\n\"consumerProperties\": {\n\n\"bootstrap.servers\": \"kafka0.zenatix.com:31090,kafka1.zenatix.com:31091,kafka2.zenatix.com:31092\"\n\n}\n\n}\n\n}\n**Important Points**\n-   Compression - 1:10\n-   Number of hyper threads\n-   Concurrency of queries\n\n4 gb/s per hyperthread - how much data you are consuming-   Servers\n\nI3.extra large - 3 data server\n\n1 middlemaanger\n\n2 query server - m5 large\n\nMaster - m5 large\n\nm4.xlarge\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Druid/Documentation/","title":"Documentation"},"frontmatter":{"draft":false},"rawBody":"# Documentation\n\nCreated: 2019-02-06 16:16:54 +0500\n\nModified: 2019-02-06 16:19:45 +0500\n\n---\n\nGetting Started\n-   [Design](http://druid.io/docs/latest/design/index.html)\n    -   [What is Druid?](http://druid.io/docs/latest/design/index.html#what-is-druid)\n    -   [When should I use Druid](http://druid.io/docs/latest/design/index.html#when-to-use-druid)\n    -   [Architecture](http://druid.io/docs/latest/design/index.html#architecture)\n    -   [Datasources & Segments](http://druid.io/docs/latest/design/index.html#datasources-and-segments)\n    -   [Query processing](http://druid.io/docs/latest/design/index.html#query-processing)\n    -   [External dependencies](http://druid.io/docs/latest/design/index.html#external-dependencies)\n    -   [Ingestion overview](http://druid.io/docs/latest/ingestion/index.html)\n-   [Quickstart](http://druid.io/docs/latest/tutorials/index.html)\n    -   [Tutorial: Loading a file](http://druid.io/docs/latest/tutorials/tutorial-batch.html)\n    -   [Tutorial: Loading stream data from Kafka](http://druid.io/docs/latest/tutorials/tutorial-kafka.html)\n    -   [Tutorial: Loading a file using Hadoop](http://druid.io/docs/latest/tutorials/tutorial-batch-hadoop.html)\n    -   [Tutorial: Loading stream data using HTTP push](http://druid.io/docs/latest/tutorials/tutorial-tranquility.html)\n    -   [Tutorial: Querying data](http://druid.io/docs/latest/tutorials/tutorial-query.html)-   Further tutorials\n    -   [Tutorial: Rollup](http://druid.io/docs/latest/tutorials/tutorial-rollup.html)\n    -   [Tutorial: Configuring retention](http://druid.io/docs/latest/tutorials/tutorial-retention.html)\n    -   [Tutorial: Updating existing data](http://druid.io/docs/latest/tutorials/tutorial-update-data.html)\n    -   [Tutorial: Compacting segments](http://druid.io/docs/latest/tutorials/tutorial-compaction.html)\n    -   [Tutorial: Deleting data](http://druid.io/docs/latest/tutorials/tutorial-delete-data.html)\n    -   [Tutorial: Writing your own ingestion specs](http://druid.io/docs/latest/tutorials/tutorial-ingestion-spec.html)\n    -   [Tutorial: Transforming input data](http://druid.io/docs/latest/tutorials/tutorial-transform-spec.html)-   [Clustering](http://druid.io/docs/latest/tutorials/cluster.html)\n\nData Ingestion\n-   [Ingestion overview](http://druid.io/docs/latest/ingestion/index.html)\n-   [Data Formats](http://druid.io/docs/latest/ingestion/data-formats.html)\n-   [Tasks Overview](http://druid.io/docs/latest/ingestion/tasks.html)\n-   [Ingestion Spec](http://druid.io/docs/latest/ingestion/ingestion-spec.html)\n    -   [Transform Specs](http://druid.io/docs/latest/ingestion/transform-spec.html)\n    -   [Firehoses](http://druid.io/docs/latest/ingestion/firehose.html)\n-   [Schema Design](http://druid.io/docs/latest/ingestion/schema-design.html)\n-   [Schema Changes](http://druid.io/docs/latest/ingestion/schema-changes.html)\n-   [Batch File Ingestion](http://druid.io/docs/latest/ingestion/batch-ingestion.html)\n    -   [Native Batch Ingestion](http://druid.io/docs/latest/ingestion/native_tasks.html)\n    -   [Hadoop Batch Ingestion](http://druid.io/docs/latest/ingestion/hadoop.html)\n-   [Stream Ingestion](http://druid.io/docs/latest/ingestion/stream-ingestion.html)\n    -   [Kafka Indexing Service (Stream Pull)](http://druid.io/docs/latest/development/extensions-core/kafka-ingestion.html)\n    -   [Stream Push](http://druid.io/docs/latest/ingestion/stream-push.html)\n-   [Compaction](http://druid.io/docs/latest/ingestion/compaction.html)\n-   [Updating Existing Data](http://druid.io/docs/latest/ingestion/update-existing-data.html)\n-   [Deleting Data](http://druid.io/docs/latest/ingestion/delete-data.html)\n-   [Task Locking & Priority](http://druid.io/docs/latest/ingestion/locking-and-priority.html)\n-   [Task Reports](http://druid.io/docs/latest/ingestion/reports.html)\n-   [FAQ](http://druid.io/docs/latest/ingestion/faq.html)\n-   [Misc. Tasks](http://druid.io/docs/latest/ingestion/misc-tasks.html)\n\nQuerying\n-   [Overview](http://druid.io/docs/latest/querying/querying.html)\n-   [Timeseries](http://druid.io/docs/latest/querying/timeseriesquery.html)\n-   [TopN](http://druid.io/docs/latest/querying/topnquery.html)\n-   [GroupBy](http://druid.io/docs/latest/querying/groupbyquery.html)\n-   [Time Boundary](http://druid.io/docs/latest/querying/timeboundaryquery.html)\n-   [Segment Metadata](http://druid.io/docs/latest/querying/segmentmetadataquery.html)\n-   [DataSource Metadata](http://druid.io/docs/latest/querying/datasourcemetadataquery.html)\n-   [Search](http://druid.io/docs/latest/querying/searchquery.html)\n-   [Select](http://druid.io/docs/latest/querying/select-query.html)\n-   [Scan](http://druid.io/docs/latest/querying/scan-query.html)-   Components\n    -   [Datasources](http://druid.io/docs/latest/querying/datasource.html)\n    -   [Filters](http://druid.io/docs/latest/querying/filters.html)\n    -   [Aggregations](http://druid.io/docs/latest/querying/aggregations.html)\n    -   [Post Aggregations](http://druid.io/docs/latest/querying/post-aggregations.html)\n    -   [Granularities](http://druid.io/docs/latest/querying/granularities.html)\n    -   [DimensionSpecs](http://druid.io/docs/latest/querying/dimensionspecs.html)\n    -   [Context](http://druid.io/docs/latest/querying/query-context.html)-   [Multi-value dimensions](http://druid.io/docs/latest/querying/multi-value-dimensions.html)\n-   [SQL](http://druid.io/docs/latest/querying/sql.html)\n-   [Lookups](http://druid.io/docs/latest/querying/lookups.html)\n-   [Joins](http://druid.io/docs/latest/querying/joins.html)\n-   [Multitenancy](http://druid.io/docs/latest/querying/multitenancy.html)\n-   [Caching](http://druid.io/docs/latest/querying/caching.html)\n-   [Sorting Orders](http://druid.io/docs/latest/querying/sorting-orders.html)\n-   [Virtual Columns](http://druid.io/docs/latest/querying/virtual-columns.html)\n\nDesign\n-   [Overview](http://druid.io/docs/latest/design/index.html)-   Storage\n    -   [Segments](http://druid.io/docs/latest/design/segments.html)\n-   Node Types\n    -   [Historical](http://druid.io/docs/latest/design/historical.html)\n    -   [Broker](http://druid.io/docs/latest/design/broker.html)\n    -   [Coordinator](http://druid.io/docs/latest/design/coordinator.html)\n    -   [Indexing Service](http://druid.io/docs/latest/design/indexing-service.html)\n        -   [Overlord](http://druid.io/docs/latest/design/overlord.html)\n        -   [MiddleManager](http://druid.io/docs/latest/design/middlemanager.html)\n        -   [Peons](http://druid.io/docs/latest/design/peons.html)\n    -   [Realtime (Deprecated)](http://druid.io/docs/latest/design/realtime.html)\n-   Dependencies\n    -   [Deep Storage](http://druid.io/docs/latest/dependencies/deep-storage.html)\n    -   [Metadata Storage](http://druid.io/docs/latest/dependencies/metadata-storage.html)\n    -   [ZooKeeper](http://druid.io/docs/latest/dependencies/zookeeper.html)\n\nOperations\n-   [API Reference](http://druid.io/docs/latest/operations/api-reference.html)\n    -   [Coordinator](http://druid.io/docs/latest/operations/api-reference.html#coordinator)\n    -   [Overlord](http://druid.io/docs/latest/operations/api-reference.html#overlord)\n    -   [MiddleManager](http://druid.io/docs/latest/operations/api-reference.html#middlemanager)\n    -   [Peon](http://druid.io/docs/latest/operations/api-reference.html#peon)\n    -   [Broker](http://druid.io/docs/latest/operations/api-reference.html#broker)\n    -   [Historical](http://druid.io/docs/latest/operations/api-reference.html#historical)\n-   [Including Extensions](http://druid.io/docs/latest/operations/including-extensions.html)\n-   [Data Retention](http://druid.io/docs/latest/operations/rule-configuration.html)\n-   [Metrics and Monitoring](http://druid.io/docs/latest/operations/metrics.html)\n-   [Alerts](http://druid.io/docs/latest/operations/alerts.html)\n-   [Updating the Cluster](http://druid.io/docs/latest/operations/rolling-updates.html)\n-   [Different Hadoop Versions](http://druid.io/docs/latest/operations/other-hadoop.html)\n-   [Performance FAQ](http://druid.io/docs/latest/operations/performance-faq.html)\n-   [Dump Segment Tool](http://druid.io/docs/latest/operations/dump-segment.html)\n-   [Insert Segment Tool](http://druid.io/docs/latest/operations/insert-segment-to-db.html)\n-   [Pull Dependencies Tool](http://druid.io/docs/latest/operations/pull-deps.html)\n-   [Recommendations](http://druid.io/docs/latest/operations/recommendations.html)\n-   [TLS Support](http://druid.io/docs/latest/operations/tls-support.html)\n-   [Password Provider](http://druid.io/docs/latest/operations/password-provider.html)\n\nConfiguration\n-   [Configuration Reference](http://druid.io/docs/latest/configuration/index.html)\n-   [Recommended Configuration File Organization](http://druid.io/docs/latest/configuration/index.html#recommended-configuration-file-organization)\n-   [JVM Configuration Best Practices](http://druid.io/docs/latest/configuration/index.html#jvm-configuration-best-practices)\n-   [Common Configuration](http://druid.io/docs/latest/configuration/index.html#common-configurations)\n-   [Coordinator](http://druid.io/docs/latest/configuration/index.html#coordinator)\n-   [Overlord](http://druid.io/docs/latest/configuration/index.html#overlord)\n-   [MiddleManager & Peons](http://druid.io/docs/latest/configuration/index.html#middle-manager-and-peons)\n-   [Broker](http://druid.io/docs/latest/configuration/index.html#broker)\n-   [Historical](http://druid.io/docs/latest/configuration/index.html#historical)\n-   [Caching](http://druid.io/docs/latest/configuration/index.html#cache-configuration)\n-   [General Query Configuration](http://druid.io/docs/latest/configuration/index.html#general-query-configuration)\n-   [Configuring Logging](http://druid.io/docs/latest/configuration/logging.html)\n\nDevelopment\n-   [Overview](http://druid.io/docs/latest/development/overview.html)\n-   [Libraries](http://druid.io/docs/latest/development/libraries.html)\n-   [Extensions](http://druid.io/docs/latest/development/extensions.html)\n-   [JavaScript](http://druid.io/docs/latest/development/javascript.html)\n-   [Build From Source](http://druid.io/docs/latest/development/build.html)\n-   [Versioning](http://druid.io/docs/latest/development/versioning.html)\n-   [Integration](http://druid.io/docs/latest/development/integrating-druid-with-other-technologies.html)-   Experimental Features\n    -   [Overview](http://druid.io/docs/latest/development/experimental.html)\n    -   [Approximate Histograms and Quantiles](http://druid.io/docs/latest/development/extensions-core/approximate-histograms.html)\n    -   [Datasketches](http://druid.io/docs/latest/development/extensions-core/datasketches-extension.html)\n    -   [Geographic Queries](http://druid.io/docs/latest/development/geo.html)\n    -   [Router](http://druid.io/docs/latest/development/router.html)\n    -   [Kafka Indexing Service](http://druid.io/docs/latest/development/extensions-core/kafka-ingestion.html)\n\nMisc\n-   [Druid Expressions Language](http://druid.io/docs/latest/misc/math-expr.html)\n-   [Papers & Talks](http://druid.io/docs/latest/misc/papers-and-talks.html)\n-   [Thanks](http://druid.io/thanks.html)\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Druid/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-12-15 18:25:30 +0500\n\nModified: 2021-06-28 12:58:00 +0500\n\n---\n\n**Plywood**\n\nPlywood is a JavaScript library that simplifies building interactive visualizations and applications for large data sets. Plywood acts as a middle-layer between data visualizations and data stores.\nPlywood is architected around the principles of nested[Split-Apply-Combine](http://www.jstatsoft.org/article/view/v040i01/v40i01.pdf), a powerful divide-and-conquer algorithm that can be used to construct all types of data visualizations. Plywood comes with its own[expression language](https://github.com/implydata/plywood/blob/master/docs/expressions.md)where a single Plywood expression can translate to multiple database queries, and where results are returned in a nested data structure so they can be easily consumed by visualization libraries such as[D3.js](http://d3js.org/).\nYou can use Plywood in the browser and/or in node.js to easily create your own visualizations and applications.\nPlywood also acts as a very advanced query planner for Druid, and Plywood will determine the most optimal way to execute Druid queries.\n<https://github.com/implydata/plywood>\n\n## Turnilo**\n\nBusiness intelligence, data exploration and visualization web application for Druid\n\n<https://github.com/allegro/turnilo>\n\n## Tranquility**\n\nTranquility helps you send real-time event streams to Druid and handles partitioning, replication, service discovery, and schema rollover, seamlessly and without downtime.\n\n<https://github.com/druid-io/tranquility>\n\n## Imply Pivot**\n-   Imply Pivot is a web-based analytics application on Druid\n-   It provides interactive, point-and-click visualizations as well as a SQL query UI\n-   It is only available with the Imply distribution of Druid\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Druid/Paper/","title":"Paper"},"frontmatter":{"draft":false},"rawBody":"# Paper\n\nCreated: 2019-07-18 22:58:49 +0500\n\nModified: 2019-07-19 20:04:10 +0500\n\n---\n\n1.  Realtime Node\n\nReal-time nodes encapsulate the functionality to ingest and query event streams. Events indexed via these nodes are immediately available for querying. The nodes are only concerned with events for some small time range and periodically hand off immutable batches of events they have collected over this small time range to other nodes in the Druid cluster that are specialized in dealing with batches of immutable events. Real-time nodes leverage Zookeeper [19] for coordination with the rest of the Druid cluster. The nodes announce their online state and the data they serve in Zookeeper.\n\nReal-time nodes maintain an in-memory index buffer for all in- coming events. These indexes are incrementally populated as events are ingested and the indexes are also directly queryable. Druid be- haves as a row store for queries on events that exist in this JVM heap-based buffer. To avoid heap overflow problems, real-time nodes persist their in-memory indexes to disk either periodically or after some maximum row limit is reached. This persist process converts data stored in the in-memory buffer to a column oriented storage format described in Section 4. Each persisted index is im- mutable and real-time nodes load persisted indexes into off-heap memory such that they can still be queried. This process is de- scribed in detail in [33] and is illustrated in Figure 2.\n\nOn a periodic basis, each real-time node will schedule a back- ground task that searches for all locally persisted indexes. The task merges these indexes together and builds an immutable block of data that contains all the events that have been ingested by a real- time node for some span of time. We refer to this block of data as a \"segment\". During the handoff stage, a real-time node uploads this segment to a permanent backup storage, typically a distributed file system such as S3 [12] or HDFS [36], which Druid refers to as \"deep storage\". The ingest, persist, merge, and handoff steps are fluid; there is no data loss during any of the processes.\n2.  Historical Nodes\n\nHistorical nodes encapsulate the functionality to load and serve the immutable blocks of data (segments) created by real-time nodes. In many real-world workflows, most of the data loaded in a Druid cluster is immutable and hence, historical nodes are typically the main workers of a Druid cluster. Historical nodes follow a shared- nothing architecture and there is no single point of contention among the nodes. The nodes have no knowledge of one another and are operationally simple; they only know how to load, drop, and serve immutable segments.\n\nSimilar to real-time nodes, historical nodes announce their on- line state and the data they are serving in Zookeeper. Instructions to load and drop segments are sent over Zookeeper and contain infor- mation about where the segment is located in deep storage and how to decompress and process the segment. Before a historical node downloads a particular segment from deep storage, it first checks a local cache that maintains information about what segments already exist on the node. If information about a segment is not present in the cache, the historical node will proceed to download the segment from deep storage. Once pro- cessing is complete, the segment is announced in Zookeeper. At this point, the segment is queryable. The local cache also allows for historical nodes to be quickly updated and restarted. On startup, the node examines its cache and immediately serves whatever data it finds. Historical nodes can support read consistency because they only deal with immutable data. Immutable data blocks also enable a sim- ple parallelization model: historical nodes can concurrently scan and aggregate immutable blocks without blocking.\n\n*Tiers*\n\nHistorical nodes can be grouped in different tiers, where all nodes in a given tier are identically configured. Different performance and fault-tolerance parameters can be set for each tier. The purpose of tiered nodes is to enable higher or lower priority segments to be dis- tributed according to their importance. For example, it is possible to spin up a \"hot\" tier of historical nodes that have a high num- ber of cores and large memory capacity. The \"hot\" cluster can be configured to download more frequently accessed data. A parallel \"cold\" cluster can also be created with much less powerful backing hardware. The \"cold\" cluster would only contain less frequently accessed segments.\n3.  Broker Nodes\n\nBroker nodes act as query routers to historical and real-time nodes. Broker nodes understand the metadata published in Zookeeper about what segments are queryable and where those segments are located. Broker nodes route incoming queries such that the queries hit the right historical or real-time nodes. Broker nodes also merge partial results from historical and real-time nodes before returning a final consolidated result to the caller.\n*Caching*\n\nBroker nodes contain a cache with a LRU [31, 20] invalidation strategy. The cache can use local heap memory or an external distributed key/value store such as Memcached [16]. Each time a bro- ker node receives a query, it first maps the query to a set of seg- ments. Results for certain segments may already exist in the cache and there is no need to recompute them. For any results that do not exist in the cache, the broker node will forward the query to the correct historical and real-time nodes. Once historical nodes return their results, the broker will cache these results on a per segment ba- sis for future use.\n\nReal-time data is never cached and hence requests for real-time data will al- ways be forwarded to real-time nodes. Real-time data is perpetually changing and caching the results is unreliable.\n\nThe cache also acts as an additional level of data durability. In the event that all historical nodes fail, it is still possible to query results if those results already exist in the cache.\n4.  Coordinator Nodes\n\nDruid coordinator nodes are primarily in charge of data manage- ment and distribution on historical nodes. The coordinator nodes tell historical nodes to load new data, drop outdated data, replicate data, and move data to load balance. Druid uses a multi-version concurrency control swapping protocol for managing immutable segments in order to maintain stable views. If any immutable seg- ment contains data that is wholly obsoleted by newer segments, the outdated segment is dropped from the cluster. Coordinator nodes undergo a leader-election process that determines a single node that runs the coordinator functionality. The remaining coordinator nodes act as redundant backups.\n\nA coordinator node runs periodically to determine the current state of the cluster. It makes decisions by comparing the expected state of the cluster with the actual state of the cluster at the time of the run. As with all Druid nodes, coordinator nodes maintain a Zookeeper connection for current cluster information. Coordinator nodes also maintain a connection to a MySQL database that con- tains additional operational parameters and configurations. One of the key pieces of information located in the MySQL database is a table that contains a list of all segments that should be served by historical nodes. This table can be updated by any service that cre- ates segments, for example, real-time nodes. The MySQL database also contains a rule table that governs how segments are created, destroyed, and replicated in the cluster.\n*Rules*\n\nRules govern how historical segments are loaded and dropped from the cluster. Rules indicate how segments should be assigned to different historical node tiers and how many replicates of a segment should exist in each tier. Rules may also indicate when segments should be dropped entirely from the cluster. Rules are usually set for a period of time\nLoadBalancing\n\nThese query patterns suggest replicating recent historical seg- ments at a higher rate, spreading out large segments that are close in time to different historical nodes, and co-locating segments from different data sources.\n**Storage Engine**\n-   In-memory storage engine\n-   Memory mapped storage engine\n**Query API**\n\nDruid has its own query language and accepts queries as POST requests. Broker, historical, and real-time nodes all share the same query API.\n**References**\n\n<http://static.druid.io/docs/druid.pdf>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/MongoDB/Commands/","title":"Commands"},"frontmatter":{"draft":false},"rawBody":"# Commands\n\nCreated: 2019-05-06 22:44:15 +0500\n\nModified: 2020-07-27 12:38:44 +0500\n\n---\n\nservices:\n\nmongo:\n\nstdin_open: true\n\ntty: true\n\nrestart: \"no\"\n\nimage: mongo\n\ncontainer_name: mongo\n\nenvironment:\n\nMONGO_INITDB_ROOT_USERNAME: root\n\nMONGO_INITDB_ROOT_PASSWORD: example\nmongo_express:\n\nstdin_open: true\n\ntty: true\n\nrestart: \"no\"\n\nimage: mongo-express\n\ncontainer_name: mongo_express\n\nports:\n\n- 8081:8081\n\nenvironment:\n\nME_CONFIG_MONGODB_ADMINUSERNAME: root\n\nME_CONFIG_MONGODB_ADMINPASSWORD: example\nbrew install mongodb\n# start mongodb server\n\nbrew services start mongo\n**# start mongo client**\n\nmongo\n\nmongo -u root -p example\n**Mongo DB Queries**\n\ndb.stats()\n\nuse db_name\n\nuse user_device_sms #create db user_device_sms\n\ndb #show current db\n\nshow dbs #show databases, only shows database with atleast one document\n\ndb.movie.insert({'name':'End game'}) #automatically creates a collection movie if not present\n\ndb.dropDatabase()\ndb.createUser({user:\"deepak\", pwd:\"12345\", roles:[\"readWrite\", \"dbAdmin\"]})\ndb.createCollection(name, options)\n\nshow collections\n\ndb.createCollection(\"customers\")\n\ndb.createCollection(\"mycol\", { capped : true, autoIndexId : true, size :\n6142800, max : 10000 } )\n\ndb.customers.insert({\"first_name\":\"Deepak\",\"last_name\":\"Sood\"});\n\ndb.COLLECTION_NAME.drop()\ndb.COLLECTION_NAME.find()\n\ndb.COLLECTION_NAME.find().pretty()\n\ndb.sms_collection.find({device_id: \"009906bcc9ed9d86\"})\n\ndb.sms_collection.find({device_id: \"009906bcc9ed9d86\"}, {device_id: 1})\n\ndb.sms_collection.find({}, {device_id: 1})\ndb.COLLECTION_NAME.update(SELECTION_CRITERIA, UPDATED_DATA) #updates the values in the existing document.\n\ndb.COLLECTION_NAME.save({_id:ObjectId(),NEW_DATA}) #replaces the existing document with the new document passed in the save() method.\n\ndb.COLLECTION_NAME.remove(DELLETION_CRITTERIA) #remove a document from the collection\n\ndb.COLLECTION_NAME.remove(DELETION_CRITERIA,1) #If there are multiple records and you want to delete only the first record, then setjustOneparameter inremove()method.\n\ndb.COLLECTION_NAME.find().limit(NUMBER)\n\ndb.COLLECTION_NAME.find().limit(NUMBER).skip(NUMBER) #accepts number type argument and is used to skip the number of documents.\n\ndb.COLLECTION_NAME.find().sort({KEY:1}) #1 is used for ascending order while -1 is used for descending order.\ndb.customers.find().forEach(function(doc){print(\"Cust Name: \"+doc.first_name)}); # looping through all data\ndb.collection.createIndex( { name: -1 } )\n\ndb.sms_collection.getIndexes()\ndb.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)\n**RDBMS Where Clause Equivalents in MongoDB**\n\nTo query the document on the basis of some condition, you can use following operations.\n\n| **Operation**       | **Syntax**                  | **Example**                                          | **RDBMS Equivalent**           |\n|----------|--------------------|--------------------------------|----------|\n| Equality            | {key:value}         | db.mycol.find({\"by\":\"tutorials point\"}).pretty() | where by = 'tutorials point' |\n| Less Than           | {key:{$lt:value}}  | db.mycol.find({\"likes\":{$lt:50}}).pretty()        | where likes < 50              |\n| Less Than Equals    | {key:{$lte:value}} | db.mycol.find({\"likes\":{$lte:50}}).pretty()       | where likes <= 50             |\n| Greater Than        | {key:{$gt:value}}  | db.mycol.find({\"likes\":{$gt:50}}).pretty()        | where likes > 50              |\n| Greater Than Equals | {key:{$gte:value}} | db.mycol.find({\"likes\":{$gte:50}}).pretty()       | where likes >= 50             |\n| Not Equals          | {key:{$ne:value}}  | db.mycol.find({\"likes\":{$ne:50}}).pretty()        | where likes != 50              |\n**Others**\n-   VS Code extension for mongodb\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/MongoDB/Data-Types/","title":"Data Types"},"frontmatter":{"draft":false},"rawBody":"# Data Types\n\nCreated: 2019-05-06 23:00:05 +0500\n\nModified: 2020-07-22 14:00:04 +0500\n\n---\n\nMongoDB supports many datatypes. Some of them are âˆ’\n-   **Stringâˆ’** This is the most commonly used datatype to store the data. String in MongoDB must be UTF-8 valid.\n-   **Integerâˆ’** This type is used to store a numerical value. Integer can be 32 bit or 64 bit depending upon your server.\n-   **Booleanâˆ’** This type is used to store a boolean (true/ false) value.\n-   **Doubleâˆ’** This type is used to store floating point values.\n-   **Min/ Max keysâˆ’** This type is used to compare a value against the lowest and highest BSON (Binary JSON) elements.\n-   **Arraysâˆ’** This type is used to store arrays or list or multiple values into one key.\n-   **Timestampâˆ’** ctimestamp. This can be handy for recording when a document has been modified or added.\n-   **Objectâˆ’** This datatype is used for embedded documents.\n-   **Nullâˆ’** This type is used to store a Null value.\n-   **Symbolâˆ’** This datatype is used identically to a string; however, it's generally reserved for languages that use a specific symbol type.\n-   **Dateâˆ’** This datatype is used to store the current date or time in UNIX time format. You can specify your own date time by creating object of Date and passing day, month, year into it.\n-   **Object IDâˆ’** This datatype is used to store the document's ID.\n-   **Binary dataâˆ’** This datatype is used to store binary data.\n-   **Codeâˆ’** This datatype is used to store JavaScript code into the document.\n-   **Regular expressionâˆ’** This datatype is used to store regular expression.\n**BSON (Binary JSON)**\n\nBSON[/ËˆbiËsÉ™n/](https://en.wikipedia.org/wiki/Help:IPA/English)is a[computer](https://en.wikipedia.org/wiki/Computer)data interchange format. The name \"BSON\" is based on the term[JSON](https://en.wikipedia.org/wiki/JSON)and stands for \"Binary JSON\". It is a binary form for representing simple or complex[data structures](https://en.wikipedia.org/wiki/Data_structure)including[associative arrays](https://en.wikipedia.org/wiki/Associative_array)(also known as name-value pairs), integer indexed arrays, and a suite of fundamental scalar types. BSON originated in 2009 at[MongoDB](https://en.wikipedia.org/wiki/MongoDB). Several scalar data types are of specific interest to MongoDB and the format is used both as a data storage and network transfer format for the MongoDB database, but it can be used independently outside of MongoDB. Implementations are available in a variety of languages such as [C](https://en.wikipedia.org/wiki/C_(programming_language)), [C++](https://en.wikipedia.org/wiki/C%2B%2B), [C#](https://en.wikipedia.org/wiki/C_Sharp_(programming_language)), [D](https://en.wikipedia.org/wiki/D_(programming_language)), [Delphi](https://en.wikipedia.org/wiki/Delphi_(IDE)), [Erlang](https://en.wikipedia.org/wiki/Erlang_(programming_language)), [Go](https://en.wikipedia.org/wiki/Go_(programming_language)), [Haskell](https://en.wikipedia.org/wiki/Haskell_(programming_language)), [Java](https://en.wikipedia.org/wiki/Java_(programming_language)), [JavaScript](https://en.wikipedia.org/wiki/JavaScript),[Lua](https://en.wikipedia.org/wiki/Lua_(programming_language)),[OCaml](https://en.wikipedia.org/wiki/OCaml),[Perl](https://en.wikipedia.org/wiki/Perl),[PHP](https://en.wikipedia.org/wiki/PHP),[Python](https://en.wikipedia.org/wiki/Python_(programming_language)),[Ruby](https://en.wikipedia.org/wiki/Ruby_(programming_language)),[Rust](https://en.wikipedia.org/wiki/Rust_(programming_language)),[Scala](https://en.wikipedia.org/wiki/Scala_(programming_language)),[Smalltalk](https://en.wikipedia.org/wiki/Smalltalk), and[Swift](https://en.wikipedia.org/wiki/Swift_(programming_language)).\n**JSON vs BSON**\n\nJSON (JavaScript Object Notation)---like XML, for example---is a human-readable standard used for data exchange. JSON has become the most widely used standard for data exchange on the web. JSON supports data types like booleans, numbers, strings, and arrays.\nBSON, however, is the binary encoding that MongoDB uses to store its documents. It is similar to JSON, but it extends JSON to support more data types, likeDate. BSON documents, unlike JSON documents, are ordered. BSON usually takes less space than JSON and is faster to traverse. BSON, since it is binary, is also quicker to encode and decode.\n**Capped Collections**\n\n[Capped collections](https://docs.mongodb.com/manual/reference/glossary/#term-capped-collection)are fixed-size collections that support high-throughput operations that insert and retrieve documents based on insertion order. Capped collections work in a way similar to circular buffers: once a collection fills its allocated space, it makes room for new documents by overwriting the oldest documents in the collection.\n<https://docs.mongodb.com/manual/core/capped-collections>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/MongoDB/Indexes/","title":"Indexes"},"frontmatter":{"draft":false},"rawBody":"# Indexes\n\nCreated: 2020-07-26 15:19:12 +0500\n\nModified: 2020-07-26 15:19:31 +0500\n\n---\n\nIndexes support the efficient execution of queries in MongoDB. Without indexes, MongoDB must perform acollection scan, i.e. scan every document in a collection, to select those documents that match the query statement. If an appropriate index exists for a query, MongoDB can use the index to limit the number of documents it must inspect.\nIndexes are special data structuresthat store a small portion of the collection's data set in an easy to traverse form. The index stores the value of a specific field or set of fields, ordered by the value of the field. The ordering of the index entries supports efficient equality matches and range-based query operations. In addition, MongoDB can return sorted results by using the ordering in the index.\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/MongoDB/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2020-07-22 13:54:29 +0500\n\nModified: 2020-07-27 01:24:51 +0500\n\n---\n\n<https://docs.mongodb.com/manual/core/index-ttl>\n\nTTL indexes are special single-field indexes that MongoDB can use to automatically remove documents from a collection after a certain amount of time or at a specific clock time. Data expiration is useful for certain types of information like machine generated event data, logs, and session information that only need to persist in a database for a finite amount of time.\n**mongo-express**\n\nWeb-based MongoDB admin interface written with Node.js, Express and Bootstrap3\n**Features**\n-   Connect to multiple databases\n-   View/add/delete databases\n-   View/add/rename/delete collections\n-   View/add/update/delete documents\n-   Preview audio/video/image assets inline in collection view\n-   Nested and/or large objects are collapsible for easy overview\n-   Async on-demand loading of big document properties (>100KB default) to keep collection view fast\n-   GridFS support - add/get/delete incredibly large files\n-   Use BSON data types in documents\n-   Mobile / Responsive - Bootstrap 3 works passably on small screens when you're in a bind\n-   Connect and authenticate to individual databases\n-   Authenticate as admin to view all databases\n-   Database blacklist/whitelist\n-   Custom CA and CA validation disabling\n-   Supports replica sets\n<https://github.com/mongo-express/mongo-express>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/MongoDB/Overview/","title":"Overview"},"frontmatter":{"draft":false},"rawBody":"# Overview\n\nCreated: 2019-05-06 22:21:37 +0500\n\nModified: 2020-07-22 13:54:11 +0500\n\n---\n\n**Database**\n\nDatabase is a physical container for collections. Each database gets its own set of files on the file system. A single MongoDB server typically has multiple databases.\n**Collection**\n\nCollection is a group of MongoDB documents. It is the equivalent of an RDBMS table. A collection exists within a single database. Collections do not enforce a schema. Documents within a collection can have different fields. Typically, all documents in a collection are of similar or related purpose.\n**Document**\n\nA document is a set of key-value pairs. Documents have dynamic schema. Dynamic schema means that documents in the same collection do not need to have the same set of fields or structure, and common fields in a collection's documents may hold different types of data.\nThe following table shows the relationship of RDBMS terminology with MongoDB.\n\n| RDBMS                      | MongoDB                                                   |\n|-------------------------|-----------------------------------------------|\n| Database                   | Database                                                  |\n| Table                      | Collection                                                |\n| Tuple/Row                  | Document                                                  |\n| column                     | Field                                                     |\n| Table Join                 | Embedded Documents                                        |\n| Primary Key                | Primary Key (Default key _id provided by mongodb itself) |\n| Database Server and Client |                                                          |\n| Mysqld/Oracle              | mongod                                                    |\n| mysql/sqlplus              | mongo                                                     |\n**Sample Document**\n\n{\n_id: ObjectId(7df78ad8902c)\ntitle: 'MongoDB Overview',\ndescription: 'MongoDB is no sql database',\nby: 'tutorials point',\nurl: '<http://www.tutorialspoint.com>',\ntags: ['mongodb', 'database', 'NoSQL'],\nlikes: 100,\ncomments: [\n{\nuser:'user1',\nmessage: 'My first comment',\ndateCreated: new Date(2011,1,20,2,15),\nlike: 0\n},\n{\nuser:'user2',\nmessage: 'My second comments',\ndateCreated: new Date(2011,1,25,7,45),\nlike: 5\n}\n]\n}\n_idis a 12 bytes hexadecimal number which assures the uniqueness of every document. You can provide _id while inserting the document. If you don't provide then MongoDB provides a unique id for every document. These 12 bytes first 4 bytes for the current timestamp, next 3 bytes for machine id, next 2 bytes for process id of MongoDB server and remaining 3 bytes are simple incremental VALUE.\n# Data Modelling\n\nData in MongoDB has a flexible schema.documents in the same collection. They do not need to have the same set of fields or structure, and common fields in a collection's documents may hold different types of data.\n**Some considerations while designing Schema in MongoDB**\n-   Design your schema according to user requirements.\n-   Combine objects into one document if you will use them together. Otherwise separate them (but make sure there should not be need of joins).\n-   Duplicate the data (but limited) because disk space is cheap as compare to compute time.\n-   Do joins while write, not on read.\n-   Optimize your schema for most frequent use cases.\n-   Do complex aggregation in the schema.\n**Projection**\n\nIn MongoDB, projection means selecting only the necessary data rather than selecting whole of the data of a document. If a document has 5 fields and you need to show only 3, then select only 3 fields from them.\n\nfind() method is used for projection.\n**Indexing**\n\nIndexes support the efficient resolution of queries. Without indexes, MongoDB must scan every document of a collection to select those documents that match the query statement. This scan is highly inefficient and require MongoDB to process a large volume of data.\nIndexes are special data structures, that store a small portion of the data set in an easy-to-traverse form. The index stores the value of a specific field or set of fields, ordered by the value of the field as specified in the index.\n**Aggregations**\n\nAggregations operations process data records and return computed results. Aggregation operations group values from multiple documents together, and can perform a variety of operations on the grouped data to return a single result. In SQL count(*) and with group by is an equivalent of mongodb aggregation.\n**Replication**\n\nMongoDB achieves replication by the use of replica set. A replica set is a group ofmongodinstances that host the same data set. In a replica, one node is primary node that receives all write operations. All other instances, such as secondaries, apply operations from the primary so that they have the same data set. Replica set can have only one primary node.\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/MongoDB/pymongo/","title":"pymongo"},"frontmatter":{"draft":false},"rawBody":"# pymongo\n\nCreated: 2020-07-26 13:11:52 +0500\n\nModified: 2021-06-22 17:14:44 +0500\n\n---\n\n<https://github.com/mongodb/mongo-python-driver>\n\n<https://pymongo.readthedocs.io/en/stable>\n<https://docs.mongoengine.org/guide/connecting.html>\npymongo==3.10.1\n**MongoEngine**\n\nMongoEngine is a Python Object-Document Mapper for working with MongoDB.\n<https://pypi.org/project/mongoengine>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/Best-Practices/","title":"Best Practices"},"frontmatter":{"draft":false},"rawBody":"# Best Practices\n\nCreated: 2020-03-24 12:09:39 +0500\n\nModified: 2021-10-14 20:59:54 +0500\n\n---\n-   [Introduction](https://redislabs.com/redis-best-practices/introduction/)\n-   [Indexing Patterns](https://redislabs.com/redis-best-practices/indexing-patterns/)\n\n<https://redis.io/topics/indexes>-   [Sorted Sets as Indexes](https://redislabs.com/redis-best-practices/indexing-patterns/sorted-sets-indexes/)\n-   [Lexicographical Encoding](https://redislabs.com/redis-best-practices/indexing-patterns/lexicographical-encoding/)\n-   [Geospatial](https://redislabs.com/redis-best-practices/indexing-patterns/geospatial/)\n-   [IP Range Indexing](https://redislabs.com/redis-best-practices/indexing-patterns/ip-range-indexing/)\n-   [Full Text Search](https://redislabs.com/redis-best-practices/indexing-patterns/full-text-search/)\n-   [Partitioned Index](https://redislabs.com/redis-best-practices/indexing-patterns/partitioned-index/)-   [Communication Patterns](https://redislabs.com/redis-best-practices/communication-patterns/)\n    -   [Event Queue](https://redislabs.com/redis-best-practices/communication-patterns/event-queue/)\n    -   [Redlock](https://redislabs.com/redis-best-practices/communication-patterns/redlock/)\n\nIn a system, sometimes you must lock a resource. This might be to make critical modifications that cannot be resolved in any concurrent way. The goals for locks are:\n-   One worker (and only one) worked to be able to acquire rights to a resource\n-   Be able to release this lock reliably\n-   Not deadlock any resource meaning that a resource should be unlocked after a given time period.\nRedis is a good option locking since has a simple key-based data model, each shard is single-threaded, and is quite quick. There is a well-established, canonical implementations of locking using Redis called Redlock.-   [Pub/Sub](https://redislabs.com/redis-best-practices/communication-patterns/pub-sub/)\n-   [Distributed Events](https://redislabs.com/redis-best-practices/communication-patterns/distributed-events/)-   [Data Storage Patterns](https://redislabs.com/redis-best-practices/data-storage-patterns/)\n    -   [JSON Storage](https://redislabs.com/redis-best-practices/data-storage-patterns/json-storage/)\n    -   [Object->Hash Storage](https://redislabs.com/redis-best-practices/data-storage-patterns/object-hash-storage/)\n-   [Time Series Patterns](https://redislabs.com/redis-best-practices/time-series/)\n    -   [Sorted Set Time Series](https://redislabs.com/redis-best-practices/time-series/sorted-set-time-series/)\n    -   [Lexicographic Sorted Set Time Series](https://redislabs.com/redis-best-practices/time-series/lexicographic-sorted-set-time-series/)\n    -   [Time Series with Bitfields](https://redislabs.com/redis-best-practices/time-series/time-series-bitfields/)\n-   [Basic Rate Limiting Pattern](https://redislabs.com/redis-best-practices/basic-rate-limiting/)\n-   [Bloom Filter Pattern](https://redislabs.com/redis-best-practices/bloom-filter-pattern/)\n-   [Counting](https://redislabs.com/redis-best-practices/counting/)\n    -   [Bit Counting Pattern](https://redislabs.com/redis-best-practices/counting/bit-counting-pattern/)\n    -   [HyperLogLog](https://redislabs.com/redis-best-practices/counting/hyperloglog/)\n-   [Lua Helpers](https://redislabs.com/redis-best-practices/lua-helpers/)\n<https://redislabs.com/redis-best-practices>\n\n## Best practices and performance tuning**\n\n**TCP-KeepAlive**\n\nKeepalive is a method to allow the same TCP connection for HTTP conversation instead of opening a new one with each new request.\n**Pipelining**\n\nPipelining facilitates a client to send multiple requests to the server without waiting for the replies at all and finally reads the reply in a single step.\nPipelines are a subclass of the base Redis class that provide support for buffering multiple commands to the server in a single request. They can be used to dramatically increase the performance of groups of commands by reducing the number of back-and-forth TCP packets between the client and server.\n<https://github.com/andymccurdy/redis-py#pipelines>\nPipelining isn't a silver bullet - you need to understand what it does before you use it. What pipelining does is batch several operations that are sent as bulk, as is their response from the server. What you gain is that the network round trip time for each operation is replaced by that of the batch. But infinitely-sized batches are a real drain on resource - you need to keep their size small enough to be effective. As a rule of thumb I usually try to aim to 60KB per pipeline and since every data is different, so does the number of actual operations in a pipeline. Assuming that your key and its value are ~1KB, you need to callpipeline.execute()every 60 operations or so.\n<https://stackoverflow.com/questions/32149626/how-to-insert-billion-of-data-to-redis-efficiently>\n\n## Mass Insertion / Bulk Inserts**\n\n<https://redis.io/topics/mass-insert>\n\n## Max-Connection**\n\ndefine the maximum connection limit to the Redis Server.\n**Overcommit memory**\n\nOvercommit memory is a kernel parameter which checks if the memory is available or not. If the overcommit memory value is 0 then there is a chance that your Redis will get OOM (Out of Memory) error.\necho 'vm.overcommit_memory = 1' >> /etc/sysctl.conf\n| **Config Option** | **Value**         | **Description**                                                                                                                                                                                                     |\n|---------------|-----------|-----------------------------------------------|\n| maxmemory         | 70% of the system | maxmemory should be 70 percent of the system so that it will not take all the resource of the server.                                                                                                               |\n| maxmemory-policy  | volatile-lru      | It adds a random key with an expiry time                                                                                                                                                                            |\n| loglevel          | notice            | Loglevel should be \"notice\", so that log will not take too much resource                                                                                                                                            |\n| timeout           | 300               | There should be a timeout value as well in redis configuration which prevents redis from spending too much time on the connection. It closes the connection of the client if it is ideal for more than 300 seconds. |\n**Memory Optimizations**\n\n[Compress Values](https://docs.redislabs.com/latest/ri/memory-optimizations/compress-values/)\n\nRedis and clients are typically IO bound and the IO costs are typically at least 2 orders of magnitude in respect to the rest of the request/reply sequence. **Redis by default does not compress any value that is stored in it**, hence it becomes important to compress your data before storing in Redis. This helps in reducing the payload which in return gives you higher throughput, lower latency and higher savings in your cost.\n[Use Smaller Keys](https://docs.redislabs.com/latest/ri/memory-optimizations/use-smaller-keys/)\n\nRedis keys can play a devil in increasing the memory consumption for your Redis instances. In general, you should always prefer descriptive keys but if you have a large dataset having millions of keys then these large keys can eat a lot of your money. How to Convert to Smaller Keys In a well written application, switching to shorter keys usually involves updating a few constant strings in the application code.\n[Switch to 32 Bits](https://docs.redislabs.com/latest/ri/memory-optimizations/switch-to-32-bits/)\n\nRedis gives you the following statistics for a 64-bit machine. An empty instance uses ~ 3MB of memory. 1 Million small Keys -> String Value pairs use ~ 85MB of memory. 1 Million Keys -> Hash value, representing an object with 5 fields, use ~ 160 MB of memory. 64-bit has more memory available as compared to a 32-bit machine. **But if you are sure that your data size does not exceed 3 GB then storing in 32 bits is a good option.**\n[Upgrade Redis Version](https://docs.redislabs.com/latest/ri/memory-optimizations/upgrade-redis-version/)\n\nRedis 4.0 is the latest version that has been launched. It contains various big improvements compared to the previous versions. It supports mixed RDB+AOF Format. Improvement in memory usage and performance. New Memory Command has been introduced. Active Memory Defragmentation. Faster Redis Cluster key creation. Trade Offs Redis 4.0 is still not a stable release but is a very battle tested release, so Redis 3.2 is a better pick for critical applications till Redis 4.\n[Use Better Serializer](https://docs.redislabs.com/latest/ri/memory-optimizations/use-better-serializer/)\n\nRedis does not have any specific data type to store the serialized objects, they are stored as byte array in Redis. If we are using regular means of serializing our java,python and PHP objects, they can be of larger size which impacts the memory consumption and latency. Which Serializers to Use Instead of default serializer of your programming language (java serialzed objects, python pickle, PHP serialize etc), switch to a better library.\n[Combine Smaller Strings to Hashes](https://docs.redislabs.com/latest/ri/memory-optimizations/combine-smaller-strings-to-hash/)\n\nStrings data type has an overhead of about 90 bytes on a 64 bit machine. In other words, calling set foo bar uses about 96 bytes, of which 90 bytes is overhead. You should use the String data type only if: The value is at least greater than 100 bytes You are storing encoded data in the string - JSON encoded or Protocol buffer You are using the string data type as an array or a bitset If you are not doing any of the above, then use Hashes.\n[Switch from Set to Intset](https://docs.redislabs.com/latest/ri/memory-optimizations/switch-set-to-intset/)\n\nSets that contain only integers are extremely efficient memory wise. If your set contains strings, try to use integers by mapping string identifiers to integers. You can either use enums in your programming language, or you can use a redis hash data structure to map values to integers. Once you switch to integers, Redis uses the IntSet encoding internally. This encoding is extremely memory efficient. By default, the value of set-max-intset-entries is 512, but you can set this value in redis.\n[Switch to bloom filter or hyperloglog](https://docs.redislabs.com/latest/ri/memory-optimizations/switch-to-bloom-filters/)\n\nUnique items can be difficult to count. Usually this means storing every unique item then recalling this information somehow. With Redis, this can be accomplished by using a set and a single command, however both the storage and time complexity of this with very large sets is prohibitive. HyperLogLog provides a probabilistic alternative. If your set contains a very large number of elements, and you are only using the set for existence checks or to eliminate duplicates - then you benefit by using a bloom filter.\n[Shard Big Hashes to Small Hashes](https://docs.redislabs.com/latest/ri/memory-optimizations/shard-big-hash-to-small-hash/)\n\nIf you have a hash with large number of key, value pairs, and if each key, value pair is small enough - break it into smaller hashes to save memory. To shard a HASH table, we need to choose a method of partitioning our data. Hashes themselves have keys which can be used for partitioning the keys into different shards. The number of shards are determined by the total number of keys we want to store and the shard size.\n[Convert Hashtable to Ziplist for Hashes](https://docs.redislabs.com/latest/ri/memory-optimizations/convert-hash-to-ziplist/)\n\nHashes have two types of encoding- HashTable and Ziplist. The decision of storing in which of the data structures in done based on the two configurations Redis provides - hash-max-ziplist-entries and hash-max-ziplist-values. By default the redis conf has these settings as: hash-max-ziplist-entries = 512 hash-max-ziplist-values = 64 So if any value for a key exceeds the two configurations it is stored automatically as a Hashtable instead of a Ziplist.\n[Convert to a List Instead of Hash](https://docs.redislabs.com/latest/ri/memory-optimizations/convert-to-list-instead-of-hash/)\n\nA Redis Hash stores field names and values. If you have thousands of small hash objects with similar field names, the memory used by field names adds up. To prevent this, consider using a list instead of a hash. The field names become indexes into the list. While this may save memory, you should only use this approach if you have thousands of hashes, and if each of those hashes have similar fields.\n[Compress Field Names](https://docs.redislabs.com/latest/ri/memory-optimizations/compress-field-names/)\n\nRedis Hash consists of Fields and their values. Like values, field name also consumes memory, so it is required to keep in mind while assigning field names. If you have a large number of hashes with similar field names, the memory adds up significantly. To reduce memory usage, you can use smaller field names. What do We Mean By Compress Field Names Referring to the previous example in convert hashes to list, we had a hash having user details.\n[Avoid Dynamic Lua Script](https://docs.redislabs.com/latest/ri/memory-optimizations/avoid-dynamic-lua-script/)\n\nRefrain from generating dynamic scripts, which can cause your Lua cache to grow and get out of control. Memory is consumed as we have scripts loaded. The memory consumption are because of the following factors. Memory used by the server.lua_scripts dictionary holding original text memory used internally by Lua to keep the compiled byte-code. So If you have to use dynamic scripting, then just use plain EVAL, as there's no point in loading them first.\n\n[Enable Compression for List](https://docs.redislabs.com/latest/ri/memory-optimizations/enable-compression-for-list/)\n\nList is just a link list of arrays, where none of the arrays are compressed. By default, redis does not compress elements inside a list. However, if you use long lists, and mostly access elements from the head and tail only, then you can enable compression. We have two configurations: List-max-ziplist-size: 8kb(default) List-compression-depth: 0,1,2 (0 by default) A configuration change in redis.conf list-compression-depth=1 helps you achieve compression. What is compression-depth Compression depth is the number of list nodes from each end of the list to leave untouched before we start compressing inner nodes.\n[Reclaim Expired Keys Memory Faster](https://docs.redislabs.com/latest/ri/memory-optimizations/reclaim-expired-keys-memory-faster/)\n\nWhen you set an expiry on a key, redis does not expire it at that instant. Instead, it uses a randomized algorithm to find out keys that should be expired. Since this algorithm is random, there are chances that the keys are not expired. This means that redis consumes memory to hold keys that have already expired. The moment the key is accessed, it is deleted. If there are only a few keys that have expired and redis hasn't deleted them - it is fine.\n<https://docs.redislabs.com/latest/ri/memory-optimizations>\n\n<https://redis.io/topics/memory-optimization>\n\n<https://cloud.google.com/memorystore/docs/redis/memory-management-best-practices>\n\n<https://www.datadoghq.com/pdf/Understanding-the-Top-5-Redis-Performance-Metrics.pdf>\n\n## Performance Metrics**\n\n1.  Memory Usage: used_memory\n\n2.  Number of commands processed: total_commands_processed\n\n3.  Latency\n\n4.  Fragmentation Ratio\n\n5.  Evictions\n**Parsers**\n\nParser classes provide a way to control how responses from the Redis server are parsed. redis-py ships with two parser classes, the PythonParser and the HiredisParser. By default, redis-py will attempt to use the HiredisParser if you have the hiredis module installed and will fallback to the PythonParser otherwise.\nHiredis is a C library maintained by the core Redis team. Pieter Noordhuis was kind enough to create Python bindings. Using Hiredis can provide up to a 10x speed improvement in parsing responses from the Redis server. The performance increase is most noticeable when retrieving many pieces of data, such as from LRANGE or SMEMBERS operations.\n\n$ pip install hiredis\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/Commands/","title":"Commands"},"frontmatter":{"draft":false},"rawBody":"# Commands\n\nCreated: 2020-03-20 23:41:17 +0500\n\nModified: 2022-11-21 15:35:38 +0500\n\n---\n\n**Installation**\n\nbrew install redis\n\napt-get install redis\ndocker run --name redis -p 6379:6379 redis\ndocker run --name redis -e ALLOW_EMPTY_PASSWORD=yes --rm -p 6379:6379 bitnami/redis:latest\n\ndocker exec -it redis bash\nredis:\n\nstdin_open: true\n\ntty: true\n\nrestart: \"no\"\n\nimage: bitnami/redis:5.0.8\n\ncontainer_name: redis\n\nenv_file:\n\n- decision_engine.env\n\nports:\n\n- 6379:6379\n\nhealthcheck:\n\ntest: \"redis-cli -h localhost -p 6379 ping\"\n\ninterval: 10s\n\ntimeout: 10s\n\nretries: 5\n\nvolumes:\n\n- ./data/redis:/bitnami/redis/data\n**Redis Insight**\n\ndocker run -rm -it -v redisinsight:/db -p 8001:8001 redislabs/redisinsight\n**Kubernetes**\n\n<https://github.com/bitnami/charts/tree/master/bitnami/redis>\n\n<https://raw.githubusercontent.com/bitnami/charts/master/bitnami/redis/values-production.yaml>\n\nhelm upgrade --install redis --values k8s/infra/redis-values-production.yaml --namespace apps bitnami/redis\n| Redis                                                                                                                                                                    | Redis Cluster                                                                                                                                                                    |\n|-------------------------------|-----------------------------------------|\n| Supports multiple databases                                                                                                                                              | Supports only one database. Better if you have a big dataset                                                                                                                     |\n| Single write point (single master)                                                                                                                                       | Multiple write points (multiple masters)                                                                                                                                         |\n| ![Redis Topology](media/Redis_Commands-image1.png) |**Commands**\n\nredis-cli ping\n\n#staging decision-engine redis-cli -h localhost -p 6379 -a 'a6ad92769ef04b711eea18dccfff85ea' ping\n\nredis-cli -h redis -p 6379 -a a6ad92769ef04b711eea18dccfff85ea ping\n\nredis-cli -h redis-dashboard -p 6379 -a a6ad92769ef04b711eea18dccfff85ea\n\n**#decision engine** redis-cli -h localhost -p 6379 -a a6ad92769ef04b711eea18dccfff85ea\n\n**#streams** redis-cli -h localhost -p 6379 -a y2Tb8FaxGyk6qm1s\n**# find out all keys with no ttl set**\n\nredis-cli -a a6ad92769ef04b711eea18dccfff85ea --no-auth-warning --scan | while read LINE ; do TTL=`redis-cli --no-auth-warning -a a6ad92769ef04b711eea18dccfff85ea ttl \"$LINE\"`; if [ $TTL -eq -1 ]; then echo \"$LINE\"; fi; done;\n**DML**\n\n**CONFIG GET ***\n```\ninfo # <https://redis.io/commands/info>\n\nconfig set maxmemory [value]\nconfig set maxmemory 2gb\nconfig set maxmemory 5gb\nconfig set maxmemory 48gb\nconfig set maxmemory 80gb\n\nmaxmemory-policy\n```\n\n### Cleanups\n\n**BGREWRITEAOF #Compress AOF**\n**auto-aof-rewrite-percentage**\n\n**CONFIG SET auto-aof-rewrite-percentage 50**\n\n<https://www.oreilly.com/library/view/redis-4x-cookbook/9781783988167/64284aa9-a324-4383-b9f4-9db3ae95ffb4.xhtml>\n\n## DDL\n```\n# -n for setting database\n\nredis-cli -h redis-dashboard -p 6379 -a DGfYvYv5b55LwMmBiPgctk1CtKvxlouQ1jqNn70sQ -n 1\nredis-cli -a DGfYvYv5b55LwMmBiPgctk1CtKvxlouQ1jqNn70sQ -p 6379. FLUSHALL\n>>> redis-cli\n\n>>> subscribe channel_name\n\n>>> publish channel_name \"message\"\n\n>>> flushdb\n>>> keys *\n\n**>>> keys sms:key:***\n> set mykey somevalue\n\n> set mykey 100 ex 10 # mykey will expire after 10 seconds\n\n> get mykey\n\n> incr counter\n\n> incrby counter 50\n\n> mset a 10 b 20 c 30\n\n> mget a b c\n\n> del mykey\n> exists mykey\n> type mykey\n\n> expire mykey 5 # 5 second expiry time for key\n\n> ttl key # how many seconds to expire\n\n> pexpire mykey 5 # 5 milliseconds time for key\n\n> pttl key # how many milliseconds to expire\n\n> rpush mylist 1 2 3 4 5 \"foo bar\"\n> lrange mylist 0 -1\n\n> rpop mylist\n\n> lpop mylist\n\n> ltrim mylist 0 2 # removes elements other than index 0 to 2\n\n> brpop tasks 5\n\n> blpop tasks 5\n\n> rpoplpush\n\n> brpoplpush\n\n> llen mylist\n\n> hmset user:1000 username antirez birthyear 1977 verified 1\n> hget user:1000 username\n\n> hgetall user:1000\n\n> DEBUG OBJECT key #show size of key\n**>** scan 0 MATCH sms:score:*\n\n**>** sscan myset 0 match f*\n\n**> hscan seen: 0**\n\n**>** zscan queue:default 0\n<https://redis.io/commands/scan>ACL GENPASS\n\nACL GETUSER default\nSome of the commands that are considered dangerous include: **FLUSHDB, FLUSHALL, KEYS, PEXPIRE, DEL, CONFIG, SHUTDOWN, BGREWRITEAOF, BGSAVE, SAVE, SPOP, SREM, RENAME, and DEBUG.**\n```"},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/Documentation/","title":"Documentation"},"frontmatter":{"draft":false},"rawBody":"# Documentation\n\nCreated: 2020-03-28 00:36:27 +0500\n\nModified: 2020-08-19 01:15:52 +0500\n\n---\n\n**Programming with Redis**\n-   [The full list of commands](https://redis.io/commands)implemented by Redis, along with thorough documentation for each of them.\n-   [Pipelining](https://redis.io/topics/pipelining): Learn how to send multiple commands at once, saving on round trip time.\n\nsendmultiple commandsto the server without waiting for the replies at all, and finally read the replies in a single step.\n-   [Redis Pub/Sub](https://redis.io/topics/pubsub): Redis is a fast and stable Publish/Subscribe messaging system\n-   [Redis Lua scripting](https://redis.io/commands/eval): Redis Lua scripting feature documentation.\n-   [Debugging Lua scripts](https://redis.io/topics/ldb): Redis 3.2 introduces a native Lua debugger for Redis scripts.\n-   [Memory optimization](https://redis.io/topics/memory-optimization): Understand how Redis uses RAM and learn some tricks to use less of it.\n-   [Expires](https://redis.io/commands/expire): Redis allows to set a time to live different for every key so that the key will be automatically removed from the server when it expires.\n-   [Redis as an LRU cache](https://redis.io/topics/lru-cache): How to configure and use Redis as a cache with a fixed amount of memory and auto eviction of keys.\n-   [Redis transactions](https://redis.io/topics/transactions): It is possible to group commands together so that they are executed as a single transaction.\n-   [Mass insertion of data](https://redis.io/topics/mass-insert): How to add a big amount of pre existing or generated data to a Redis instance in a short time.\n-   [Partitioning](https://redis.io/topics/partitioning): How to distribute your data among multiple Redis instances.\n-   [Distributed locks](https://redis.io/topics/distlock): Implementing a distributed lock manager with Redis.\n-   [Redis keyspace notifications](https://redis.io/topics/notifications): Get notifications of keyspace events via Pub/Sub (Redis 2.8 or greater).\n-   [Creating secondary indexes with Redis](https://redis.io/topics/indexes): Use Redis data structures to create secondary indexes, composed indexes and traverse graphs.\n\n<https://redis.io/topics/indexes>\n\n## Redis modules API**\n-   [Introduction to Redis modules](https://redis.io/topics/modules-intro). A good place to start learing about Redis 4.0 modules programming.\n-   [Implementing native data types](https://redis.io/topics/modules-native-types). Modules scan implement new data types (data structures and more) that look like built-in data types. This documentation covers the API to do so.\n-   [Blocking operations](https://redis.io/topics/modules-blocking-ops)with modules. This is still an experimental API, but a very powerful one to write commands that can block the client (without blocking Redis) and can execute tasks in other threads.\n-   [Redis modules API reference](https://redis.io/topics/modules-api-ref). Directly generated from the top comments in the source code insidesrc/module.c. Contains many low level details about API usage.\n**Tutorials & FAQ**\n-   [Introduction to Redis data types](https://redis.io/topics/data-types-intro): This is a good starting point to learn the Redis API and data model.\n-   [Introduction to Redis streams](https://redis.io/topics/streams-intro): A detailed description of the Redis 5 new data type, the Stream.\n-   [Writing a simple Twitter clone with PHP and Redis](https://redis.io/topics/twitter-clone)\n-   [Auto complete with Redis](http://autocomplete.redis.io/)\n-   [Data types short summary](https://redis.io/topics/data-types): A short summary of the different types of values that Redis supports, not as updated and info rich as the first tutorial listed in this section.\n-   [FAQ](https://redis.io/topics/faq): Some common questions about Redis.\n**Administration**\n-   [Redis-cli](https://redis.io/topics/rediscli): Learn how to master the Redis command line interface, something you'll be using a lot in order to administer, troubleshoot and experiment with Redis.\n-   [Configuration](https://redis.io/topics/config): How to configure redis.\n-   [Replication](https://redis.io/topics/replication): What you need to know in order to set up master-replicas replication.\n-   [Persistence](https://redis.io/topics/persistence): Know your options when configuring Redis' durability.\n-   [Redis Administration](https://redis.io/topics/admin): Selected administration topics.\n-   [Security](https://redis.io/topics/security): An overview of Redis security.\n-   [Encryption](https://redis.io/topics/encryption): How to encrypt Redis client-server communication.\n-   [Signals Handling](https://redis.io/topics/signals): How Redis handles signals.\n-   [Connections Handling](https://redis.io/topics/clients): How Redis handles clients connections.\n-   [High Availability](https://redis.io/topics/sentinel): Redis Sentinel is the official high availability solution for Redis.\n-   [Latency monitoring](https://redis.io/topics/latency-monitor): Redis integrated latency monitoring and reporting capabilities are helpful to tune Redis instances for low latency workloads.\n-   [Benchmarks](https://redis.io/topics/benchmarks): See how fast Redis is in different platforms.\n-   [Redis Releases](https://redis.io/topics/releases): Redis development cycle and version numbering.\n**Embedded and IoT**\n-   [Redis on ARM and Raspberry Pi](https://redis.io/topics/ARM): Starting with Redis 4.0 ARM and the Raspberry Pi are officially supported platforms. This page contains general information and benchmarks.\n-   [A reference implementation of Redis for IoT and Edge Computing can be found here](https://redislabs.com/redis-enterprise/redis-edge/).\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2020-03-24 11:54:17 +0500\n\nModified: 2021-03-24 15:54:36 +0500\n\n---\n\n**Redis Time Series**\n\nRedisTimeSeries simplifies the use of Redis for time-series use cases like IoT, stock prices, and telemetry.\nWith RedisTimeSeries, ingest and query millions of metrics and events per second using an optimal data structure. Advanced techniques such as downsampling and aggregation ensure a small memory footprint without impacting performance. Use a variety of queries for visualization and monitoring with built-in connectors to popular tools like Grafana, Prometheus, and Telegraf.-   Downsampling and retention\n-   Aggregation, range queries, and special counter operations\n<https://redislabs.com/redis-enterprise/redis-time-series>\n\n<https://redislabs.com/blog/redistimeseries-ga-making-4th-dimension-truly-immersive>\n\n## RedisAI**\n\n<https://oss.redislabs.com/redisai>\n\n## RedisEdge**\n\nRedisEdge from Redis Labs is a purpose-built, multi-model database for the demanding conditions at the Internet of Things (IoT) edge. It can ingest millions of writes per second with <1ms latency and a very small footprint (<5MB), so it easily resides in constrained compute environments. It can run on a variety of edge devices and sensors ranging from ARM32 to x64-based hardware. RedisEdge bundles open source Redis (version 5 with Redis Streams) with the RedisAI and RedisTimeSeries modules, along with RedisGears for inter-module communication.\n\n![RedisEdge --- EDGE Redis Time Streams + Series Gears ](media/Redis_Others-image1.png)\n**Redis Insight**\n\nInspect your Redis data, monitor health, and perform runtime server configuration with a browser-based management interface for your Redis deployment\ndocker run -v redisinsight:/db -p 8001:8001 redislabs/redisinsight\n<https://redislabs.com/redisinsight>\n\n<https://docs.redislabs.com/latest/ri>\n\n## Redis Bloom**\n\nRedisBloom extends Redis core to support additional probabilistic data structures. It allows for solving computer science problems at a constant memory space with extremely fast processing and a low error rate. It supports scalable Bloom and Cuckoo Filters to determine (with a given degree of certainty) whether an item is present or absent from a collection. Count-Mins-Sketch is used to count the frequency of the different items in sub-linear space and TopK allows Redis to count top k events in a (close to) deterministic manner.\n<https://redislabs.com/redis-enterprise/redis-bloom>\n\n## Redis Json**\n\nRedisJSON provides fast in-memory manipulation of JSON documents at high velocity and volume. With RedisJSON, you can natively store document data in a hierarchical, tree-like format to scale and query documents efficiently, significantly improving performance over storing and manipulating JSON with Lua and core Redis data structures.\n<https://redislabs.com/redis-enterprise/redis-json>\n\n<https://redislabs.com/blog/redis-as-a-json-store>\n\n## Redis Graph**\n\nRedisGraph is based on a unique approach and architecture that translates Cypher queries to matrix operations executed over a GraphBLAS engine. This new design allows use cases like social graph operation, fraud detection, and real-time recommendation to be executed 10x -- 600x faster than any other graph database.\n<https://redislabs.com/redis-enterprise/redis-graph>\n\n## RediSearch**\n\nRediSearch is a fast search engine that enables you to query your recently indexed Redis data to answer any kind of complex question. Use it as a secondary index for datasets hosted on other data stores, a fast text search or auto-complete engine, and as a search engine that powers other modules like RedisGraph and RedisTimeSeries.\nRich with features, RediSearch supports many capabilities including ranking, boolean queries, geo-filters, synonyms, numeric filters and ranges, aggregation, and more, and even allows you to add your own custom scoring code.\n<https://redislabs.com/redis-enterprise/redis-search>\n\n## Redis on Flash (SSD)**\n\nRedis on Flash (RoF) offers users of[Redis Enterprise Software](https://docs.redislabs.com/latest/rs/)and[Redis Enterprise Cloud](https://redislabs.com/redis-enterprise-cloud/)the unique ability to have very large Redis databases but at significant cost savings. Where standard Redis databases must all be in RAM, Redis on Flash enables your Redis databases to span both RAM and dedicated flash memory (SSD).Whilst keys are always stored in RAM, RoF intelligently manages the location of their values (RAM vs Flash) in the database via a LRU-based (least-recently-used) mechanism. Hot values are stored in RAM, but infrequently used, or warm values, are ejected to flash memory. This enables you to have much larger datasets with RAM-like latency and performance, but at dramatically lower cost than an all-RAM database.\n<https://docs.redislabs.com/latest/rs/concepts/memory-architecture/redis-flash>\n\n## Security**\n\n<https://redis.io/topics/security>\n\n<https://redis.io/topics/acl>\n\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/Redis-Concepts/","title":"Redis Concepts"},"frontmatter":{"draft":false},"rawBody":"# Redis Concepts\n\nCreated: 2020-03-20 23:56:32 +0500\n\nModified: 2021-04-07 21:05:55 +0500\n\n---\n\n**Redis keys**\n\nRedis keys are binary safe, this means that you can use any binary sequence as a key, from a string like \"foo\" to the content of a JPEG file. The empty string is also a valid key.\nA few other rules about keys:\n-   Very long keys are not a good idea. For instance a key of 1024 bytes is a bad idea not only memory-wise, but also because the lookup of the key in the dataset may require several costly key-comparisons. Even when the task at hand is to match the existence of a large value, hashing it (for example with SHA1) is a better idea, especially from the perspective of memory and bandwidth.\n-   Very short keys are often not a good idea. There is little point in writing \"u1000flw\" as a key if you can instead write \"user:1000:followers\". The latter is more readable and the added space is minor compared to the space used by the key object itself and the value object. While short keys will obviously consume a bit less memory, your job is to find the right balance.\n-   Try to stick with a schema. For instance \"object-type:id\" is a good idea, as in \"user:1000\". Dots or dashes are often used for multi-word fields, as in \"comment:1234:reply.to\" or \"comment:1234:reply-to\".\n-   The maximum allowed key size is 512 MB.\n**Redis Strings**\n\nValues can be strings (including binary data) of every kind, for instance you can store a jpeg image inside a value. A value can't be bigger than 512 MB.\nNote that[SET](https://redis.io/commands/set)will replace any existing value already stored into the key, in the case that the key already exists, even if the key is associated with a non-string value. So[SET](https://redis.io/commands/set)performs an assignment.\nThe[SET](https://redis.io/commands/set)command has interesting options, that are provided as additional arguments. For example, I may ask[SET](https://redis.io/commands/set)to fail if the key already exists, or the opposite, that it only succeed if the key already exists:\n\n> set mykey newval nx\n(nil)\n> set mykey newval xx\nOK\nEven if strings are the basic values of Redis, there are interesting operations you can perform with them. For instance, one is atomic increment:\n\n> set counter 100\nOK\n> incr counter\n(integer) 101\n> incr counter\n(integer) 102\n> incrby counter 50\n(integer) 152\nThe[INCR](https://redis.io/commands/incr)command parses the string value as an integer, increments it by one, and finally sets the obtained value as the new value. There are other similar commands like[INCRBY](https://redis.io/commands/incrby),[DECR](https://redis.io/commands/decr)and[DECRBY](https://redis.io/commands/decrby). Internally it's always the same command, acting in a slightly different way.\nWhat does it mean that INCR is atomic? That even multiple clients issuing INCR against the same key will never enter into a race condition. For instance, it will never happen that client 1 reads \"10\", client 2 reads \"10\" at the same time, both increment to 11, and set the new value to 11. The final value will always be 12 and the read-increment-set operation is performed while all the other clients are not executing a command at the same time.\nThere are a number of commands for operating on strings. For example the[GETSET](https://redis.io/commands/getset)command sets a key to a new value, returning the old value as the result. You can use this command, for example, if you have a system that increments a Redis key using[INCR](https://redis.io/commands/incr)every time your web site receives a new visitor. You may want to collect this information once every hour, without losing a single increment. You can[GETSET](https://redis.io/commands/getset)the key, assigning it the new value of \"0\" and reading the old value back.\n\nThe ability to set or retrieve the value of multiple keys in a single command is also useful for reduced latency. For this reason there are the[MSET](https://redis.io/commands/mset)and[MGET](https://redis.io/commands/mget)commands:\n\n> mset a 10 b 20 c 30\nOK\n> mget a b c\n1) \"10\"\n2) \"20\"\n3) \"30\"\n\nWhen[MGET](https://redis.io/commands/mget)is used, Redis returns an array of values.\n**Redis Expires**\n-   They can be set both using seconds or milliseconds precision.\n-   However the expire time resolution is always 1 millisecond.\n-   Information about expires are replicated and persisted on disk, the time virtually passes when your Redis server remains stopped (this means that Redis saves the date at which a key will expire).\n**How Redis expires keys**\n\nRedis keys are expired in two ways: a passive way, and an active way.\nA key is passively expired simply when some client tries to access it, and the key is found to be timed out.\nOf course this is not enough as there are expired keys that will never be accessed again. These keys should be expired anyway, so periodically Redis tests a few keys at random among keys with an expire set. All the keys that are already expired are deleted from the keyspace.\nSpecifically this is what Redis does 10 times per second:\n\n1.  Test 20 random keys from the set of keys with an associated expire.\n\n2.  Delete all the keys found expired.\n\n3.  If more than 25% of keys were expired, start again from step 1.\nThis is a trivial probabilistic algorithm, basically the assumption is that our sample is representative of the whole key space, and we continue to expire until the percentage of keys that are likely to be expired is under 25%\nThis means that at any given moment the maximum amount of keys already expired that are using memory is at max equal to max amount of write operations per second divided by 4.\n<https://redis.io/commands/expire#how-redis-expires-keys>\n<https://redis.io/topics/data-types-intro>\n\n## Scan**\n\n**Time complexity:**O(1) for every call. O(N) for a complete iteration, including enough command calls for the cursor to return back to 0. N is the number of elements inside the collection.\nThe[SCAN](https://redis.io/commands/scan)command and the closely related commands[SSCAN](https://redis.io/commands/sscan),[HSCAN](https://redis.io/commands/hscan)and[ZSCAN](https://redis.io/commands/zscan)are used in order to incrementally iterate over a collection of elements.\n-   [SCAN](https://redis.io/commands/scan)iterates the set of keys in the currently selected Redis database.\n-   [SSCAN](https://redis.io/commands/sscan)iterates elements of Sets types.\n-   [HSCAN](https://redis.io/commands/hscan)iterates fields of Hash types and their associated values.\n-   [ZSCAN](https://redis.io/commands/zscan)iterates elements of Sorted Set types and their associated scores.\nThese commands allow for incremental iteration, returning only a small number of elements per call, they can be used in production without the downside of commands likeKEYSorSMEMBERSthat may block the server for a long time (even several seconds) when called against big collections of keys or elements.\nSCAN is a cursor based iterator. This means that at every call of the command, the server returns an updated cursor that the user needs to use as the cursor argument in the next call.\nAn iteration starts when the cursor is set to 0, and terminates when the cursor returned by the server is 0.\nStarting an iteration with a cursor value of 0, and calling[SCAN](https://redis.io/commands/scan)until the returned cursor is 0 again is called afull iteration.\n> sscan myset 0 match f*\n\n> redis-cli -a a6ad92769ef04b711eea18dccfff85ea --no-auth-warning --scan | while read LINE ; do TTL=`redis-cli --no-auth-warning -a a6ad92769ef04b711eea18dccfff85ea ttl \"$LINE\"`; if [ $TTL -eq -1 ]; then echo \"$LINE\"; fi; done;\n<https://redis.io/commands/scan>\n\n## Persistence**\n-   The RDB persistence performs point-in-time snapshots of your dataset at specified intervals.\n-   The AOF persistence logs every write operation received by the server, that will be played again at server startup, reconstructing the original dataset. Commands are logged using the same format as the Redis protocol itself, in an append-only fashion. Redis is able to rewrite the log in the background when it gets too big.\n-   If you wish, you can disable persistence completely, if you want your data to just exist as long as the server is running.\n-   It is possible to combine both AOF and RDB in the same instance. Notice that, in this case, when Redis restarts the AOF file will be used to reconstruct the original dataset since it is guaranteed to be the most complete.\n**AOF - Append Only File**\n\nIt's the change-log style persistent format.\n\nAOF is actually a persistence technique in which an RDB file is generated once and all the data is appended to it as it comes\n**RDB - Redis Database Backup**\n\nIt's the snapshot style persistence format.\n\nRDB file is a dump of all user data stored in an internal, compressed serialization format at a particular timestamp which is used for point-in-time recovery (recovery from a timestamp).\n**Compress AOF**\n\nBGREWRITEAOF\n<https://redis.io/commands/bgrewriteaof>\n<https://stackoverflow.com/questions/39953542/aof-and-rdb-backups-in-redis>\n\n[**https://redis.io/topics/persistence**](https://redis.io/topics/persistence)\n\n<https://redislabs.com/ebook/part-2-core-concepts/chapter-4-keeping-data-safe-and-ensuring-performance/4-1-persistence-options>\n\n## Redis Keyspace Notifications**\n\nKeyspace notifications allow clients to subscribe to Pub/Sub channels in order to receive events affecting the Redis data set in some way.\nExamples of events that can be received are:\n-   All the commands affecting a given key.\n-   All the keys receiving an LPUSH operation.\n-   All the keys expiring in the database 0.\nEvents are delivered using the normal Pub/Sub layer of Redis, so clients implementing Pub/Sub are able to use this feature without modifications.\nBecause Redis Pub/Sub isfire and forgetcurrently there is no way to use this feature if your application demandsreliable notificationof events, that is, if your Pub/Sub client disconnects, and reconnects later, all the events delivered during the time the client was disconnected are lost.\nIn the future there are plans to allow for more reliable delivering of events, but probably this will be addressed at a more general level either bringing reliability to Pub/Sub itself, or allowing Lua scripts to intercept Pub/Sub messages to perform operations like pushing the events into a list.\n<https://redis.io/topics/notifications>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/Redis-Data-Types/","title":"Redis Data Types"},"frontmatter":{"draft":false},"rawBody":"# Redis Data Types\n\nCreated: 2020-03-20 23:18:22 +0500\n\nModified: 2021-01-09 04:26:53 +0500\n\n---\n\nRedis is not aplainkey-value store, it is actually adata structures server, supporting different kinds of values. What this means is that, while in traditional key-value stores you associate string keys to string values, in Redis the value is not limited to a simple string, but can also hold more complex data structures. The following is the list of all the data structures supported by Redis\n-   **Binary-safe strings**\n-   **Lists:** collections of string elements sorted according to the order of insertion. They are basicallylinked lists.\n-   **Sets:** collections of unique, unsorted string elements.\n-   **Sorted sets,** similar to Sets but where every string element is associated to a floating number value, calledscore. The elements are always taken sorted by their score, so unlike Sets it is possible to retrieve a range of elements (for example you may ask: give me the top 10, or the bottom 10).\n-   **Hashes,** which are maps composed of fields associated with values. Both the field and the value are strings. This is very similar to Ruby or Python hashes.\n-   **Bit arrays (or simply bitmaps):** it is possible, using special commands, to handle String values like an array of bits: you can set and clear individual bits, count all the bits set to 1, find the first set or unset bit, and so forth.\n-   **HyperLogLogs:** this is a probabilistic data structure which is used in order to estimate the cardinality of a set.\n-   **Streams:** append-only collections of map-like entries that provide an abstract log data type.\n**Redis Lists**\n\nRedis lists are implemented via Linked Lists. This means that even if you have millions of elements inside a list, the operation of adding a new element in the head or in the tail of the list is performedin constant time. The speed of adding a new element with the[LPUSH](https://redis.io/commands/lpush)command to the head of a list with ten elements is the same as adding an element to the head of list with 10 million elements.\nRedis Lists are implemented with linked lists because for a database system it is crucial to be able to add elements to a very long list in a very fast way. Redis Lists can be taken at constant length in constant time.\nWhen fast access to the middle of a large collection of elements is important, there is a different data structure that can be used, called sorted sets. Sorted sets will be covered later in this tutorial.\nThe[LPUSH](https://redis.io/commands/lpush)command adds a new element into a list, on the left (at the head), while the[RPUSH](https://redis.io/commands/rpush)command adds a new element into a list, on the right (at the tail). Finally the[LRANGE](https://redis.io/commands/lrange)command extracts ranges of elements from lists:\n\n> rpush mylist A\n> lpush mylist first\n> lrange mylist 0 -1\n\nNote that[LRANGE](https://redis.io/commands/lrange)takes two indexes, the first and the last element of the range to return. Both the indexes can be negative, telling Redis to start counting from the end: so -1 is the last element, -2 is the penultimate element of the list, and so forth.\nAn important operation defined on Redis lists is the ability topop elements. Popping elements is the operation of both retrieving the element from the list, and eliminating it from the list, at the same time. You can pop elements from left and right, similarly to how you can push elements in both sides of the list:\n\n> rpush mylist a b c\n> rpop mylist\n> lpop mylist\n**Common use cases for lists**\n-   Remember the latest updates posted by users into a social network.\n-   Communication between processes, using a consumer-producer pattern where the producer pushes items into a list, and a consumer (usually aworker) consumes those items and executed actions. Redis has special list commands to make this use case both more reliable and efficient.\n**Capped Lists**\n\nRedis allows us to use lists as a capped collection, only remembering the latest N items and discarding all the oldest items using the [LTRIM](https://redis.io/commands/ltrim) command.\nThe[LTRIM](https://redis.io/commands/ltrim)command is similar to[LRANGE](https://redis.io/commands/lrange), butinstead of displaying the specified range of elementsit sets this range as the new list value. All the elements outside the given range are removed.\nNote: while[LRANGE](https://redis.io/commands/lrange)is technically anO(N)command, accessing small ranges towards the head or the tail of the list is a constant time operation.\n**Blocking operations on lists**\n\nLists have a special feature that make them suitable to implement queues, and in general as a building block for inter process communication systems: blocking operations.\nImagine you want to push items into a list with one process, and use a different process in order to actually do some kind of work with those items. This is the usual producer / consumer setup, and can be implemented in the following simple way:\n-   To push items into the list, producers call[LPUSH](https://redis.io/commands/lpush).\n-   To extract / process items from the list, consumers call[RPOP](https://redis.io/commands/rpop).\nHowever it is possible that sometimes the list is empty and there is nothing to process, so[RPOP](https://redis.io/commands/rpop)just returns NULL. In this case a consumer is forced to wait some time and retry again with[RPOP](https://redis.io/commands/rpop). This is calledpolling, and is not a good idea in this context because it has several drawbacks:\n\n1.  Forces Redis and clients to process useless commands (all the requests when the list is empty will get no actual work done, they'll just return NULL).\n\n2.  Adds a delay to the processing of items, since after a worker receives a NULL, it waits some time. To make the delay smaller, we could wait less between calls to[RPOP](https://redis.io/commands/rpop), with the effect of amplifying problem number 1, i.e. more useless calls to Redis.\nSo Redis implements commands called[BRPOP](https://redis.io/commands/brpop)and[BLPOP](https://redis.io/commands/blpop)which are versions of[RPOP](https://redis.io/commands/rpop)and[LPOP](https://redis.io/commands/lpop)able to block if the list is empty: they'll return to the caller only when a new element is added to the list, or when a user-specified timeout is reached.\nThis is an example of a[BRPOP](https://redis.io/commands/brpop)call we could use in the worker:\n\n> brpop tasks 5\n1) \"tasks\"\n2) \"do_something\"\n\nIt means: \"wait for elements in the listtasks, but return if after 5 seconds no element is available\".\nNote that you can use 0 as timeout to wait for elements forever, and you can also specify multiple lists and not just one, in order to wait on multiple lists at the same time, and get notified when the first list receives an element.\nA few things to note about[BRPOP](https://redis.io/commands/brpop):\n\n1.  Clients are served in an ordered way: the first client that blocked waiting for a list, is served first when an element is pushed by some other client, and so forth.\n\n2.  The return value is different compared to[RPOP](https://redis.io/commands/rpop): it is a two-element array since it also includes the name of the key, because[BRPOP](https://redis.io/commands/brpop)and[BLPOP](https://redis.io/commands/blpop)are able to block waiting for elements from multiple lists.\n\n3.  If the timeout is reached, NULL is returned.\nThere are more things you should know about lists and blocking ops. We suggest that you read more on the following:\n-   It is possible to build safer queues or rotating queues using[RPOPLPUSH](https://redis.io/commands/rpoplpush).\n-   There is also a blocking variant of the command, called[BRPOPLPUSH](https://redis.io/commands/brpoplpush).\n**Automatic creation and removal of keys**\n\nSo far in our examples we never had to create empty lists before pushing elements, or removing empty lists when they no longer have elements inside. It is Redis' responsibility to delete keys when lists are left empty, or to create an empty list if the key does not exist and we are trying to add elements to it, for example, with[LPUSH](https://redis.io/commands/lpush).\nThis is not specific to lists, it applies to all the Redis data types composed of multiple elements -- Streams, Sets, Sorted Sets and Hashes.\nBasically we can summarize the behavior with three rules:\n\n1.  When we add an element to an aggregate data type, if the target key does not exist, an empty aggregate data type is created before adding the element.\n\n2.  When we remove elements from an aggregate data type, if the value remains empty, the key is automatically destroyed. The Stream data type is the only exception to this rule.\n\n3.  Calling a read-only command such as[LLEN](https://redis.io/commands/llen)(which returns the length of the list), or a write command removing elements, with an empty key, always produces the same result as if the key is holding an empty aggregate type of the type the command expects to find.\n**Redis Hashes**\n\nRedis hashes look exactly how one might expect a \"hash\" to look, with field-value pairs:\n\n> hmset user:1000 username antirez birthyear 1977 verified 1\nOK\n> hget user:1000 username\n\"antirez\"\n> hget user:1000 birthyear\n\"1977\"\n> hgetall user:1000\n1) \"username\"\n2) \"antirez\"\n3) \"birthyear\"\n4) \"1977\"\n5) \"verified\"\n6) \"1\"\nWhile hashes are handy to representobjects, actually the number of fields you can put inside a hash has no practical limits (other than available memory)\nThe command[HMSET](https://redis.io/commands/hmset)sets multiple fields of the hash, while[HGET](https://redis.io/commands/hget)retrieves a single field.[HMGET](https://redis.io/commands/hmget)is similar to[HGET](https://redis.io/commands/hget)but returns an array of values:\n\n> hmget user:1000 username birthyear no-such-field\n1) \"antirez\"\n2) \"1977\"\n3) (nil)\nThere are commands that are able to perform operations on individual fields as well, like[HINCRBY](https://redis.io/commands/hincrby):\n\n> hincrby user:1000 birthyear 10\n(integer) 1987\n> hincrby user:1000 birthyear 10\n(integer) 1997\nIt is worth noting that small hashes (i.e., a few elements with small values) are encoded in special way in memory that make them very memory efficient.\n**Redis Sets**\n\nRedis Sets are unordered collections of strings. The[SADD](https://redis.io/commands/sadd)command adds new elements to a set. It's also possible to do a number of other operations against sets like testing if a given element already exists, performing the intersection, union or difference between multiple sets, and so forth.\n\n> sadd myset 1 2 3\n(integer) 3\n> smembers myset\n1. 3\n2. 1\n3. 2\n\n> sscan myset 0 match f*\nHere I've added three elements to my set and told Redis to return all the elements. As you can see they are not sorted -- Redis is free to return the elements in any order at every call, since there is no contract with the user about element ordering.\nRedis has commands to test for membership. For example, checking if an element exists:\n\n> sismember myset 3\n(integer) 1\n> sismember myset 30\n(integer) 0\n\n\"3\" is a member of the set, while \"30\" is not.\nSets are good for expressing relations between objects. For instance we can easily use sets in order to implement tags.\nThere are other non trivial operations that are still easy to implement using the right Redis commands. For instance we may want a list of all the objects with the tags 1, 2, 10, and 27 together. We can do this using the[SINTER](https://redis.io/commands/sinter)command, which performs the intersection between different sets.\nIn addition to intersection you can also perform unions, difference, extract a random element, and so forth.\n\nThe command to extract an element is called[SPOP](https://redis.io/commands/spop), and is handy to model certain problems.\nset command that provides the number of elements inside a set. This is often called thecardinality of a setin the context of set theory, so the Redis command is called[SCARD](https://redis.io/commands/scard).\nWhen you need to just get random elements without removing them from the set, there is the[SRANDMEMBER](https://redis.io/commands/srandmember)command suitable for the task. It also features the ability to return both repeating and non-repeating elements.\n| **Command** | **Example use and description**                                                                                                                                                                                                                                       |\n|---------------|---------------------------------------------------------|\n| SADD        | SADD key-name item [item ...]--- Adds the items to the set and returns the number of items added that weren't already present                                                                                                                                      |\n| SREM        | SREM key-name item [item ...]--- Removes the items and returns the number of items that were removed                                                                                                                                                               |\n| SISMEMBER   | SISMEMBER key-name item--- Returns whether the item is in the SET                                                                                                                                                                                                    |\n| SCARD       | SCARD key-name--- Returns the number of items in the SET                                                                                                                                                                                                             |\n| SMEMBERS    | SMEMBERS key-name--- Returns all of the items in the SET as a Python set                                                                                                                                                                                             |\n| SRANDMEMBER | SRANDMEMBER key-name [count]--- Returns one or more random items from the SET. When count is positive, Redis will return count distinct randomly chosen items, and when count is negative, Redis will return count randomly chosen items that may not be distinct. |\n| SPOP        | SPOP key-name--- Removes and returns a random item from the SET                                                                                                                                                                                                      |\n| SMOVE       | SMOVE source-key dest-key item--- If the item is in the source, removes the item from the source and adds it to the destination, returning if the item was moved                                                                                                     |\nOperations for combining and manipulatingSETs in Redis\n\n| **Command** | **Example use and description**                                                                                                                                                   |\n|-------------|-----------------------------------------------------------|\n| SDIFF       | SDIFF key-name [key-name ...]--- Returns the items in the first SET that weren't in any of the other SETs (mathematical set difference operation)                              |\n| SDIFFSTORE  | SDIFFSTORE dest-key key-name [key-name ...]--- Stores at the dest-key the items in the first SET that weren't in any of the other SETs (mathematical set difference operation) |\n| SINTER      | SINTER key-name [key-name ...]--- Returns the items that are in all of the SETs (mathematical set intersection operation)                                                      |\n| SINTERSTORE | SINTERSTORE dest-key key-name [key-name ...]--- Stores at the dest-key the items that are in all of the SETs (mathematical set intersection operation)                         |\n| SUNION      | SUNION key-name [key-name ...]--- Returns the items that are in at least one of the SETs (mathematical set union operation)                                                    |\n| SUNIONSTORE | SUNIONSTORE dest-key key-name [key-name ...]--- Stores at the dest-key the items that are in at least one of the SETs (mathematical set union operation)                       |\n**Redis Sorted sets (ZSET)**\n\nSorted sets are a data type which is similar to a mix between a Set and a Hash. Like sets, sorted sets are composed of unique, non-repeating string elements, so in some sense a sorted set is a set as well.\nHowever while elements inside sets are not ordered, every element in a sorted set is associated with a floating point value, calledthe score(this is why the type is also similar to a hash, since every element is mapped to a value).\nMoreover, elements in a sorted sets aretaken in order(so they are not ordered on request, order is a peculiarity of the data structure used to represent sorted sets). They are ordered according to the following rule:\n-   If A and B are two elements with a different score, then A > B if A.score is > B.score.\n-   If A and B have exactly the same score, then A > B if the A string is lexicographically greater than the B string. A and B strings can't be equal since sorted sets only have unique elements.\nImplementation note: Sorted sets are implemented via a dual-ported data structure containing both a skip list and a hash table, so every time we add an element Redis performs anO(log(N))operation. That's good, but when we ask for sorted elements Redis does not have to do any work at all, it's already all sorted\n\n> zadd hackers 1940 \"Alan Kay\"\n\n> zrange hackers 0 -1\n\n> zrevrange hackers 0 -1\n\n> zrange hackers 0 -1 withscores\n\n> zrangebyscore hackers -inf 1950\n\n> zremrangebyscore hackers 1940 1960\n\n> zrank hackers \"Anita Borg\"\n\n> zrevrank hackers \"Anita Borg\"\n| **Command**   | **What it does**                                                   |\n|------------------|------------------------------------------------------|\n| ZADD          | Adds member with the given score to the ZSET                       |\n| ZRANGE        | Fetches the items in the ZSET from their positions in sorted order |\n| ZRANGEBYSCORE | Fetches items in the ZSET based on a range of scores               |\n| ZREM          | Removes the item from the ZSET, if it exists                       |-   Expire an item in list after 2 mins\n    -   Set expire on item if no new data comes then expire the whole key\n    -   When inserting new data remove the data before 2 mins if available\nzadd test_key 1588056957 test_value_1 (add an item to sortedset)\n\nzadd test_key 1588057150 test_value_2 (add an item to sortedset)\n\nzrangebyscore test_key 1588056957 1588057150 (to sortedset items)\n\nzrangebyscore test_key 1588056957 1588057150 withscores (to sortedset items with score)\n\nzremrangebyscore test_key 1588056950 1588056957 (to remove item)\n**Lexicographical scores**\n\nWith recent versions of Redis 2.8, a new feature was introduced that allows getting ranges lexicographically, assuming elements in a sorted set are all inserted with the same identical score (elements are compared with the Cmemcmpfunction, so it is guaranteed that there is no collation, and every Redis instance will reply with the same output).\nThe main commands to operate with lexicographical ranges are [ZRANGEBYLEX](https://redis.io/commands/zrangebylex), [ZREVRANGEBYLEX](https://redis.io/commands/zrevrangebylex), [ZREMRANGEBYLEX](https://redis.io/commands/zremrangebylex) and [ZLEXCOUNT](https://redis.io/commands/zlexcount).\n**Updating the score: leader boards**\n\nJust a final note about sorted sets before switching to the next topic. Sorted sets' scores can be updated at any time. Just calling[ZADD](https://redis.io/commands/zadd)against an element already included in the sorted set will update its score (and position) withO(log(N))time complexity. As such, sorted sets are suitable when there are tons of updates.\nBecause of this characteristic a common use case is leader boards. The typical application is a Facebook game where you combine the ability to take users sorted by their high score, plus the get-rank operation, in order to show the top-N users, and the user rank in the leader board (e.g., \"you are the #4932 best score here\").\n**Bitmaps**\n\nBitmaps are not an actual data type, but a set of bit-oriented operations defined on the String type. Since strings are binary safe blobs and their maximum length is 512 MB, they are suitable to set up to 232different bits.\n\nBit operations are divided into two groups: constant-time single bit operations, like setting a bit to 1 or 0, or getting its value, and operations on groups of bits, for example counting the number of set bits in a given range of bits (e.g., population counting).\nSince bitmap operations don't have a data structure of their own, there isn't a special data structure to describe. The Redis strings themselves are implemented as a binary safe string. Redis string data structure is internally called Simple Dynamic String (SDS). It is essentially a nativechar []with some additional book keeping information.\nOne of the biggest advantages of bitmaps is that they often provide extreme space savings when storing information. For example in a system where different users are represented by incremental user IDs, it is possible to remember a single bit information (for example, knowing whether a user wants to receive a newsletter) of 4 billion of users using just 512 MB of memory.\nBits are set and retrieved using the[SETBIT](https://redis.io/commands/setbit)and[GETBIT](https://redis.io/commands/getbit)commands:\n\n> setbit key 10 1\n(integer) 1\n> getbit key 10\n(integer) 1\n> getbit key 11\n(integer) 0\nThe[SETBIT](https://redis.io/commands/setbit)command takes as its first argument the bit number, and as its second argument the value to set the bit to, which is 1 or 0. The command automatically enlarges the string if the addressed bit is outside the current string length.\n[GETBIT](https://redis.io/commands/getbit)just returns the value of the bit at the specified index. Out of range bits (addressing a bit that is outside the length of the string stored into the target key) are always considered to be zero.\nThere are three commands operating on group of bits:\n\n1.  [BITOP](https://redis.io/commands/bitop)performs bit-wise operations between different strings. The provided operations are AND, OR, XOR and NOT.\n\n2.  [BITCOUNT](https://redis.io/commands/bitcount)performs population counting, reporting the number of bits set to 1.\n\n3.  [BITPOS](https://redis.io/commands/bitpos)finds the first bit having the specified value of 0 or 1.\nBoth[BITPOS](https://redis.io/commands/bitpos)and[BITCOUNT](https://redis.io/commands/bitcount)are able to operate with byte ranges of the string, instead of running for the whole length of the string. The following is a trivial example of[BITCOUNT](https://redis.io/commands/bitcount)call:\n\n> setbit key 0 1\n(integer) 0\n> setbit key 100 1\n(integer) 0\n> bitcount key\n(integer) 2\nCommon use cases for bitmaps are:\n-   Real time analytics of all kinds.\n-   Storing space efficient but high performance boolean information associated with object IDs.\nFor example imagine you want to know the longest streak of daily visits of your web site users. You start counting days starting from zero, that is the day you made your web site public, and set a bit with[SETBIT](https://redis.io/commands/setbit)every time the user visits the web site. As a bit index you simply take the current unix time, subtract the initial offset, and divide by the number of seconds in a day (normally, 3600*24).\nThis way for each user you have a small string containing the visit information for each day. With[BITCOUNT](https://redis.io/commands/bitcount)it is possible to easily get the number of days a given user visited the web site, while with a few[BITPOS](https://redis.io/commands/bitpos)calls, or simply fetching and analyzing the bitmap client-side, it is possible to easily compute the longest streak.\nBITOP AND andbc b c\n\nbitop not nota a\n\nBITOP OR orbc b c\n\nBITOP XOR xorbc b c\nBitmaps are trivial to split into multiple keys, for example for the sake of sharding the data set and because in general it is better to avoid working with huge keys. To split a bitmap across different keys instead of setting all the bits into a key, a trivial strategy is just to store M bits per key and obtain the key name withbit-number/M and the Nth bit to address inside the key withbit-number MOD M.\n**HyperLogLogs**\n\nA HyperLogLog is a probabilistic data structure used in order to count unique things (technically this is referred to estimating the cardinality of a set). Usually counting unique items requires using an amount of memory proportional to the number of items you want to count, because you need to remember the elements you have already seen in the past in order to avoid counting them multiple times. However there is a set of algorithms that trade memory for precision: you end with an estimated measure with a standard error, which in the case of the Redis implementation is less than 1%. The magic of this algorithm is that you no longer need to use an amount of memory proportional to the number of items counted, and instead can use a constant amount of memory! 12k bytes in the worst case, or a lot less if your HyperLogLog (We'll just call them HLL from now) has seen very few elements.\nHLLs in Redis, while technically a different data structure, are encoded as a Redis string, so you can call[GET](https://redis.io/commands/get)to serialize a HLL, and[SET](https://redis.io/commands/set)to deserialize it back to the server.\nConceptually the HLL API is like using Sets to do the same task. You would[SADD](https://redis.io/commands/sadd)every observed element into a set, and would use[SCARD](https://redis.io/commands/scard)to check the number of elements inside the set, which are unique since[SADD](https://redis.io/commands/sadd)will not re-add an existing element.\nWhile you don't reallyadd itemsinto an HLL, because the data structure only contains a state that does not include actual elements, the API is the same:\n-   Every time you see a new element, you add it to the count with[PFADD](https://redis.io/commands/pfadd).\n-   Every time you want to retrieve the current approximation of the unique elementsaddedwith[PFADD](https://redis.io/commands/pfadd)so far, you use the[PFCOUNT](https://redis.io/commands/pfcount).\n    > pfadd hll a b c d\n    (integer) 1\n    > pfcount hll\n    (integer) 4\nAn example of use case for this data structure is counting unique queries performed by users in a search form every day.\nRedis is also able to perform the union of HLLs\n**Other Features**\n-   It is possible to[iterate the key space of a large collection incrementally](https://redis.io/commands/scan).\n-   It is possible to run[Lua scripts server side](https://redis.io/commands/eval)to improve latency and bandwidth.\n-   Redis is also a[Pub-Sub server](https://redis.io/topics/pubsub).\n<https://redis.io/topics/data-types-intro>\n\n## Redis single-Â­â€argument commands and their corresponding multi-Â­â€argument alternatives**\n\n| **Single-Â­â€Argument Command** | **Single-Â­â€Argument Description**  | **Multi-Â­â€Argument Alternative** | **Multi-Â­â€Argument Description**             |\n|---------------|------------------------|---------------|------------------|\n| **SET**                      | Set the value of a key            | MSET                            | Set multiple keys to multiple values        |\n| **GET**                      | Get the value of a key            | MGET                            | Get the values of all the given keys        |\n| **LSET**                     | Set value of an element in a list | LPUSH, RPUSH                    | Prepend/append multiple values to a list    |\n| **LINDEX**                   | Get an element from a list        | LRANGE                          | Get a range of elements from a list         |\n| **HSET**                     | Set the string value of a hash    | HMSET                           | Set multiple hash fields to multiple values |\n| **HGET**                     | Get the value of a hash field     | HMGET                           | Get the values of all the given hash fields |\n**Pipeline commands**\n\nAnother way to reduce latency associated with high command volume is to pipeline several commands together so that you reduce latency due to network usage. Rather than sending 10 client commands to the Redis server individually and taking the network latency hit 10 times, pipelining the commands will send them all at once and pay the network latency cost only once. Pipelining commands is supported by the Redis server and by most clients. This is only beneficial if network latency is significantly larger than your instance's\n![throughput vs command with or without redis pipeline](media/Redis_Redis-Data-Types-image1.png)\n<https://redis.io/topics/pipelining>\n\n## Redis commands with high time complexity**\n\n| **Command**     | **Description**                                 | **Improve Performance By**                                                                   |\n|--------------|------------------------------|-----------------------------|\n| **ZINTERSTORE** | intersect multiple sorted sets and store result | reducing the number of sets and/or the number of elements in resulting set                   |\n| **SINTERSTORE** | intersect multiple sets and store result        | reducing size of smallest set and/or the number of sets                                      |\n| **SINTER**      | intersect multiple sets                         | reducing the size of smallest set and/or the number of sets                                  |\n| **MIGRATE**     | transfer key from one Redis instance to another | reducing the number of objects stored as values and/or their average size                    |\n| **DUMP**        | return serialized value for a given key         | reducing the number of objects stored asvalues and/or their average size                     |\n| **ZREM**        | remove one or more members from sorted set      | reducing the number of elements to remove and/or the size of the sorted set                  |\n| **ZUNIONSTORE** | add multiple sorted sets and store result       | reducing the total size of the sorted setsand/or the number of elements in the resulting set |\n| **SORT**        | sort elements in list, set, or sorted set       | reducing the number of element to sort and/or the number of returned elements                |\n| **SDIFFSTORE**  | subtract multiple sets and store result         | reducing the number of elements in all sets                                                  |\n| **SDIFF**       | subtract multiple sets                          | reducing the number of elements in all sets                                                  |\n| **SUNION**      | add multiple sets                               | reducing the number elements in all sets                                                     |\n| **LSET**        | set value of an element in a list               | reducing the length of the list                                                              |\n| **LREM**        | remove elements from a list                     | reducing the length of the list                                                              |\n| **LRANGE**      | get range of elements from a list               | reduce the start offset and/or the number of elements in range                               |\n\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/Redis-Eviction-Policies/","title":"Redis Eviction Policies"},"frontmatter":{"draft":false},"rawBody":"# Redis Eviction Policies\n\nCreated: 2020-07-20 20:01:14 +0500\n\nModified: 2020-11-01 12:20:09 +0500\n\n---\n\nThe exact behavior Redis follows when themaxmemorylimit is reached is configured using themaxmemory-policyconfiguration directive.\nThe following policies are available:\n-   **noeviction:** return errors when the memory limit was reached and the client is trying to execute commands that could result in more memory to be used (most write commands, but[DEL](https://redis.io/commands/del)and a few more exceptions).\n-   **allkeys-lru:** evict keys by trying to remove the less recently used (LRU) keys first, in order to make space for the new data added.\n-   **volatile-lru:** evict keys by trying to remove the less recently used (LRU) keys first, but only among keys that have anexpire set, in order to make space for the new data added.\n-   **allkeys-random:** evict keys randomly in order to make space for the new data added.\n-   **volatile-random:** evict keys randomly in order to make space for the new data added, but only evict keys with anexpire set.\n-   **volatile-ttl:** evict keys with anexpire set, and try to evict keys with a shorter time to live (TTL) first, in order to make space for the new data added.\nThe policies**volatile-lru,volatile-random**and**volatile-ttl**behave like**noeviction**if there are no keys to evict matching the prerequisites.\nPicking the right eviction policy is important depending on the access pattern of your application, however you can reconfigure the policy at runtime while the application is running, and monitor the number of cache misses and hits using the Redis[INFO](https://redis.io/commands/info)output in order to tune your setup.\nIn general as a rule of thumb:\n-   Use the**allkeys-lru**policy when you expect a power-law distribution in the popularity of your requests, that is, you expect that a subset of elements will be accessed far more often than the rest.**This is a good pick if you are unsure.**\n-   Use the**allkeys-random**if you have a cyclic access where all the keys are scanned continuously, or when you expect the distribution to be uniform (all elements likely accessed with the same probability).\n-   Use the**volatile-ttl**if you want to be able to provide hints to Redis about what are good candidate for expiration by using different TTL values when you create your cache objects.\nThe**volatile-lruandvolatile-random**policies are mainly useful when you want to use a single instance for both caching and to have a set of persistent keys. However it is usually a better idea to run two Redis instances to solve such a problem.\nIt is also worth noting that setting an expire to a key costs memory, so using a policy like**allkeys-lru**is more memory efficient since there is no need to set an expire for the key to be evicted under memory pressure.\n**How the eviction process works**\n\nIt is important to understand that the eviction process works like this:\n-   A client runs a new command, resulting in more data added.\n-   Redis checks the memory usage, and if it is greater than themaxmemorylimit , it evicts keys according to the policy.\n-   A new command is executed, and so forth.\nSo we continuously cross the boundaries of the memory limit, by going over it, and then by evicting keys to return back under the limits.\nIf a command results in a lot of memory being used (like a big set intersection stored into a new key) for some time the memory limit can be surpassed by a noticeable amount.\n**Approximated LRU algorithm**\n\nRedis LRU algorithm is not an exact implementation. This means that Redis is not able to pick thebest candidatefor eviction, that is, the access that was accessed the most in the past. Instead it will try to run an approximation of the LRU algorithm, by sampling a small number of keys, and evicting the one that is the best (with the oldest access time) among the sampled keys.\nHowever since Redis 3.0 the algorithm was improved to also take a pool of good candidates for eviction. This improved the performance of the algorithm, making it able to approximate more closely the behavior of a real LRU algorithm.\nWhat is important about the Redis LRU algorithm is that youare able to tunethe precision of the algorithm by changing the number of samples to check for every eviction. This parameter is controlled by the following configuration directive:\n\nmaxmemory-samples 5\n**LFU Mode (Least Frequently Used)**\n\nIf you think at LRU, an item that was recently accessed but is actually almost never requested, will not get expired, so the risk is to evict a key that has an higher chance to be requested in the future. LFU does not have this problem, and in general should adapt better to different access patterns.\nTo configure the LFU mode, the following policies are available:\n-   **volatile-lfu**Evict using approximated LFU among the keys with an expire set.\n-   **allkeys-lfu**Evict any key using approximated LFU.\nLFU is approximated like LRU: it uses a probabilistic counter, called a[Morris counter](https://en.wikipedia.org/wiki/Approximate_counting_algorithm)in order to estimate the object access frequency using just a few bits per object, combined with a decay period so that the counter is reduced over time: at some point we no longer want to consider keys as frequently accessed, even if they were in the past, so that the algorithm can adapt to a shift in the access pattern.\n<https://redis.io/topics/lru-cache>\n\n<https://tokers.github.io/posts/lru-and-lfu-in-redis-memory-eviction>\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/Redis-Queues/","title":"Redis Queues"},"frontmatter":{"draft":false},"rawBody":"# Redis Queues\n\nCreated: 2020-08-19 01:12:35 +0500\n\nModified: 2021-01-09 04:32:50 +0500\n\n---\n\nConceptually, a Stream in Redis is a list where you can append entries. Each entry has a unique ID and a value. The ID is auto-generated by default, and it includes a timestamp. The value is a hash. You can query ranges or use blocking commands to read entries as they come. Typical of Redis, you can combine different ingredients to get the result you need. As Niklaus Wirth once said, programs are algorithms plus data structures, and Redis already gives you a bit of both.\nRedis streams are ideal for building history preserving message brokers, message queues, unified logs, and chat systems. Unlike Pub/Sub messages which are fire and forget, Redis streams preserve messages in perpetuity. Redis streams implement consumer groups, a feature that allows a group of clients to cooperate when consuming elements from a stream. For example, consumers in a group can lookup items by ID, have to acknowledge the processing of an item, or claim ownership of a pending message.\n<https://aws.amazon.com/redis/Redis_Streams>\n\n## Message queues**\n\nA message queue is conceptually a list. A producer pushes an element from one side, a consumer reads from the other. Multiple producers and consumers can interact with the same queue. In Redis, a rudimentary message queue can be easily implemented with the commands LPUSH (which means \"push from the left\") and RPOP (which means \"pop from the right\"). In the best-case scenario ---the happy path--- the consumer pops an item, works on it, and once it's done, the customer is ready to consume and process the next item. A slight improvement would be to use a blocking command for reading. So instead of RPOP you could use BRPOP. If the list is empty, the consumer blocks and waits for an element to arrive. If the timeout elapses, the consumer can retry. So far, so good for this simplistic implementation. The problem, though, doesn't lie in the happy path. The issue is what happens when a process crashes while processing an item.\n**Reliable queues**\n\nA queue is reliable if it can recover from a failure scenario. If a consumer crashes and the item it was processing is lost, the system is unreliable. A command was added to a previous version of Redis that is tailor-made for this exact situation. The command is BRPOPLPUSH. It not only pops an item, as discussed in the previous implementation, but it also pushes the item to another list. With the commands LPUSH and BRPOPLPUSH, you can design a reliable message queue\n<https://aws.amazon.com/redis/Redis_Streams_MQ>\n\n## Queueing Solutions**\n\n1.  Celery has an optional redis broker.<http://celeryproject.org>\n\n2.  resque is an extremely popular redis task queue using redis.<https://github.com/defunkt/resque>\n\n3.  RQ is a simple and small redis based queue that aims to \"take the good stuff from celery and resque\" and be much simpler to work with.<http://python-rq.org>\n\n## RQ**\n\nRQ (Redis Queue) is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis and it is designed to have a low barrier to entry. It should be integrated in your web stack easily.\npip install rq\n**# Application**\n```\nfrom rq.job import Job\njob = redis_queue.enqueue(some_long_function, data)\n\njob = queue.enqueue(count_words_at_url, '<http://nvie.com>')\n```\n**Scheduling jobs**\n\n# Schedule job to run at 9:15, October 10th\njob = queue.**enqueue_at**(datetime(2019, 10, 8, 9, 15), say_hello)\n\n# Schedule job to run in 10 seconds\njob = queue.**enqueue_in**(timedelta(seconds=10), say_hello)\n**Retrying failed jobs**\n\nfrom rq import Retry\n\n# Retry up to 3 times, failed job will be requeued immediately\nqueue.enqueue(say_hello, retry=Retry(max=3))\n\n# Retry up to 3 times, with configurable intervals between retries\nqueue.enqueue(say_hello, retry=Retry(max=3, interval=[10, 30, 60]))\n**Some interesting job attributes include**\n-   job.get_status()Possible values arequeued,started,deferred,finished, andfailed\n-   job.func_name\n-   job.argsarguments passed to the underlying job function\n-   job.kwargskey word arguments passed to the underlying job function\n-   **job.result**stores the return value of the job being executed, will returnNoneprior to job execution. Results are kept according to theresult_ttlparameter (500 seconds by default).\n-   job.enqueued_at (job.enqueued_at.isoformat())\n-   job.started_at (job.started_at.isoformat())\n-   job.ended_at\n-   job.exc_infostores exception information if job doesn't finish successfully.\n-   job.id\n**# get job**\n\njob = Job.fetch(job_id, connection=redis_conn)\n**# Worker**\n\n\"\"\"Sets up the redis connection and the redis queue.\"\"\"\n```\nimport os\n\nimport redis\n\nfrom rq import Queue\nredis_conn = redis.Redis(\n\nhost=os.getenv(\"REDIS_HOST\", \"127.0.0.1\"),\n\nport=os.getenv(\"REDIS_PORT\", \"6379\"),\n\npassword=os.getenv(\"REDIS_PASSWORD\", \"\"),\n\n)\nredis_queue = Queue(connection=redis_conn)\nIf you use RQ's scheduling features, you need to run RQ workers with the scheduler component enabled\n\nrq worker --with-scheduler\nrq worker --url redis://:a6ad92769ef04b711eea18dccfff85ea@redis:6379\nCommands\n\nkeys *\n\n1) \"rq:job:61cd0099-f14e-407c-b1c0-f3ce46e5ab67\"\n\n2) \"rq:queue:default\"\n\n3) \"rq:queues\"\n\ntype rq:job:61cd0099-f14e-407c-b1c0-f3ce46e5ab67\n\n>>> hash\n\nhgetall rq:job:61cd0099-f14e-407c-b1c0-f3ce46e5ab67\ntype rq:queue:default\n\n>>> list\n\nlrange rq:queue:default 0 -1\ntype rq:queues\n\n>>> set\n\nsmembers rq:queues\n<https://github.com/rq/rq>\n\n<https://pypi.org/project/rq>\n\n<https://python-rq.org>\n\n<https://python-rq.org/docs>\n\n<https://python-rq.org/patterns>\n\n<https://python-rq.org/patterns/django>\n\n<https://python-rq.org/patterns/sentry>\n```\n\n**Test app - <https://github.com/edkrueger/rq-flask-template>**\n**Tools - RQ**\n\n1.  <https://github.com/rq/rq-scheduler>\n\n[RQ Scheduler](https://github.com/rq/rq-scheduler)is a small package that adds job scheduling capabilities to[RQ](https://github.com/nvie/rq), a[Redis](http://redis.io/)based Python queuing library.\n\n2.  <https://github.com/Parallels/rq-dashboard>\n\nrq-dashboardis a general purpose, lightweight,[Flask](https://flask.palletsprojects.com/)-based web front-end to monitor your[RQ](http://python-rq.org/)queues, jobs, and workers in realtime.\n\n3.  <https://github.com/pranavgupta1234/rqmonitor>\n\nRQ Monitor is Flask based more actionable and dynamic web frontend for monitoring your RQ.\n**Delayed Tasks**\n\nThere are a few different ways that we could potentially add delays to our queue items. Here are the three most straightforward ones:\n-   We could include an execution time as part of queue items, and if a worker process sees an item with an execution time later than now, it can wait for a brief period and then re-enqueue the item.\n-   The worker process could have a local waiting list for any items it has seen that need to be executed in the future, and every time it makes a pass through its while loop, it could check that list for any outstanding items that need to be executed.\n-   Normally when we talk about times, we usually start talking about*ZSETs*. What if, for any item we wanted to execute in the future, we added it to a*ZSET*instead of a*LIST*, with its score being the time when we want it to execute? We then have a process that checks for items that should be executed now, and if there are any, the process removes it from the*ZSET*, adding it to the proper*LIST*queue.\nWe can't wait/re-enqueue items as described in the first, because that'll waste the worker process's time. We also can't create a local waiting list as described in the second option, because if the worker process crashes for an unrelated reason, we lose any pending work items it knew about. We'll instead use a secondaryZSETas described in the third option, because it's simple, straightforward, and we can use a lock to ensure that the move is safe.\n**# requiements.txt**\n\nrpqueue==0.33.2\n**# tasks.py**\n```\nimport requests\n\nimport rpqueue\n\nfrom rpqueue import task\nrpqueue.set_redis_connection_settings('redis', '6379', 0, 'a6ad92769ef04b711eea18dccfff85ea')\n@task\n\ndef call_sms_model(cust_id):\n\npayload = {'cust_id': cust_id}\n\nresp = requests.get('http://decision_engine:5000/score', params=payload)\n\nprint(f'request: {cust_id}, resp status code: {resp.status_code}, text: {resp.text}')\n**# tasks_runner.py**\n\nimport rpqueue\n\nfrom tasks import call_sms_model\nrpqueue.set_redis_connection_settings('redis', '6379', 0, 'a6ad92769ef04b711eea18dccfff85ea')\n\nrpqueue.execute_tasks(queues=None, threads_per_process=1, processes=1, wait_per_thread=1, module='tasks')\n\n# python -m rpqueue.run --module=tasks --host=redis --port=6379 --db=0 --password=a6ad92769ef04b711eea18dccfff85ea\n<https://redislabs.com/ebook/part-2-core-concepts/chapter-6-application-components-in-redis/6-4-task-queues/6-4-2-delayed-tasks>\n\n<https://github.com/josiahcarlson/rpqueue>\n\n<https://josiahcarlson.github.io/rpqueue>\n\n<https://github.com/Parallels/rq-dashboard>\n```\n"},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/Redis-Streams---PUBSUB/","title":"Redis Streams / PUBSUB"},"frontmatter":{"draft":false},"rawBody":"# Redis Streams / PUBSUB\n\nCreated: 2020-03-21 17:39:59 +0500\n\nModified: 2022-05-30 14:45:55 +0500\n\n---\n\n**Messaging**\n\n**Redis Streams**doubles as a communication channel for building streaming architectures and as a log-like data structure for persisting data, making Streams the perfect solution for event sourcing.\n**Redis Pub/Sub**is an extremely lightweight messaging protocol designed for broadcasting live notifications within a system. It's ideal for propagating short-lived messages when low latency and huge throughput are critical.\n**Redis Lists and Redis Sorted Sets**are the basis for implementing message queues. They can be used both directly to build bespoke solutions, or via a framework that makes message processing more idiomatic for your programming language of choice.\n<https://redislabs.com/solutions/use-cases/messaging>\nThe Stream is a new data type introduced with Redis 5.0, which models alog data structurein a more abstract way, however the essence of the log is still intact: like a log file, often implemented as a file open in append only mode, Redis streams are primarily an append only data structure. At least conceptually, because being Redis Streams an abstract data type represented in memory, they implement more powerful operations, to overcome the limits of the log file itself.\nWhat makes Redis streams the most complex type of Redis, despite the data structure itself being quite simple, is the fact that it implements additional, non mandatory features: a set of blocking operations allowing consumers to wait for new data added to a stream by producers, and in addition to that a concept calledConsumer Groups.\nConsumer groups were initially introduced by the popular messaging system called Kafka. Redis reimplements a similar idea in completely different terms, but the goal is the same: to allow a group of clients to cooperate consuming a different portion of the same stream of messages.\n**Commands**\n\n# show a list of all streams\n\nSCAN 0 TYPE stream\n\nXLEN dead:letter:queue\n\nXREVRANGE dead:letter:queue + - COUNT 1\n\nXINFO STREAM customer:profile:updated\n\nXINFO STREAM customer:profile:updated FULL\n**Streams basics**\n\nBecause streams are an append only data structure, the fundamental write command, calledXADD, appends a new entry into the specified stream. A stream entry is not just a string, but is instead composed of one or multiple field-value pairs. This way, each entry of a stream is already structured, like an append only file written in CSV format where multiple separated fields are present in each line.\n\n> XADD mystream * sensor-id 1234 temperature 19.8\n1518951480106-0\nThe above call to theXADDcommand adds an entrysensor-id: 1234, temperature: 19.8to the stream at keymystream, using an auto-generated entry ID, which is the one returned by the command, specifically1518951480106-0. It gets as first argument the key namemystream, the second argument is the entry ID that identifies every entry inside a stream. However, in this case, we passed*because we want the server to generate a new ID for us. Every new ID will be monotonically increasing, so in more simple terms, every new entry added will have a higher ID compared to all the past entries. Auto-generation of IDs by the server is almost always what you want, and the reasons for specifying an ID explicitly are very rare. We'll talk more about this later. The fact that each Stream entry has an ID is another similarity with log files, where line numbers, or the byte offset inside the file, can be used in order to identify a given entry. Returning back at ourXADDexample, after the key name and ID, the next arguments are the field-value pairs composing our stream entry.\nIt is possible to get the number of items inside a Stream just using theXLENcommand:\n\n> XLEN mystream\n(integer) 1\n**Entry IDs**\n\nThe entry ID returned by theXADDcommand, and identifying univocally each entry inside a given stream, is composed of two parts:\n\nmillisecondsTime - sequenceNumber\n\nThe milliseconds time part is actually the local time in the local Redis node generating the stream ID, however if the current milliseconds time happens to be smaller than the previous entry time, then the previous entry time is used instead, so if a clock jumps backward the monotonically incrementing ID property still holds. The sequence number is used for entries created in the same millisecond. Since the sequence number is 64 bit wide, in practical terms there are no limits in the number of entries that can be generated within the same millisecond.\nThe format of such IDs may look strange at first, and the gentle reader may wonder why the time is part of the ID. The reason is that Redis streams support range queries by ID. Because the ID is related to the time the entry is generated, this gives the ability to query for ranges of time basically for free. We will see this soon while covering theXRANGEcommand.\nIf for some reason the user needs incremental IDs that are not related to time but are actually associated to another external system ID, as previously already observed, theXADDcommand can take an explicit ID instead of the*wildcard ID that triggers auto-generation, like in the following examples:\n\n> XADD somestream 0-1 field value\n0-1\n> XADD somestream 0-2 foo bar\n0-2\nNote that in this case, the minimum ID is 0-1 and that the command will not accept an ID equal or smaller than a previous one:\n\n> XADD somestream 0-1 foo bar\n(error) ERR The ID specified in XADD is equal or smaller than the target stream top item\n**Getting data from Streams**\n\nNow we are finally able to append entries in our stream viaXADD. However, while appending data to a stream is quite obvious, the way streams can be queried in order to extract data is not so obvious. If we continue with the analogy of the log file, one obvious way is to mimic what we normally do with the Unix commandtail -f, that is, we may start to listen in order to get the new messages that are appended to the stream. Note that unlike the blocking list operations of Redis, where a given element will reach a single client which is blocking in apop styleoperation likeBLPOP, with streams we want that multiple consumers can see the new messages appended to the Stream, like manytail -fprocesses can see what is added to a log. Using the traditional terminology we want the streams to be able tofan outmessages to multiple clients.\nHowever, this is just one potential access mode. We could also see a stream in quite a different way: not as a messaging system, but as atime series store. In this case, maybe it's also useful to get the new messages appended, but another natural query mode is to get messages by ranges of time, or alternatively to iterate the messages using a cursor to incrementally check all the history. This is definitely another useful access mode.\nFinally, if we see a stream from the point of view of consumers, we may want to access the stream in yet another way, that is, as a stream of messages that can be partitioned to multiple consumers that are processing such messages, so that groups of consumers can only see a subset of the messages arriving in a single stream. In this way, it is possible to scale the message processing across different consumers, without single consumers having to process all the messages: each consumer will just get different messages to process. This is basically what Kafka (TM) does with consumer groups. Reading messages via consumer groups is yet another interesting mode to read from a Redis stream.\nRedis streams support all the three query modes described above via different commands.\n**Querying by range: XRANGE and XREVRANGE**\n\nTo query the stream by range we are only required to specify two IDs,startandend. The range returned will include the elements having start or end as ID, so the range is inclusive. The two special IDs-and+respectively means the smallest and the greatest ID possible.\n\n> XRANGE mystream - +\n1) 1) 1518951480106-0\n2) 1) \"sensor-id\"\n2) \"1234\"\n3) \"temperature\"\n4) \"19.8\"\n2) 1) 1518951482479-0\n2) 1) \"sensor-id\"\n2) \"9999\"\n3) \"temperature\"\n4) \"18.2\"\nSinceXRANGEcomplexity isO(log(N))to seek, and thenO(M)to return M elements, with a small count the command has a logarithmic time complexity, which means that each step of the iteration is fast. SoXRANGEis also the de factostreams iteratorand does not require anXSCANcommand.\nThe commandXREVRANGEis the equivalent ofXRANGEbut returning the elements in inverted order, so a practical use forXREVRANGEis to check what is the last item in a Stream\n**Listening for new items with XREAD**\n\nWhen we do not want to access items by a range in a stream, usually what we want instead is tosubscribeto new items arriving to the stream. This concept may appear related to Redis Pub/Sub, where you subscribe to a channel, or to Redis blocking lists, where you wait for a key to get new elements to fetch, but there are fundamental differences in the way you consume a stream:\n\na.  A stream can have multiple clients (consumers) waiting for data. Every new item, by default, will be delivered toevery consumerthat is waiting for data in a given stream. This behavior is different than blocking lists, where each consumer will get a different element. However, the ability tofan outto multiple consumers is similar to Pub/Sub.\n\nb.  While in Pub/Sub messages arefire and forgetand are never stored anyway, and while when using blocking lists, when a message is received by the client it ispopped(effectively removed) from the list, streams work in a fundamentally different way. All the messages are appended in the stream indefinitely (unless the user explicitly asks to delete entries): different consumers will know what is a new message from its point of view by remembering the ID of the last message received.\n\nc.  Streams Consumer Groups provide a level of control that Pub/Sub or blocking lists cannot achieve, with different groups for the same stream, explicit acknowledge of processed items, ability to inspect the pending items, claiming of unprocessed messages, and coherent history visibility for each single client, that is only able to see its private past history of messages.\nThe command that provides the ability to listen for new messages arriving into a stream is calledXREAD.\n\n> XREAD COUNT 2 STREAMS mystream 0\n1) 1) \"mystream\"\n2) 1) 1) 1519073278252-0\n2) 1) \"foo\"\n2) \"value_1\"\n2) 1) 1519073279157-0\n2) 1) \"foo\"\n2) \"value_2\"\nApart from the fact thatXREADcan access multiple streams at once, and that we are able to specify the last ID we own to just get newer messages, in this simple form the command is not doing something so different compared toXRANGE. However, the interesting part is that we can turnXREADin ablocking commandeasily, by specifying theBLOCKargument:\n\n> XREAD BLOCK 0 STREAMS mystream $\nNote that when theBLOCKoption is used, we do not have to use the special ID$. We can use any valid ID. If the command is able to serve our request immediately without blocking, it will do so, otherwise it will block. Normally if we want to consume the stream starting from new entries, we start with the ID$, and after that we continue using the ID of the last message received to make the next call, and so forth.\n**Consumer groups**\n\nWhen the task at hand is to consume the same stream from different clients, thenXREADalready offers a way tofan-outto N clients, potentially also using slaves in order to provide more read scalability. However in certain problems what we want to do is not to provide the same stream of messages to many clients, but to provide adifferent subsetof messages from the same stream to many clients. An obvious case where this is useful is the case of slow to process messages: the ability to have N different workers that will receive different parts of the stream allows us to scale message processing, by routing different messages to different workers that are ready to do more work.\nRedis uses a concept calledconsumer groups. It is very important to understand that Redis consumer groups have nothing to do from the point of view of the implementation with Kafka (TM) consumer groups, but they are only similar from the point of view of the concept they implement\nA consumer group is like apseudo consumerthat gets data from a stream, and actually serves multiple consumers, providing certain guarantees:\n\na.  Each message is served to a different consumer so that it is not possible that the same message is delivered to multiple consumers.\n\nb.  Consumers are identified, within a consumer group, by a name, which is a case-sensitive string that the clients implementing consumers must choose. This means that even after a disconnect, the stream consumer group retains all the state, since the client will claim again to be the same consumer. However, this also means that it is up to the client to provide a unique identifier.\n\nc.  Each consumer group has the concept of thefirst ID never consumedso that, when a consumer asks for new messages, it can provide just messages that were never delivered previously.\n\nd.  Consuming a message however requires explicit acknowledge using a specific command, to say: this message was correctly processed, so can be evicted from the consumer group.\n\ne.  A consumer group tracks all the messages that are currently pending, that is, messages that were delivered to some consumer of the consumer group, but are yet to be acknowledged as processed. Thanks to this feature, when accessing the history of messages of a stream, each consumerwill only see messages that were delivered to it.\nActually, it is even possible for the same stream to have clients reading without consumer groups via**XREAD**, and clients reading via**XREADGROUP**in different consumer groups.\n**Creating a consumer group**\n\nXGROUP CREATE mystream mygroup $\n**Recovering from permanent failures**\n\n**Claiming and the delivery counter**\n\n**Streams observability**\n\nXINFO STREAM mystream\n\nXINFO GROUPS mystream\n\nXINFO CONSUMERS mystream mygroup\n\nXINFO HELP\n**XPENDING**\n\nFetching data from a stream via a consumer group, and not acknowledging such data, has the effect of creating*pending entries*.The[XACK](https://redis.io/commands/xack)command will immediately remove the pending entry from the Pending Entries List (PEL) since once a message is successfully processed, there is no longer need for the consumer group to track it and to remember the current owner of the message.\nThe[XPENDING](https://redis.io/commands/xpending)command is the interface to inspect the list of pending messages, and is as thus a very important command in order to observe and understand what is happening with a streams consumer groups: what clients are active, what messages are pending to be consumed, or to see if there are idle messages\n<https://redis.io/commands/xpending>\n\n## Differences with Kafka partitions**\n\nThe partitions are onlylogicaland the messages are just put into a single Redis key, so the way the different clients are served is based on who is ready to process new messages, and not from which partition clients are reading. For instance, if the consumer C3 at some point fails permanently, Redis will continue to serve C1 and C2 all the new messages arriving, as if now there are only twologicalpartitions.\nSimilarly, if a given consumer is much faster at processing messages than the other consumers, this consumer will receive proportionally more messages in the same unit of time. This is possible since Redis tracks all the unacknowledged messages explicitly, and remembers who received which message and the ID of the first message never delivered to any consumer.\nHowever, this also means that in Redis if you really want to partition messages in the same stream into multiple Redis instances, you have to use multiple keys and some sharding system such as Redis Cluster or some other application-specific sharding system. A single Redis stream is not automatically partitioned to multiple instances.\nWe could say that schematically the following is true:\n-   If you use 1 stream -> 1 consumer, you are processing messages in order.\n-   If you use N streams with N consumers, so that only a given consumer hits a subset of the N streams, you can scale the above model of 1 stream -> 1 consumer.\n-   If you use 1 stream -> N consumers, you are load balancing to N consumers, however in that case, messages about the same logical item may be consumed out of order, because a given consumer may process message 3 faster than another consumer is processing message 4.\nSo basically Kafka partitions are more similar to using N different Redis keys. While Redis consumer groups are a server-side load balancing system of messages from a given stream to N different consumers.\n**Capped Streams**\n\nMany applications do not want to collect data into a stream forever. Sometimes it is useful to have at maximum a given number of items inside a stream, other times once a given size is reached, it is useful to move data from Redis to a storage which is not in memory and not as fast but suited to take the history for potentially decades to come. Redis streams have some support for this. One is theMAXLENoption of theXADDcommand.\n\nXADD mystream MAXLEN 2 * value 1\nUsingMAXLENthe old entries are automatically evicted when the specified length is reached, so that the stream is taken at a constant size.\n**Special IDs in the streams API**\n\nWe have-,+,$,>and*, and all have a different meaning, and most of the times, can be used in different contexts.\n**Persistence, replication and message safety**\n\nA Stream, like any other Redis data structure, is asynchronously replicated to slaves and persisted into AOF and RDB files.\n**Removing single items from a stream**\n\n**Zero length streams**\n\n**Total latency of consuming a message**\n\n**How serving blocked consumers work**\n\n**Latency tests results**\n<https://redis.io/topics/streams-intro>\n\n<https://events.redislabs.com/sessions/build-message-bus-redis-streams-fastapi>\n\n[Delayed Message Processing with Redis Streams - RedisConf 2020](https://www.youtube.com/watch?v=hkGYRYe5NE8)\n**PUBSUB**\n\n[SUBSCRIBE](https://redis.io/commands/subscribe),[UNSUBSCRIBE](https://redis.io/commands/unsubscribe)and[PUBLISH](https://redis.io/commands/publish)implement the[Publish/Subscribe messaging paradigm](http://en.wikipedia.org/wiki/Publish/subscribe)where (citing Wikipedia) senders (publishers) are not programmed to send their messages to specific receivers (subscribers). Rather, published messages are characterized into channels, without knowledge of what (if any) subscribers there may be. Subscribers express interest in one or more channels, and only receive messages that are of interest, without knowledge of what (if any) publishers there are. This decoupling of publishers and subscribers can allow for greater scalability and a more dynamic network topology.\n<https://redis.io/topics/pubsub>\n\n## PUBSUB vs Streams**\n\n**Data storage**\n\nPub/Sub is a Publisher/Subscriber platform, it's not data storage. Published messages evaporate, regardless if there was any subscriber.\nIn Redis Streams, stream is a data type, a data structure on its own right. Messages or entries are stored in memory and stay there until commanded to be deleted.\n**Sync/Async communication**\n\nPub/Sub is synchronous communication. All parties need to be active at the same time to be able to communicate. Here Redis is a pure synchronous messaging broker.\nRedis Streams allows for both synchronous (XREADwithBLOCKand the special$ID) and asynchronous communication. It is like Pub/Sub, but with the ability to resume on disconnection without losing messages.\n**Delivery Semantics**\n\nPub/Sub is At-most-once, i.e. \"fire and forget\".\nRedis Streams allows for both At-most-once or At-least-once (explicit acknowledgement sent by the receiver)\n**Blocking mode for consumers**\n\nPub/Sub is blocking-mode only. Once subscribed to a channel, the client is put into subscriber mode and it cannot issue commands (except for[P]SUBSCRIBE,[P]UNSUBSCRIBE,PINGandQUIT), it has become read-only.\nRedis Streams allows consumers to read messages in blocking mode or not.\n**Fan-out**\n\nPub/Sub is fan-out only. All active clients get all messages.\nRedis Streams allows fan-out (withXREAD), but also to provide a different subset of messages from the same stream to many clients. This allows scaling message processing, by routing different messages to different workers, in a way that it is not possible that the same message is delivered to multiple consumers. This last scenario is achieved withconsumer groups.\n\nRedis Streams provide many more features, like time-stamps, field-value pairs, ranges, etc. It doesn't mean you should always go for Streams. If your use-case can be achieved with Pub/Sub, it is better for you to use Pub/Sub then. With Streams, you have to care for memory usage."},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/redis-py/","title":"redis-py"},"frontmatter":{"draft":false},"rawBody":"# redis-py\n\nCreated: 2019-11-05 12:51:49 +0500\n\nModified: 2022-09-23 16:02:11 +0500\n\n---\n\n**# check pending list in redis**\n\n**pip install ipython**\n\n**ipython**\n```\nfrom config.redis_setup import redis_client\n\ndef get_pending(queue):\nlast_id = redis_client.xinfo_groups(queue)[0][\"last-delivered-id\"]\nreturn len(redis_client.xread({queue: last_id})[0][1])\nget_pending(\"send:pinpoint_sms\")\n\nget_pending(\"send:sms\")\n\nget_pending(\"send:offer_sms\")\n\nget_pending(\"send:push_notification\")\n\nget_pending(\"send:offer_push_notification\")\n\nget_pending(\"send:whatsapp_sms\")\n\nget_pending(\"send:whatsapp_offer_sms\")\npip install redis==3.4.1\nimport redis\n\nr = redis.Redis(host='redis', port=6379, db=0, password='password')\n\nr.set('foo', 'bar')\n\nr.get('foo')\nfor key, value in enumerate(r.keys('test:*')):\n\nredis_key = value.decode(\"utf-8\")\n\nsize = r.debug_object(redis_key)\n\nprint(f'{key},{redis_key},{r.ttl(redis_key)},{r.llen(redis_key)},{size[\"serializedlength\"]},{size[\"ql_uncompressed_size\"]}')\nfor key, value in enumerate(r.keys('test:*')):\n\nprint(key, value)\nclass redis.Redis(host=u'localhost',port=6379,db=0, password=None, socket_timeout=None, socket_connect_timeout=None, socket_keepalive=None, socket_keepalive_options=None, connection_pool=None, unix_socket_path=None, encoding=u'utf-8', encoding_errors=u'strict', charset=None, errors=None, decode_responses=False, retry_on_timeout=False, ssl=False, ssl_keyfile=None, ssl_certfile=None, ssl_cert_reqs=u'required', ssl_ca_certs=None, max_connections=None, single_connection_client=False, health_check_interval=0)\nget(name)\n\nReturn the value at keyname, or None if the key doesn't exist\ngetset(name,value)\n\nSets the value at keynametovalueand returns the old value at key name atomically\nexists(*names)\n\nReturns the number ofnamesthat exist\nkeys(pattern=u'*')\n\nReturns a list of keys matchingpattern\nappend(key,value)\n\nAppends the stringvalueto the value atkey. Ifkeydoesn't already exist, create it with a value ofvalue. Returns the new length of the value atkey.\nset(name,value,ex=None,px=None,nx=False,xx=False, keepttl=False)\n\nSet the value at keynametovalue\n\nexsets an expire flag on keynameforexseconds.\n\npxsets an expire flag on keynameforpxmilliseconds.\n\nnxif set to True, set the value at keynametovalueonly if it does not exist.\n\nxxif set to True, set the value at keynametovalueonly if it already exists.\n\nkeepttlif True, retain the time to live associated with the key. (Available since Redis 6.0)\nrpush(name,*values)\n\nPushvaluesonto the tail of the listname\nrpushx(name,value)\n\nPushvalueonto the tail of the listnameifnameexists\nlrange(name,start,end)\n\nReturn a slice of the listnamebetween positionstartandend\n\nstartandendcan be negative numbers just like Python slicing notation\nttl(name)\n\nReturns the number of seconds until the keynamewill expire\ntype(name)\n\nReturns the type of keyname\n**Sentinel**\n\nredis-py can be used together with[Redis Sentinel](https://redis.io/topics/sentinel)to discover Redis nodes. You need to have at least one Sentinel daemon running in order to use redis-py's Sentinel support.\n<https://redis-py.readthedocs.io/en/latest>\n\n<https://realpython.com/python-redis>\n\n<https://pypi.org/project/redis>\n\n<https://github.com/andymccurdy/redis-py>\n\n## # purge queue**\n\nz = redis_client.xpending_range('send:offer_sms', 'offer_sms_consumer', \"-\", \"+\", 10000)\n\nfor i in z:\n\nredis_client.xack('send:offer_sms', 'offer_sms_consumer', i['message_id'])\n```\n\n**Redis-OM**\n\n<https://github.com/redis/redis-om-python>\n\n<https://redis.com/blog/introducing-redis-om-client-libraries>\n"},{"fields":{"slug":"/Databases/Others/Data-Warehousing/Architecture/","title":"Architecture"},"frontmatter":{"draft":false},"rawBody":"# Architecture\n\nCreated: 2020-02-22 23:56:00 +0500\n\nModified: 2020-02-23 00:03:31 +0500\n\n---\n\nThere are mainly three types of Datawarehouse Architectures: -\n**Single-tier architecture**\n\nThe objective of a single layer is to minimize the amount of data stored. This goal is to remove data redundancy. This architecture is not frequently used in practice.\n**Two-tier architecture**\n\nTwo-layer architecture separates physically available sources and data warehouse. This architecture is not expandable and also not supporting a large number of end-users. It also has connectivity problems because of network limitations.\n**Three-tier architecture**\n\nThis is the most widely used architecture.\nIt consists of the Top, Middle and Bottom Tier.\n\na.  **Bottom Tier:**The database of the Datawarehouse servers as the bottom tier. It is usually a relational database system. Data is cleansed, transformed, and loaded into this layer using back-end tools.\n\nb.  **Middle Tier:**The middle tier in Data warehouse is an OLAP server which is implemented using either ROLAP or MOLAP model. For a user, this application tier presents an abstracted view of the database. This layer also acts as a mediator between the end-user and the database.\n\nc.  **Top-Tier:**The top tier is a front-end client layer. Top tier is the tools and API that you connect and get data out from the data warehouse. It could be Query tools, reporting tools, managed query tools, Analysis tools and Data mining tools.\n**Architecture Components**\n\n![Data Warehouse Architecture Components](media/Data-Warehousing_Architecture-image1.png)\n**Data Warehouse Tech Stack**\n\n<table>\n<colgroup>\n<col style=\"width: 20%\" />\n<col style=\"width: 79%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>ITEM</th>\n<th>DESCRIPTION</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><p>Metadata</p>\n<p>Repository</p></td>\n<td>A software tool that contains data that describes other data. Here are the two kinds of metadata: business metadata and technical metadata.</td>\n</tr>\n<tr class=\"even\">\n<td>Data Modeling Tool</td>\n<td>A software tool that enables the design of data and databases through graphical means. This tool provides a detailed design capability that includes the design of tables, columns, relationships, rules, and business definitions.</td>\n</tr>\n<tr class=\"odd\">\n<td>Data Profiling Tool</td>\n<td><p>A software tool that supports understanding data through exploration and comparison. This tool accesses the data and explores it, looking for patterns such as typical values, outlying values, ranges, and allowed values. It is meant to help you</p>\n<p>better understand the content and quality of the data.</p></td>\n</tr>\n<tr class=\"even\">\n<td><p>Data Integration</p>\n<p>Tools</p></td>\n<td>ETL (extract, transfer &amp; load) tools, as well as realtime integration tools like the ESB (enterprise service bus) software tools. These tools copy data from place to place and also scrub and clean the data.</td>\n</tr>\n<tr class=\"odd\">\n<td><p>RDBMS (Relational</p>\n<p>Database</p>\n<p>Management</p>\n<p>System)</p></td>\n<td>Software that stores data in a relational format using SQL (Structured Query Language). This is really the Database system that is going to maintain robust data and store it. It is also important to the expandability of the system.</td>\n</tr>\n<tr class=\"even\">\n<td><p>MOLAP</p>\n<p>(Multidimensional</p>\n<p>OLAP)</p></td>\n<td>Database software designed for data mart-type operations. This software organizes data into multiple dimensions, known as \"cubes,\" to support analytics.</td>\n</tr>\n<tr class=\"odd\">\n<td>Big Data Store</td>\n<td>Software that manages huge amounts of data (relational databases, for example) that other types of software cannot. This Big Data tends to be unstructured and consists of text, images, video, and audio.</td>\n</tr>\n<tr class=\"even\">\n<td><p>Reporting and</p>\n<p>Query Tools</p></td>\n<td>Business-intelligence software tools that select data through query and present it as reports and/or graphical displays. The business or analyst will be able to explore the data-exploration sanction. These tools also help produce reports and outputs that are desired and needed to understand the data.</td>\n</tr>\n<tr class=\"odd\">\n<td>Data Mining Tools</td>\n<td>Software tools that find patterns in stores of data or databases. These tools are useful for predictive analytics and optimization analytics.</td>\n</tr>\n</tbody>\n</table>\n\n## More (dzone refcardz) -**\n-   Data\n-   Data Modeling\n-   Normalized Data\n-   Atomic Data Warehouse\n-   Supporting Tables\n-   Dimensional Database\n-   Facts\n-   Dimension\n-   Data Integration\n    -   Extract Transform Load (ETL)\n    -   Extract Load Transform (ELT)\n    -   Change Data Capture (CDC)\n\n"},{"fields":{"slug":"/Databases/Others/Data-Warehousing/Characteristics/","title":"Characteristics"},"frontmatter":{"draft":false},"rawBody":"# Characteristics\n\nCreated: 2020-02-22 23:56:41 +0500\n\nModified: 2020-02-22 23:57:55 +0500\n\n---\n\n**Characteristics of Data warehouse**\n\nA data warehouse has following characteristics:\n-   Subject-Oriented\n-   Integrated\n-   Time-variant\n-   Non-volatile\n**Subject-Oriented**\n\nA data warehouse is subject oriented as it offers information regarding a theme instead of companies' ongoing operations. These subjects can be sales, marketing, distributions, etc.\nA data warehouse never focuses on the ongoing operations. Instead, it put emphasis on modeling and analysis of data fordecision making. It also provides a simple and concise view around the specific subject by excluding data which not helpful to support the decision process.\n**Integrated**\n\nIn Data Warehouse, integration means the establishment of a common unit of measure for all similar data from the dissimilar database. The data also needs to be stored in the Datawarehouse in common and universally acceptable manner.\nA data warehouse is developed by integrating data from varied sources like a mainframe, relational databases, flat files, etc. Moreover, it must keep consistent naming conventions, format, and coding.\nThis integration helps in effective analysis of data. Consistency in naming conventions, attribute measures, encoding structure etc. have to be ensured. Consider the following example:\n\n![Application A Gender: m,f Balance: Bal on Hand Application B Gender: 0,1 Date (yymmdd) Balance: Current Bal Application C Gender: male, fem Date (mmddyyyy) Balance: CashOnhand Data Integration Issues Data Warehouse Gender: Transformation Date( Julian) and Cleansing Balance: Balance Dec. fixed (13,2) O Guru99.com ](media/Data-Warehousing_Characteristics-image1.png)\nIn the above example, there are three different application labeled A, B and C. Information stored in these applications are Gender, Date, and Balance. However, each application's data is stored different way.\n-   In Application A gender field store logical values like M or F\n-   In Application B gender field is a numerical value,\n-   In Application C application, gender field stored in the form of a character value.\n-   Same is the case with Date and balance\nHowever, after transformation and cleaning process all this data is stored in common format in the Data Warehouse.\n**Time-Variant**\n\nThe time horizon for data warehouse is quite extensive compared with operational systems. The data collected in a data warehouse is recognized with a particular period and offers information from the historical point of view. It contains an element of time, explicitly or implicitly.\nOne such place where Datawarehouse data display time variance is in in the structure of the record key. Every primary key contained with the DW should have either implicitly or explicitly an element of time. Like the day, week month, etc.\nAnother aspect of time variance is that once data is inserted in the warehouse, it can't be updated or changed.\n**Non-volatile**\n\nData warehouse is also non-volatile means the previous data is not erased when new data is entered in it.\nData is read-only and periodically refreshed. This also helps to analyze historical data and understand what & when happened. It does not require transaction process, recovery and concurrency control mechanisms.\nActivities like delete, update, and insert which are performed in an operational application environment are omitted in Data warehouse environment. Only two types of data operations performed in the Data Warehousing are\n\na.  Data loading\n\nb.  Data access\nHere, are some major differences between Application and Data Warehouse\n\n| **Operational Application**                                                                                                    | **Data Warehouse**                                                        |\n|--------------------------------------------|----------------------------|\n| Complex program must be coded to make sure that data upgrade processes maintain high integrity of the final product.           | This kind of issues does not happen because data update is not performed. |\n| Data is placed in a normalized form to ensure minimal redundancy.                                                              | Data is not stored in normalized form.                                    |\n| Technology needed to support issues of transactions, data recovery, rollback, and resolution as its deadlock is quite complex. | It offers relative simplicity in technology.                              |\n<https://www.guru99.com/data-warehouse-architecture.html>\n\n"},{"fields":{"slug":"/Databases/Others/Data-Warehousing/Concepts/","title":"Concepts"},"frontmatter":{"draft":false},"rawBody":"# Concepts\n\nCreated: 2020-02-22 23:59:16 +0500\n\nModified: 2021-12-08 21:03:26 +0500\n\n---\n\n![Data Warehouse Architecture Presentation Transaction Databases Operational Systems External Cleansing & Transaction Data Warehouse Databse Meta Data Query Tools Data Mart Data Mart Analytic application ReporÃ¼ng/ Analysis/ Mining Tools Interactive Reports Ad-hoc reports Static reports @ guru9.com ](media/Data-Warehousing_Concepts-image1.png)\n\nThe data warehouse is based on an RDBMS server which is a central information repository that is surrounded by some key components to make the entire environment functional, manageable and accessible\nThere are mainly five components of Data Warehouse:\n**Data Warehouse Database**\n\nThe central database is the foundation of the data warehousing environment. This database is implemented on the RDBMS technology. Although, this kind of implementation is constrained by the fact that traditional RDBMS system is optimized for transactional database processing and not for data warehousing. For instance, ad-hoc query, multi-table joins, aggregates are resource intensive and slow down performance.\nHence, alternative approaches to Database are used as listed below-\n-   In a datawarehouse, relational databases are deployed in parallel to allow for scalability. Parallel relational databases also allow shared memory or shared nothing model on various multiprocessor configurations or massively parallel processors.\n-   New index structures are used to bypass relational table scan and improve speed.\n-   Use of multidimensional database (MDDBs) to overcome any limitations which are placed because of the relational data model. Example: Essbase from Oracle.\n**Sourcing, Acquisition, Clean-up and Transformation Tools (ETL)**\n\nThe data sourcing, transformation, and migration tools are used for performing all the conversions, summarizations, and all the changes needed to transform data into a unified format in the datawarehouse. They are also called Extract, Transform and Load (ETL) Tools.\nTheir functionality includes:\n-   Anonymize data as per regulatory stipulations.\n-   Eliminating unwanted data in operational databases from loading into Data warehouse.\n-   Search and replace common names and definitions for data arriving from different sources.\n-   Calculating summaries and derived data\n-   In case of missing data, populate them with defaults.\n-   De-duplicated repeated data arriving from multiple datasources.\nThese Extract, Transform, and Load tools may generate cron jobs, background jobs, Cobol programs, shell scripts, etc. that regularly update data in datawarehouse. These tools are also helpful to maintain the Metadata.\nThese ETL Tools have to deal with challenges of Database & Data heterogeneity.\n**Metadata**\n\nThe name Meta Data suggests some high- level technological concept. However, it is quite simple. Metadata is data about data which defines the data warehouse. It is used for building, maintaining and managing the data warehouse.\nIn the Data Warehouse Architecture, meta-data plays an important role as it specifies the source, usage, values, and features of data warehouse data. It also defines how data can be changed and processed. It is closely connected to the data warehouse.\nFor example, a line in sales database may contain:\n\n4030 KJ732 299.90\nThis is a meaningless data until we consult the Meta that tell us it was\n-   Model number: 4030\n-   Sales Agent ID: KJ732\n-   Total sales amount of $299.90\nTherefore, Meta Data are essential ingredients in the transformation of data into knowledge.\nMetadata helps to answer the following questions\n-   What tables, attributes, and keys does the Data Warehouse contain?\n-   Where did the data come from?\n-   How many times do data get reloaded?\n-   What transformations were applied with cleansing?\nMetadata can be classified into following categories:\n\na.  Technical Meta Data: This kind of Metadata contains information about warehouse which is used by Data warehouse designers and administrators.\n\nb.  Business Meta Data:This kind of Metadata contains detail that gives end-users a way easy to understand information stored in the data warehouse.\n**Query Tools**\n\nOne of the primary objects of data warehousing is to provide information to businesses to make strategic decisions. Query tools allow users to interact with the data warehouse system.\nThese tools fall into four different categories:\n\na.  Query and reporting tools\n\nQuery and reporting tools can be further divided into\n-   Reporting tools\n\nReporting tools can be further divided into production reporting tools and desktop report writer.\n\n1.  Report writers: This kind of reporting tool are tools designed for end-users for their analysis.\n\n2.  Production reporting: This kind of tools allows organizations to generate regular operational reports. It also supports high volume batch jobs like printing and calculating. Some popular reporting tools are Brio, Business Objects, Oracle, PowerSoft, SAS Institute.-   Managed query tools\n\nThis kind of access tools helps end users to resolve snags in database and SQL and database structure by inserting meta-layer between users and database.\nb.  Application Development tools\n\nSometimes built-in graphical and analytical tools do not satisfy the analytical needs of an organization. In such cases, custom reports are developed using Application development tools.\nc.  Data mining tools\n\nData mining is a process of discovering meaningful new correlation, pattens, and trends by mining large amount data. Data mining tools are used to make this process automatic.\nd.  OLAP tools\n\nThese tools are based on concepts of a multidimensional database. It allows users to analyse the data using elaborate and complex multidimensional views.\n**Data warehouse Bus Architecture**\n\nData warehouse Bus determines the flow of data in your warehouse. The data flow in a data warehouse can be categorized as Inflow, Upflow, Downflow, Outflow and Meta flow.\nWhile designing a Data Bus, one needs to consider the shared dimensions, facts across data marts.\n**Data Marts**\n\nA data mart is an access layer which is used to get data out to the users. It is presented as an option for large size data warehouse as it takes less time and money to build. However, there is no standard definition of a data mart is differing from person to person.\nIn a simple word Data mart is a subsidiary of a data warehouse. The data mart is used for partition of data which is created for the specific group of users.\nData marts could be created in the same database as the Datawarehouse or a physically separate Database.\n**Data warehouse Architecture Best Practices**\n\nTo design Data Warehouse Architecture, you need to follow below given best practices:\n-   Use a data model which is optimized for information retrieval which can be the dimensional mode, denormalized or hybrid approach.\n-   Need to assure that Data is processed quickly and accurately. At the same time, you should take an approach which consolidates data into a single version of the truth.\n-   Carefully design the data acquisition and cleansing process for Data warehouse.\n-   Design a MetaData architecture which allows sharing of metadata between components of Data Warehouse\n-   Consider implementing an ODS model when information retrieval need is near the bottom of the data abstraction pyramid or when there are multiple operational sources required to be accessed.\n-   One should make sure that the data model is integrated and not just consolidated. In that case, you should consider 3NF data model. It is also ideal for acquiring ETL and Data cleansing tools\n**Fact tables**\n\ntypically contain point-in-time transactional data. Each row in the table can be extremely simple and is often represented as a unit of transaction. Because of their simplicity, they are often the source of truth tables from which business metrics are derived. For example, fact tables that track transaction-like events such as bookings, reservations, alterations, cancellations, and more.\n**Fact tables**store observations or events, and can be sales orders, stock balances, exchange rates, temperatures, etc. A fact table contains dimension key columns that relate to dimension tables, and numeric measure columns. The dimension key columns determine thedimensionalityof a fact table, while the dimension key values determine thegranularityof a fact table. For example, consider a fact table designed to store sale targets that has two dimension key columns**Date**and**ProductKey**. It's easy to understand that the table has two dimensions. The granularity, however, can't be determined without considering the dimension key values. In this example, consider that the values stored in the**Date**column are the first day of each month. In this case, the granularity is at month-product level.\n**Dimension tables**\n\ntypically contain slowly changing attributes of specific entities, and attributes sometimes can be organized in a hierarchical structure. These attributes are often called \"dimensions\", and can be joined with the fact tables, as long as there is a foreign key available in the fact table. Ex - dimension tables such as users, listings, and markets that can help to slice and dice data.\n**Dimension tables**describe business entities---thethingsyou model. Entities can include products, people, places, and concepts including time itself. The most consistent table you'll find in a star schema is a date dimension table. A dimension table contains a key column (or columns) that acts as a unique identifier, and descriptive columns.\n**Dimension tables supportfilteringandgrouping**\n\n**Fact tables supportsummarization**\n**Fact table vs Dimension table**\n\n![Dimension Table Product Product_Key Product_Name Brand Colour Fact Table Order_Measure Product _ Key Time_Key Store_Key Order_Dollars Order_Quantity Store Store_Key City Store_name Phone_no Dimension Table Dimension Table Time Time_Key Date Day Month ](media/Data-Warehousing_Concepts-image2.jpg)\n| **Fact Table**                                                             | **Dimension Table**                                                                                  |\n|--------------------------------|----------------------------------------|\n| Fact table contains the measuring on the attributes of a dimension table.  | Dimension table contains the attributes on that truth table calculates the metric.                   |\n| In fact table, There is less attributes than dimension table.              | While in dimension table, There is more attributes than fact table.                                  |\n| In fact table, There is more records than dimension table.                 | While in dimension table, There is less records than fact table.                                     |\n| Fact table forms a vertical table.                                         | While dimension table forms a horizontal table.                                                      |\n| The attribute format of fact table is in numerical format and text format. | While the attribute format of dimension table is in text format.                                     |\n| It comes after dimension table.                                            | While it comes before fact table.                                                                    |\n| The number of fact table is less than dimension table in a schema.         | While the number of dimension is more than fact table in a schema.                                   |\n| It is used for analysis purpose and decision making.                       | While the main task of dimension table is to store the information about a business and its process. |\n<https://www.guru99.com/data-warehouse-architecture.html>\n\n<https://www.geeksforgeeks.org/difference-between-fact-table-and-dimension-table>"},{"fields":{"slug":"/Databases/Others/Data-Warehousing/Databases/","title":"Databases"},"frontmatter":{"draft":false},"rawBody":"# Databases\n\nCreated: 2020-12-15 22:39:11 +0500\n\nModified: 2020-12-15 22:45:01 +0500\n\n---\n\n1.  SnowFlake\n\n2.  AWS Redshift\n\n3.  Snowflake\n\n4.  AWS Athena\n\n5.  Google BigQuery\n\n6.  Elastic\n\n7.  Hadoop\n\n8.  Druid/Impy\n\n9.  MemSQL\n\n10. Presto11. **FireBolt**\n\n**A new class of cloud data warehouses built for AWS**\nFirebolt has completely redesigned the cloud data warehouse to deliver a super fast, incredibly efficient analytics experience at any scale\nFirebolt's serverless architecture connects to your S3 data lake as its data source and to the entire data ecosystem using standard SQL as its destination\n![Operations Engine Customer dashboards SQL SQL Data science SQL Meta layer (DBMS) Compute layer (isolated resources) Engine Engine Engine Engine Data prep Engine Storage layer ( aws S3 data lake) ](media/Data-Warehousing_Databases-image1.png)\n\n"},{"fields":{"slug":"/Databases/Others/Data-Warehousing/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2020-02-22 23:45:09 +0500\n\nModified: 2021-04-03 21:00:54 +0500\n\n---\n\n**What is a Slowly Changing Dimension?**\n\nA Slowly Changing Dimension (SCD) is a dimension that stores and manages both current and historical data over time in a data warehouse. It is considered and implemented as one of the most critical ETL (Extract Transform Load) tasks in tracking the history of dimension records.\n\nThere are three types of SCDs and you can use Warehouse Builder to define, deploy, and load all three types of SCDs.\n**Type 1 SCDs - Overwriting**\n\nIn a Type 1 SCD the new data overwrites the existing data. Thus the existing data is lost as it is not stored anywhere else. This is the default type of dimension you create. You do not need to specify any additional information to create a Type 1 SCD.\n**Type 2 SCDs - Creating another dimension record**\n\nA Type 2 SCD retains the full history of values. When the value of a chosen attribute changes, the current record is closed. A new record is created with the changed data values and this new record becomes the current record. Each record contains the effective time and expiration time to identify the time period between which the record was active.\n**Type 3 SCDs - Creating a current value field**\n\nA Type 3 SCD stores two versions of values for certain selected level attributes. Each record stores the previous value and the current value of the selected attribute. When the value of any of the selected attributes changes, the current value is stored as the old value and the new value becomes the current value.\n**Change Data Capture (CDC)**\n\nIn[databases](https://en.wikipedia.org/wiki/Database),change data capture(CDC) is a set of software[design patterns](https://en.wikipedia.org/wiki/Design_pattern_(computer_science))used to determine (and track) the data that has changed so that action can be taken using the changed data. CDC is also an approach to data integration that is based on the identification, capture and delivery of the changes made to enterprise data sources.\nCDC solutions occur most often in[data-warehouse](https://en.wikipedia.org/wiki/Data_warehouse)environments since capturing and preserving the state of data across time is one of the core functions of a data warehouse, but CDC can be utilized in any database or data repository system.\n**Tools**\n\n**Debezium**\n\nDebezium is an open source distributed platform for change data capture. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.\n<https://debezium.io>\n\n## AWS DMS (Data Migration Service)**\n<https://en.wikipedia.org/wiki/Change_data_capture>\n"},{"fields":{"slug":"/Databases/Others/Data-Warehousing/Warehouse-Schemas/","title":"Warehouse Schemas"},"frontmatter":{"draft":false},"rawBody":"# Warehouse Schemas\n\nCreated: 2020-02-22 23:36:06 +0500\n\nModified: 2021-12-08 21:00:46 +0500\n\n---\n\nMultidimensional schema is especially designed to model data warehouse systems. The schemas are designed to address the unique needs of very large databases designed for the analytical purpose (OLAP).\n**Types of Data Warehouse Schema**\n\nFollowing are 3 chief types of multidimensional schemas each having its unique advantages.\n-   Star Schema\n-   Snowflake Schema\n-   Galaxy Schema\n**What is a Star Schema?**\n\nIn theStar Schema, the center of the star can have one fact table and a number of associated dimension tables. It is known as star schema as its structure resembles a star. The star schema is the simplest type of Data Warehouse schema. It is also known as Star Join Schema and is optimized for querying large data sets.\n\n![Dimension Table Dealer Dealer_lD Location ID Country_ID Dealer NM Dealer CNTCT Dimension Table Branch Dim Branch ID Name Address Country Fact Table Revenue Dealer ID Model ID Branch_ID Date ID Units Sold Revenue Dimension Table Date Dim Date ID Year Month Quarter Date Dimension Table Product Product ID Product Name Model_lD Variant ID ](media/Data-Warehousing_Warehouse-Schemas-image1.png)\n**Example of Star Schema**\n\nFor example, as you can see in the above-given image that fact table is at the center which contains keys to every dimension table like Dealer_ID, Model ID, Date_ID, Product_ID, Branch_ID & other attributes like Units sold and revenue.\n**Characteristics of Star Schema**\n-   Every dimension in a star schema is represented with the only one-dimension table\n-   The dimension table should contain the set of attributes\n-   The dimension table is joined to the fact table using a foreign key\n-   The dimension table are not joined to each other\n-   Fact table would contain key and measure\n-   The Star schema is easy to understand and provides optimal disk usage\n-   The dimension tables arenot normalized. For instance, in the above figure, Country_ID does not have Country lookup table as an OLTP design would have\n-   The schema is widely supported by BI Tools\n[**https://docs.microsoft.com/en-us/power-bi/guidance/star-schema**](https://docs.microsoft.com/en-us/power-bi/guidance/star-schema)\n\n**Book -** The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling(3rd edition, 2013) by Ralph Kimball et al.\nStar schemais a mature modeling approach widely adopted by relational data warehouses. It requires modelers to classify their model tables as eitherdimensionorfact.\n**What is a Snowflake Schema?**\n\nA Snowflake Schema is an extension of a Star Schema, and it adds additional dimensions. It is called snowflake because its diagram resembles a Snowflake\nThe dimension tables arenormalizedwhich splits data into additional tables. In the following example, Country is further normalized into an individual table.\n\n![mens on a Location Location_ID R ion Dimension Table Country Country_ID Countr _ Name Dimension Table Dealer Dealer_1D Location_ID Country_ID Dealer NM Dealer_CNTCT Dimension Table Branch Dim Branch ID Name Add ress Country Fact Table Revenue Dealer ID Model ID Branch ID Date ID Un its_Sold Revenue Dimension Table Date Dim Date_lD Year Month Quarter Date Dimension Table Product Product_ID Product_Name Model_lD Variant_ID Dimension Table Variant Variant_lD Variant_Name Fuel type ](media/Data-Warehousing_Warehouse-Schemas-image2.png)\n**Characteristics of Snowflake Schema:**\n-   The main benefit of the snowflake schema it uses smaller disk space.\n-   Easier to implement a dimension is added to the Schema\n-   Due to multiple tables query performance is reduced\n-   The primary challenge that you will face while using the snowflake Schema is that you need to perform more maintenance efforts because of the more lookup tables.\n**Star Vs Snowflake Schema: Key Differences**\n\n| **Star Schema**                                                                                                        | **Snow Flake Schema**                                                                                             |\n|-------------------------------------|-----------------------------------|\n| Hierarchies for the dimensions are stored in the dimensional table.                                                    | Hierarchies are divided into separate tables.                                                                     |\n| It contains a fact table surrounded by dimension tables.                                                               | One fact table surrounded by dimension table which are in turn surrounded by dimension table                      |\n| In a star schema, only single join creates the relationship between the fact table and any dimension tables.           | A snowflake schema requires many joins to fetch the data.                                                         |\n| Simple DB Design.                                                                                                      | Very Complex DB Design.                                                                                           |\n| Denormalized Data structure and query also run faster.                                                                 | Normalized Data Structure.                                                                                        |\n| High level of Data redundancy                                                                                          | Very low-level data redundancy                                                                                    |\n| Single Dimension table contains aggregated data.                                                                       | Data Split into different Dimension Tables.                                                                       |\n| Cube processing is faster.                                                                                             | Cube processing might be slow because of the complex join.                                                        |\n| Offers higher performing queries using Star Join Query Optimization. Tables may be connected with multiple dimensions. | The Snow Flake Schema is represented by centralized fact table which unlikely connected with multiple dimensions. |\n**What is a Galaxy schema?**\n\nA Galaxy Schema contains two fact table that shares dimension tables. It is also called Fact Constellation Schema. The schema is viewed as a collection of stars hence the name Galaxy Schema.\n\n![Dimension Table Dealer]D Location_lD Country_lD Dealer _NM Dealer_CNTCT Fact Table Revenue Dealer ID Branch ID Date D Units Sold Revenue Dimension Table Branch Dim Branch _ID Name Address Country imension Table Date Dim Date ID Year Month Qiarter Date Fact Table Product Product_D Product_Name Variant_lD Dimension Table Product ID Product_Name Model ID Variant D ](media/Data-Warehousing_Warehouse-Schemas-image3.png)\n\nAs you can see in above figure, there are two facts table\n\n1.  Revenue\n\n2.  Product\nIn Galaxy schema shares dimensions are called Conformed Dimensions.\n**Characteristics of Galaxy Schema:**\n-   The dimensions in this schema are separated into separate dimensions based on the various levels of hierarchy.\n-   For example, if geography has four levels of hierarchy like region, country, state, and city then Galaxy schema should have four dimensions.\n-   Moreover, it is possible to build this type of schema by splitting the one-star schema into more Star schemes.\n-   The dimensions are large in this schema which is needed to build based on the levels of hierarchy.\n-   This schema is helpful for aggregating fact tables for better understanding.\n**What is Star Cluster Schema?**\n\n![](media/Data-Warehousing_Warehouse-Schemas-image4.png)\nSnowflake schema contains fully expanded hierarchies. However, this can add complexity to the Schema and requires extra joins. On the other hand, star schema contains fully collapsed hierarchies, which may lead to redundancy. So, the best solution may be a balance between these two schemas which is star cluster schema design.\nOverlapping dimensions can be found as forks in hierarchies. A fork happens when an entity acts as a parent in two different dimensional hierarchies. Fork entities then identified as classification with one-to-many relationships.\n**Summary:**\n-   Multidimensional schema is especially designed to model data warehouse systems\n-   The star schema is the simplest type of Data Warehouse schema.\n-   A Snowflake Schema is an extension of a Star Schema, and it adds additional dimensions.\n-   In a star schema, only single join creates the relationship between the fact table and any dimension tables.\n-   Star schema contains a fact table surrounded by dimension tables.\n-   Snow flake schema is surrounded by dimension table which are in turn surrounded by dimension table\n-   A snowflake schema requires many joins to fetch the data.\n-   A Galaxy Schema contains two fact table that shares dimension tables. It is also called Fact Constellation Schema.\n-   Star cluster schema contains attributes of Star schema and Slow flake schema.\n<https://www.guru99.com/star-snowflake-data-warehousing.html>\n\n<https://www.geeksforgeeks.org/difference-between-star-schema-and-snowflake-schema>\n\n"},{"fields":{"slug":"/Databases/Others/MemSQL/Intro/","title":"Intro"},"frontmatter":{"draft":false},"rawBody":"# Intro\n\nCreated: 2020-04-26 00:34:47 +0500\n\nModified: 2021-06-06 16:47:55 +0500\n\n---\n\nMemSQL is a distributed in-memory relational database designed for both transactional and analytical workloads.\n**History**\n\nMemSQL was a Y-combinator graduate start-up founded in 2011.\n**Checkpoints**\n\n[Non-Blocking](https://dbdb.io/browse?checkpoints=non-blocking)[Consistent](https://dbdb.io/browse?checkpoints=consistent)\nMemSQL uses multi-version concurrency control and it's natural to create a consistent(non-fuzzy) snapshot without the need to block ongoing transactions.\n**Concurrency Control**\n\n[Multi-version Concurrency Control (MVCC)](https://dbdb.io/browse?concurrency-control=multi-version-concurrency-control-mvcc)\nMemSQL uses multi-version concurrency control. Reads are not blocked, writes acquire row-level locks.\n**Data Model**\n\n[Relational](https://dbdb.io/browse?data-model=relational)[Key/Value](https://dbdb.io/browse?data-model=keyvalue)\nMemSQL is a distributed relational database. It also supports two-column key/value store.\n**Indexes**\n\n[Skip List](https://dbdb.io/browse?indexes=skip-list)[Hash Table](https://dbdb.io/browse?indexes=hash-table)\nSkip list is the default index type in MemSQL. Skip list is lock free and thus leads extremely fast insert performance, and O(lg(n)) expected lookup/insert/delete performance. Unlike B+ tree, skip list is singly linked, thus reserve scan leads to twice as costly as forward scan. Skip list involves more pointer chasing than B+ tree which could potentially lead to more cache misses. In MemSQL, heuristics are applied to organize nearby nodes on the same physical page to mitigate penalties caused by pointer chasing. Lock-free hash table is also supported in MemSQL to perform fast exact-match queries.\n**Isolation Levels**\n\n[Read Committed](https://dbdb.io/browse?isolation-levels=read-committed)\nMemSQL supports read committed isolation level.\n**Joins**\n\n[Nested Loop Join](https://dbdb.io/browse?joins=nested-loop-join)[Hash Join](https://dbdb.io/browse?joins=hash-join)[Sort-Merge Join](https://dbdb.io/browse?joins=sort-merge-join)[Broadcast Join](https://dbdb.io/browse?joins=broadcast-join)\nNested loop join, index-nested loop join, merge join and hash join are supported in MemSQL. Joins between two Columnstore tables are often executed as sort merge join. For distributed join queries, if two tables are joined with identical shard key, the join will be performed locally; otherwise dataset is broadcast to other nodes via the network.\n**Logging**\n\n[Physical Logging](https://dbdb.io/browse?logging=physical-logging)\nMemSQL implements write-ahead-logging which records only committed transactions. It uses a transaction buffer pool as back-pressure mechanism so that a worker thread doesn't generate indefinite amount of logs. Replication is implemented based on log recovery.\n**Query Compilation**\n\n[Code Generation](https://dbdb.io/browse?query-compilation=code-generation)[JIT Compilation](https://dbdb.io/browse?query-compilation=jit-compilation)\nInstead of the traditional interpreter based execution model, MemSQL 5 comes with a new code generation architecture, which compiles a SQL query to LLVM to machine code. When the MemSQL server encounters a SQL query, it parses SQL into AST and extracts parameters from the query, which is then transformed into a MemSQL-specific intermediate representation inMemSQL Plan Language(MPL). MemSQL then flattens MPL AST into a more compact format asMemSQL Bytecode(MBC). Plans in MBC format is then transformed into LLVM Bitcode, which LLVM uses to generate machine code. Such code generation architecture enables many low-level optimizations and avoids much less unnecessary work compared to interpreter-based execution. Compiled plans are also cached on disk for future use.\n**Query Execution**\n\n[Tuple-at-a-Time Model](https://dbdb.io/browse?query-execution=tuple-at-a-time-model)[Vectorized Model](https://dbdb.io/browse?query-execution=vectorized-model)\nMemSQL uses Tuple-at-a-Time Model for rowstore query execution, uses Vectorized Model for columnstore.\n**Query Interface**\n\n[SQL](https://dbdb.io/browse?query-interface=sql)\nMemSQL supports a subset of MySQL syntax, plus extensions for distributed SQL, geospatial and JSON queries. MySQL wire protocol is supported.\n**Storage Architecture**\n\n[Disk-oriented](https://dbdb.io/browse?storage-architecture=disk-oriented)[In-Memory](https://dbdb.io/browse?storage-architecture=in-memory)\nIn MemSQL, rowstore is completely in-memory and columnstore is disk-backed.\n**Storage Model**\n\n[Decomposition Storage Model (Columnar)](https://dbdb.io/browse?storage-model=decomposition-storage-model-columnar)[N-ary Storage Model (Row/Record)](https://dbdb.io/browse?storage-model=n-ary-storage-model-rowrecord)\nIn MemSQL, row segments in rowstore are stored in N-ary storage model in-memory. Column segments in columnstore are stored in decomposition storage model. Clustered columnar indexes are created for columnstore and compression is applied.\n**Stored Procedures**\n\n[Not Supported](https://dbdb.io/browse?stored-procedures=not-supported)\nStored procedure is not supported.\n**System Architecture**\n\n[Shared-Nothing](https://dbdb.io/browse?system-architecture=shared-nothing)\nMemSQL has a two-tier, clustered architecture. The nodes in upper tier areaggregators, which are cluster-aware query routers. One special node calledMaster Aggregatoris responsible for clustering monitoring. The nodes in lower tier areleaves, which store and process partitioned shards. The aggregator sends extended SQL to leaves to perform distributed query execution.\n**Views**\n\n[Virtual Views](https://dbdb.io/browse?views=virtual-views)\nViews in MemSQL is not materialzed and cannot be written into.\n<https://dbdb.io/db/memsql>\n"},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Aurora/Aurora-Documentation/","title":"Aurora Documentation"},"frontmatter":{"draft":false},"rawBody":"# Aurora Documentation\n\nCreated: 2020-02-17 15:28:28 +0500\n\nModified: 2021-03-15 10:49:56 +0500\n\n---\n\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html>\n\n## Integration Aurora MySQL with AWS Services > Saving data into text files in Amazon S3**\n\nYou can use the**SELECT INTO OUTFILE S3**statement to query data from an Amazon Aurora MySQL DB cluster and save it directly into text files stored in an Amazon S3 bucket. You can use this functionality to skip bringing the data down to the client first, and then copying it from the client to Amazon S3. The**LOAD DATA FROM S3**statement can use the files created by this statement to load data into an Aurora DB cluster.\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.SaveIntoS3.html>\n\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.LoadFromS3.html>\n-- move data to s3\n\nSELECT * FROM equifax_raw_response WHERE inserted_on BETWEEN '2016-01-01' AND '2019-08-31'\n\nINTO OUTFILE S3 's3-ap-south-1://stashfin-migration-data/rds/equifax_raw_response/equifax_raw_response_2016-01-01_to_2019-08-31'\n\nFIELDS TERMINATED BY ','\n\nLINES TERMINATED BY 'n'\n\nMANIFEST ON;\nSELECT * FROM equifax_raw_response WHERE inserted_on LIMIT 100\n\nINTO OUTFILE S3 's3-ap-south-1://stashfin-migration-data/rds/equifax_raw_response/equifax_raw_response_escaped'\n\nCHARACTER SET utf8mb4\n\nFIELDS TERMINATED BY ','\n\nOPTIONALLY ENCLOSED BY '\"'\n\nESCAPED BY ''\n\nLINES TERMINATED BY 'n'\n\nMANIFEST ON\n\nOVERWRITE ON;\n-- Load Back data from s3\n\nLOAD DATA FROM S3 MANIFEST 's3-ap-south-1://stashfin-migration-data/rds/equifax_raw_response/equifax_raw_response_2016-01-01_to_2019-08-31.manifest'\n\nINTO TABLE equifax_raw_response\n\nFIELDS TERMINATED BY ','\n\nLINES TERMINATED BY 'n'\n\n(xml_insert_id, customer_id, load_id, request_data, xml_string, inserted_on, inserted_by, s3_key_request, s3_key_response, is_success);\nSELECT\n\n[ALL | DISTINCT | DISTINCTROW ]\n\n[HIGH_PRIORITY]\n\n[STRAIGHT_JOIN]\n\n[SQL_SMALL_RESULT] [SQL_BIG_RESULT] [SQL_BUFFER_RESULT]\n\n[SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS]\n\nselect_expr [, select_expr ...]\n\n[FROM table_references\n\n[PARTITION partition_list]\n\n[WHERE where_condition]\n\n[GROUP BY {col_name | expr | position}\n\n[ASC | DESC], ... [WITH ROLLUP]]\n\n[HAVING where_condition]\n\n[ORDER BY {col_name | expr | position}\n\n[ASC | DESC], ...]\n\n[LIMIT {[offset,] row_count | row_count OFFSET offset}]\n\n[PROCEDURE procedure_name(argument_list)]\n\nINTO OUTFILE S3 's3_uri'\n\n[CHARACTER SET charset_name]\n\n[export_options]\n\n[MANIFEST {ON | OFF}]\n\n[OVERWRITE {ON | OFF}]\nexport_options:\n\n[{FIELDS | COLUMNS}\n\n[TERMINATED BY 'string']\n\n[[OPTIONALLY] ENCLOSED BY 'char']\n\n[ESCAPED BY 'char']\n\n]\n\n[LINES\n\n[STARTING BY 'string']\n\n[TERMINATED BY 'string']\n\n]\nLOAD DATA FROM S3 [FILE | PREFIX | MANIFEST] 'S3-URI'\n\n[REPLACE | IGNORE]\n\nINTO TABLE tbl_name\n\n[PARTITION (partition_name,...)]\n\n[CHARACTER SET charset_name]\n\n[{FIELDS | COLUMNS}\n\n[TERMINATED BY 'string']\n\n[[OPTIONALLY] ENCLOSED BY 'char']\n\n[ESCAPED BY 'char']\n\n]\n\n[LINES\n\n[STARTING BY 'string']\n\n[TERMINATED BY 'string']\n\n]\n\n[IGNORE number {LINES | ROWS}]\n\n[(col_name_or_user_var,...)]\n\n[SET col_name = expr,...]\n39385044 row(s) affected246.881 sec/4 minutes\n**Parallel query for Aurora MySQL**\n\nAurora MySQL parallel query is an optimization that parallelizes some of the I/O and computation involved in processing data-intensive queries. The work that is parallelized includes retrieving rows from storage, extracting column values, and determining which rows match the conditions in the WHERE clause and join clauses. This data-intensive work is delegated (in database optimization terms,pushed down) to multiple nodes in the Aurora distributed storage layer. Without parallel query, each query brings all the scanned data to a single node within the Aurora MySQL cluster (the head node) and performs all the query processing there.\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-mysql-parallel-query.html>\n\n<https://aws.amazon.com/blogs/aws/new-parallel-query-for-amazon-aurora>\n\n## Cloning Database**\n\nUsing database cloning, you can quickly and cost-effectively create clones of all of the databases within an Aurora DB cluster. The clone databases require only minimal additional space when first created.\nDatabase cloning uses acopy-on-write protocol,in which data is copied at the time that data changes, either on the source databases or the clone databases. You can make multiple clones from the same DB cluster. You can also create additional clones from other clones. For more information on how the copy-on-write protocol works in the context of Aurora storage, see[Copy-on-Write Protocol for Database Cloning](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html#Aurora.Managing.Clone.Protocol).\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html>\n\n## Backtracking an Amazon Aurora DB**\n\nBacktracking lets you rewind the Aurora DB cluster to the time you specify. With backtracking enabled, Aurora keeps a record of changes to your database and allows you to switch to a previous consistent state. With this feature you can easily undo mistakes. For example, if by accident you perform a destructive action, such as a DELETE without a WHERE clause, you can quickly backtrack to a state before the accident. Unlike restoring from a snapshot or automated backup---a slower operation, backtracking lets you move back and forth in time in a matter of minutes.\nBacktracking is not a replacement for backing up your DB cluster so that you can restore it to a point in time.\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html>\n\n<https://aws.amazon.com/getting-started/tutorials/aurora-cloning-backtracking>\n\n## References**\n\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithParamGroups.html>\n\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Reference.html#AuroraMySQL.Reference.ParameterGroups>\n"},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Aurora/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2020-03-02 13:26:53 +0500\n\nModified: 2021-04-07 20:54:03 +0500\n\n---\n\n**Auditing**\n\n<https://aws.amazon.com/premiumsupport/knowledge-center/advanced-audit-aurora-mysql-cloudwatch>\n\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Auditing.html>\n\n## Autoscaling**\n\nTo meet your connectivity and workload requirements, Aurora Auto Scaling dynamically adjusts the number of Aurora Replicas provisioned for an Aurora DB cluster using single-master replication. Aurora Auto Scaling is available for both Aurora MySQL and Aurora PostgreSQL. Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances.\nYou define and apply a scaling policy to an Aurora DB cluster. Thescaling policydefines the minimum and maximum number of Aurora Replicas that Aurora Auto Scaling can manage. Based on the policy, Aurora Auto Scaling adjusts the number of Aurora Replicas up or down in response to actual workloads, determined by using Amazon CloudWatch metrics and target values.\n**Monitoring and Event Notifications**\n\n<https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html>\n\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/MonitoringOverview.html>\n\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Monitoring.html>\n\n<https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-rds-monitoring-integration>\n\n<https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-rds-enhanced-monitoring-integration>\n\n<https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_PerfInsights.UsingDashboard.html>\n\n## Performance Monitoring**\n\n**Why does Performance Insights measure database load in sessions?**\n\nSessions, in this case, is shorthand for \"average active sessions,\" sometimes abbreviated \"AAS.\" An active session is a database connection that has submitted a request to the database but has not yet received the response. Measuring the average number of active concurrent sessions over time provides a clear picture of the load on the database.\n**The Performance Insights dashboard**\n\nThe dashboard is divided into two parts:\n-   At the top, aload chartshows the recent history of database load in units of average active sessions (AAS).\n-   On the bottom, atop activity tableshows what is contributing to database load for the time interval on the load chart.\nBy default, the load chart is color coded by wait type. Breaking down database load by wait types can help you understand what kind of database mechanisms are chiefly contributing to the load. Top activity shows SQL statements by default. Understanding what SQL is chiefly contributing to the load can help you understand what parts of your application are responsible for bottlenecks.\n**Tuning a database load bottleneck**\n\nAsessionis database terminology for a database connection serving an application. Sessions that are waiting during database requests contribute to database load. A session might wait for a response for many reasons. A common reason is that the request is executing and using CPU to complete. However, the request could also be waiting for I/O to complete, for a lock, for writes to storage, for space in a buffer area, or for any number of other reasons. These various wait states appear in the load chart as stacked color areas. The colors correspond to wait states that can be seen in the legend on the right side. The most prominent colors in the load chart are the greatest contributors to database load.\nAn important visual cue in the load chart is theMax CPUline. This line represents the number of vCPUs (virtual central processing units) on the host. If more sessions are active in CPU wait than there are vCPUs, it means that the instance is running beyond CPU capacity. Whenever the overall load goes over theMax CPUline, there might be a bottleneck. The bottleneck could be because of CPU saturation, or it could be caused by one of the many other ways that sessions wait in a database.\n**CPU bottleneck**\n\nIn the preceding examples,there are two vCPU cores, so only two sessions can concurrently run on the CPU without queueing. If three sessions run on the CPU concurrently, then, at any given point in time, at least one of them is waiting in the run queue and therefore not getting work done.\n<https://aws.amazon.com/blogs/database/analyzing-amazon-rds-database-workload-with-performance-insights>\n\n## Metrics**\n\n**Freeable memory**\n\nIt includes cached memory and memory used for buffers (besides what's really free/unused). They'll all be freed if an application requests more memory than what's free.\nIf you have a lot of freeable memory you can downgrade to a smaller instance. If you are running out of freeable memory, then it is time to upgrade.\nThe freeable memory includes the amount of physical memory left unused by the system plus the total amount of buffer or page cache memory that are free and available.\nSo it's freeable memory across the entire system. While MySQL is the main consumer of memory on the host we do have internal processes in addition to the OS that use up a small amount of additional memory.\nIf you see your freeable memory near 0 or also start seeing swap usage then you may need to scale up to a larger instance class or adjust MySQL memory settings. For example decreasing the innodb_buffer_pool_size (by default set to 75% of physical memory) is one way example of adjusting MySQL memory settings.\n"},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Aurora/Storage/","title":"Storage"},"frontmatter":{"draft":false},"rawBody":"# Storage\n\nCreated: 2020-02-25 17:47:33 +0500\n\nModified: 2022-11-18 16:12:39 +0500\n\n---\n\n[AWS re:Invent 2019: [REPEAT 1] Amazon Aurora storage demystified: How it all works (DAT309-R1)](https://www.youtube.com/watch?v=DrtwAOND1Pc)\n**Cost**\n\n| Storage Rate | $0.11per GB-month           |\n|--------------|-------------------------------|\n| I/O Rate     | $0.22per 1 million requests |\nS3 - $0.025per GB\n**Storage**\n\nThe minimumstorageis 10GB. Based on yourdatabaseusage, your AmazonAurora storagewill automatically grow, up to 64 TB, in 10GB increments with no impact todatabaseperformance. There is no need to provisionstoragein advance\n![Quick recap: Database B+ Tree Root Interme diate Leaf 1 Data is organized in emory as fixed sized \"pages\", e.g. 16KB (aka \"buffer-pool\") Leaf 2 Interme diate Leaf 3 Leaf 4 Pages are serialized into durable storage (aka \"checkpoint\") periodically Root Interme diate Interme Leaf 1 Leaf 2 Leaf 3 Leaf 4 diate ](media/AWS-Aurora_Storage-image1.png)\n![Quick recap: DO-REDO-UNDO protocol Data is modified \"in-place\" in the buffer-pool using a DO/REDO/UNDO operation Log records with before and after images are stored in a write-ahead log (WAL) New state Old state DO Log record New state Log record Old state Log record Old state UNDO New state REDO ](media/AWS-Aurora_Storage-image2.png)\n![Quick recap: Crash Recovery Pages on durable storage TXI Log records on durable storage Checkpoint TX2 System failure System recovery TX2 and TX3 are redone by using the REDO procedure is undone by using the REDO/UNDO procedure 4 4 ](media/AWS-Aurora_Storage-image3.png)\n![Quick recap: I/Os required for persistence Pages on durable storage Log record write: typically few bytes Torn page protection write: page sized, e.g. 16KB Checkpoint write: page sized, e.g. 16KB User data change size << I/O size (32KB+) Databases are all about I/O Log records on durable storage ](media/AWS-Aurora_Storage-image4.png)\n**Cloud native database architecture**\n\n**Traditional database architecture**\n-   Databases are all about I/O\n-   Design principles for > 40 years\n    -   Increase I/O bandwidth\n    -   Decrease number of I/Os\n**Aurora approach: Log is the database**\n\n![Log stream from beginning of the database to ti t2 t3 t4 G Any version of a database page can be constructed using the log stream Blue-page at t5 can be created using log records from ti and t5 ](media/AWS-Aurora_Storage-image5.jpg)\n**Aurora approach: Offload checkpointing to the storage fleet**![Aurora approach: Offload checkpointing to the storage fleet Relying only on log stream for page reads is not practical (too slow) Solution: Use periodic checkpoints Database instance is burdened with checkpointing task Solution: Use a distributed storage fleet for continuous checkpointing ](media/AWS-Aurora_Storage-image6.png)\n![Aurora approach: compute & storage separation Compute & storage have different lifetimes Compute instances â€¢ fail and are replaced â€¢ are shut down to save cost â€¢ are scaled up/down/out on the basis of load needs Storaqe, on the other hand, has to be long-lived Decouple compute and storage for scalability, availability, durability Compute SQL Transactions Caching Logging Network storage ](media/AWS-Aurora_Storage-image7.png)\n![Aurora uses service-oriented architecture We built a log-structured distributed storage system that is multi-tenant, multi-attach, and purpose-built for databases SQL Transactions Caching I. Logging + storage Annazon DynamoD8 53 Amazon Route 53 Amazon EC2 Amazon SS ](media/AWS-Aurora_Storage-image8.png)\n![I/O flow in Amazon Aurora storage node Storage node Incoming queue Database instance Peer storage nodes Log records ACK Update Queue Group GC Data Pages Coa lesce Continuous backup S3 backup Scrub Peer-to-peer gossip H ot log (j) Receive log records and add to in-memory queue and durably persist log records @ ACK to the database @ Organize records and identify gaps in log @ Gossip with peers to fill in holes @ Coalesce log records into new page versions @ Periodically stage log and new page versions to S3 @ Periodically garbage collect old versions @ Periodically validate CRC codes on blocks Note: â€¢ All steps are asynchronous â€¢ Only steps 1 and 2 are in the foreground latency path ](media/AWS-Aurora_Storage-image9.png)\n**Durability at scale**\n\n![Uncorrelated and independent failures At scale there are continuous independent failures due to failing nodes, disks, and switches. The solution is replication One common straw man: Replicate 3-ways with 1 copy per AZ Use write and read quorums of 2/3 Availability Zone 1 Availability Zone 2 Availability Zone 3 Shared storage volume x Storage nodes with SSDs ](media/AWS-Aurora_Storage-image10.png)\n![What about AZ failure? Still have 2/3 copies Can establish quorum No data loss Availability Zone 1 Availability Zone 2 Availability Zone 3 Shared storage volume 11 â€¢ â€¢ â€¢ Storage nodes with SSDs ](media/AWS-Aurora_Storage-image11.png)\n![What about AZ + 1 failures? Losing 1 node in an AZ while another AZ is down Lose 2/3 copies Lose quorum Lose data Availability Zone 1 Availability Zone 2 Availability Zone 3 Shared storage volume Storage nodes with SSDs ](media/AWS-Aurora_Storage-image12.png)\n![Aurora tolerates AZ + 1 failures Replicate 6-ways with 2 copies per AZ Write quorum of 4/6 What if an AZ fails? Still have 4/6 copies Maintain write availability What if there is an AZ + 1 failure ? Still have 3 copies No data loss Rebuild failed copy by copying from 3 copies Recover write availability Availability Zone 1 Availability Zone 2 Availability Zone 3 Shared storage volume Storage nodes with SSDs ](media/AWS-Aurora_Storage-image13.png)\n**Aurora uses segmented storage**\n-   Partition volume into n fixed-size segments\n    -   Replicate each segment 6 ways into a protection group (PG)\n-   Trade-off between likelihood of faults and time to repair\n    -   If segments are too small, failures are more likely\n    -   If segments are too big, repairs take too long\n-   Choose the biggest size that lets us repair \"fast enough\"\n    -   We currently picked a segment size of 10 GB, as we repair a 10-GB segment in less than a minute\n![Fast and reversible membership changes Use quorum sets, and epochs to â€¢ Enable quicker transitions with epoch advances â€¢ Create richer temporary quorums during changes â€¢ Reverse changes by more quorum transitions Epoch 1: All nodes are healthy Epoch 2: Node F is in a suspect state; second quorum group is formed with node G; both quorums are active Epoch 3: Node F is confirmed unhealthy; new quorum group with node G is active ](media/AWS-Aurora_Storage-image14.png)\n**Performance results**\n\n![Aurora I/O profile MySQL with replica 3 Replica Primary instance instance 4 Amazon EBS Amazon EBS 2 5 Amazon EBS Amazon EBS mirror Amuon MySQL I/O profile for 30-min Sysbench run 780K transactions â€¢ Average 7.4 1/Os per transaction Log Aurora Primary instance ASYNC 4/6 QUORUM AZ 2 Replica instance Replica instance 11 Continuous backup Aurora 10 profile for 30-min Sysbench run 27M transactions: 35x more 0.95 1/Os per transaction amplification): 7.7* less Bi nlog Data Double-write Frm files ](media/AWS-Aurora_Storage-image15.png)\n![Write and read throughput Aurora MySQL is 5x faster than MySQL 250,000 200,000 200,000 170,000 150,000 100,000 5Q000 9,536 Write 800,000 705,000 705,000 5.592 â€¢Aurora 5.7 â€¢Aurora 56 'My-SQL 5.7 â€¢MySQL 56 700,000 600,000 500,000 400,000 300,000 200,000 100,000 Aurora 5.7 â€¢ Aurora 5.6 290,787 . MySQL 5.7 .MySQL 56 Using Sysbench with 250 tables and 200,000 rows per table on R4.16XL ](media/AWS-Aurora_Storage-image16.png)\n**Global databases**\n\n![Global physical replication Primary region AZ 3 Replica instance backup Amazon AZ 2 Replica instance AZ 1 O Primary instance o O Replication Replication Fleet Secondary region Replica instance Replication agent Replication Fleet AZ 2 Replica instance ASYNC 4/6 quorum Replica instance ASYNC 4/6 quorum 0 backup @ Primary instance sends log records in parallel to storage nodes, replica instances, and replication server @ Replication server streams log records to replication agent in secondary region @ Replication agent sends log records in parallel to storage nodes and replica instances @ Replication server pulls log records from storage nodes to catch up after outages Up to 150K writes/second; negligible performance impact <1 second cross-region replica lag under heavy load <1 minute to accept full read-write workloads after region failure ](media/AWS-Aurora_Storage-image17.png)\n![Global replication performance Logical vs. physical replication 250,000 200,000 150,000 100,000 50,000 ---QPS ---Lag Logical replication 600 500 400 200 100 250,000 200,000 150,000 100,000 50,000 ---QPS ---Lag Physical replication 5.00 4.50 4.00 3.50 3.00 2.50 0 2.00 1.50 1.00 0.50 0.00 ](media/AWS-Aurora_Storage-image18.png)\n![Global databases Primary region (US West) Primary instance o ASYNC 4/6 quorum o Continuous backup Continuous Inserts O Repl ication 0 Secondary region (US East) Replica instance ASYNC 4/6 quorum Replication agent Continuous Reads backup ](media/AWS-Aurora_Storage-image19.png)\n**Fast database cloning**\n\n![Fast database cloning Create a copy of a database without duplicate storage costs Creation of a clone is instantaneous because it doesn't require deep copy Data copy happens only on write, when original and cloned volume data differ Typical use cases Clone a production database to run tests Reorganize a database â€¢ Save a point-in-time snapshot for analysis without impacting production system Dev/test applications Production applications oo Benchmarks Production applications oo Production database ](media/AWS-Aurora_Storage-image20.png)\n![Database cloning: How does it work? Source database Cloned database Page Page 2 Page 3 Page 4 Page 5 As databases diverge, new pages are added appropriately to each database while still referencing pages common to both databases Shared distributed storage system Protection group 1 Page Page Page 1 3 5 Protection qroup 2 ](media/AWS-Aurora_Storage-image21.png)\n**Database backtrack**\n\n![Database backtrack t to Invisible Invisible Rewind to t3 Rewind to ti Backtrack is a quick way to bring the database to a particular point in time without having to restore from backups Rewinding the database to quickly recover from unintentional DML/DDL operations Rewind multiple times to determine the desired point in time in the database state; for example, quickly iterate over schema changes without having to restore multiple times ](media/AWS-Aurora_Storage-image22.png)\n"},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Redshift/Architecture/","title":"Architecture"},"frontmatter":{"draft":false},"rawBody":"# Architecture\n\nCreated: 2020-02-18 11:41:39 +0500\n\nModified: 2022-06-02 19:21:01 +0500\n\n---\n\n![Compute Node 1 Node Slices Client Applications Leader Node Compute Node n Node Slices Data Warehouse Cluster ](media/AWS-Redshift_Architecture-image1.png)\nRedshift is meant to work in a Cluster formation. A typical Redshift Cluster has two or more**Compute Nodes**which are coordinated through a**Leader Node**.All client applications communicate with the cluster only with the Leader Node.\n1.  **Leader Node**\n\nThe Leader Node in an Amazon Redshift Cluster manages all external and internal communication. It is responsible for preparing query execution plans whenever a query is submitted to the cluster. Once the query execution plan is ready, the Leader Node distributes query execution code on the compute nodes and assigns slices of data to each to compute node for computation of results.\nLeader Node distributes query load to compute node only when the query involves accessing data stored on the compute nodes. Otherwise, the query is executed on the Leader Node itself. There are several functions in Redshift architecture which are always executed on the Leader Node. You can read[SQL Functions Supported on the Leader Node](http://docs.aws.amazon.com/redshift/latest/dg/c_sql-functions-leader-node.html)for more information on these functions.\n2.  **Compute Nodes**\n\nCompute Nodes are responsible for the actual execution of queries and have data stored with them. They execute queries and return intermediate results to the Leader Node which further aggregates the results.\nThere are two types of Compute Nodes available in Redshift architecture:\n-   Dense Storage (DS) --Dense Storage nodes allow you to create large data warehouses using Hard Disk Drives (HDDs) for a low price point.\n-   Dense Compute (DC) --Dense Compute nodes allow you to create high-performance data warehouses using Solid-State Drives (SSDs).\nA more detailed explanation of how responsibilities are divided among Leader and Compute Nodes are depicted in the diagram below:\n\n![Redshift Architecture - Leader and Compute Nodes](media/AWS-Redshift_Architecture-image2.png)\n3.  **Node slices**\n\nA compute node consist of slices. Each Slice has a portion of Compute Node's memory and disk assigned to it where it performs Query Operations. The Leader Node is responsible for assigning a Query code and data to a slice for execution. Slices once assigned query load work in parallel to generate query results.\nData is distributed among the Slices on the basis of[Distribution Style and Distribution Key](https://hevodata.com/blog/redshift-distribution-keys/)of a particular table. An even distribution of data enables Redshift to assign workload evenly to slices and maximizes the benefit of parallel processing.\nNumber of Slices per Compute Node is decided on the basis of the type of node. You can find more information onClusters and Nodes.\n4.  **Massively parallel processing (MPP)**\n\nAmazon Redshift architecture allows it to use Massively parallel processing (MPP) for fast processing even for the most complex queries and a huge amount of data set. Multiple compute nodes execute the same query code on portions of data to maximize parallel processing.\n5.  **Columnar Data Storage**\n\nData in Amazon Redshift data warehouse is stored in a columnar fashion which drastically reduces the I/O on disks. Columnar storage reduces the number of disk I/O requests and minimizes the amount of data loaded into the memory to execute a query. Reduction in I/O speeds up query execution and loading less data means Redshift can perform more in-memory processing.\nRedshift uses Sort Keys to sort columns and filter out chunks of data while executing queries. You can read more about Sort Keys in our post on[Choosing the best Sort Keys](https://hevodata.com/blog/redshift-sort-keys-choosing-best-sort-style/)\n6.  **Data Compression**\n\nData compression is one of the important factors in ensuring query performance. It reduces storage footprint and enables loading of large amounts of data in the memory fast. Owing to Columnar storage, Redshift can use adaptive compression encoding depending on the column data type. Read more about using compression encodings in[Compression Encodings in Redshift](http://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html).\n7.  **Query Optimizer**\n\nRedshift's Query Optimizer generate query plans that are MPP-aware and takes advantage of Columnar Data Storage. Query Optimizer uses analyzed information about tables to generate efficient query plans for execution. Read more about[Analyze](https://hevodata.com/blog/redshift-vacuum-and-analyze/)to know how to make the best of Query Optimizer.\n<https://hevodata.com/blog/redshift-architecture>\n\n<https://docs.aws.amazon.com/redshift/latest/dg/c_high_level_system_architecture.html>"},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Redshift/Deep-dive---Best-practices/","title":"Deep dive / Best practices"},"frontmatter":{"draft":false},"rawBody":"# Deep dive / Best practices\n\nCreated: 2020-02-24 23:43:56 +0500\n\nModified: 2022-12-02 09:41:04 +0500\n\n---\n\n**Insert performance**\n\n<https://stackoverflow.com/questions/16485425/aws-redshift-jdbc-insert-performance>\n\n## Optimizations / Best practices**\n\n<https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-redshift-announces-automatic-table-optimization>\n\n[**https://docs.aws.amazon.com/redshift/latest/dg/c_designing-tables-best-practices.html**](https://docs.aws.amazon.com/redshift/latest/dg/c_designing-tables-best-practices.html)\n\n[**https://docs.aws.amazon.com/redshift/latest/dg/c_designing-queries-best-practices.html**](https://docs.aws.amazon.com/redshift/latest/dg/c_designing-queries-best-practices.html)\n<https://www.youtube.com/watch?v=TJDtQom7SAA>\n\n<https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift>\n\n<https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift>\n\n<https://aws.amazon.com/blogs/big-data/amazon-redshift-engineerings-advanced-table-design-playbook-preamble-prerequisites-and-prioritization>\n\n## Data storage, ingestion and ELT**\n-   **Redundancy**\n\nAmazon Redshift utilizes locally attached storage devices\n\nCompute nodes have 2 to 3 times the advertised storage capacity\nGlobal commit ensures all permanent tables have written blocks to another node in the cluster to ensure data redundancy\nAsynchronously backup blocks to Amazon S3 - always consistent snapshot\n\nEvery 5 GB of changed data or eight hours\n\nUser on-demand manual snapshots\n\nTo disable backups at the table level: CREATE TABLE example (id int) BACKUP NO;\nTemporary tables\n\nBlocks are not mirrored to the remote partition - two-times faster write performance\n\nDo not trigger a full commit or backups\n-   **Transactions**\n\nAmazon Redshift is a fully transactional, ACID complaint data warehourse\n-   Isolation level is serializable\n-   Two phase commits (local and global commit phases)\nDesign consideration\n-   Because of the expense of commit overhead, limit commits by explicitly creating transactions-   **Data ingestion: COPY statement**\n\nIngestion throughput\n\nEach slice's query processors can load one file at a time:\n-   Streaming decompression\n-   Parse\n-   Distribute\n-   Write\nRealizing only partial node usage at 6.25% of slices are active\nNumber of input files should be a multiple of the number of slices\nSplitting the single file into 16 input files, all slices are working to maximize ingestion performance\nCOPY continues to scale linearly as you add nodes\nRecommendation is to use delimited files - 1 MB to 1 GB after gzip compression-   **Best practices: COPY ingestion**\n\nDelimited files are recommend\n-   Pick a simple delimites '|' or ',' or tabs\n-   Pick a simple NULL character (N)\n-   Use double quotes and an escape character ('') for varchars\n-   UTF-8 varchar columns take four bytes per char\nSplit files into a nuber that is a multiple of the total number of slices in the Amazon Redshift cluster\n\nSELECT count(slice) from stv_slices;-   **Data ingestion: Amazon Redshift Spectrum**\n\nUse INSERT INTO SELECT against external Amazon S3 tables\n-   Aggregate incoming data\n-   Select subset of columns and/or rows\n-   Manipulate incoming column data with SQL\nBest practices:\n-   Save cluster resources for querying and reporting rather than on ELT\n-   Filtering/aggregating incoming data can improve performance over COPY\nDesign considerations\n-   Repeated reads against Amazon S3 are not transactional\n-   $5/TB of (compressed) data scanned-   **Design considerations: Data ingestion**\n\nDesigned for large writes\n-   Batch processing system, optimized for processing massive amounts of data\n-   1 MB size plus immutable blocks means that we clone blocks on write so as not to introduce fragmentation\n-   Small write (~1~10 rows) has similar cost to a larger write (~100K rows)\nUPDATE and DELETE\n-   Immutable blocks means that we only logically delete rows on UPDATE or DELETE\n-   Must VACUUM or DEEP COPY to remove ghost rows from table\n![Data ingestion: Deduplication/UPSERT 2016-09-01 2017-10-20 2016-09-14 2017-10-20 2017-04-01 2017-10-10 2017-05-14 2017-11-29 Table: deep_dive s3://bucket/dd.csv aid 2 3 4 loc SFO JFK SFO JFK dt aid 2 5 6 loc SFO JFK SJC SEA dt ](media/AWS-Redshift_Deep-dive---Best-practices-image1.png)\n\n![aid 2 3 4 5 6 Table: deep_dive 2017-10-20 2017-10-20 2017-04-01 2017-05-14 2017-10-10 2017-11-29 s3://bucket/dd.csv loc SFO JFK SFO JFK SJC SEA dt aid 2 5 6 loc SFO JFK SJC SEA dt 2017-10-20 2017-10-20 2017-10-10 2017-11-29 ](media/AWS-Redshift_Deep-dive---Best-practices-image2.png)\nSteps\n\n1.  Load CSV data into a staging table\n\n2.  Delete duplicate data from the production table\n\n3.  Insert (or append) data from the staging into the production table\nCreate a Transaction\n\n![BEGIN ; CREATE TEMP TABLE staging (LIKE deep _ dive) ; COPY staging FROM s 3: / / bucket/ dd. creds COMPUPDATE OFF; DELETE FROM deep_dive d USING staging s WHERE d. aid = s. aid; INSERT INTO deep dive SELECT * FROM staging; DROP TABLE staging; COMMIT ; ](media/AWS-Redshift_Deep-dive---Best-practices-image3.png)\n**Best practices: ELT**\n\nWrap workflow/statements in an explicit transaction\n\nConsider using DROP TABLE or TRUNCATE instead of DELETE\n\nStaging tables:\n-   Use temporary table or permanent table with the \"BACKUP NO\" option\n-   If possible use DISTSTYLE KEY on both the staging table and production table to speed up the the INSERT INTO SELECT statement\n-   Turn off automatic compression - COMPUPDATE OFF\n-   Copy compression settings from production table or use ANALYZE COMPRESSION statement\n    -   Use CREATE TABLE LIKE or write encodings into the DDL\n-   For copying a large number of rows (> hundreds of millions) consider using ALTER TABLE APPEND instead of INSERT INTO SELECT\nVACUUM and ANALYZE\n\nVACCUM will globally sort the table and remove rows that are marked as deleted\n-   For tables with a sort key, ingestion operations will locally sort new data and write it into the unsorted region\nANALYZE collects table statistics for optimal query planning\nBest Practices:\n-   VACUUM should be run only as necessary\n    -   Typically nightly or weekly\n    -   Consider Deep Copy (recreating and copying data) for larger or wide tables\n-   ANALYZE can be run periodically after ingestion on just the columns taht WHERE predicates are filtered on\n-   Utility to VACUUM and ANALYZE all the tables in the cluster\n**Workload management (WLM) and query monitoring rules**\n\nAllows for the separation of different query workloads\nGoals\n-   Prioritize important queries\n-   Throttle/abort less important queries\nControl concurrent number of executing of queries\n\nDivide cluster memory\n\nSet query timeouts to abort long running queries\n**Terminology and Concepts**\n\n**WLM attributes**\n\nQueues:\n-   Assigned a percentage of cluster memory\n-   SQL queries execute in queue based on\n    -   Use group: which groups the user belongs to\n    -   Query group session level variable\nShort query acceleration (SQA):\n-   Automatically detech short running queries and run them within the short query queue if queuing occurs\n**Queue attributes**\n-   Query slots (or Concurrency)\n    -   Devision of memory within a WLM queue, correlated with the number of simultaneous running queries\n    -   WLM_QUERY_SLOT_COUNT is a session level variable\n        -   Useful to increase for memory intensive operations (example: large COPY, VACUUM, larege INSERT INTO SELECT)\n-   Concurrency Scaling\n    -   When queues are full, queries are routed to transient Amazon Redshift clusters\n**Workload management: Example**\n\n**Use Case:**\n-   Light ingestion/ELT on a continuous cadence of 10 minutes\n-   Peak reporting workload during business hours (7 a.m. - 7 p.m.)\n-   Heavy ingestion / ELT nightly (11 p.m. - 2 a.m.)\n**User types**\n-   Business reporting and dashboards\n-   Analysts and data science teams\n-   Database administrators\n**Create a queue for each workload type**\n\n![Queue name Ingestion Dashboard Default (Analysts) Memory Concurrency Timeout (seconds) 20% 50% 25% 2 10 3 None 120 None Concurrency Scaling Never Automatic Never or Automatic ](media/AWS-Redshift_Deep-dive---Best-practices-image4.jpg)-   Unallocated memory goes into a general pool that can be used by any queue\n-   Enable: Short Query Acceleration\n-   Hidden superuser queue can be used by admins manually switched into:\n\nSET query_group TO 'superuser'\n-   The superuser queue has a single slot, the equivalent of 5-7% memory allocation, and no timeout\n**Query monitoring rules (QMR)**\n-   Extension of workload management (WLM)\n-   Allow the automatic handling of runaway (poorly written) queries\n-   Rules applied to a WLM queue allow queries to be\n    -   LOGGED\n    -   ABORTED\n    -   HOPPED\n-   Goals\n    -   Protect against wasteful use of the cluster\n    -   Log resource-intensive queries\n**WLM and QMR**\n-   Keep the number of WLM queues to a minimum, typically just three queues to avoid having unused queues\n-   Use WLM to limit ingestion/ELT concurrency to two to three\n-   To maximize query throughput, use WLM to throttle the number of concurrent queries to 15 or less\n-   Use QMR rather than WLM to set query timeouts\n-   Use QMR to log long running queries\n-   Save the superuser queue for administration tasks and cenceling queries\n<https://github.com/awslabs/amazon-redshift-utils>\n\n[AWS re:Invent 2018: [REPEAT 1] Deep Dive and Best Practices for Amazon Redshift (ANT401-R1)](https://www.youtube.com/watch?v=TJDtQom7SAA)\n**ETL in Redshift**\n\n**AWS Data pipeline**\n\nAWS data pipeline can integrate with all the available AWS services and provides templates with Redshift or S3 as a target. If you are imagining an ELT system, it would be worthwhile to use the Redshift target template directly so that the data is loaded directly to Redshift. That said, internally even this will use S3 as intermediate storage, but the end-user does not have to worry about that. One issue with AWS data pipeline is that it normally works in the batch mode as periodically scheduled jobs and this does not do a good job in case the data extraction needs to be in real-time or near-real-time.\n**AWS RDS Sync with Redshift**\n\nIf your source data is in RDS and wants to sync the RDS directly to Redshift in real-time, AWS offers partner solutions like Attunity and Flydata which works based on bin log streaming.\n**COPY Command**\n\nThe COPY command loads data into Amazon Redshift tables from either data files or Amazon DynamoDB tables.The copied files may reside in an S3 bucket, an EMR cluster or on a remote host accessed via SSH.Data is loadable from fixed-width, character-delimited text files, including CSV, AVRO and JSON format. Default format is character-delimited UTF-8 text files, delimited by the pipe (|) char.Rows to be copied may not be larger than 4 MB from any single source.Root users and IAM users must have INSERT privileges to modify Redshift (RS) tables.-   Use a single COPY command to load from multiple files\n-   Split your load data\n    -   When you load compressed data, with the COPY command from multiple files, the data loads in parallel. This divides the workload among the nodes in your cluster. When you load all the data from a single large, compressed file, Amazon Redshift is forced to perform a serialized load, which is much slower.\n    -   In contrast, when you load delimited data from a large, uncompressed file, Amazon Redshift makes use of multiple slices. These slices work in parallel, automatically. This provides fast load performance. Specifically, when Amazon Redshift loads uncompressed, delimited data, data is split into ranges and handled by slices in each node.\n    -   If you intend to load data from a large, compressed file, we recommend that you split your data into smaller files that are about equal size, from 1 MB to 1 GB after compression. For optimum parallelism, the ideal file size is 1--125 MB after compression. Make the number of files a multiple of the number of slices in your cluster.\n<https://docs.aws.amazon.com/redshift/latest/dg/c_loading-data-best-practices.html>\n\n## Copy Job**\n\nAutomatically loads the new files detected in the specified Amazon S3 path\n\n<https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-redshift-supports-auto-copy-amazon-s3>\n\n## AWS Glue**\n**Redshift ETL Best Practices**\n\n1.  While using the COPY command of Redshift, it is always better to use it on multiple source files rather than one big file. This helps in parallelly loading them and can save a lot of time. Using a manifest file is recommended in case of loading from multiple files.\n\n2.  In cases where data needs to be deleted after a specific time, it is better to use the time series tables. This involves creating different tables for different time windows and dropping them when that data is not needed. Dropping a table is way faster than deleting rows from one master table.\n\n3.  Execute 'VACUUM' command at regular intervals and after the large update or delete operations. Also, use 'ANALYZE' command frequently to ensure that database statistics are updated.\n\n4.  AWS workload management queues enable the user to prioritize different kinds of Redshift loads by creating separate queues for each type of job. It is recommended to create a separate queue for all the ETL jobs and a separate one for reporting jobs.\n\n5.  For transformations that span across multiple SQL statements, it is recommended to execute 'commit' command after the complete group is executed rather than committing after each statement. For example, let's say there are two INSERT statements in one of your ETL steps. It is better to use the COMMIT statement after both the statements than using a COMMIT after each statement.\n\n6.  While using intermediate tables and transferring data between an intermediate table and master table, it is better to use 'ALTER table APPEND' command to insert data from the temporary table to the target table. This command is generally faster than using \"CREATE TABLE AS\" or \"INSERT INTO\" statements. It is to be noted that 'ALTER table APPEND' command empties the source table.\n\n7.  If there is a need to extract a large amount of data from Redshift and save to S3 or other storage, it is better to use 'UNLOAD' command rather than 'SELECT' command since the former command will be executed parallelly by all the nodes saving a lot of time.\n\n"},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Redshift/Documentation/","title":"Documentation"},"frontmatter":{"draft":false},"rawBody":"# Documentation\n\nCreated: 2020-02-17 23:01:55 +0500\n\nModified: 2021-12-08 21:55:36 +0500\n\n---\n\n**Designing Tables > Choosing a column compression type > Compression Encoding**\n-   [Raw Encoding](https://docs.aws.amazon.com/redshift/latest/dg/c_Raw_encoding.html)\n-   [AZ64 Encoding](https://docs.aws.amazon.com/redshift/latest/dg/az64-encoding.html)\n-   [Byte-Dictionary Encoding](https://docs.aws.amazon.com/redshift/latest/dg/c_Byte_dictionary_encoding.html)\n-   [Delta Encoding](https://docs.aws.amazon.com/redshift/latest/dg/c_Delta_encoding.html)\n-   [LZO Encoding](https://docs.aws.amazon.com/redshift/latest/dg/lzo-encoding.html)\n-   [Mostly Encoding](https://docs.aws.amazon.com/redshift/latest/dg/c_MostlyN_encoding.html)\n-   [Runlength Encoding](https://docs.aws.amazon.com/redshift/latest/dg/c_Runlength_encoding.html)\n-   [Text255 and Text32k Encodings](https://docs.aws.amazon.com/redshift/latest/dg/c_Text255_encoding.html)\n-   [Zstandard Encoding](https://docs.aws.amazon.com/redshift/latest/dg/zstd-encoding.html)\nA compression encoding specifies the type of compression that is applied to a column of data values as rows are added to a table.\nIf no compression is specified in a CREATE TABLE or ALTER TABLE statement, Amazon Redshift automatically assigns compression encoding as follows:\n-   Columns that are defined as sort keys are assigned RAW compression.\n-   Columns that are defined as BOOLEAN, REAL, or DOUBLE PRECISION data types are assigned RAW compression.\n-   Columns that are defined as SMALLINT, INTEGER, BIGINT, DECIMAL, DATE, TIMESTAMP, or TIMESTAMPTZ data types are assigned AZ64 compression.\n-   Columns that are defined as CHAR or VARCHAR data types are assigned LZO compression.\n<https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html>\n\n## Concepts**\n\n1.  **Blocks**\n    -   **Column data is persisted to 1 MB immutable blocks**\n    -   **Blocks are individually encoded with 1 of 12 encodings**\n    -   **A full block can contain millions of values**\n\n2.  **Zone maps**\n    -   **Goal: eliminates unnecessary I/O**\n    -   **In-memory block metadata**\n        -   **Contains per-block min and max values**\n        -   **All blocks automatically have zone maps**\n        -   **Effectively prunes blocks that cannot contain data for a given query**\n\n3.  **Materialize columns**\n\n**Goal:** Make queries run faster by leveraging zonemaps on the fact tables\nFrequently filtered and unchanging dimension values should be materialized within fact tables\n-   Time dimension tables do not allow for range restricted scans on fact tables\n-   Materializing temporal values in fact table can give significant performance gains\n4.  **Slices**\n    -   **A slice can be thought of like a virtual compute node**\n        -   **Unit of data partitioning**\n        -   **Parallel query processing**\n    -   **Facts about slices**\n        -   **Each compute node has either 2 or 16 slices**\n        -   **Table rows are distributed to slices**\n        -   **A slice processes only its own data**\n5.  **Best Practices: Table design summary**\n\n    a.  Add compression to columns\n\n    b.  Add sort keys on the primary columns that are filtered on\n\n    c.  Materialize often filtered columns from dimension tables into fact tables\n\n    d.  Materialize often calculated values into tables\n\n    e.  Co-locate large tables using DISTSTYLE KEY if the columns do not cause skew\n\n    f.  Avoid distribution keys on temporal columns\n\n    g.  Keep data types as wide as necessay (but no longer than necessary)\n\n        i.  VARCHAR, CHAR, and NUMERIC\n**Redshift Sort Key**\n\nRedshift Sort Key determines the order in which rows in a table are stored. Query performance is improved when Sort keys are properly used as it enables query optimizer to read fewer chunks of data filtering out the majority of it.\nRedshift Sort Keys allow skipping large chunks of data during query processing. Fewer data to scan means a shorter processing time, thereby improving the query's performance.\n**Types**\n-   **Compound sort keys**\n\nThese are made up of all the columns that are listed in the Redshift sort keys definition during the creation of the table, in the order that they are listed. Therefore, it is advisable to put the most frequently used column at the first in the list. COMPOUND is the default sort type. Compound sort keys might speed up joins, GROUP BY and ORDER BY operations, and window functions that use PARTITION BY.-   **Interleaved sort keys**\n\nInterleaved sort gives equal weight to each column in the Redshift sort keys. As a result, it can significantly improve query performance where the query uses restrictive predicates (equality operator in WHERE clause) on secondary sort columns.\n**Choosing Sorting Keys**\n-   UseInterleaved Sort Keywhen you plan to use one column as Sort Key or when WHERE clauses in your query have highly selective restrictive predicates. Or if the tables are huge. You may want to check table statistics by querying the STV_BLOCKLIST system table. Look for the tables with a high number of 1MB blocks per slice and distributed over all slices.\n-   UseCompound Sort Key,when you have more that one column as Sort Key, when your query includes JOINS, GROUP BY, ORDER BY and PARTITION BY when your table size is small.\n-   Don't use aninterleaved sort keyon columns with monotonically increasing attributes, like an identity column, dates or timestamps.\n<https://hevodata.com/blog/redshift-sort-keys-choosing-best-sort-style>\n\n## Data sorting**\n-   Goal: make queries run faster by increasing the effectiveness of zone maps and reducing I/O\n-   Impact: enables range-restricted scans to prune blocks by leveraging zone maps\n-   Achieved with the table property SORTKEY defined on one or more columns\n-   Optimal sort key is dependent on:\n    -   Query patterns\n    -   Business requirements\n    -   Data profile\n-   Design considerations\n    -   Sort keys are less beneficial on small tables\n    -   Define four or less sort key columns - more will result in marginal gains and increased ingestion overhead\n-   Less then 10000 rows, don't sort, since all will be in a single zone map\n<https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html>\n-   To have Amazon Redshift choose the appropriate sort order, specifyAUTOfor the sort key.\n-   If recent data is queried most frequently, specify the timestamp column as the leading column for the sort key.\n    Queries are more efficient because they can skip entire blocks that fall outside the time range.\n-   If you do frequent range filtering or equality filtering on one column, specify that column as the sort key.\n    Amazon Redshift can skip reading entire blocks of data for that column. It can do so because it tracks the minimum and maximum column values stored on each block and can skip blocks that don't apply to the predicate range.\n-   If you frequently join a table, specify the join column as both the sort key and the distribution key.\n    Doing this enables the query optimizer to choose a sort merge join instead of a slower hash join. Because the data is already sorted on the join key, the query optimizer can bypass the sort phase of the sort merge join.\n-   Place the sort key on columns that are frequently filtered on placing the low cardinality columns first\n    -   On most fact tables, the first sort key columns should be a temporal column\n    -   Columns added to a sort key after a high-cardinality column are not effective\n\n<https://github.com/awsdocs/amazon-redshift-developer-guide/blob/master/doc_source/c_best-practices-sort-key.md>\n\n## Redshift Distribution Key (DIST Keys)**\n\nRedshiftDistributionKeys([DIST Keys](http://docs.aws.amazon.com/redshift/latest/dg/t_Distributing_data.html)) determine where data is stored in Redshift. Clusters store data fundamentally across the compute nodes. Query performance suffers when a large amount of data is stored on a single node.\nThe query optimizer distributes less number of rows to the compute nodes to perform joins and aggregation on query execution. This redistribution of data can include shuffling of the entire tables across all the nodes.\n\nUneven distribution of data across computing nodes leads to the skewness of the work a node has to do and you don't want an under-utilised compute node. So the distribution of the data should be uniform. Distribution is per table. So you can select a different distribution style for each of the tables you are going to have in your database.\n**Types of Distribution Styles**\n\nAmazon Redshift supports three kinds of table distribution styles.\n-   **Even distribution**\n\nThis is the default distribution styles of a table.InEven DistributiontheLeadernode of the cluster distributes the data of a table evenly across all slices, using a round-robin approach.\n-   **Key distribution**\n\nThe data is distributed across slices by the leader node matching the values of a designated column. So all the entries with the same value in the column end up in the same slice.\n-   **All distribution**\n\nLeader node maintains a copy of the table on all the computing nodes resulting in more space utilisation. Since all the nodes have a local copy of the data, the query does not require copying data across the network. This results in faster query operations. The negative side of usingALLis that a copy of the table is on every node in the cluster. This takes up too much of space and increases the time taken byCopy commandto upload data into Redshift.\n**Choosing the right Distribution Styles**\n\nThe motive in selecting a table distribution style is to minimize the impact of the redistribution by relocating the data where it was prior to the query execution. Choosing the right KEY is not as straightforward as it may seem. In fact, setting wrong DISTKEY can even worsen the query performance.\nChoose columns used in the query that leads to least skewness as the DISTKEY. The good choice is the column with maximum distinct values, such as the timestamp. Avoid columns with few distinct values, such as months of the year, payment card types.-   If the table(e.g. fact table) is highly de-normalised and no JOIN is required, choose theEVENstyle.\n-   ChooseALLstyle for small tables that do not often change. For example, a table containing telephone ISD codes against the country name.\n-   It is beneficial to select aKEYdistribution if a table is used in JOINS. Also, consider the other joining tables and their distribution style.\n-   If one particular node contains the skew data, the processing on this node will be slower. This results in much longer total query processing time. This query under skewed configuration may take even longer than the query made against the table without a DISTKEY\n<https://hevodata.com/blog/redshift-distribution-keys>\n\n## Data Distribution**\n\nDistribution style is a table property which dictates how that table's data is distributed throughout the cluster\n-   KEY: value is hashed, same value goes to same location (slice)\n-   ALL: full table data goes to the first slice of every node\n-   EVEN: round robin\n-   AUTO: Combines EVEN and ALL\n\nGoals:\n-   Distribute data evenly for parallel processing\n-   Minimize data movement during query processing\nSummary\n-   KEY\n    -   Goals\n        -   Optimize JOIN performance between large tables by distributing on columns used in the ON clause\n        -   Optimize INSERT INTO SELECT performance\n        -   Optimize GROUP BY performance\n    -   The column that is being distributed on should have a high cardinality and not cause row skew\n-   ALL\n    -   Goals\n        -   Optimize Join performance with dimension tables\n        -   Reduces disk usage on small tables\n        -   Small and medium size dimension tables (< 3M rows)\n-   EVEN\n    -   If neither KEY or ALL apply\n-   AUTO\n    -   Default Distribution - Combines DISTSTYLE ALL and EVEN\n<https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html>\n\n<https://docs.aws.amazon.com/redshift/latest/dg/t_Distributing_data.html>\n"},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Redshift/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2020-02-18 11:31:40 +0500\n\nModified: 2022-01-03 13:20:24 +0500\n\n---\n\n**Column level access controls**\n\n<https://aws.amazon.com/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift>\n![AQUA for Amazon Redshift - Advanced Query Accelerator A new distributed and hardware-accelerated processing layer that will make Amazon Redshift 10x faster than any other cloud data warehouse without increasing cost Compute Node AQUA Node Custom AWS-designed processor Compute Node Compute Node Parallel execution AQUA Node Custom AQUA Node Custom AWS-designed AWS-designed processor processor Compute Node AQUA Node Custom AWS-designed processor Preview Minimize data movement over the network by pushing down operations to AQUA Nodes AQUA Nodes with custom AWS-designed analytics processors to make operations (compression, encryption, filtering, and aggregations) faster than traditional CPUs Available only with RA3, no code changes required. Available in preview. ](media/AWS-Redshift_Others-image1.png)\n\n<https://aws.amazon.com/blogs/aws/new-aqua-advanced-query-accelerator-for-amazon-redshift>\n![Amazon Redshift Federated Query Query and join data from one or more RDS and Aurora PostgreSQL databases Analytics on operational data without data movement and ETL delays Integrate operational data with data warehouse and S3 data lake Flexible and easy way to ingest data avoiding complex ETL pipelines Intelligent distribution of computation to remote sources to optimize performance Redshift V JDBC/ODBC 1 1 1 Aurora PostgreSQL S3 Data lake 1 1 1 Previ RDS PostgreSQL ](media/AWS-Redshift_Others-image2.png)\n![Materialized views Compute once, query many times Materialized View loc_sales Speed up queries by orders of magnitude â€¢ Joins, filters, aggregations, and projections store_info Simplify and accelerate ETL/BI pipelines â€¢ Incremental refresh User triggered maintenance Easier and faster migration to Amazon Redshift store sl loc NY owner Joe Ann Lisa loc NY total_sales 12.00 10.00 sales item store cust il i2 s2 cl Previeu rice 12.0 3.0 7.0 ](media/AWS-Redshift_Others-image3.png)\n![Stored procedures support to simplify migrations Use Schema Conversion Tool to automatically convert your stored procedures Migrating to Amazon Redshift is even easier! Amazon Redshift supports Stored Procedures in PL/pgSQL format Stored procedures used for ETL, data validation, and custom business logic close to data. CREATE OR REPLACE PROCEDURE int, f2 varchar) AS $$ BEGIN RAISE INFO 'fl = %, f2 END; $$ LANGUAGE pipgsql; call test_sp1(5, 'abc'); INFO: fl = 5, f2 = abc % , fl, f2; ](media/AWS-Redshift_Others-image4.png)\n![Spatial processing Spatial Analytics at scale --- ingest, store and analyze spatial data Seamlessly integrate spatial and business data Get new dimension of insights and value New data type GEOMETRY 40+ SQL spatial functions Accessors, Constructors, Predicates Insert Client + Select Copy S3, or local ](media/AWS-Redshift_Others-image5.png)\n\nAmazon Redshift spatial provides location-based analytics for rich insights into your data. It seamlessly integrates spatial and business data to provide analytics for decision making.[Amazon Redshift launched native spatial data processing support in November 2019](https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-redshift-announces-support-spatial-data/), with a polymorphic data type GEOMETRY and several key SQL spatial functions. We now support GEOGRAPHY data type, and our library of SQL spatial functions has grown to 80. We support all the common spatial datatypes and standards, including Shapefiles, GeoJSON, WKT, WKB, eWKT, and eWKB.\n**Vaccum and Analyze commands**\n\n<https://hevodata.com/blog/redshift-vacuum-and-analyze>\n\n<https://docs.aws.amazon.com/redshift/latest/dg/t_Reclaiming_storage_space202.html>\n\n<https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html>\n\n## Auto WLM with queue priority**\n\n<https://docs.aws.amazon.com/redshift/latest/dg/automatic-wlm.html>\n\n## Others**\n\nSTL Load Errors - <https://docs.aws.amazon.com/redshift/latest/dg/r_STL_LOAD_ERRORS.html>\n\n<https://docs.aws.amazon.com/redshift/latest/dg/c_serial_isolation.html>"},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Redshift/Pricing---Sizing/","title":"Pricing / Sizing"},"frontmatter":{"draft":false},"rawBody":"# Pricing / Sizing\n\nCreated: 2020-12-23 02:00:47 +0500\n\nModified: 2021-12-08 23:39:19 +0500\n\n---\n\n**Node Types**\n\nAmazon Redshift offers different node types to accommodate your workloads, and we recommend choosing RA3 or DC2 depending on the required performance, data size and its growth.\n**RA3 nodes with managed storage**\n\nAllow you to optimize your data warehouse by scaling and paying for compute and managed storage independently. With RA3, you choose the number of nodes based on your performance requirements and only pay for the managed storage that you use. You should size your RA3 cluster based on the amount of data you process daily.\nRedshift managed storage uses large, high-performance SSDs in each RA3 node for fast local storage and Amazon S3 for longer-term durable storage. If the data in a node grows beyond the size of the large local SSDs, Redshift managed storage automatically offloads that data to Amazon S3. You pay the same low rate for Redshift managed storage regardless of whether the data sits in high-performance SSDs or S3. For workloads that require ever-growing storage, managed storage lets you automatically scale your data warehouse storage capacity without adding and paying for additional nodes.\n**Managed, Analytics-Optimized Storage**\n\nThe new managed storage is equally exciting. There's a cache of large-capacity, high-performance SSD-based storage on each instance, backed by[S3](https://aws.amazon.com/s3/), for scale, performance, and durability. The storage system uses multiple cues, including data block temperature, data blockage, and workload patterns, to manage the cache for high performance. Data is automatically placed into the appropriate tier, and you need not do anything special to benefit from the caching or the other optimizations. You pay the same low price for SSD and S3 storage, and you can scale the storage capacity of your data warehouse without adding and paying for additional instances.\n<https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#rs-ra3-node-types>\n\n## DC2 nodes**\n\nallow you to have compute-intensive data warehouses with local SSD storage included. You choose the number of nodes you need based on data size and performance requirements. DC2 nodes store your data locally for high performance, and as the data size grows, you can add more compute nodes to increase the storage capacity of the cluster. **For datasets under 1TB uncompressed,** we recommend DC2 node types for the best performance at the lowest price. If you expect your data to grow, we recommend using RA3 nodes so you can size compute and storage independently to achieve the best price and performance.\n**DS2 nodes**\n\nenable you to create large data warehouses using hard disk drives (HDDs), and we recommend using RA3 nodes instead. If you are using DS2 nodes, see[Overview of RA3 Node Types](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#rs-ra3-node-types)in the Cluster Management Guide for upgrade guidelines. Customers using eight or more nodes of DS2.xlarge, or any number of DS2.8xlarge nodes, can now upgrade to RA3 and get 2x more storage and better performance for the same on-demand cost.\n<https://aws.amazon.com/redshift/pricing>\n<table style=\"width:100%;\">\n<colgroup>\n<col style=\"width: 22%\" />\n<col style=\"width: 8%\" />\n<col style=\"width: 11%\" />\n<col style=\"width: 8%\" />\n<col style=\"width: 19%\" />\n<col style=\"width: 11%\" />\n<col style=\"width: 18%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th><strong>vCPU</strong></th>\n<th><strong>Memory</strong></th>\n<th><strong>Slices</strong></th>\n<th><strong>Addressable storage capacity</strong></th>\n<th><strong>I/O</strong></th>\n<th><strong>Price</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><strong>Dense Compute DC2</strong></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>dc2.large</td>\n<td>2</td>\n<td>15 GiB</td>\n<td>2</td>\n<td><p>0.16TB SSD</p>\n<p>= 160 GB</p></td>\n<td>0.60 GB/s</td>\n<td>$0.315per Hour <strong>0.315 * 8 = $2.52 per Hour</strong></td>\n</tr>\n<tr class=\"odd\">\n<td>dc2.8xlarge</td>\n<td>32</td>\n<td>244 GiB</td>\n<td>32</td>\n<td>2.56TB SSD</td>\n<td>7.50 GB/s</td>\n<td>$6.10per Hour</td>\n</tr>\n<tr class=\"even\">\n<td><strong>Dense Storage DS2</strong></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"odd\">\n<td>ds2.xlarge</td>\n<td>4</td>\n<td>31 GiB</td>\n<td>2</td>\n<td>2TB HDD</td>\n<td>0.40 GB/s</td>\n<td>$1.19per Hour</td>\n</tr>\n<tr class=\"even\">\n<td>ds2.8xlarge</td>\n<td>36</td>\n<td>244 GiB</td>\n<td>16</td>\n<td>16TB HDD</td>\n<td>3.30 GB/s</td>\n<td>$9.50per Hour</td>\n</tr>\n<tr class=\"odd\">\n<td><strong>RA3 with Redshift Managed Storage*</strong></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>ra3.xlplus</td>\n<td>4</td>\n<td>32 GiB</td>\n<td></td>\n<td>32TB RMS</td>\n<td>0.65 GB/sec</td>\n<td><p>$1.235per Hour</p>\n<p><strong>1.235* 2 = 2.47 per hour</strong></p></td>\n</tr>\n<tr class=\"odd\">\n<td>ra3.4xlarge</td>\n<td>12</td>\n<td>96 GiB</td>\n<td></td>\n<td>64TB RMS</td>\n<td>2.00 GB/s</td>\n<td>$3.706per Hour</td>\n</tr>\n<tr class=\"even\">\n<td>ra3.16xlarge</td>\n<td>48</td>\n<td>384 GiB</td>\n<td></td>\n<td>64TB RMS</td>\n<td>8.00 GB/s</td>\n<td>$14.827per Hour</td>\n</tr>\n</tbody>\n</table>\n\n## Migrate from DC2 to DS2**\n-   **50% cheaper**\n-   **2x query time**\n<https://medium.com/tensult/how-to-migrate-aws-redshift-dc2-to-ds2-node-cluster-73f320dc57f6>\n![Sizing Amazon Redshift cluster for production Estimate the uncompressed size of the incoming data Assume 3x compression (actual can be > 4x) Target 30-40% free space (resize to add/remove storage as needed) Disk utilization should be at least 15% and less than 80% Based on performance requirements, pick SSD or HDD If required, additional nodes can be added for increased performance Example: 20TB of uncompressed data 6.67 TB compressed Depending on performance requirements, recommendation: â€¢ 4xDC2.8xlarge or 5xDS2.xlarge = NIOTB of capacity ](media/AWS-Redshift_Pricing---Sizing-image1.png)\n**Resizing Amazon Redshift**\n\nClassic Resize\n-   Data is transferred from old cluster to new cluster (within hours)\n-   Change node types\n-   Enable / disable full disk encryption\nElastic Resize\n-   Nodes are added/removed to/from existing cluster (within minutes)\n![Classic resize Redirect DNS/bounce connections Leader node DC2.8XL Instances Node 1 JDBC/OOBC Leader Node 2 48-slice cluster Node 3 Node 1 Node 2 Node 3 Node 4 I I I I Ill 64-slice cluster â€¢ Source cluster is placed into read-only mode during resize All data is copied and re-distributed on the target cluster Allows for changing node types ](media/AWS-Redshift_Pricing---Sizing-image2.png)\n![Elastic Resize Etas tk requested Elastic Node 1 JDBC/OOBC Leader Node Node 2 Node 3 DC2.8XL Instances Amazon S3 48 Slice Cluster â€¢ Slices are redistributed to/from nodes Node 4 Inflight queries/connections â€¢ are put on hold â€¢ Some queries within transactions maybe rollback ](media/AWS-Redshift_Pricing---Sizing-image3.png)\n![Elastic Resize 4 fro m SS Data transfer Node 1 JOBC/OOBC Leader Node Node 2 Node 3 Node 4 DC2.8XL Instances Amazon S3 48 Slice Cluster Etas tk requested â€¢ Cluster is fully available; data transfer continues in the background Hot blocks are moved first ](media/AWS-Redshift_Pricing---Sizing-image4.png)\n![Elastic resize node increments nstance ype DC2 large DS2 xlarge DC2 8x1arge DS2 8x1arge Allowe Increments 2x or 1/2 original cluster size only Can allow Â± single or multiple nodes Max chÃ¥hgefrOm original size 2x, 1/2 size 2x, 1/2 size xampIÃ©/VaIicfSiÂ±es for 4 node cluster ](media/AWS-Redshift_Pricing---Sizing-image5.png)\n![When to use elastic vs. classic resize Elastic resize Scale up and down for workload spikes Incrementally add/remove storage Change cluster instance type (SSD ) If elastic resize is not an option because of sizing limits Limited availability during resize < 5 minutes (parked connections) Classic resize 1-24 hours (read-only) ](media/AWS-Redshift_Pricing---Sizing-image6.png)\n**Best Practices: Cluster Sizing**\n\nUse at least two computes nodes (multi-node cluster) in production for data mirrioring\n-   Leader node is given for no additional cost\nMaintain at least 20% free space or three times the size of the largest table\n-   Scratch space for usage, rewriting tables\n-   Free space is required for vacuum to re-sort table\n-   Temporary tables used fo intermediate query results\nThe maximum number of available Amazon Redshift Spectrum nodes is a function of the number of slices in the Amazon Redshift cluster\nIf you're using DC1 instances, upgrade to the DC2 instance type\n-   Same price as DC1, significantly faster\n-   Reserved instances can be migrated without additional cost in the AWS console\n"},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Redshift/Redshift-SQL-Queries---Commands/","title":"Redshift SQL Queries / Commands"},"frontmatter":{"draft":false},"rawBody":"# Redshift SQL Queries / Commands\n\nCreated: 2021-12-05 14:04:40 +0500\n\nModified: 2022-12-11 14:27:30 +0500\n\n---\n\n<https://docs.aws.amazon.com/redshift/latest/dg/c_designing-queries-best-practices.html>\ncopy public.perfios_parsed\n\nfrom'[s3://stashfin-migration-data/bank_score_data/perfios/parsed_data/old_data/2017/01/Jan_2017](s3://stashfin-migration-data/bank_score_data/perfios/parsed_data/old_data/2017/01/Jan_2017)'\n\niam_role 'arn:aws:iam::331916247734:role/'\n\nCOMPUPDATE off FORMATASPARQUET;\ncopy colender.incred_loan_mapping from 's3://rds-s3-redshift-bucket/ETL_Colender_Dashboard/INCRED/INCRED_Loan_Mapping.csv'\n\ncredentials 'aws_access_key_id=AKIAU2R6AAK3P4L7TV7P;aws_secret_access_key=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'\n\nDELIMITER ',' IGNOREHEADER 1 FILLRECORD IGNOREBLANKLINES NULL 'nan'\n\nACCEPTINVCHARS EMPTYASNULL ESCAPE COMPUPDATE OFF\nselect version();\n**# show sizes**\n\n**SELECT**tbl,**name**, size_mb**FROM**\n\n(\n\n**SELECT**tbl,**count**(*)**AS**size_mb\n\n**FROM**stv_blocklist\n\n**GROUP****BY**tbl\n\n)\n\n**LEFT****JOIN**\n\n(**select****distinct**id,**name****FROM**stv_tbl_perm)\n\n**ON**id = tbl\n\n**ORDER****BY**size_mb**DESC**\n\n**LIMIT**10;\nSELECT TRIM(pgdb.datname) AS Database,\nTRIM(a.name) AS Table,\n((b.mbytes/part.total::decimal)*100)::decimal(5,2) AS pct_of_total,\nb.mbytes,\nb.unsorted_mbytes\nFROM stv_tbl_perm a\nJOIN pg_database AS pgdb\nON pgdb.oid = a.db_id\nJOIN ( SELECT tbl,\nSUM( DECODE(unsorted, 1, 1, 0)) AS unsorted_mbytes,\nCOUNT(*) AS mbytes\nFROM stv_blocklist\nGROUP BY tbl ) AS b\nON a.id = b.tbl\nJOIN ( SELECT SUM(capacity) AS total\nFROM stv_partitions\nWHERE part_begin = 0 ) AS part\nON 1 = 1\nWHERE a.slice = 0\nORDER BY 4 desc, db_id, name;\n**# show all users and Grant**\n\n**select** usesysid **as** user_id,\n\nusename **as** username,\n\nusecreatedb **as** db_create,\n\nusesuper **as** is_superuser,\n\nvaluntil **as** password_expiration\n\n**from** pg_user\n\n**order by** user_id;\nCREATE USER intern2_datascience WITH password 's4XfxXE8D8FqXxNH';\n\ngrant usage on schema data_analytics to developer; (important for 1st time)\ngrantselectonalltablesinschemadata_analyticstouser_name;\nGRANT SELECT on SCHEMA_NAME.TABLE_NAME TO USER_NAME;\n\nGRANT SELECT on public.sentinel_customers TO bhupesh_goyal;\n\nGRANT SELECT on public.dsa_customers TO bhupesh_goyal;\n**grant** **select**,**update**,**delete** **on** **all** tables **in** **schema** colender **to** developer;\n<https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_USER.html>\n\n## select**\n\nid,\n\ndocs,\n\nis_valid_json_array(docs),\n\n**json_array_length**(docs),\n\njson_extract_array_element_text(docs, 1) **as** json_text,\n\nis_valid_json(json_text),\n\njson_extract_path_text(json_text, 'doc_status'),\n\njson_extract_path_text(json_text, 'id')\n\n**from**\n\nsttash_website_live.fos_scan_doc_comment\n\n**limit** 1;\n\n<https://docs.aws.amazon.com/redshift/latest/dg/json-functions.html>\n\n**# getting blocking queries**\n```\nSELECT waiting.relation::regclass AS waiting_table,\n\nblocking.relation::regclass AS blocking_table,\n\nwaiting.pid AS waiting_pid,\n\nblocking.pid AS blocking_pid,\n\nwaiting.mode AS waiting_mode,\n\nblocking.mode AS blocking_mode,\n\nwaiting.GRANTED AS waiting_granted,\n\nblocking.GRANTED AS blocking_granted,\n\nwaiting.TRANSACTION AS waiting_txn,\n\nblocking.TRANSACTION AS blocking_txn\n\nFROM pg_locks AS waiting\n\nLEFT JOIN pg_locks AS blocking\n\nON ( (waiting. \"database\" = blocking. \"database\"\n\nAND (waiting.relation = blocking.relation\n\nOR blocking.relation IS NULL\n\nOR waiting.relation IS NULL))\n\nOR waiting.TRANSACTION = blocking.TRANSACTION)\n\nWHERE 1 = 1\n\nAND NOT waiting.GRANTED\n\nAND waiting.pid <> blocking.pid\n\nAND blocking_granted = 't'\n\nORDER BY blocking_granted,\n\nwaiting_granted,\n\nblocking_pid,\n\nwaiting_pid;\n\nselect a.txn_owner, a.txn_db, a.xid, a.pid, a.txn_start, a.lock_mode, a.relation as table_id,nvl(trim(c.\"name\"),d.relname) as tablename, a.granted,b.pid as blocking_pid ,datediff(s,a.txn_start,getdate())/86400||' days '||datediff(s,a.txn_start,getdate())%86400/3600||' hrs '||datediff(s,a.txn_start,getdate())%3600/60||' mins '||datediff(s,a.txn_start,getdate())%60||' secs' as txn_duration from svv_transactions a left join (select pid,relation,granted from pg_locks group by 1,2,3) b on a.relation=b.relation and a.granted='f' and b.granted='t' left join (select * from stv_tbl_perm where slice=0) c on a.relation=c.id left join pg_class d on a.relation=d.oid where a.relation is not null;\nSELECT * from stl_query where pid=4887;\n\nSELECT pg_terminate_backend(4887);\n<https://aws.amazon.com/premiumsupport/knowledge-center/redshift-high-cpu-usage>\n\nWhile the queries are running,[retrieve locking information](https://aws.amazon.com/premiumsupport/knowledge-center/prevent-locks-blocking-queries-redshift/). To identify long-running sessions, use the following SQL query:\n\nselect *,datediff(s,txn_start,getdate())/86400||' days '||datediff(s,txn_start,getdate())%86400/3600||' hrs '||datediff(s,txn_start,getdate())%3600/60||' mins '||datediff(s,txn_start,getdate())%60||' secs'\nfrom svv_transactions where lockable_object_type='transactionid' and pid <> pg_backend_pid() order by 3;\nThen, run[PG_TERMINATE_BACKEND](https://docs.aws.amazon.com/redshift/latest/dg/PG_TERMINATE_BACKEND.html)to stop any long-running transactions. To prevent these sessions from remaining open, be sure that all transactions are closed. For example, make sure that all transactions starting with a[BEGIN](https://docs.aws.amazon.com/redshift/latest/dg/r_BEGIN.html)statement are also accompanied by anENDorCOMMITstatement.\n\nThen, run the following SQL query to identify queries consuming high CPU:\n\nselect stq.userid, stq.query, trim(stq.label) as label, stq.xid, stq.pid, svq.service_class,\nquery_cpu_usage_percent as \"cpu_%\",starttime, endtime, datediff(s,starttime, endtime) as duration_s,\nsubstring(stq.querytxt,1,100) as querytext from stl_query stq\njoin svl_query_metrics svq on stq.query=svq.query\nwhere query_cpu_usage_percent is not null and starttime > sysdate - 1\norder by query_cpu_usage_percent desc;\nTo analyze segment and slice-level execution steps for each query, run the following query:\n\nselect query, segment, step, label ,is_rrscan as rrS, is_diskbased as disk, is_delayed_scan as DelayS, min(start_time) as starttime, max(end_time) as endtime, datediff(ms, min(start_time), max(end_time)) as \"elapsed_msecs\", sum(rows) as row_s , sum(rows_pre_filter) as rows_pf, CASE WHEN sum(rows_pre_filter) = 0 THEN 100 ELSE sum(rows)::float/sum(rows_pre_filter)::float*100 END as pct_filter, SUM(workmem)/1024/1024 as \"Memory(MB)\", SUM(bytes)/1024/1024 as \"MB_produced\" from svl_query_report where query in (query_ids) group by query, segment, step, label , is_rrscan, is_diskbased , is_delayed_scan order by query, segment, step, label;-   Reduce query concurrency per queue to provide more memory to each query slot. This reduction helps queries that require more memory to run more efficiently.\n-   Enable[short query acceleration](https://docs.aws.amazon.com/redshift/latest/dg/wlm-short-query-acceleration.html)(SQA) to prioritize short-running queries over long-running queries.\n**UNLOAD**\n\n<https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html>\n\n## Redshift Spectrum**\n\nSELECT * FROM SVV_EXTERNAL_DATABASES;\n\nSELECT * FROM SVV_EXTERNAL_SCHEMAS;\n\nSELECT * FROM SVV_EXTERNAL_TABLES;\n\nSELECT * FROM SVV_EXTERNAL_PARTITIONS;\n\nSELECT * FROM SVV_EXTERNAL_COLUMNS;\ncreate external schema spectrum_schema from data catalog\n\ndatabase 'pinpointanalytics'\n\niam_role 'arn:aws:iam::331916247734:role/service-role/AmazonRedshift-CommandsAccessRole-20211208T161606';\ndrop schema spectrum_schema;\nCREATE EXTERNAL TABLE spectrum_schema.test_loan_data (\n\nid integer,\n\nloan_status integer)\n\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n\nLOCATION 's3://data-team-reporting/test/';\nSELECT * FROM spectrum_schema.test_loan_data limit 10;\n```"},{"fields":{"slug":"/Databases/SQL-Databases/MySQL/11.-MySQL-Data-Types/","title":"11. MySQL Data Types"},"frontmatter":{"draft":false},"rawBody":"# 11. MySQL Data Types\n\nCreated: 2020-04-20 08:51:00 +0500\n\nModified: 2021-06-23 14:46:39 +0500\n\n---\n\n**11.1 Numeric Data Types**\n\n11.1.1 Numeric Data Type Syntax\n\n11.1.2 Integer Types (Exact Value) - INTEGER, INT, SMALLINT, TINYINT,\n\nMEDIUMINT, BIGINT\n\n11.1.3 Fixed-Point Types (Exact Value) - DECIMAL, NUMERIC\n\n11.1.4 Floating-Point Types (Approximate Value) - FLOAT, DOUBLE\n\n11.1.5 Bit-Value Type - BIT\n\n11.1.6 Numeric Type Attributes\n\n11.1.7 Out-of-Range and Overflow Handling\n**11.2 Date and Time Data Types**\n\n11.2.1 Date and Time Data Type Syntax\n\n11.2.2 The DATE, DATETIME, and TIMESTAMP Types\n\n11.2.3 The TIME Type\n\n11.2.4 The YEAR Type\n\n11.2.5 2-Digit YEAR(2) Limitations and Migrating to 4-Digit YEAR\n\n11.2.6 Automatic Initialization and Updating for TIMESTAMP and DATETIME\n\n11.2.7 Fractional Seconds in Time Values\n\n11.2.8 Conversion Between Date and Time Types\n\n11.2.9 2-Digit Years in Dates\n**11.3 String Data Types**\n\n11.3.1 String Data Type Syntax\n\n11.3.2 The CHAR and VARCHAR Types\n\n11.3.3 The BINARY and VARBINARY Types\n\n11.3.4 The BLOB and TEXT Types\n\n11.3.5 The ENUM Type\n\n11.3.6 The SET Type\n**11.4 Spatial Data Types**\n\n11.4.1 Spatial Data Types\n\n11.4.2 The OpenGIS Geometry Model\n\n11.4.3 Supported Spatial Data Formats\n\n11.4.4 Geometry Well-Formedness and Validity\n\n11.4.5 Creating Spatial Columns\n\n11.4.6 Populating Spatial Columns\n\n11.4.7 Fetching Spatial Data\n\n11.4.8 Optimizing Spatial Analysis\n\n11.4.9 Creating Spatial Indexes\n\n11.4.10 Using Spatial Indexes\n**11.5 The JSON Data Type**\n\nTheJSONdata type provides these advantages over storing JSON-format strings in a string column:\n-   **Automatic validation** of JSON documents stored inJSONcolumns. Invalid documents produce an error.\n-   **Optimized storage format**. JSON documents stored inJSONcolumns are converted to an internal format that permits quick read access to document elements. When the server later must read a JSON value stored in this binary format, the value need not be parsed from a text representation. The binary format is structured to enable the server to look up subobjects or nested values directly by key or array index without reading all values before or after them in the document.\n<https://dev.mysql.com/doc/refman/8.0/en/json.html>\n11.6 Data Type Default Values\n\n**11.7 Data Type Storage Requirements**\n\n| **Type**  | **Storage** | **Minimum Value**     | **Maximum Value**     |\n|-----------|-------------|-----------------------|-----------------------|\n|          | **(Bytes)** | **(Signed/Unsigned)** | **(Signed/Unsigned)** |\n| TINYINT   | 1           | -128                  | 127                   |\n|          |            | 0                     | 255                   |\n| SMALLINT  | 2           | -32768                | 32767                 |\n|          |            | 0                     | 65535                 |\n| MEDIUMINT | 3           | -8388608              | 8388607               |\n|          |            | 0                     | 16777215              |\n| INT       | 4           | -2147483648           | 2147483647            |\n|          |            | 0                     | 4294967295            |\n| BIGINT    | 8           | -9223372036854775808  | 9223372036854775807   |\n|          |            | 0                     | 18446744073709551615  |\n| FLOAT(***p***)                                                                                  | 4 bytes if 0 <=***p***<= 24, 8 bytes if 25 <=***p***<= 53 |\n|---------------------------------|---------------------------------------|\n| [FLOAT](https://dev.mysql.com/doc/refman/8.0/en/floating-point-types.html)                      | 4 bytes                                                           |\n| DOUBLE [PRECISION],[REAL](https://dev.mysql.com/doc/refman/8.0/en/floating-point-types.html) | 8 bytes                                                           |\n| DECIMAL(***M***,***D***),NUMERIC(***M***,***D***)                                              | Varies; see following discussion                                  |\n| BIT(***M***)                                                                                    | approximately (***M***+7)/8 bytes                                 |\n**int(11)**\n\nThe number in the parenthesis i.e()does not determines the max and min values that can be stored in the integer field. The max and min values that can be stored are always fixed. It is just**display widthof integer** data type.\n**INT UNSIGNED**\n\n**INT(11) ZEROFILL**\n| **Data Type**                                                      | **Storage Required as of MySQL 5.6.4** |\n|----------------------|--------------------------------------------------|\n| [YEAR](https://dev.mysql.com/doc/refman/8.0/en/year.html)          | 1 byte                                 |\n| [DATE](https://dev.mysql.com/doc/refman/8.0/en/datetime.html)      | 3 bytes                                |\n| [TIME](https://dev.mysql.com/doc/refman/8.0/en/time.html)          | 3 bytes + fractional seconds storage   |\n| [DATETIME](https://dev.mysql.com/doc/refman/8.0/en/datetime.html)  | 5 bytes + fractional seconds storage   |\n| [TIMESTAMP](https://dev.mysql.com/doc/refman/8.0/en/datetime.html) | 4 bytes + fractional seconds storage   |\n| **Fractional Seconds Precision** | **Storage Required** |\n|----------------------------------|----------------------|\n| 0                                | 0 bytes              |\n| 1, 2                             | 1 byte               |\n| 3, 4                             | 2 bytes              |\n| 5, 6                             | 3 bytes              |\n| **Data Type**                                                                                                                    | **Storage Required**                                                                                                                                                                                                                                                                                                                                                                                                    |\n|----------------------------|--------------------------------------------|\n| CHAR(***M***)                                                                                                                    | The compact family of InnoDB row formats optimize storage for variable-length character sets. See[COMPACT Row Format Storage Characteristics](https://dev.mysql.com/doc/refman/8.0/en/innodb-row-format.html#innodb-compact-row-format-characteristics). Otherwise,***M***Ã—***w***bytes,<=***M***<=255, where***w***is the number of bytes required for the maximum-length character in the character set. |\n| BINARY(***M***)                                                                                                                  | ***M***bytes, 0<=***M***<=255                                                                                                                                                                                                                                                                                                                                                                                    |\n| VARCHAR(***M***),VARBINARY(***M***)                                                                                             | ***L***+ 1 bytes if column values require 0 âˆ’ 255 bytes,***L***+ 2 bytes if values may require more than 255 bytes                                                                                                                                                                                                                                                                                                   |\n| [TINYBLOB](https://dev.mysql.com/doc/refman/8.0/en/blob.html),[TINYTEXT](https://dev.mysql.com/doc/refman/8.0/en/blob.html)     | ***L***+ 1 bytes, where***L***< 2^8^                                                                                                                                                                                                                                                                                                                                                                                |\n| [BLOB](https://dev.mysql.com/doc/refman/8.0/en/blob.html),[TEXT](https://dev.mysql.com/doc/refman/8.0/en/blob.html)             | ***L***+ 2 bytes, where***L***< 2^16^                                                                                                                                                                                                                                                                                                                                                                               |\n| [MEDIUMBLOB](https://dev.mysql.com/doc/refman/8.0/en/blob.html),[MEDIUMTEXT](https://dev.mysql.com/doc/refman/8.0/en/blob.html) | ***L***+ 3 bytes, where***L***< 2^24^                                                                                                                                                                                                                                                                                                                                                                               |\n| [LONGBLOB](https://dev.mysql.com/doc/refman/8.0/en/blob.html),[LONGTEXT](https://dev.mysql.com/doc/refman/8.0/en/blob.html)     | ***L***+ 4 bytes, where***L***< 2^32^                                                                                                                                                                                                                                                                                                                                                                               |\n| ENUM('***value1***','***value2***',...)                                                                                     | 1 or 2 bytes, depending on the number of enumeration values (65,535 values maximum)                                                                                                                                                                                                                                                                                                                                     |\n| SET('***value1***','***value2***',...)                                                                                      | 1, 2, 3, 4, or 8 bytes, depending on the number of set members (64 members maximum)                                                                                                                                                                                                                                                                                                                                     |\n**TEXT**\n-   fixed max size of 65535 characters (you cannot limit the max size)\n-   takes 2 +cbytes of disk space, wherecis the length of the stored string.\n-   cannot be (fully) part of an index. One would need to specify a prefix length.\n**VARCHAR(M)**\n-   variable max size ofMcharacters\n-   Mneeds to be between 1 and 65535\n-   takes 1 +cbytes (forMâ‰¤ 255) or 2 +c(for 256 â‰¤Mâ‰¤ 65535) bytes of disk space wherecis the length of the stored string\n-   can be part of an index\n<https://dev.mysql.com/doc/refman/8.0/en/storage-requirements.html>\n\n<https://chartio.com/resources/tutorials/understanding-strorage-sizes-for-mysql-text-data-types>\n\n## 11.8 Choosing the Right Type for a Column**\n\nFor optimum storage, you should try to use the most precise type in all cases. For example, if an integer column is used for values in the range from1to99999,MEDIUMINT UNSIGNEDis the best type. Of the types that represent all the required values, this type uses the least amount of storage.\nAll basic calculations (+,-,*, and/) with[DECIMAL](https://dev.mysql.com/doc/refman/8.0/en/fixed-point-types.html)columns are done with precision of 65 decimal (base 10) digits. See[Section11.1.1, \"Numeric Data Type Syntax\"](https://dev.mysql.com/doc/refman/8.0/en/numeric-type-syntax.html).\nIf accuracy is not too important or if speed is the highest priority, the[DOUBLE](https://dev.mysql.com/doc/refman/8.0/en/floating-point-types.html)type may be good enough. For high precision, you can always convert to a fixed-point type stored in a[BIGINT](https://dev.mysql.com/doc/refman/8.0/en/integer-types.html). This enables you to do all calculations with 64-bit integers and then convert results back to floating-point values as necessary.\n**11.9 Using Data Types from Other Database Engines**\n<https://dev.mysql.com/doc/refman/8.0/en/data-types.html>\n\n![MySQL Data Types](media/MySQL_11.-MySQL-Data-Types-image1.jpg)\n\n<https://www.mysqltutorial.org/mysql-data-types.aspx>\n\n## Optimizing data types**\n\n[**8.4.2.1 Optimizing for Numeric Data**](https://dev.mysql.com/doc/refman/8.0/en/optimize-numeric.html)\n-   For unique IDs or other values that can be represented as either strings or numbers, **prefer numeric columns to string columns**. Since large numeric values can be stored in fewer bytes than the corresponding strings, it is faster and takes less memory to transfer and compare them.\n-   If you are using numeric data, it is faster in many cases to access information from a database (using a live connection) than to access a text file. Information in the database is likely to be stored in a more compact format than in the text file, so accessing it involves fewer disk accesses. You also save code in your application because you can avoid parsing the text file to find line and column boundaries.\n[**8.4.2.2 Optimizing for Character and String Types**](https://dev.mysql.com/doc/refman/8.0/en/optimize-character.html)\n-   Use binary collation order for fast comparison and sort operations, when you do not need language-specific collation features. You can use the[BINARY](https://dev.mysql.com/doc/refman/8.0/en/cast-functions.html#operator_binary)operator to use binary collation within a particular query.\n-   When comparing values from different columns, declare those columns with the same character set and collation wherever possible, to avoid string conversions while running the query.\n-   For column values less than 8KB in size, use **binaryVARCHARinstead ofBLOB**. TheGROUP BYandORDER BYclauses can generate temporary tables, and these temporary tables can use theMEMORYstorage engine if the original table does not contain anyBLOBcolumns.\n-   If a table contains string columns such as name and address, but many queries do not retrieve those columns, consider **splitting the string columns into a separate table** and using join queries with a foreign key when necessary. When MySQL retrieves any value from a row, it reads a data block containing all the columns of that row (and possibly other adjacent rows). Keeping each row small, with only the most frequently used columns, allows more rows to fit in each data block. Such compact tables reduce disk I/O and memory usage for common queries.\n-   When you use a randomly generated value as a primary key in anInnoDBtable, prefix it with an ascending value such as the current date and time if possible. When consecutive primary values are physically stored near each other,InnoDBcan insert and retrieve them faster.\n[**8.4.2.3 Optimizing for BLOB Types**](https://dev.mysql.com/doc/refman/8.0/en/optimize-blob.html)\n-   When storing a large blob containing textual data, consider **compressing** it first. Do not use this technique when the entire table is compressed byInnoDBorMyISAM.\n-   For a table with several columns, to reduce memory requirements for queries that do not use the BLOB column, consider **splitting the BLOB column into a separate table** and referencing it with a join query when needed.\n-   Since the performance requirements to retrieve and display a BLOB value might be very different from other data types, you could put the BLOB-specific table on a different storage device or even a separate database instance. For example, to retrieve a BLOB might require a large sequential disk read that is better suited to a traditional hard drive than to an[SSD device](https://dev.mysql.com/doc/refman/8.0/en/glossary.html#glos_ssd).\n-   **a binaryVARCHARcolumn is sometimes preferable to an equivalent BLOB column.**\n-   Rather than testing for equality against a very long text string, you can store a hash of the column value in a separate column, index that column, and test the hashed value in queries. (Use theMD5()orCRC32()function to produce the hash value.) Since hash functions can produce duplicate results for different inputs, you still include a clause AND blob_column = long_string_value in the query to guard against false matches; the performance benefit comes from the smaller, easily scanned index for the hashed values.\n<https://www.oreilly.com/library/view/high-performance-mysql/9781449332471/ch04.html>\n\n<https://dev.mysql.com/doc/refman/8.0/en/optimize-data-types.html>\n\n"},{"fields":{"slug":"/Databases/SQL-Databases/MySQL/Connection-Handling/","title":"Connection Handling"},"frontmatter":{"draft":false},"rawBody":"# Connection Handling\n\nCreated: 2020-03-14 13:52:34 +0500\n\nModified: 2020-12-02 23:07:41 +0500\n\n---\n\nThe MySQL Server (mysqld) executes as a single OSprocess, with multiplethreadsexecuting concurrent activities. MySQL does not have its own thread implementation, but relies on the thread implementation of the underlying OS. When a userconnectsto the database auser threadis created inside mysqld and this user thread executes user queries, sending results back to the user, until the userdisconnects.\nWhen more and more users connect to the database, more and more user threads execute in parallel. As long as all user threads execute as if they are alone we can say that the system (MySQL) scales well. But at some point we reach a limit and adding more user threads will not be useful or efficient.\n**Connect and Disconnect**\n\nConnectionscorrespond toSessionsin SQL standard terminology. A client connects to the MySQL Server and stays connected until it does a disconnect. Figure 1 illustrates what happens when a MySQL Client connects to a MySQL Server.\n\n![Clients Thread Cache poo Connection Requests Receiver Thread mysqld User Threads THDs ](media/MySQL_Connection-Handling-image1.png)\n**Clients**\n\nA MySQL Client is a command line tool or an application that talks to the MySQL Server over the MySQL Client-Server protocol using the[libmysqlclient](https://dev.mysql.com/doc/refman/8.0/en/c-api-implementations.html)library or some of the many MySQL connectors. A single multi-threaded client can open many connections to the server, but for simplicity we here sayone client opens one connectionto the server.\n**Connection Requests**\n\nThe MySQL Clients sendconnection requeststo the MySQL Server. A connection request is simply a TCP-IP connect message sent to port 3306 on the server host machine.\n**Receiver Thread**\n\nIncoming connection requests are queued and then processed by thereceiver threadone by one. The only job of the receiver thread is to create auser thread, further processing is done by the user thread.\n**Thread Cache**\n\nThe receiver thread will either create a new OS thread or reuse an existing \"free\" OS thread if found in the thread cache. The thread cache used to be important for connect speed when OS threads were costly to create. Nowadays creating an OS thread is relatively cheap and the thread cache can perhaps be said to be legacy. The[thread_cache_size](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_thread_cache_size)default value is calculated as 8 + (max_connections / 100) and is rarely changed. It might make sense to try increasing the thread cache in cases where number of connections fluctuates between having very few connections and having many connections.\n**User Thread**\n\nIt is the user thread that handles the[client-server protocol](https://dev.mysql.com/doc/dev/mysql-server/latest/PAGE_PROTOCOL.html), e.g. sends back the initial handshake packet. Thisuser threadwill allocate and initialize the corresponding THD, and then continue with capability negotiation and authentication. In this process the user credentials are stored in the THD'ssecurity context. If everything goes well in the[connection phase](https://dev.mysql.com/doc/dev/mysql-server/latest/page_protocol_connection_phase.html), the user thread will enter the[command phase](https://dev.mysql.com/doc/dev/mysql-server/latest/page_protocol_command_phase.html).\n**THD**\n\nThe connection is represented by a data structure called the THD which is created when the connection is established and deleted when the connection is dropped. There is always a one-to-one correspondence between a user connection and a THD, i.e. THDs are not reused across connections. The size of the THD is ~10K and its definition is found in[sql_class.h](https://dev.mysql.com/doc/dev/mysql-server/latest/classTHD.html). The THD is a large data structure which is used to keep track of various aspects of execution state. Memory rooted in the THD will grow significantly during query execution, but exactly how much it grows will depend upon the query. For memory planning purposes we recommend to plan for ~10MB per connection on average.\n![mysqld Client User Thread Que Result THD ](media/MySQL_Connection-Handling-image2.png)\nFigure 2 illustrates the command phase. Here, the client sends queries to the server and get results back in several rounds. In general, a sequence of statements can be enclosed by a start transaction and a commit/rollback. In this case there is a need to keep track of thetransaction context. In auto-commit mode, each statement will be executed as a transaction (each statement constitutes the full transaction context). In addition there is thesession context, i.e. the session can hold session variables, user variables, and temporary tables. Thus, as long as the context is relevant for executing queries, all queries on a connection must use the same THD.\n![mysqld Client COM Thread Cache er Thread QUIT THD ](media/MySQL_Connection-Handling-image3.png)\nFigure 3 illustrates what happens when a MySQL Client disconnects from a MySQL Server. The Client sends a[COM_QUIT](https://dev.mysql.com/doc/dev/mysql-server/latest/page_protocol_com_quit.html)command which causes the server to close the socket. A disconnect can also happen when either side closes its end of the socket. Upon a disconnect the user thread will clean up, deallocate the THD, and finally put itself in the Thread Cache as \"suspended\" if there are free slots. If there are no free slots, the user thread will be \"terminated\".\n**Short Lived Connections**\n\nA short lived connection is a connection that is only open for a short period of time. This is typically the case for PHP applications, the client opens a connection, executes a simple query, and then closes the connection. Due to its architecture, MySQL is really good at accepting new connections at a high speed, up to 80,000 connects/disconnects per second\n**Long Lived Connections**\n\nA long lived connection is a connection that is open \"indefinitely\". For example one might have a Web server or an Application server opening many connections to the MySQL server and keeping them open until the client (Web/Application server) is stopped, perhaps for months.\n**What limits threads concurrency**\n\nA thread will happily execute instructions until it needs to wait for something or until it has used its timeshare as decided by the OS scheduler. There are three things a thread might need to wait for: A mutex, a database lock, or IO.\n**Conclusion**\n-   MySQL is very good at handling many clients connecting and disconnecting to the database at a high frequency, up to 80 thousand connect and disconnects per second\n-   MySQL scales well on multi-core CPUs and can deliver up to 2 million primary key look-ups per second on 48 CPU cores.\n-   Rule of thumb: Max number of connections = 4 times available CPU cores\n-   Efficient use of connections will depend upon user load, useful number of user connections can even be lower than number of CPU cores when the bottleneck is somewhere else than on the threading\n-   Check out your own load by doubling the number of connections until TPS no longer increases and latency starts to increase\n<https://mysqlserverteam.com/mysql-connection-handling-and-scaling>\n"},{"fields":{"slug":"/Databases/SQL-Databases/MySQL/Documentation/","title":"Documentation"},"frontmatter":{"draft":false},"rawBody":"# Documentation\n\nCreated: 2020-02-10 12:42:49 +0500\n\nModified: 2022-11-18 15:53:59 +0500\n\n---\n\n1.3 Overview of the MySQL Database Management System\n\n1.3.1 What is MySQL?\n\n1.3.2 The Main Features of MySQL\n\n1.3.3 History of MySQL\n\n1.4 What Is New in MySQL 5.7\n\n1.5 Server and Status Variables and Options Added, Deprecated, or Removed in MySQL\n\n5.7\n\n1.6 MySQL Information Sources\n\n1.6.1 MySQL Websites\n\n1.6.2 MySQL Community Support at the MySQL Forums\n\n1.6.3 MySQL Enterprise\n\n1.7 How to Report Bugs or Problems\n\n1.8 MySQL Standards Compliance\n\n1.8.1 MySQL Extensions to Standard SQL\n\n1.8.2 MySQL Differences from Standard SQL\n\n1.8.3 How MySQL Deals with Constraints\n\n1.9 Credits\n\n1.9.1 Contributors to MySQL\n\n1.9.2 Documenters and translators\n\n1.9.3 Packages that support MySQL\n\n1.9.4 Tools that were used to create MySQL\n\n1.9.5 Supporters of MySQL\n\n2 Installing and Upgrading MySQL\n\n2.1 General Installation Guidance\n\n2.1.1 Which MySQL Version and Distribution to Install\n\n2.1.2 How to Get MySQL\n\n2.1.3 Verifying Package Integrity Using MD5 Checksums or GnuPG\n\n2.1.4 Installation Layouts\n\n2.1.5 Compiler-Specific Build Characteristics\n\n2.2 Installing MySQL on Unix/Linux Using Generic Binaries\n\n2.3 Installing MySQL on Microsoft Windows\n\n2.3.1 MySQL Installation Layout on Microsoft Windows\n\n2.3.2 Choosing an Installation Package\n\n2.3.3 MySQL Installer for Windows\n\n2.3.4 Installing MySQL on Microsoft Windows Using a noinstall ZIP Archive\n\n2.3.5 Troubleshooting a Microsoft Windows MySQL Server Installation\n\n2.3.6 Windows Postinstallation Procedures\n\n2.3.7 Windows Platform Restrictions\n\n2.4 Installing MySQL on macOS\n\n2.4.1 General Notes on Installing MySQL on macOS\n\n2.4.2 Installing MySQL on macOS Using Native Packages\n\n2.4.3 Installing a MySQL Launch Daemon\n\n2.4.4 Installing and Using the MySQL Preference Pane\n\n2.5 Installing MySQL on Linux\n\n2.5.1 Installing MySQL on Linux Using the MySQL Yum Repository\n\n2.5.2 Replacing a Third-Party Distribution of MySQL Using the MySQL Yum\n\nRepository\n\n2.5.3 Installing MySQL on Linux Using the MySQL APT Repository\n\n2.5.4 Installing MySQL on Linux Using the MySQL SLES Repository\n\n2.5.5 Installing MySQL on Linux Using RPM Packages from Oracle\n\n2.5.6 Installing MySQL on Linux Using Debian Packages from Oracle\n\n2.5.7 Deploying MySQL on Linux with Docker\n\n2.5.8 Installing MySQL on Linux from the Native Software Repositories\n\n2.5.9 Installing MySQL on Linux with Juju\n\n2.5.10 Managing MySQL Server with systemd\n\n2.6 Installing MySQL Using Unbreakable Linux Network (ULN)\n\n2.7 Installing MySQL on Solaris\n\n2.7.1 Installing MySQL on Solaris Using a Solaris PKG\n\n2.8 Installing MySQL on FreeBSD\n\n2.9 Installing MySQL from Source\n\n2.9.1 Source Installation Methods\n\n2.9.2 Source Installation Prerequisites\n\n2.9.3 MySQL Layout for Source Installation\n\n2.9.4 Installing MySQL Using a Standard Source Distribution\n\n2.9.5 Installing MySQL Using a Development Source Tree\n\n2.9.6 Configuring SSL Library Support\n\n2.9.7 MySQL Source-Configuration Options\n\n2.9.8 Dealing with Problems Compiling MySQL\n\n2.9.9 MySQL Configuration and Third-Party Tools\n\n2.10 Postinstallation Setup and Testing\n\n2.10.1 Initializing the Data Directory\n\n2.10.2 Starting the Server\n\n2.10.3 Testing the Server\n\n2.10.4 Securing the Initial MySQL Account\n\n2.10.5 Starting and Stopping MySQL Automatically\n\n2.11 Upgrading MySQL\n\n2.11.1 Before You Begin\n\n2.11.2 Upgrade Paths\n\n2.11.3 Changes in MySQL 5.7\n\n2.11.4 Upgrading MySQL Binary or Package-based Installations on Unix/Linux\n\n2.11.5 Upgrading MySQL with the MySQL Yum Repository\n\n2.11.6 Upgrading MySQL with the MySQL APT Repository\n\n2.11.7 Upgrading MySQL with the MySQL SLES Repository\n\n2.11.8 Upgrading MySQL on Windows\n\n2.11.9 Upgrading a Docker Installation of MySQL\n\n2.11.10 Upgrading MySQL with Directly-Downloaded RPM Packages\n\n2.11.11 Upgrade Troubleshooting\n\n2.11.12 Rebuilding or Repairing Tables or Indexes\n\n2.11.13 Copying MySQL Databases to Another Machine\n\n2.12 Downgrading MySQL\n\n2.12.1 Before You Begin\n\n2.12.2 Downgrade Paths\n\n2.12.3 Downgrade Notes\n\n2.12.4 Downgrading Binary and Package-based Installations on Unix/Linux\n\n2.12.5 Downgrade Troubleshooting\n\n2.13 Perl Installation Notes\n\n2.13.1 Installing Perl on Unix\n\n2.13.2 Installing ActiveState Perl on Windows\n\n2.13.3 Problems Using the Perl DBI/DBD Interface\n\n3 Tutorial\n\n3.1 Connecting to and Disconnecting from the Server\n\n3.2 Entering Queries\n\n3.3 Creating and Using a Database\n\n3.3.1 Creating and Selecting a Database\n\n3.3.2 Creating a Table\n\n3.3.3 Loading Data into a Table\n\n3.3.4 Retrieving Information from a Table\n\n3.4 Getting Information About Databases and Tables\n\n3.5 Using mysql in Batch Mode\n\n3.6 Examples of Common Queries\n\n3.6.1 The Maximum Value for a Column\n\n3.6.2 The Row Holding the Maximum of a Certain Column\n\n3.6.3 Maximum of Column per Group\n\n3.6.4 The Rows Holding the Group-wise Maximum of a Certain Column\n\n3.6.5 Using User-Defined Variables\n\n3.6.6 Using Foreign Keys\n\n3.6.7 Searching on Two Keys\n\n3.6.8 Calculating Visits Per Day\n\n3.6.9 Using AUTO_INCREMENT\n\n3.7 Using MySQL with Apache\n\n4 MySQL Programs\n\n4.1 Overview of MySQL Programs\n\n4.2 Using MySQL Programs\n\n4.2.1 Invoking MySQL Programs\n\n4.2.2 Specifying Program Options\n\n4.2.3 Command Options for Connecting to the Server\n\n4.2.4 Connecting to the MySQL Server Using Command Options\n\n4.2.5 Connection Compression Control\n\n4.2.6 Setting Environment Variables\n\n4.3 Server and Server-Startup Programs\n\n4.3.1 mysqld --- The MySQL Server\n\n4.3.2 mysqld_safe --- MySQL Server Startup Script\n\n4.3.3 mysql.server --- MySQL Server Startup Script\n\n4.3.4 mysqld_multi --- Manage Multiple MySQL Servers\n\n4.4 Installation-Related Programs\n\n4.4.1 comp_err --- Compile MySQL Error Message File\n\n4.4.2 mysql_install_db --- Initialize MySQL Data Directory\n\n4.4.3 mysql_plugin --- Configure MySQL Server Plugins\n\n4.4.4 mysql_secure_installation --- Improve MySQL Installation Security\n\n4.4.5 mysql_ssl_rsa_setup --- Create SSL/RSA Files\n\n4.4.6 mysql_tzinfo_to_sql --- Load the Time Zone Tables\n\n4.4.7 mysql_upgrade --- Check and Upgrade MySQL Tables\n\n4.5 Client Programs\n\n4.5.1 mysql --- The MySQL Command-Line Client\n\n4.5.2 mysqladmin --- Client for Administering a MySQL Server\n\n4.5.3 mysqlcheck --- A Table Maintenance Program\n\n4.5.4 mysqldump --- A Database Backup Program\n\n4.5.5 mysqlimport --- A Data Import Program\n\n4.5.6 mysqlpump --- A Database Backup Program\n\n4.5.7 mysqlshow --- Display Database, Table, and Column Information\n\n4.5.8 mysqlslap --- Load Emulation Client\n\n4.6 Administrative and Utility Programs\n\n4.6.1 innochecksum --- Offline InnoDB File Checksum Utility\n\n4.6.2 myisam_ftdump --- Display Full-Text Index information\n\n4.6.3 myisamchk --- MyISAM Table-Maintenance Utility\n\n4.6.4 myisamlog --- Display MyISAM Log File Contents\n\n4.6.5 myisampack --- Generate Compressed, Read-Only MyISAM Tables\n\n4.6.6 mysql_config_editor --- MySQL Configuration Utility\n\n4.6.7 mysqlbinlog --- Utility for Processing Binary Log Files\n\n4.6.8 mysqldumpslow --- Summarize Slow Query Log Files\n\n4.7 Program Development Utilities\n\n4.7.1 mysql_config --- Display Options for Compiling Clients\n\n4.7.2 my_print_defaults --- Display Options from Option Files\n\n4.7.3 resolve_stack_dump --- Resolve Numeric Stack Trace Dump to Symbols\n\n4.8 Miscellaneous Programs\n\n4.8.1 lz4_decompress --- Decompress mysqlpump LZ4-Compressed Output\n\n4.8.2 perror --- Display MySQL Error Message Information\n\n4.8.3 replace --- A String-Replacement Utility\n\n4.8.4 resolveip --- Resolve Host name to IP Address or Vice Versa\n\n4.8.5 zlib_decompress --- Decompress mysqlpump ZLIB-Compressed Output\n\n4.9 Environment Variables\n\n4.10 Unix Signal Handling in MySQL\n\n5 MySQL Server Administration\n\n5.1 The MySQL Server\n\n5.1.1 Configuring the Server\n\n5.1.2 Server Configuration Defaults\n\n5.1.3 Server Option, System Variable, and Status Variable Reference\n\n5.1.4 Server System Variable Reference\n\n5.1.5 Server Status Variable Reference\n\n5.1.6 Server Command Options\n\n5.1.7 Server System Variables\n\n5.1.8 Using System Variables\n\n5.1.9 Server Status Variables\n\n5.1.10 Server SQL Modes\n\n5.1.11 IPv6 Support\n\n5.1.12 MySQL Server Time Zone Support\n\n5.1.13 Server-Side Help Support\n\n5.1.14 Server Tracking of Client Session State Changes\n\n5.1.15 The Server Shutdown Process\n\n5.2 The MySQL Data Directory\n\n5.3 The mysql System Database\n\n5.4 MySQL Server Logs\n\n5.4.1 Selecting General Query Log and Slow Query Log Output Destinations\n\n5.4.2 The Error Log\n\n5.4.3 The General Query Log\n\n5.4.4 The Binary Log\n\n5.4.5 The Slow Query Log\n\n5.4.6 The DDL Log\n\n5.4.7 Server Log Maintenance\n\n5.5 MySQL Server Plugins\n\n5.5.1 Installing and Uninstalling Plugins\n\n5.5.2 Obtaining Server Plugin Information\n\n5.5.3 MySQL Enterprise Thread Pool\n\n5.5.4 The Rewriter Query Rewrite Plugin\n\n5.5.5 Version Tokens\n\n5.6 MySQL Server User-Defined Functions\n\n5.6.1 Installing and Uninstalling User-Defined Functions\n\n5.6.2 Obtaining User-Defined Function Information\n\n5.7 Running Multiple MySQL Instances on One Machine\n\n5.7.1 Setting Up Multiple Data Directories\n\n5.7.2 Running Multiple MySQL Instances on Windows\n\n5.7.3 Running Multiple MySQL Instances on Unix\n\n5.7.4 Using Client Programs in a Multiple-Server Environment\n\n5.8 Tracing mysqld Using DTrace\n\n5.8.1 mysqld DTrace Probe Reference\n\n6 Security\n\n6.1 General Security Issues\n\n6.1.1 Security Guidelines\n\n6.1.2 Keeping Passwords Secure\n\n6.1.3 Making MySQL Secure Against Attackers\n\n6.1.4 Security-Related mysqld Options and Variables\n\n6.1.5 How to Run MySQL as a Normal User\n\n6.1.6 Security Issues with LOAD DATA LOCAL\n\n6.1.7 Client Programming Security Guidelines\n\n6.2 Access Control and Account Management\n\n6.2.1 Account User Names and Passwords\n\n6.2.2 Privileges Provided by MySQL\n\n6.2.3 Grant Tables\n\n6.2.4 Specifying Account Names\n\n6.2.5 Access Control, Stage 1: Connection Verification\n\n6.2.6 Access Control, Stage 2: Request Verification\n\n6.2.7 Adding Accounts, Assigning Privileges, and Dropping Accounts\n\n6.2.8 Reserved Accounts\n\n6.2.9 When Privilege Changes Take Effect\n\n6.2.10 Assigning Account Passwords\n\n6.2.11 Password Management\n\n<https://dev.mysql.com/doc/refman/5.7/en/password-management.html>\nMySQL supports these password-management capabilities:\n-   Password expiration, to require passwords to be changed periodically.\n-   Password reuse restrictions, to prevent old passwords from being chosen again.\n-   Password verification, to require that password changes also specify the current password to be replaced.\n-   **Dual passwords**, to enable clients to connect using either a primary or secondary password.\n-   Password strength assessment, to require strong passwords.\n-   Random password generation, as an alternative to requiring explicit administrator-specified literal passwords.\n-   Password failure tracking, to enable temporary account locking after too many consecutive incorrect-password login failures.\n<https://dev.mysql.com/doc/mysql-security-excerpt/8.0/en/password-management.html>\n6.2.12 Server Handling of Expired Passwords\n\n6.2.13 Pluggable Authentication\n\n6.2.14 Proxy Users\n\n6.2.15 Account Locking\n\n**6.2.16 Setting Account Resource Limits**\n\nOne means of restricting client use of MySQL server resources is to set the global [max_user_connections](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_max_user_connections) system variable to a nonzero value. This limits the number of simultaneous connections that can be made by any given account, but places no limits on what a client can do once connected. In addition, setting[max_user_connections](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_max_user_connections)does not enable management of individual accounts. Both types of control are of interest to MySQL administrators.\nTo address such concerns, MySQL permits limits for individual accounts on use of these server resources:\n-   The number of queries an account can issue per hour\n-   The number of updates an account can issue per hour\n-   The number of times an account can connect to the server per hour\n-   The number of simultaneous connections to the server by an account\n<https://dev.mysql.com/doc/refman/8.0/en/user-resources.html>\n6.2.17 Troubleshooting Problems Connecting to MySQL\n\n6.2.18 SQL-Based Account Activity Auditing\n\n6.3 Using Encrypted Connections\n\n6.3.1 Configuring MySQL to Use Encrypted Connections\n\n6.3.2 Encrypted Connection TLS Protocols and Ciphers\n\n6.3.3 Creating SSL and RSA Certificates and Keys\n\n6.3.4 SSL Library-Dependent Capabilities\n\n6.3.5 Connecting to MySQL Remotely from Windows with SSH\n\n6.4 Security Plugins\n\n6.4.1 Authentication Plugins\n\n6.4.2 The Connection-Control Plugins\n\n6.4.3 The Password Validation Plugin\n\n6.4.4 The MySQL Keyring\n\n6.4.5 MySQL Enterprise Audit\n\n6.4.6 MySQL Enterprise Firewall\n\n6.4.7 MySQL Enterprise Data Masking and De-Identification\n\n7 Backup and Recovery\n\n7.1 Backup and Recovery Types\n\n7.2 Database Backup Methods\n\n7.3 Example Backup and Recovery Strategy\n\n7.3.1 Establishing a Backup Policy\n\n7.3.2 Using Backups for Recovery\n\n7.3.3 Backup Strategy Summary\n\n7.4 Using mysqldump for Backups\n\n7.4.1 Dumping Data in SQL Format with mysqldump\n\n7.4.2 Reloading SQL-Format Backups\n\n7.4.3 Dumping Data in Delimited-Text Format with mysqldump\n\n7.4.4 Reloading Delimited-Text Format Backups\n\n7.4.5 mysqldump Tips\n\n7.5 Point-in-Time (Incremental) Recovery Using the Binary Log\n\n7.5.1 Point-in-Time Recovery Using Event Times\n\n7.5.2 Point-in-Time Recovery Using Event Positions\n\n7.6 MyISAM Table Maintenance and Crash Recovery\n\n7.6.1 Using myisamchk for Crash Recovery\n\n7.6.2 How to Check MyISAM Tables for Errors\n\n7.6.3 How to Repair MyISAM Tables\n\n7.6.4 MyISAM Table Optimization\n\n7.6.5 Setting Up a MyISAM Table Maintenance Schedule\n\n8 Optimization\n\n8.1 Optimization Overview\n\n8.2 Optimizing SQL Statements\n\n8.2.1 Optimizing SELECT Statements\n\n8.2.2 Optimizing Subqueries, Derived Tables, and View References\n\n8.2.3 Optimizing INFORMATION_SCHEMA Queries\n\n8.2.4 Optimizing Data Change Statements\n\n8.2.5 Optimizing Database Privileges\n\n8.2.6 Other Optimization Tips\n\n8.3 Optimization and Indexes\n\n8.3.1 How MySQL Uses Indexes\n\n8.3.2 Primary Key Optimization\n\n8.3.3 Foreign Key Optimization\n\n8.3.4 Column Indexes\n\n8.3.5 Multiple-Column Indexes\n\n8.3.6 Verifying Index Usage\n\n8.3.7 InnoDB and MyISAM Index Statistics Collection\n\n8.3.8 Comparison of B-Tree and Hash Indexes\n\n8.3.9 Use of Index Extensions\n\n8.3.10 Optimizer Use of Generated Column Indexes\n\n8.3.11 Indexed Lookups from TIMESTAMP Columns\n\n8.4 Optimizing Database Structure\n\n8.4.1 Optimizing Data Size\n\n8.4.2 Optimizing MySQL Data Types\n\n8.4.3 Optimizing for Many Tables\n\n8.4.4 Internal Temporary Table Use in MySQL\n\n8.4.5 Limits on Number of Databases and Tables\n\n8.4.6 Limits on Table Size\n\n8.4.7 Limits on Table Column Count and Row Size\n-   MySQL has hard limit of 4096 columns per table, but the effective maximum may be less for a given table.\n-   [InnoDB](https://dev.mysql.com/doc/refman/8.0/en/innodb-storage-engine.html)has a limit of 1017 columns per table.\n-   The internal representation of a MySQL table has a maximum row size limit of 65,535 bytes\n\n<https://dev.mysql.com/doc/refman/8.0/en/column-count-limit.html>\n\n8.5 Optimizing for InnoDB Tables\n\n8.5.1 Optimizing Storage Layout for InnoDB Tables\n\n8.5.2 Optimizing InnoDB Transaction Management\n\n8.5.3 Optimizing InnoDB Read-Only Transactions\n\n8.5.4 Optimizing InnoDB Redo Logging\n\n8.5.5 Bulk Data Loading for InnoDB Tables\n\n8.5.6 Optimizing InnoDB Queries\n\n8.5.7 Optimizing InnoDB DDL Operations\n\n8.5.8 Optimizing InnoDB Disk I/O\n\n8.5.9 Optimizing InnoDB Configuration Variables\n\n8.5.10 Optimizing InnoDB for Systems with Many Tables\n\n8.6 Optimizing for MyISAM Tables\n\n8.6.1 Optimizing MyISAM Queries\n\n8.6.2 Bulk Data Loading for MyISAM Tables\n\n8.6.3 Optimizing REPAIR TABLE Statements\n\n8.7 Optimizing for MEMORY Tables\n\n8.8 Understanding the Query Execution Plan\n\n8.8.1 Optimizing Queries with EXPLAIN\n\n8.8.2 EXPLAIN Output Format\n\n8.8.3 Extended EXPLAIN Output Format\n\n8.8.4 Obtaining Execution Plan Information for a Named Connection\n\n8.8.5 Estimating Query Performance\n\n8.9 Controlling the Query Optimizer\n\n8.9.1 Controlling Query Plan Evaluation\n\n8.9.2 Switchable Optimizations\n\n8.9.3 Optimizer Hints\n\n8.9.4 Index Hints\n\n8.9.5 The Optimizer Cost Model\n\nTo generate execution plans, the optimizer uses a cost model that is based on estimates of the cost of various operations that occur during query execution. The optimizer has a set of compiled-in default\"cost constants\" available to it to make decisions regarding execution plans.\n<https://dev.mysql.com/doc/refman/5.7/en/cost-model.html>\n8.10 Buffering and Caching\n\n8.10.1 InnoDB Buffer Pool Optimization\n\n8.10.2 The MyISAM Key Cache\n\n8.10.3 The MySQL Query Cache\n\n8.10.4 Caching of Prepared Statements and Stored Programs\n\n**8.11 Optimizing Locking Operations**\n-   Internal locking is performed within the MySQL server itself to manage contention for table contents by multiple threads. This type of locking is internal because it is performed entirely by the server and involves no other programs.\n-   External locking occurs when the server and other programs lock[MyISAM](https://dev.mysql.com/doc/refman/5.7/en/myisam-storage-engine.html)table files to coordinate among themselves which program can access the tables at which time.\n8.11.1 Internal Locking Methods\n\nMySQL uses[**row-level locking**](https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_row_lock)forInnoDBtables to support simultaneous write access by multiple sessions, making them suitable for multi-user, highly concurrent, and OLTP applications.\n\nTo avoid[deadlocks](https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_deadlock)when performing multiple concurrent write operations on a singleInnoDBtable, acquire necessary locks at the start of the transaction by issuing aSELECT ... FOR UPDATEstatement for each group of rows expected to be modified, even if the data change statements come later in the transaction. If transactions modify or lock more than one table, issue the applicable statements in the same order within each transaction. Deadlocks affect performance rather than representing a serious error, becauseInnoDBautomatically[detects](https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_deadlock_detection)deadlock conditions and rolls back one of the affected transactions.\n\nOn high concurrency systems, **deadlock detection can cause a slowdown when numerous threads wait for the same lock.** At times, it may be **more efficient to disable deadlock detection and rely on the[innodb_lock_wait_timeout](https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_lock_wait_timeout)setting for transaction rollback** when a deadlock occurs. Deadlock detection can be disabled using the [innodb_deadlock_detect](https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_deadlock_detect) configuration option.\n\nAdvantages of row-level locking:\n-   Fewer lock conflicts when different sessions access different rows.\n-   Fewer changes for rollbacks.\n-   Possible to lock a single row for a long time.\nMySQL uses[**table-level locking**](https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_table_lock)forMyISAM,MEMORY, andMERGEtables\n8.11.2 Table Locking Issues\n\n8.11.3 Concurrent Inserts\n\n8.11.4 Metadata Locking\n8.11.5 External Locking\n\n8.12 Optimizing the MySQL Server\n\n8.12.1 System Factors\n\n8.12.2 Optimizing Disk I/O\n\n8.12.3 Using Symbolic Links\n\n8.12.4 Optimizing Memory Use\n\n8.12.5 Optimizing Network Use\n\n8.13 Measuring Performance (Benchmarking)\n\n8.13.1 Measuring the Speed of Expressions and Functions\n\n8.13.2 Using Your Own Benchmarks\n\n8.13.3 Measuring Performance with performance_schema\n\n8.14 Examining Thread Information\n\n8.14.1 Thread Command Values\n\n8.14.2 General Thread States\n\n8.14.3 Query Cache Thread States\n\n8.14.4 Replication Master Thread States\n\n8.14.5 Replication Slave I/O Thread States\n\n8.14.6 Replication Slave SQL Thread States\n\n8.14.7 Replication Slave Connection Thread States\n\n8.14.8 NDB Cluster Thread States\n\n8.14.9 Event Scheduler Thread States\n\n9 Language Structure\n\n9.1 Literal Values\n\n9.1.1 String Literals\n\n9.1.2 Numeric Literals\n\n9.1.3 Date and Time Literals\n\n9.1.4 Hexadecimal Literals\n\n9.1.5 Bit-Value Literals\n\n9.1.6 Boolean Literals\n\n9.1.7 NULL Values\n\n9.2 Schema Object Names\n\n9.2.1 Identifier Length Limits\n\n9.2.2 Identifier Qualifiers\n\n9.2.3 Identifier Case Sensitivity\n\n9.2.4 Mapping of Identifiers to File Names\n\n9.2.5 Function Name Parsing and Resolution\n\n9.3 Keywords and Reserved Words\n\n9.4 User-Defined Variables\n\n9.5 Expressions\n\n9.6 Comment Syntax\n\n10 Character Sets, Collations, Unicode\n\nSQLServercollationrefers to a set of character and character encoding rules, and influences how information is stored according to the order in the data page, how data is matched by comparing two columns, and how information is arranged in the T-SQLquery statement.\n10.1 Character Sets and Collations in General\n\n10.2 Character Sets and Collations in MySQL\n\n10.2.1 Character Set Repertoire\n\n10.2.2 UTF-8 for Metadata\n\n10.3 Specifying Character Sets and Collations\n\n10.3.1 Collation Naming Conventions\n\n10.3.2 Server Character Set and Collation\n\n10.3.3 Database Character Set and Collation\n\n10.3.4 Table Character Set and Collation\n\n10.3.5 Column Character Set and Collation\n\n10.3.6 Character String Literal Character Set and Collation\n\n10.3.7 The National Character Set\n\n10.3.8 Character Set Introducers\n\n10.3.9 Examples of Character Set and Collation Assignment\n\n10.3.10 Compatibility with Other DBMSs\n\n10.4 Connection Character Sets and Collations\n\n10.5 Configuring Application Character Set and Collation\n\n10.6 Error Message Character Set\n\n10.7 Column Character Set Conversion\n\n10.8 Collation Issues\n\n10.8.1 Using COLLATE in SQL Statements\n\n10.8.2 COLLATE Clause Precedence\n\n10.8.3 Character Set and Collation Compatibility\n\n10.8.4 Collation Coercibility in Expressions\n\n10.8.5 The binary Collation Compared to _bin Collations\n\n10.8.6 Examples of the Effect of Collation\n\n10.8.7 Using Collation in INFORMATION_SCHEMA Searches\n\n10.9 Unicode Support\n\n10.9.1 The utf8mb4 Character Set (4-Byte UTF-8 Unicode Encoding)\n\n10.9.2 The utf8mb3 Character Set (3-Byte UTF-8 Unicode Encoding)\n\n10.9.3 The utf8 Character Set (Alias for utf8mb3)\n\n10.9.4 The ucs2 Character Set (UCS-2 Unicode Encoding)\n\n10.9.5 The utf16 Character Set (UTF-16 Unicode Encoding)\n\n10.9.6 The utf16le Character Set (UTF-16LE Unicode Encoding)\n\n10.9.7 The utf32 Character Set (UTF-32 Unicode Encoding)\n\n10.9.8 Converting Between 3-Byte and 4-Byte Unicode Character Sets\n\n10.10 Supported Character Sets and Collations\n\n10.10.1 Unicode Character Sets\n\n10.10.2 West European Character Sets\n\n10.10.3 Central European Character Sets\n\n10.10.4 South European and Middle East Character Sets\n\n10.10.5 Baltic Character Sets\n\n10.10.6 Cyrillic Character Sets\n\n10.10.7 Asian Character Sets\n\n10.10.8 The Binary Character Set\n\n10.11 Restrictions on Character Sets\n\n10.12 Setting the Error Message Language\n\n10.13 Adding a Character Set\n\n10.13.1 Character Definition Arrays\n\n10.13.2 String Collating Support for Complex Character Sets\n\n10.13.3 Multi-Byte Character Support for Complex Character Sets\n\n10.14 Adding a Collation to a Character Set\n\n10.14.1 Collation Implementation Types\n\n10.14.2 Choosing a Collation ID\n\n10.14.3 Adding a Simple Collation to an 8-Bit Character Set\n\n10.14.4 Adding a UCA Collation to a Unicode Character Set\n\n10.15 Character Set Configuration\n\n10.16 MySQL Server Locale Support\n\n12 Functions and Operators\n\n12.1 Function and Operator Reference\n\n12.2 Type Conversion in Expression Evaluation\n\n12.3 Operators\n\n12.3.1 Operator Precedence\n\n12.3.2 Comparison Functions and Operators\n\n12.3.3 Logical Operators\n\n12.3.4 Assignment Operators\n\n12.4 Control Flow Functions\n\n12.5 Numeric Functions and Operators\n\n12.5.1 Arithmetic Operators\n\n12.5.2 Mathematical Functions\n\n12.6 Date and Time Functions\n\n12.7 String Functions and Operators\n\n12.7.1 String Comparison Functions and Operators\n\n12.7.2 Regular Expressions\n\n12.7.3 Character Set and Collation of Function Results\n\n12.8 What Calendar Is Used By MySQL?\n\n12.9 Full-Text Search Functions\n\n12.9.1 Natural Language Full-Text Searches\n\n12.9.2 Boolean Full-Text Searches\n\n12.9.3 Full-Text Searches with Query Expansion\n\n12.9.4 Full-Text Stopwords\n\n12.9.5 Full-Text Restrictions\n\n12.9.6 Fine-Tuning MySQL Full-Text Search\n\n12.9.7 Adding a Collation for Full-Text Indexing\n\n12.9.8 ngram Full-Text Parser\n\n12.9.9 MeCab Full-Text Parser Plugin\n\n12.10 Cast Functions and Operators\n\n12.11 XML Functions\n\n12.12 Bit Functions and Operators\n\n12.13 Encryption and Compression Functions\n\n12.14 Locking Functions\n\n12.15 Information Functions\n\n12.16 Spatial Analysis Functions\n\n12.16.1 Spatial Function Reference\n\n12.16.2 Argument Handling by Spatial Functions\n\n12.16.3 Functions That Create Geometry Values from WKT Values\n\n12.16.4 Functions That Create Geometry Values from WKB Values\n\n12.16.5 MySQL-Specific Functions That Create Geometry Values\n\n12.16.6 Geometry Format Conversion Functions\n\n12.16.7 Geometry Property Functions\n\n12.16.8 Spatial Operator Functions\n\n12.16.9 Functions That Test Spatial Relations Between Geometry Objects\n\n12.16.10 Spatial Geohash Functions\n\n12.16.11 Spatial GeoJSON Functions\n\n12.16.12 Spatial Convenience Functions\n\n12.17 JSON Functions\n\n12.17.1 JSON Function Reference\n\n12.17.2 Functions That Create JSON Values\n\n12.17.3 Functions That Search JSON Values\n\n12.17.4 Functions That Modify JSON Values\n\n12.17.5 Functions That Return JSON Value Attributes\n\n12.17.6 JSON Utility Functions\n\n12.18 Functions Used with Global Transaction Identifiers (GTIDs)\n\n12.19 MySQL Enterprise Encryption Functions\n\n12.19.1 MySQL Enterprise Encryption Installation\n\n12.19.2 MySQL Enterprise Encryption Usage and Examples\n\n12.19.3 MySQL Enterprise Encryption Function Reference\n\n12.19.4 MySQL Enterprise Encryption Function Descriptions\n\n12.20 Aggregate (GROUP BY) Functions\n\n12.20.1 Aggregate (GROUP BY) Function Descriptions\n\n12.20.2 GROUP BY Modifiers\n\n12.20.3 MySQL Handling of GROUP BY\n\n12.20.4 Detection of Functional Dependence\n\n12.21 Miscellaneous Functions\n\n12.22 Precision Math\n\n12.22.1 Types of Numeric Values\n\n12.22.2 DECIMAL Data Type Characteristics\n\n12.22.3 Expression Handling\n\n12.22.4 Rounding Behavior\n\n12.22.5 Precision Math Examples\n\n13 SQL Statements\n\n13.1 Data Definition Statements\n\n13.1.1 ALTER DATABASE Statement\n\n13.1.2 ALTER EVENT Statement\n\n13.1.3 ALTER FUNCTION Statement\n\n13.1.4 ALTER INSTANCE Statement\n\n13.1.5 ALTER LOGFILE GROUP Statement\n\n13.1.6 ALTER PROCEDURE Statement\n\n13.1.7 ALTER SERVER Statement\n\n13.1.8 ALTER TABLE Statement\n\n13.1.9 ALTER TABLESPACE Statement\n\n13.1.10 ALTER VIEW Statement\n\n13.1.11 CREATE DATABASE Statement\n\n13.1.12 CREATE EVENT Statement\n\n13.1.13 CREATE FUNCTION Statement\n\n13.1.14 CREATE INDEX Statement\n\n13.1.15 CREATE LOGFILE GROUP Statement\n\n13.1.16 CREATE PROCEDURE and CREATE FUNCTION Statements\n\n13.1.17 CREATE SERVER Statement\n\n13.1.18 CREATE TABLE Statement\n\n13.1.19 CREATE TABLESPACE Statement\n\n13.1.20 CREATE TRIGGER Statement\n\n13.1.21 CREATE VIEW Statement\n\n13.1.22 DROP DATABASE Statement\n\n13.1.23 DROP EVENT Statement\n\n13.1.24 DROP FUNCTION Statement\n\n13.1.25 DROP INDEX Statement\n\n13.1.26 DROP LOGFILE GROUP Statement\n\n13.1.27 DROP PROCEDURE and DROP FUNCTION Statements\n\n13.1.28 DROP SERVER Statement\n\n13.1.29 DROP TABLE Statement\n\n13.1.30 DROP TABLESPACE Statement\n\n13.1.31 DROP TRIGGER Statement\n\n13.1.32 DROP VIEW Statement\n\n13.1.33 RENAME TABLE Statement\n\n13.1.34 TRUNCATE TABLE Statement\n\n13.2 Data Manipulation Statements\n\n13.2.1 CALL Statement\n\n13.2.2 DELETE Statement\n\n13.2.3 DO Statement\n\n13.2.4 HANDLER Statement\n\n13.2.5 INSERT Statement\n\n13.2.6 LOAD DATA Statement\n\n13.2.7 LOAD XML Statement\n\n13.2.8 REPLACE Statement\n\n13.2.9 SELECT Statement\n\n13.2.10 Subqueries\n\n13.2.11 UPDATE Statement\n\n13.3 Transactional and Locking Statements\n\n13.3.1 START TRANSACTION, COMMIT, and ROLLBACK Statements\n\nInInnoDB, all user activity occurs inside a transaction. If[autocommit](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_autocommit)mode is enabled, each SQL statement forms a single transaction on its own. By default, MySQL starts the session for each new connection with[autocommit](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_autocommit)enabled, so MySQL does a commit after each SQL statement if that statement did not return an error. If a statement returns an error, the commit or rollback behavior depends on the error. See[Section15.21.4, \"InnoDB Error Handling\"](https://dev.mysql.com/doc/refman/8.0/en/innodb-error-handling.html).\nA session that has[autocommit](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_autocommit)enabled can perform a multiple-statement transaction by starting it with an explicit[START TRANSACTION](https://dev.mysql.com/doc/refman/8.0/en/commit.html)or[BEGIN](https://dev.mysql.com/doc/refman/8.0/en/commit.html)statement and ending it with a[COMMIT](https://dev.mysql.com/doc/refman/8.0/en/commit.html)or[ROLLBACK](https://dev.mysql.com/doc/refman/8.0/en/commit.html)statement. See[Section13.3.1, \"START TRANSACTION, COMMIT, and ROLLBACK Statements\"](https://dev.mysql.com/doc/refman/8.0/en/commit.html).\nIf[autocommit](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_autocommit)mode is disabled within a session withSET autocommit = 0, the session always has a transaction open. A[COMMIT](https://dev.mysql.com/doc/refman/8.0/en/commit.html)or[ROLLBACK](https://dev.mysql.com/doc/refman/8.0/en/commit.html)statement ends the current transaction and a new one starts.\nIf a session that has[autocommit](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_autocommit)disabled ends without explicitly committing the final transaction, MySQL rolls back that transaction.\nSome statements implicitly end a transaction, as if you had done a[COMMIT](https://dev.mysql.com/doc/refman/8.0/en/commit.html)before executing the statement. For details, see[Section13.3.3, \"Statements That Cause an Implicit Commit\"](https://dev.mysql.com/doc/refman/8.0/en/implicit-commit.html).\n\nA[COMMIT](https://dev.mysql.com/doc/refman/8.0/en/commit.html)means that the changes made in the current transaction are made permanent and become visible to other sessions. A[ROLLBACK](https://dev.mysql.com/doc/refman/8.0/en/commit.html)statement, on the other hand, cancels all modifications made by the current transaction. Both[COMMIT](https://dev.mysql.com/doc/refman/8.0/en/commit.html)and[ROLLBACK](https://dev.mysql.com/doc/refman/8.0/en/commit.html)release allInnoDBlocks that were set during the current transaction.\n<https://dev.mysql.com/doc/refman/8.0/en/innodb-autocommit-commit-rollback.html>\n13.3.2 Statements That Cannot Be Rolled Back\n\n13.3.3 Statements That Cause an Implicit Commit\n\n13.3.4 SAVEPOINT, ROLLBACK TO SAVEPOINT, and RELEASE SAVEPOINT\n\nStatements\n\n13.3.5 LOCK TABLES and UNLOCK TABLES Statements\n\n13.3.6 SET TRANSACTION Statement\n\n13.3.7 XA Transactions\n\n13.4 Replication Statements\n\n13.4.1 SQL Statements for Controlling Master Servers\n\n13.4.2 SQL Statements for Controlling Slave Servers\n\n13.4.3 SQL Statements for Controlling Group Replication\n\n13.5 Prepared Statements\n\n13.5.1 PREPARE Statement\n\n13.5.2 EXECUTE Statement\n\n13.5.3 DEALLOCATE PREPARE Statement\n\n13.6 Compound Statements\n\n13.6.1 BEGIN\n\n13.6.2 Statement Labels\n\n13.6.3 DECLARE Statement\n\n13.6.4 Variables in Stored Programs\n\n13.6.5 Flow Control Statements\n\n13.6.6 Cursors\n\n13.6.7 Condition Handling\n\n13.7 Database Administration Statements\n\n13.7.1 Account Management Statements\n\n13.7.2 Table Maintenance Statements\n\n13.7.3 Plugin and User-Defined Function Statements\n\n13.7.4 SET Statements\n\n13.7.5 SHOW Statements\n\n13.7.6 Other Administrative Statements\n\n13.8 Utility Statements\n\n13.8.1 DESCRIBE Statement\n\n13.8.2 EXPLAIN Statement\n\n13.8.3 HELP Statement\n\n13.8.4 USE Statement\n\n14 The InnoDB Storage Engine\n\n14.1 Introduction to InnoDB\n\n14.1.1 Benefits of Using InnoDB Tables\n\n14.1.2 Best Practices for InnoDB Tables\n\n14.1.3 Verifying that InnoDB is the Default Storage Engine\n\n14.1.4 Testing and Benchmarking with InnoDB\n\n14.1.5 Turning Off InnoDB\n\n14.2 InnoDB and the ACID Model\n\n14.3 InnoDB Multi-Versioning\n\n14.4 InnoDB Architecture\n\n14.5 InnoDB In-Memory Structures\n\n14.5.1 Buffer Pool\n\n14.5.2 Change Buffer\n\n14.5.3 Adaptive Hash Index\n\n14.5.4 Log Buffer\n\n14.6 InnoDB On-Disk Structures\n\n14.6.1 Tables\n\n14.6.2 Indexes\n\n14.6.3 Tablespaces\n\n14.6.4 InnoDB Data Dictionary\n\n14.6.5 Doublewrite Buffer\n\n14.6.6 Redo Log\n\n14.6.7 Undo Logs\n\n14.7 InnoDB Locking and Transaction Model\n\n14.7.1 InnoDB Locking\n\n14.7.2 InnoDB Transaction Model\n\n14.7.3 Locks Set by Different SQL Statements in InnoDB\n\n14.7.4 Phantom Rows\n\n14.7.5 Deadlocks in InnoDB\n\n14.8 InnoDB Configuration\n\n14.8.1 InnoDB Startup Configuration\n\n14.8.2 Configuring InnoDB for Read-Only Operation\n\n14.8.3 InnoDB Buffer Pool Configuration\n\n14.8.4 Configuring the Memory Allocator for InnoDB\n\n14.8.5 Configuring Thread Concurrency for InnoDB\n\n14.8.6 Configuring the Number of Background InnoDB I/O Threads\n\n14.8.7 Using Asynchronous I/O on Linux\n\n14.8.8 Configuring InnoDB I/O Capacity\n\n14.8.9 Configuring Spin Lock Polling\n\n14.8.10 Purge Configuration\n\n14.8.11 Configuring Optimizer Statistics for InnoDB\n\n14.8.12 Configuring the Merge Threshold for Index Pages\n\n14.9 InnoDB Table and Page Compression\n\n**14.9.1 InnoDB Table Compression**\n\n<https://dev.mysql.com/doc/refman/8.0/en/innodb-compression-usage.html>\n\n<https://dev.mysql.com/doc/refman/8.0/en/innodb-compression-internals.html>\n14.9.2 InnoDB Page Compression\n\n14.10 InnoDB File-Format Management\n\n14.10.1 Enabling File Formats\n\n14.10.2 Verifying File Format Compatibility\n\n14.10.3 Identifying the File Format in Use\n\n14.10.4 Modifying the File Format\n\n14.11 InnoDB Row Formats\n\n14.12 InnoDB Disk I/O and File Space Management\n\n14.12.1 InnoDB Disk I/O\n\n14.12.2 File Space Management\n\n14.12.3 InnoDB Checkpoints\n\n14.12.4 Defragmenting a Table\n\n14.12.5 Reclaiming Disk Space with TRUNCATE TABLE\n\n14.13 InnoDB and Online DDL\n\n14.13.1 Online DDL Operations\n\n14.13.2 Online DDL Performance and Concurrency\n\n14.13.3 Online DDL Space Requirements\n\n14.13.4 Simplifying DDL Statements with Online DDL\n\n14.13.5 Online DDL Failure Conditions\n\n14.13.6 Online DDL Limitations\n\n14.14 InnoDB Data-at-Rest Encryption\n\n14.15 InnoDB Startup Options and System Variables\n\n14.16 InnoDB INFORMATION_SCHEMA Tables\n\n14.16.1 InnoDB INFORMATION_SCHEMA Tables about Compression\n\n14.16.2 InnoDB INFORMATION_SCHEMA Transaction and Locking Information\n\n14.16.3 InnoDB INFORMATION_SCHEMA System Tables\n\n14.16.4 InnoDB INFORMATION_SCHEMA FULLTEXT Index Tables\n\n14.16.5 InnoDB INFORMATION_SCHEMA Buffer Pool Tables\n\n14.16.6 InnoDB INFORMATION_SCHEMA Metrics Table\n\n14.16.7 InnoDB INFORMATION_SCHEMA Temporary Table Info Table\n\n14.16.8 Retrieving InnoDB Tablespace Metadata from\n\nINFORMATION_SCHEMA.FILES\n\n14.17 InnoDB Integration with MySQL Performance Schema\n\n14.17.1 Monitoring ALTER TABLE Progress for InnoDB Tables Using Performance\n\nSchema\n\n14.17.2 Monitoring InnoDB Mutex Waits Using Performance Schema\n\n14.18 InnoDB Monitors\n\n14.18.1 InnoDB Monitor Types\n\n14.18.2 Enabling InnoDB Monitors\n\n14.18.3 InnoDB Standard Monitor and Lock Monitor Output\n\n14.19 InnoDB Backup and Recovery\n\n14.19.1 InnoDB Backup\n\n14.19.2 InnoDB Recovery\n\n14.20 InnoDB and MySQL Replication\n\n14.21 InnoDB memcached Plugin\n\n14.21.1 Benefits of the InnoDB memcached Plugin\n\n14.21.2 InnoDB memcached Architecture\n\n14.21.3 Setting Up the InnoDB memcached Plugin\n\n14.21.4 Security Considerations for the InnoDB memcached Plugin\n\n14.21.5 Writing Applications for the InnoDB memcached Plugin\n\n14.21.6 The InnoDB memcached Plugin and Replication\n\n14.21.7 InnoDB memcached Plugin Internals\n\n14.21.8 Troubleshooting the InnoDB memcached Plugin\n\n14.22 InnoDB Troubleshooting\n\n14.22.1 Troubleshooting InnoDB I/O Problems\n\n14.22.2 Forcing InnoDB Recovery\n\n14.22.3 Troubleshooting InnoDB Data Dictionary Operations\n\n14.22.4 InnoDB Error Handling\n\n14.23 InnoDB Limits\n\n14.24 InnoDB Restrictions and Limitations\n\n15 Alternative Storage Engines\n\n15.1 Setting the Storage Engine\n\n15.2 The MyISAM Storage Engine\n\n15.2.1 MyISAM Startup Options\n\n15.2.2 Space Needed for Keys\n\n15.2.3 MyISAM Table Storage Formats\n\n15.2.4 MyISAM Table Problems\n\n15.3 The MEMORY Storage Engine\n\n15.4 The CSV Storage Engine\n\n15.4.1 Repairing and Checking CSV Tables\n\n15.4.2 CSV Limitations\n\n15.5 The ARCHIVE Storage Engine\n\n15.6 The BLACKHOLE Storage Engine\n\n15.7 The MERGE Storage Engine\n\n15.7.1 MERGE Table Advantages and Disadvantages\n\n15.7.2 MERGE Table Problems\n\n15.8 The FEDERATED Storage Engine\n\n15.8.1 FEDERATED Storage Engine Overview\n\n15.8.2 How to Create FEDERATED Tables\n\n15.8.3 FEDERATED Storage Engine Notes and Tips\n\n15.8.4 FEDERATED Storage Engine Resources\n\n15.9 The EXAMPLE Storage Engine\n\n15.10 Other Storage Engines\n\n15.11 Overview of MySQL Storage Engine Architecture\n\n15.11.1 Pluggable Storage Engine Architecture\n\n15.11.2 The Common Database Server Layer\n\n16 Replication\n\n16.1 Configuring Replication\n\n16.1.1 Binary Log File Position Based Replication Configuration Overview\n\n16.1.2 Setting Up Binary Log File Position Based Replication\n\n16.1.3 Replication with Global Transaction Identifiers\n\n16.1.4 MySQL Multi-Source Replication\n\n16.1.5 Changing Replication Modes on Online Servers\n\n16.1.6 Replication and Binary Logging Options and Variables\n\n16.1.7 Common Replication Administration Tasks\n\n16.2 Replication Implementation\n\n16.2.1 Replication Formats\n\n16.2.2 Replication Implementation Details\n\n16.2.3 Replication Channels\n\n16.2.4 Replication Relay and Status Logs\n\n16.2.5 How Servers Evaluate Replication Filtering Rules\n\n16.3 Replication Solutions\n\n16.3.1 Using Replication for Backups\n\n16.3.2 Handling an Unexpected Halt of a Replication Slave\n\n16.3.3 Using Replication with Different Master and Slave Storage Engines\n\n16.3.4 Using Replication for Scale-Out\n\n16.3.5 Replicating Different Databases to Different Slaves\n\n16.3.6 Improving Replication Performance\n\n16.3.7 Switching Masters During Failover\n\n16.3.8 Setting Up Replication to Use Encrypted Connections\n\n16.3.9 Semisynchronous Replication\n\n16.3.10 Delayed Replication\n\n16.4 Replication Notes and Tips\n\n16.4.1 Replication Features and Issues\n\n16.4.2 Replication Compatibility Between MySQL Versions\n\n16.4.3 Upgrading a Replication Setup\n\nxvi\n\n16.4.4 Troubleshooting Replication\n\n16.4.5 How to Report Replication Bugs or Problems\n\n17 Group Replication\n\n17.1 Group Replication Background\n\n17.1.1 Replication Technologies\n\n17.1.2 Group Replication Use Cases\n\n17.1.3 Group Replication Details\n\n17.2 Getting Started\n\n17.2.1 Deploying Group Replication in Single-Primary Mode\n\n17.2.2 Deploying Group Replication Locally\n\n17.3 Monitoring Group Replication\n\n17.3.1 Group Replication Server States\n\n17.3.2 The replication_group_members Table\n\n17.3.3 The replication_group_member_stats Table\n\n17.4 Group Replication Operations\n\n17.4.1 Deploying in Multi-Primary or Single-Primary Mode\n\n17.4.2 Tuning Recovery\n\n17.4.3 Network Partitioning\n\n17.4.4 Using MySQL Enterprise Backup with Group Replication\n\n17.5 Group Replication Security\n\n17.5.1 Group Replication IP Address Whitelisting\n\n17.5.2 Group Replication Secure Socket Layer (SSL) Support\n\n17.5.3 Group Replication and Virtual Private Networks (VPNs)\n\n17.6 Group Replication System Variables\n\n17.7 Requirements and Limitations\n\n17.7.1 Group Replication Requirements\n\n17.7.2 Group Replication Limitations\n\n17.8 Frequently Asked Questions\n\n17.9 Group Replication Technical Details\n\n17.9.1 Group Replication Plugin Architecture\n\n17.9.2 The Group\n\n17.9.3 Data Manipulation Statements\n\n17.9.4 Data Definition Statements\n\n17.9.5 Distributed Recovery\n\n17.9.6 Observability\n\n17.9.7 Group Replication Performance\n\n18 MySQL Shell\n\n19 Using MySQL as a Document Store\n\n19.1 Preproduction Status --- Legal Notice\n\n19.2 Key Concepts\n\n19.3 Setting Up MySQL as a Document Store\n\n19.3.1 Installing MySQL Shell\n\n19.3.2 Starting MySQL Shell\n\n19.4 Quick-Start Guide: MySQL Shell for JavaScript\n\n19.4.1 Introduction\n\n19.4.2 Import Database Sample\n\n19.4.3 MySQL Shell\n\n19.4.4 Documents and Collections\n\n19.4.5 Relational Tables\n\n19.4.6 Documents in Tables\n\n19.5 Quick-Start Guide: MySQL Shell for Python\n\n19.5.1 Introduction\n\n19.5.2 Import Database Sample\n\n19.5.3 MySQL Shell\n\n19.5.4 Documents and Collections\n\n19.5.5 Relational Tables\n\n19.5.6 Documents in Tables\n\n19.6 Quick-Start Guide: MySQL for Visual Studio\n\n19.7 X Plugin\n\n19.7.1 Using Secure Connections with X Plugin\n\n19.7.2 X Plugin Options and Variables\n\n19.7.3 Monitoring X Plugin\n\n20 InnoDB Cluster\n\n20.1 Introducing InnoDB Cluster\n\n20.2 Creating an InnoDB Cluster\n\n20.2.1 Deployment Scenarios\n\n20.2.2 InnoDB Cluster Requirements\n\n20.2.3 Methods of Installing\n\n20.2.4 Sandbox Deployment of InnoDB Cluster\n\n20.2.5 Production Deployment of InnoDB Cluster\n\n20.2.6 Adopting a Group Replication Deployment\n\n20.3 Using MySQL Router with InnoDB Cluster\n\n20.4 Working with InnoDB Cluster\n\n20.5 Known Limitations\n\n21 MySQL NDB Cluster 7.5 and NDB Cluster 7.6\n\n21.1 NDB Cluster Overview\n\n21.1.1 NDB Cluster Core Concepts\n\n21.1.2 NDB Cluster Nodes, Node Groups, Replicas, and Partitions\n\n21.1.3 NDB Cluster Hardware, Software, and Networking Requirements\n\n21.1.4 What is New in NDB Cluster\n\n21.1.5 NDB: Added, Deprecated, and Removed Options, Variables, and Parameters\n\n21.1.6 MySQL Server Using InnoDB Compared with NDB Cluster\n\n21.1.7 Known Limitations of NDB Cluster\n\n21.2 NDB Cluster Installation\n\n21.2.1 The NDB Cluster Auto-Installer (NDB 7.5)\n\n21.2.2 The NDB Cluster Auto-Installer (NDB 7.6)\n\n21.2.3 Installation of NDB Cluster on Linux\n\n21.2.4 Installing NDB Cluster on Windows\n\n21.2.5 Initial Configuration of NDB Cluster\n\n21.2.6 Initial Startup of NDB Cluster\n\n21.2.7 NDB Cluster Example with Tables and Data\n\n21.2.8 Safe Shutdown and Restart of NDB Cluster\n\n21.2.9 Upgrading and Downgrading NDB Cluster\n\n21.3 Configuration of NDB Cluster\n\n21.3.1 Quick Test Setup of NDB Cluster\n\n21.3.2 Overview of NDB Cluster Configuration Parameters, Options, and Variables\n\n21.3.3 NDB Cluster Configuration Files\n\n21.3.4 Using High-Speed Interconnects with NDB Cluster\n\n21.4 NDB Cluster Programs\n\n21.4.1 ndbd --- The NDB Cluster Data Node Daemon\n\n21.4.2 ndbinfo_select_all --- Select From ndbinfo Tables\n\n21.4.3 ndbmtd --- The NDB Cluster Data Node Daemon (Multi-Threaded)\n\n21.4.4 ndb_mgmd --- The NDB Cluster Management Server Daemon\n\n21.4.5 ndb_mgm --- The NDB Cluster Management Client\n\n21.4.6 ndb_blob_tool --- Check and Repair BLOB and TEXT columns of NDB\n\nCluster Tables\n\n21.4.7 ndb_config --- Extract NDB Cluster Configuration Information\n\n21.4.8 ndb_cpcd --- Automate Testing for NDB Development\n\n21.4.9 ndb_delete_all --- Delete All Rows from an NDB Table\n\n21.4.10 ndb_desc --- Describe NDB Tables\n\n21.4.11 ndb_drop_index --- Drop Index from an NDB Table\n\n21.4.12 ndb_drop_table --- Drop an NDB Table\n\n21.4.13 ndb_error_reporter --- NDB Error-Reporting Utility\n\n21.4.14 ndb_import --- Import CSV Data Into NDB\n\n21.4.15 ndb_index_stat --- NDB Index Statistics Utility\n\n21.4.16 ndb_move_data --- NDB Data Copy Utility\n\n21.4.17 ndb_perror --- Obtain NDB Error Message Information\n\n21.4.18 ndb_print_backup_file --- Print NDB Backup File Contents\n\n21.4.19 ndb_print_file --- Print NDB Disk Data File Contents\n\n21.4.20 ndb_print_frag_file --- Print NDB Fragment List File Contents\n\n21.4.21 ndb_print_schema_file --- Print NDB Schema File Contents\n\n21.4.22 ndb_print_sys_file --- Print NDB System File Contents\n\n21.4.23 ndb_redo_log_reader --- Check and Print Content of Cluster Redo Log\n\n21.4.24 ndb_restore --- Restore an NDB Cluster Backup\n\n21.4.25 ndb_select_all --- Print Rows from an NDB Table\n\n21.4.26 ndb_select_count --- Print Row Counts for NDB Tables\n\n21.4.27 ndb_setup.py --- Start browser-based Auto-Installer for NDB Cluster\n\n21.4.28 ndb_show_tables --- Display List of NDB Tables\n\n21.4.29 ndb_size.pl --- NDBCLUSTER Size Requirement Estimator\n\n21.4.30 ndb_top --- View CPU usage information for NDB threads\n\n21.4.31 ndb_waiter --- Wait for NDB Cluster to Reach a Given Status\n\n21.4.32 Options Common to NDB Cluster Programs --- Options Common to NDB\n\nCluster Programs\n\n21.5 Management of NDB Cluster\n\n21.5.1 Summary of NDB Cluster Start Phases\n\n21.5.2 Commands in the NDB Cluster Management Client\n\n21.5.3 Online Backup of NDB Cluster\n\n21.5.4 MySQL Server Usage for NDB Cluster\n\n21.5.5 Performing a Rolling Restart of an NDB Cluster\n\n21.5.6 Event Reports Generated in NDB Cluster\n\n21.5.7 NDB Cluster Log Messages\n\n21.5.8 NDB Cluster Single User Mode\n\n21.5.9 Quick Reference: NDB Cluster SQL Statements\n\n21.5.10 ndbinfo: The NDB Cluster Information Database\n\n21.5.11 INFORMATION_SCHEMA Tables for NDB Cluster\n\n21.5.12 NDB Cluster Security Issues\n\n21.5.13 NDB Cluster Disk Data Tables\n\n21.5.14 Online Operations with ALTER TABLE in NDB Cluster\n\n21.5.15 Adding NDB Cluster Data Nodes Online\n\n21.5.16 Distributed Privileges Using Shared Grant Tables\n\n21.5.17 NDB API Statistics Counters and Variables\n\n21.6 NDB Cluster Replication\n\n21.6.1 NDB Cluster Replication: Abbreviations and Symbols\n\n21.6.2 General Requirements for NDB Cluster Replication\n\n21.6.3 Known Issues in NDB Cluster Replication\n\n21.6.4 NDB Cluster Replication Schema and Tables\n\n21.6.5 Preparing the NDB Cluster for Replication\n\n21.6.6 Starting NDB Cluster Replication (Single Replication Channel)\n\n21.6.7 Using Two Replication Channels for NDB Cluster Replication\n\n21.6.8 Implementing Failover with NDB Cluster Replication\n\n21.6.9 NDB Cluster Backups With NDB Cluster Replication\n\n21.6.10 NDB Cluster Replication: Multi-Master and Circular Replication\n\n21.6.11 NDB Cluster Replication Conflict Resolution\n\n21.7 NDB Cluster Release Notes\n\n22 Partitioning\n\n22.1 Overview of Partitioning in MySQL\n\n22.2 Partitioning Types\n\n22.2.1 RANGE Partitioning\n\n22.2.2 LIST Partitioning\n\n22.2.3 COLUMNS Partitioning\n\n22.2.4 HASH Partitioning\n\n22.2.5 KEY Partitioning\n\n22.2.6 Subpartitioning\n\n22.2.7 How MySQL Partitioning Handles NULL\n\n22.3 Partition Management\n\n22.3.1 Management of RANGE and LIST Partitions\n\n22.3.2 Management of HASH and KEY Partitions\n\n22.3.3 Exchanging Partitions and Subpartitions with Tables\n\n22.3.4 Maintenance of Partitions\n\n22.3.5 Obtaining Information About Partitions\n\n22.4 Partition Pruning\n\n22.5 Partition Selection\n\n22.6 Restrictions and Limitations on Partitioning\n\n22.6.1 Partitioning Keys, Primary Keys, and Unique Keys\n\n22.6.2 Partitioning Limitations Relating to Storage Engines\n\n22.6.3 Partitioning Limitations Relating to Functions\n\n22.6.4 Partitioning and Locking\n\n23 Stored Objects\n\n23.1 Defining Stored Programs\n\n23.2 Using Stored Routines\n\n23.2.1 Stored Routine Syntax\n\n23.2.2 Stored Routines and MySQL Privileges\n\n23.2.3 Stored Routine Metadata\n\n23.2.4 Stored Procedures, Functions, Triggers, and LAST_INSERT_ID()\n\n23.3 Using Triggers\n\nshow triggers;\n\nCREATE TRIGGER ins_sum BEFORE INSERT ON account FOR EACH ROW SET @sum = @sum + NEW.amount;\n\nDROP TRIGGER test.ins_sum;\nmysql> delimiter //\n\nmysql> CREATE TRIGGER upd_check BEFORE UPDATE ON account\n\n-> FOR EACH ROW\n\n-> BEGIN\n\n-> IF NEW.amount < 0 THEN\n\n-> SET NEW.amount = 0;\n\n-> ELSEIF NEW.amount > 100 THEN\n\n-> SET NEW.amount = 100;\n\n-> END IF;\n\n-> END;//\n\nmysql> delimiter ;\nA trigger is a named database object that is associated with a table, and that activates when a particular event occurs for the table. Some uses for triggers are to perform checks of values to be inserted into a table or to perform calculations on values involved in an update.\n**Does MySQL 5.6 have statement-level or row-level triggers?**\n\nIn MySQL 5.6 and MySQL 8.0, all triggers areFOR EACH ROW; that is, the trigger is activated for each row that is inserted, updated, or deleted. MySQL 5.6 does not support triggers usingFOR EACH STATEMENT.\nThe SQL standard defines two types of triggers: row-level triggers and statement-level triggers.\n-   A row-level trigger is activated for each row that is inserted, updated, or deleted. For example, if a table has 100 rows inserted, updated, or deleted, the trigger is automatically invoked 100 times for the 100 rows affected.\n-   A statement-level trigger is executed once for each transaction regardless of how many rows are inserted, updated, or deleted.\nMySQL supports only row-level triggers. It doesn't support statement-level triggers.\n<https://www.mysqltutorial.org/mysql-triggers.aspx>\n\n## When to use and not to use Triggers**\n\nTriggers are a requirement for any complex data integrity rules. These cannot be enforced anywhere except the database or you will have data integrity problems.\nThey are also the best place for auditing unless you don't want to capture all changes to the database (which is the problem of auditing from the application).\nTriggers can cause performance issues if not written carefully and not enough developers are knowledgeable enough to write them well. This is part of where they get their bad rap.\nTriggers are often slower than other means of maintaining data integrity, so if you can use a check constraint, use that instead of a trigger.\nIt is easy to write bad triggers that do stupid things like try to send emails. Do you really want to be unable to change records in the db if the email server goes down?\nIn SQL server, triggers operate on a batch of records. All too often developers think they only need to handle one record inserts, updates or deletes. That is not the only kind of data changes that happen to a database and all triggers should be tested under the conditions of 1 record change and many record changes. Forgetting to do the second test can lead to extremely poorly performing triggers or a loss of data integrity.\n**Use of database triggers**\n\n1.  To drive column values automatically.\n\n2.  To enforce complex integrity constraints.\n\n3.  To enforce complex business rules.\n\n4.  To customize complex security authorizations.\n\n5.  To maintain replicate tables.\n\n6.  To audit data modification.\n**Different Types of Triggers**\n\nA MySQLtriggeris a stored program (with queries) which is executed automatically to respond to a specific event such as insertion, updation or deletion occurring in a table.\nThere are 6 different types of triggers in MySQL:\n\n1.  Before Update Trigger\n\n2.  After Update Trigger\n\n3.  Before Insert Trigger\n\n4.  After Insert Trigger\n\n5.  Before Delete Trigger\n\n6.  After Delete Trigger\n<https://www.geeksforgeeks.org/different-types-of-mysql-triggers-with-examples>\n23.3.1 Trigger Syntax and Examples\n\n23.3.2 Trigger Metadata\n\n23.4 Using the Event Scheduler\n\nMySQL Events are tasks that run according to a schedule. Therefore, we sometimes refer to them as*scheduled*events. When you create an event, you are creating a named database object containing one or more SQL statements to be executed at one or more regular intervals, beginning and ending at a specific date and time. Conceptually, this is similar to the idea of the Unixcrontab(also known as a\"cron job\") or the Windows Task Scheduler.\n<https://www.mysqltutorial.org/mysql-triggers/working-mysql-scheduled-event>\n\n<https://dev.mysql.com/doc/refman/8.0/en/events-overview.html>\n23.4.1 Event Scheduler Overview\n\n23.4.2 Event Scheduler Configuration\n\n23.4.3 Event Syntax\n\n23.4.4 Event Metadata\n\n23.4.5 Event Scheduler Status\n\n23.4.6 The Event Scheduler and MySQL Privileges\n\n23.5 Using Views\n\n23.5.1 View Syntax\n\n23.5.2 View Processing Algorithms\n\n23.5.3 Updatable and Insertable Views\n\n23.5.4 The View WITH CHECK OPTION Clause\n\n23.5.5 View Metadata\n\n23.6 Stored Object Access Control\n\n23.7 Stored Program Binary Logging\n\n23.8 Restrictions on Stored Programs\n\n23.9 Restrictions on Views\n\n24 INFORMATION_SCHEMA Tables\n\n24.1 Introduction\n\n24.2 The INFORMATION_SCHEMA CHARACTER_SETS Table\n\n24.3 The INFORMATION_SCHEMA COLLATIONS Table\n\n24.4 The INFORMATION_SCHEMA COLLATION_CHARACTER_SET_APPLICABILITY\n\nTable\n\n24.5 The INFORMATION_SCHEMA COLUMNS Table\n\n24.6 The INFORMATION_SCHEMA COLUMN_PRIVILEGES Table\n\n24.7 The INFORMATION_SCHEMA ENGINES Table\n\n24.8 The INFORMATION_SCHEMA EVENTS Table\n\n24.9 The INFORMATION_SCHEMA FILES Table\n\n24.10 The INFORMATION_SCHEMA GLOBAL_STATUS and SESSION_STATUS Tables\n\n24.11 The INFORMATION_SCHEMA GLOBAL_VARIABLES and SESSION_VARIABLES\n\nTables\n\n24.12 The INFORMATION_SCHEMA KEY_COLUMN_USAGE Table\n\n24.13 The INFORMATION_SCHEMA ndb_transid_mysql_connection_map Table\n\n24.14 The INFORMATION_SCHEMA OPTIMIZER_TRACE Table\n\n24.15 The INFORMATION_SCHEMA PARAMETERS Table\n\n24.16 The INFORMATION_SCHEMA PARTITIONS Table\n\n24.17 The INFORMATION_SCHEMA PLUGINS Table\n\n24.18 The INFORMATION_SCHEMA PROCESSLIST Table\n\n24.19 The INFORMATION_SCHEMA PROFILING Table\n\n24.20 The INFORMATION_SCHEMA REFERENTIAL_CONSTRAINTS Table\n\n24.21 The INFORMATION_SCHEMA ROUTINES Table\n\n24.22 The INFORMATION_SCHEMA SCHEMATA Table\n\n24.23 The INFORMATION_SCHEMA SCHEMA_PRIVILEGES Table\n\n24.24 The INFORMATION_SCHEMA STATISTICS Table\n\n24.25 The INFORMATION_SCHEMA TABLES Table\n\n24.26 The INFORMATION_SCHEMA TABLESPACES Table\n\n24.27 The INFORMATION_SCHEMA TABLE_CONSTRAINTS Table\n\n24.28 The INFORMATION_SCHEMA TABLE_PRIVILEGES Table\n\n24.29 The INFORMATION_SCHEMA TRIGGERS Table\n\n24.30 The INFORMATION_SCHEMA USER_PRIVILEGES Table\n\n24.31 The INFORMATION_SCHEMA VIEWS Table\n\n24.32 INFORMATION_SCHEMA InnoDB Tables\n\n24.32.1 The INFORMATION_SCHEMA INNODB_BUFFER_PAGE Table\n\n24.32.2 The INFORMATION_SCHEMA INNODB_BUFFER_PAGE_LRU Table\n\n24.32.3 The INFORMATION_SCHEMA INNODB_BUFFER_POOL_STATS Table\n\n24.32.4 The INFORMATION_SCHEMA INNODB_CMP and INNODB_CMP_RESET\n\nTables\n\n24.32.5 The INFORMATION_SCHEMA INNODB_CMPMEM and\n\nINNODB_CMPMEM_RESET Tables\n\n24.32.6 The INFORMATION_SCHEMA INNODB_CMP_PER_INDEX and\n\nINNODB_CMP_PER_INDEX_RESET Tables\n\n24.32.7 The INFORMATION_SCHEMA INNODB_FT_BEING_DELETED Table\n\n24.32.8 The INFORMATION_SCHEMA INNODB_FT_CONFIG Table\n\n24.32.9 The INFORMATION_SCHEMA INNODB_FT_DEFAULT_STOPWORD Table .\n\n24.32.10 The INFORMATION_SCHEMA INNODB_FT_DELETED Table\n\n24.32.11 The INFORMATION_SCHEMA INNODB_FT_INDEX_CACHE Table\n\n24.32.12 The INFORMATION_SCHEMA INNODB_FT_INDEX_TABLE Table\n\n24.32.13 The INFORMATION_SCHEMA INNODB_LOCKS Table\n\n24.32.14 The INFORMATION_SCHEMA INNODB_LOCK_WAITS Table\n\n24.32.15 The INFORMATION_SCHEMA INNODB_METRICS Table\n\n24.32.16 The INFORMATION_SCHEMA INNODB_SYS_COLUMNS Table\n\n24.32.17 The INFORMATION_SCHEMA INNODB_SYS_DATAFILES Table\n\n24.32.18 The INFORMATION_SCHEMA INNODB_SYS_FIELDS Table\n\n24.32.19 The INFORMATION_SCHEMA INNODB_SYS_FOREIGN Table\n\n24.32.20 The INFORMATION_SCHEMA INNODB_SYS_FOREIGN_COLS Table\n\n24.32.21 The INFORMATION_SCHEMA INNODB_SYS_INDEXES Table\n\n24.32.22 The INFORMATION_SCHEMA INNODB_SYS_TABLES Table\n\n24.32.23 The INFORMATION_SCHEMA INNODB_SYS_TABLESTATS View\n\n24.32.24 The INFORMATION_SCHEMA INNODB_SYS_VIRTUAL Table\n\n24.32.25 The INFORMATION_SCHEMA INNODB_TEMP_TABLE_INFO Table\n\n24.32.26 The INFORMATION_SCHEMA INNODB_TRX Table\n\n24.33 INFORMATION_SCHEMA Thread Pool Tables\n\n24.33.1 The INFORMATION_SCHEMA TP_THREAD_GROUP_STATE Table\n\n24.33.2 The INFORMATION_SCHEMA TP_THREAD_GROUP_STATS Table\n\n24.33.3 The INFORMATION_SCHEMA TP_THREAD_STATE Table\n\n24.34 INFORMATION_SCHEMA Connection-Control Tables\n\n24.34.1 The INFORMATION_SCHEMA\n\nCONNECTION_CONTROL_FAILED_LOGIN_ATTEMPTS Table\n\n24.35 Extensions to SHOW Statements\n\n25 MySQL Performance Schema\n\n25.1 Performance Schema Quick Start\n\n25.2 Performance Schema Build Configuration\n\n25.3 Performance Schema Startup Configuration\n\n25.4 Performance Schema Runtime Configuration\n\n25.4.1 Performance Schema Event Timing\n\n25.4.2 Performance Schema Event Filtering\n\n25.4.3 Event Pre-Filtering\n\n25.4.4 Pre-Filtering by Instrument\n\n25.4.5 Pre-Filtering by Object\n\n25.4.6 Pre-Filtering by Thread\n\n25.4.7 Pre-Filtering by Consumer\n\n25.4.8 Example Consumer Configurations\n\n25.4.9 Naming Instruments or Consumers for Filtering Operations\n\n25.4.10 Determining What Is Instrumented\n\n25.5 Performance Schema Queries\n\n25.6 Performance Schema Instrument Naming Conventions\n\n25.7 Performance Schema Status Monitoring\n\n25.8 Performance Schema Atom and Molecule Events\n\n25.9 Performance Schema Tables for Current and Historical Events\n\n25.10 Performance Schema Statement Digests\n\n25.11 Performance Schema General Table Characteristics\n\n25.12 Performance Schema Table Descriptions\n\n25.12.1 Performance Schema Table Index\n\n25.12.2 Performance Schema Setup Tables\n\n25.12.3 Performance Schema Instance Tables\n\n25.12.4 Performance Schema Wait Event Tables\n\n25.12.5 Performance Schema Stage Event Tables\n\n25.12.6 Performance Schema Statement Event Tables\n\n25.12.7 Performance Schema Transaction Tables\n\n25.12.8 Performance Schema Connection Tables\n\n25.12.9 Performance Schema Connection Attribute Tables\n\n25.12.10 Performance Schema User-Defined Variable Tables\n\n25.12.11 Performance Schema Replication Tables\n\n25.12.12 Performance Schema Lock Tables\n\n25.12.13 Performance Schema System Variable Tables\n\n25.12.14 Performance Schema Status Variable Tables\n\n25.12.15 Performance Schema Summary Tables\n\n25.12.16 Performance Schema Miscellaneous Tables\n\n25.13 Performance Schema Option and Variable Reference\n\n25.14 Performance Schema Command Options\n\n25.15 Performance Schema System Variables\n\n25.16 Performance Schema Status Variables\n\n25.17 The Performance Schema Memory-Allocation Model\n\n25.18 Performance Schema and Plugins\n\n25.19 Using the Performance Schema to Diagnose Problems\n\n25.19.1 Query Profiling Using Performance Schema\n\n25.20 Migrating to Performance Schema System and Status Variable Tables\n\n25.21 Restrictions on Performance Schema\n\n26 MySQL sys Schema\n\n26.1 Prerequisites for Using the sys Schema\n\n26.2 Using the sys Schema\n\n26.3 sys Schema Progress Reporting\n\n26.4 sys Schema Object Reference\n\n26.4.1 sys Schema Object Index\n\n26.4.2 sys Schema Tables and Triggers\n\n26.4.3 sys Schema Views\n\n26.4.4 sys Schema Stored Procedures\n\n26.4.5 sys Schema Stored Functions\n\n27 Connectors and APIs\n\n27.1 MySQL Connector/C++\n\n27.2 MySQL Connector/J\n\n27.3 MySQL Connector/NET\n\n27.4 MySQL Connector/ODBC\n\n27.5 MySQL Connector/Python\n\n27.6 libmysqld, the Embedded MySQL Server Library\n\n27.6.1 Compiling Programs with libmysqld\n\n27.6.2 Restrictions When Using the Embedded MySQL Server\n\n27.6.3 Options with the Embedded Server\n\n27.6.4 Embedded Server Examples\n\n27.7 MySQL C API\n\n27.7.1 MySQL C API Implementations\n\n27.7.2 Example C API Client Programs\n\n27.7.3 Building and Running C API Client Programs\n\n27.7.4 C API Data Structures\n\n27.7.5 C API Function Overview\n\n27.7.6 C API Function Descriptions\n\n27.7.7 C API Prepared Statements\n\n27.7.8 C API Prepared Statement Data Structures\n\n27.7.9 C API Prepared Statement Function Overview\n\n27.7.10 C API Prepared Statement Function Descriptions\n\n27.7.11 C API Threaded Function Descriptions\n\n27.7.12 C API Embedded Server Function Descriptions\n\n27.7.13 C API Client Plugin Functions\n\n27.7.14 C API Encrypted Connection Support\n\n27.7.15 C API Multiple Statement Execution Support\n\n27.7.16 C API Prepared Statement Handling of Date and Time Values\n\n27.7.17 C API Prepared CALL Statement Support\n\n27.7.18 C API Prepared Statement Problems\n\n27.7.19 C API Automatic Reconnection Control\n\n27.7.20 C API Common Issues\n\n27.8 MySQL PHP API\n\n27.9 MySQL Perl API\n\n27.10 MySQL Python API\n\n27.11 MySQL Ruby APIs\n\n27.11.1 The MySQL/Ruby API\n\n27.11.2 The Ruby/MySQL API\n\n27.12 MySQL Tcl API\n\n27.13 MySQL Eiffel Wrapper\n\n28 Extending MySQL\n\n28.1 MySQL Internals\n\n28.1.1 MySQL Threads\n\n28.1.2 The MySQL Test Suite\n\n28.2 The MySQL Plugin API\n\n28.2.1 Types of Plugins\n\n28.2.2 Plugin API Characteristics\n\n28.2.3 Plugin API Components\n\n28.2.4 Writing Plugins\n\n28.3 MySQL Services for Plugins\n\n28.3.1 The Locking Service\n\n28.3.2 The Keyring Service\n\n28.4 Adding Functions to MySQL\n\n28.4.1 Features of the User-Defined Function Interface\n\n28.4.2 Adding a User-Defined Function\n\n28.4.3 Adding a Native Function\n\n28.5 Debugging and Porting MySQL\n\n28.5.1 Debugging a MySQL Server\n\n28.5.2 Debugging a MySQL Client\n\n28.5.3 The DBUG Package\n\n29 MySQL Enterprise Edition\n\n29.1 MySQL Enterprise Monitor Overview\n\n29.2 MySQL Enterprise Backup Overview\n\n29.3 MySQL Enterprise Security Overview\n\n29.4 MySQL Enterprise Encryption Overview\n\n29.5 MySQL Enterprise Audit Overview\n\n29.6 MySQL Enterprise Firewall Overview\n\n29.7 MySQL Enterprise Thread Pool Overview\n\n29.8 MySQL Enterprise Data Masking and De-Identification Overview\n\n30 MySQL Workbench\nA MySQL 5.7 Frequently Asked Questions\n\nA.1 MySQL 5.7 FAQ: General\n\nA.2 MySQL 5.7 FAQ: Storage Engines\n\nA.3 MySQL 5.7 FAQ: Server SQL Mode\n\nA.4 MySQL 5.7 FAQ: Stored Procedures and Functions\n\nA.5 MySQL 5.7 FAQ: Triggers\n\nA.6 MySQL 5.7 FAQ: Views\n\nA.7 MySQL 5.7 FAQ: INFORMATION_SCHEMA\n\nA.8 MySQL 5.7 FAQ: Migration\n\nA.9 MySQL 5.7 FAQ: Security\n\nA.10 MySQL 5.7 FAQ: NDB Cluster\n\nA.11 MySQL 5.7 FAQ: MySQL Chinese, Japanese, and Korean Character Sets\n\nA.12 MySQL 5.7 FAQ: Connectors & APIs\n\nA.13 MySQL 5.7 FAQ: C API, libmysql\n\nA.14 MySQL 5.7 FAQ: Replication\n\nA.15 MySQL 5.7 FAQ: MySQL Enterprise Thread Pool\n\nA.16 MySQL 5.7 FAQ: InnoDB Change Buffer\n\nA.17 MySQL 5.7 FAQ: InnoDB Data-at-Rest Encryption\n\nA.18 MySQL 5.7 FAQ: Virtualization Support\nB Errors, Error Codes, and Common Problems\n\nB.1 Error Message Sources and Components\n\nB.2 Error Information Interfaces\n\nB.3 Error Message Reference\n\nB.3.1 Server Error Message Reference\n\nB.3.2 Client Error Message Reference\n\nB.3.3 Global Error Message Reference\n\nB.4 Problems and Common Errors\n\nB.4.1 How to Determine What Is Causing a Problem\n\nB.4.2 Common Errors When Using MySQL Programs\n\nB.4.3 Administration-Related Issues\n\nB.4.4 Query-Related Issues\n\nB.4.5 Optimizer-Related Issues\n\nB.4.6 Table Definition-Related Issues\n\nB.4.7 Known Issues in MySQL\n<https://dev.mysql.com/doc/refman/8.0/en>\n"},{"fields":{"slug":"/Databases/SQL-Databases/MySQL/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2020-02-13 23:36:54 +0500\n\nModified: 2022-04-28 15:41:39 +0500\n\n---\n\n**Facts**\n\nMySQLhas hardlimitof **4096columnsper table**, but the effectivemaximummay be less for a given table. The exactcolumn limitdepends on several factors: Themaximumrow size for a table constrains the number (and possibly size) ofcolumnsbecause the total length of allcolumnscannot exceed this size.-   CREATE SCHEMA is a synonym for CREATE DATABASEas of MySQL 5.0.2.\n<https://dev.mysql.com/doc/refman/5.7/en/innodb-limits.html>\n\nA table can contain a maximum of 64[secondary indexes](https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_secondary_index).\n**mysql client**\n\nbrew install mysql-client\n\necho 'export PATH=\"/usr/local/opt/mysql-client/bin:$PATH\"' >> ~/.bash_profile\n\nsource ~/.bash_profile\napt-get install -y default-mysql-client\nmysql -u root -p kollectai -h localhost\n**mysql -u root -p q82b912h9123hbasd9 -h localhost**\n\nmysql -u developer -p Ck4DmZA3TZZF@ -h devdb-ujwy3me96v.stashfin.com -D sttash_website_LIVE\n\nmysql -u deepak_sood -p 0d43db3b6ae622cac35b76092e2bcf13 -h backup-sttash-main-db-instance-cluster.cluster-c1z93jsyca9u.ap-south-1.rds.amazonaws.com -D sttash_website_LIVE\nmysql --user=deepak_sood --host=backup-sttash-main-db-instance-cluster.cluster-c1z93jsyca9u.ap-south-1.rds.amazonaws.com --database=sttash_website_LIVE --password=0d43db3b6ae622cac35b76092e2bcf13 -A\nshow databases;\n\nuse mysql;\n\nshow tables;\n\nselect * from mysql.user;\n\nGRANT ALL PRIVILEGES ON *.* TO 'root';\n**cobar**\n\nCobar is a proxy for sharding databases and tables,compatible with MySQL protocol and MySQL SQL grama,underlying storage only support MySQL for support foreground business more simple,stable,efficient and safety\n-   **Sharding:**You can add new MySQL instance as your business grows.\n-   **High availability:**Both Cobar server and underlying MySQL is clustered,business will not suffer with single node fail.\n-   **Compatible with MySQL protocol:**Use Cobar as MySQL. You can replace MySQL with Cobar to power your application.\n\n<https://github.com/alibaba/cobar>\n\n## InnoDB**\n\n**InnoDB**is a[storage engine](https://en.wikipedia.org/wiki/Database_engine)for the[database management system](https://en.wikipedia.org/wiki/Database_management_system)[MySQL](https://en.wikipedia.org/wiki/MySQL). Since the release of MySQL 5.5.5 in 2010, it replaced[MyISAM](https://en.wikipedia.org/wiki/MyISAM)as MySQL's default table type.^[[1]](https://en.wikipedia.org/wiki/InnoDB#cite_note-1)[[2]](https://en.wikipedia.org/wiki/InnoDB#cite_note-2)^It provides the standard[ACID](https://en.wikipedia.org/wiki/ACID)-compliant[transaction](https://en.wikipedia.org/wiki/Database_transaction)features, along with[foreign key](https://en.wikipedia.org/wiki/Foreign_key)support ([Declarative Referential Integrity](https://en.wikipedia.org/wiki/Declarative_Referential_Integrity)). It is included as standard in most[binaries](https://en.wikipedia.org/wiki/Binaries)distributed by[MySQL AB](https://en.wikipedia.org/wiki/MySQL_AB), the exception being some[OEM](https://en.wikipedia.org/wiki/Original_equipment_manufacturer)versions.\n**mariadb**\n\nMariaDBis a community-developed, commercially supported[fork](https://en.wikipedia.org/wiki/Fork_(software_development))of the[MySQL](https://en.wikipedia.org/wiki/MySQL)[relational database management system](https://en.wikipedia.org/wiki/Relational_database_management_system)(RDBMS), intended to remain[free and open-source software](https://en.wikipedia.org/wiki/Free_and_open-source_software)under the[GNU General Public License](https://en.wikipedia.org/wiki/GNU_General_Public_License). Development is led by some of the original developers of MySQL, who forked it due to concerns over its[acquisition](https://en.wikipedia.org/wiki/Takeover)by[Oracle Corporation](https://en.wikipedia.org/wiki/Oracle_Corporation)in 2009.\nMariaDB intended to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL[APIs](https://en.wikipedia.org/wiki/Application_programming_interface)and commands. However, new features diverge more.It includes new[storage engines](https://en.wikipedia.org/wiki/Storage_engine)like[Aria](https://en.wikipedia.org/wiki/Aria_(storage_engine)),[ColumnStore](https://en.wikipedia.org/wiki/InfiniDB), and[MyRocks](https://en.wikipedia.org/wiki/MyRocks).\nIts lead developer/CTO is[Michael \"Monty\" Widenius](https://en.wikipedia.org/wiki/Michael_Widenius), one of the founders of[MySQL AB](https://en.wikipedia.org/wiki/MySQL_AB)and the founder of Monty Program AB. On 16 January 2008, MySQL AB announced that it had agreed to be acquired by[Sun Microsystems](https://en.wikipedia.org/wiki/Sun_Microsystems) for approximately $1 billion. The acquisition completed on 26 February 2008. Sun was then bought the following year by Oracle Corporation. MariaDB is named after Monty's younger daughter, Maria. (MySQL is named after his other daughter, My.)\n**MySQL 5 vs MySQL 8**\n\n<https://mysqlserverteam.com/whats-new-in-mysql-8-0-generally-available>\n\nUpgrade Checker - <https://mysqlserverteam.com/upgrading-to-mysql-8-0-here-is-what-you-need-to-know>\nMySQL 8.0 brought enormous changes and modifications that were pushed by the Oracle MySQL Team.\n-   Physical files have been changed. For instance, *.frm, *.TRG, *.TRN, and *.par[no longer exist](https://dev.mysql.com/worklog/task/?id=8216).\n-   Tons of new features have been added such as[CTE](https://dev.mysql.com/doc/refman/8.0/en/with.html)(Common Table Expressions),[Window Functions](https://dev.mysql.com/doc/refman/8.0/en/window-functions.html),[**Invisible Indexes**](https://dev.mysql.com/doc/refman/8.0/en/invisible-indexes.html), regexp (or Regular Expression)--the latter has been changed and now provides full Unicode support and is multibyte safe.\n-   Data dictionary has also changed. It's now incorporated with a transactional data dictionary that stores information about database objects. Unlike previous versions, dictionary data was stored in metadata files and non-transactional tables.\n-   Security has been improved with the new addition ofcaching_sha2_passwordwhich is now the default authentication replacingmysql_native_passwordand offers more flexibility but tightened security which must use either a secure connection or an unencrypted connection that supports password exchange using an RSA key pair.-   **The benchmark results reveals that there has been an impressive improvement, not only on managing read workloads, but also on a high read/write workload comparing to MySQL 5.7.**\n-   <https://dev.mysql.com/doc/refman/8.0/en/create-index.html#create-index-functional-key-parts>\n-   Resource Groups\n    -   <https://dev.mysql.com/doc/refman/5.7/en/user-resources.html>\n    -   <https://dzone.com/articles/mysql-8-load-fine-tuning-with-resource-groups>\n\n## Important**\n-   Removed Query Cache\n-   Dual Passwords\n**Resources**\n\n<https://dev.mysql.com/doc/refman/5.7/en/glossary.html>\n\n<https://github.com/shlomi-noach/awesome-mysql>\n"},{"fields":{"slug":"/Databases/SQL-Databases/MySQL/SQL---MySQL-Tools/","title":"SQL / MySQL Tools"},"frontmatter":{"draft":false},"rawBody":"# SQL / MySQL Tools\n\nCreated: 2020-12-02 23:07:43 +0500\n\nModified: 2021-11-25 19:26:25 +0500\n\n---\n\n**Monitoring**\n\n<https://www.percona.com/doc/percona-monitoring-and-management/index.html>\n\n**MySQL Diagnostic Manager (Monyog) - <https://www.webyog.com/product/monyog>**\n<https://www.eversql.com/top-5-mysql-monitoring-tools>\n\n## Testing**\n\n**mysqlslap**\n\nIt's a benchmarking tool that can help DBAs and developers load test their database servers.\nmysqlslap can emulate a large number of client connections hitting the database server at the same time. The load testing parameters are fully configurable and the results from different test runs can be used to fine-tune database design or hardware resources.\n<https://www.digitalocean.com/community/tutorials/how-to-measure-mysql-query-performance-with-mysqlslap>\n\n## Optimizations**\n\n**MySQLTuner**\n\n<https://github.com/major/MySQLTuner-perl>\n\n<https://github.com/pdufault/mysqlfragfinder/blob/master/mysqlfragfinder.sh>\n\n## Mysqlreport**\n\nMysqlreport transforms the values from SHOW STATUS into an easy-to-read report that provides an in-depth understanding of how well MySQL is running. mysqlreport is a better alternative (and practically the only alternative) to manually interpreting SHOW STATUS.\n**percona-toolkit**\n-   Verify MySQL replication integrity by checking source and replica data consistency\n-   Efficiently archive rows\n-   Find duplicate indexes\n-   Summarize MySQL and MongoDB servers\n-   Analyze MySQL queries from logs and tcpdump\n-   Analyze MongoDB query profiler\n-   Collect vital system information when problems occur\n**brew install percona-toolkit**-   [pt-align](https://www.percona.com/doc/percona-toolkit/LATEST/pt-align.html)\n-   [pt-archiver](https://www.percona.com/doc/percona-toolkit/LATEST/pt-archiver.html)\n-   [pt-config-diff](https://www.percona.com/doc/percona-toolkit/LATEST/pt-config-diff.html)\n-   [pt-diskstats](https://www.percona.com/doc/percona-toolkit/LATEST/pt-diskstats.html)\n-   [pt-duplicate-key-checker](https://www.percona.com/doc/percona-toolkit/LATEST/pt-duplicate-key-checker.html)\n-   [pt-fifo-split](https://www.percona.com/doc/percona-toolkit/LATEST/pt-fifo-split.html)\n-   [pt-find](https://www.percona.com/doc/percona-toolkit/LATEST/pt-find.html)\n-   [pt-fingerprint](https://www.percona.com/doc/percona-toolkit/LATEST/pt-fingerprint.html)\n-   [pt-fk-error-logger](https://www.percona.com/doc/percona-toolkit/LATEST/pt-fk-error-logger.html)\n-   [pt-heartbeat](https://www.percona.com/doc/percona-toolkit/LATEST/pt-heartbeat.html)\n-   [**pt-index-usage**](https://www.percona.com/doc/percona-toolkit/LATEST/pt-index-usage.html)\n\nRead queries from a log and analyze how they use indexes.-   [pt-align](https://www.percona.com/doc/percona-toolkit/LATEST/pt-align.html)\n-   [pt-archiver](https://www.percona.com/doc/percona-toolkit/LATEST/pt-archiver.html)\n-   [pt-config-diff](https://www.percona.com/doc/percona-toolkit/LATEST/pt-config-diff.html)\n-   [**pt-deadlock-logger**](https://www.percona.com/doc/percona-toolkit/LATEST/pt-deadlock-logger.html)\n\n**pt-deadlock-logger**prints information about MySQL deadlocks by polling and parsingSHOWENGINEINNODBSTATUS. When a new deadlock occurs, it's printed toSTDOUTand, if specified, saved to[--dest](https://www.percona.com/doc/percona-toolkit/LATEST/pt-deadlock-logger.html#cmdoption-pt-deadlock-logger-dest).\npt-deadlock-logger -host sttash-main-db-instance-new-cluster.cluster-ro-c1z93jsyca9u.ap-south-1.rds.amazonaws.com --user lms-website --password Rf9zdHwB9E3GHWKq2yZM-   [pt-diskstats](https://www.percona.com/doc/percona-toolkit/LATEST/pt-diskstats.html)\n-   [pt-duplicate-key-checker](https://www.percona.com/doc/percona-toolkit/LATEST/pt-duplicate-key-checker.html)\n\npt-duplicate-key-checker -host sttash-main-db-instance-new-cluster.cluster-ro-c1z93jsyca9u.ap-south-1.rds.amazonaws.com --user lms-website --password Rf9zdHwB9E3GHWKq2yZM\n-   [pt-fifo-split](https://www.percona.com/doc/percona-toolkit/LATEST/pt-fifo-split.html)\n-   [pt-find](https://www.percona.com/doc/percona-toolkit/LATEST/pt-find.html)\n-   [pt-fingerprint](https://www.percona.com/doc/percona-toolkit/LATEST/pt-fingerprint.html)\n-   [pt-fk-error-logger](https://www.percona.com/doc/percona-toolkit/LATEST/pt-fk-error-logger.html)\n-   [pt-heartbeat](https://www.percona.com/doc/percona-toolkit/LATEST/pt-heartbeat.html)\n-   [pt-index-usage](https://www.percona.com/doc/percona-toolkit/LATEST/pt-index-usage.html)\n-   [pt-ioprofile](https://www.percona.com/doc/percona-toolkit/LATEST/pt-ioprofile.html)\n-   [pt-kill](https://www.percona.com/doc/percona-toolkit/LATEST/pt-kill.html)\n-   [pt-mext](https://www.percona.com/doc/percona-toolkit/LATEST/pt-mext.html)\n-   [pt-mongodb-query-digest](https://www.percona.com/doc/percona-toolkit/LATEST/pt-mongodb-query-digest.html)\n-   [pt-mongodb-summary](https://www.percona.com/doc/percona-toolkit/LATEST/pt-mongodb-summary.html)\n-   [pt-mysql-summary](https://www.percona.com/doc/percona-toolkit/LATEST/pt-mysql-summary.html)\n\npt-mysql-summary --host localhost --user root --ask-pass > mysql-summary.txt\n\npt-mysql-summary --host sttash-main-db-instance-new-cluster.cluster-ro-c1z93jsyca9u.ap-south-1.rds.amazonaws.com --user lms-website --password Rf9zdHwB9E3GHWKq2yZM > mysql-summary.txt-   [pt-online-schema-change](https://www.percona.com/doc/percona-toolkit/LATEST/pt-online-schema-change.html)\n\npt-online-schema-change --host dailydb.snapshot.stashfin.com --user deepak_sood --password 72062eacf89016e8c2bb4fe9c4457b90 --alter='ENGINE=InnoDB' D=sttash_website_LIVE,t=elev8_offer_tmp --preserve-triggers --dry-run\nWorks by creating an empty copy of the table to alter, modifying it as desired, and then copying rows from the original table into the new table. When the copy is complete, it moves away the original table and replaces it with the new one. By default, it also drops the original table.\n<https://dev.mysql.com/doc/refman/5.7/en/innodb-online-ddl-operations.html>\n\n<https://www.percona.com/doc/percona-toolkit/3.0/pt-online-schema-change.html>-   [pt-pmp](https://www.percona.com/doc/percona-toolkit/LATEST/pt-pmp.html)\n-   [pt-query-digest](https://www.percona.com/doc/percona-toolkit/LATEST/pt-query-digest.html)\n-   [pt-secure-collect](https://www.percona.com/doc/percona-toolkit/LATEST/pt-secure-collect.html)\n-   [pt-show-grants](https://www.percona.com/doc/percona-toolkit/LATEST/pt-show-grants.html)\n-   [pt-sift](https://www.percona.com/doc/percona-toolkit/LATEST/pt-sift.html)\n-   [pt-slave-delay](https://www.percona.com/doc/percona-toolkit/LATEST/pt-slave-delay.html)\n-   [pt-slave-find](https://www.percona.com/doc/percona-toolkit/LATEST/pt-slave-find.html)\n-   [pt-slave-restart](https://www.percona.com/doc/percona-toolkit/LATEST/pt-slave-restart.html)\n-   [pt-stalk](https://www.percona.com/doc/percona-toolkit/LATEST/pt-stalk.html)\n-   [pt-summary](https://www.percona.com/doc/percona-toolkit/LATEST/pt-summary.html)\n-   [pt-table-checksum](https://www.percona.com/doc/percona-toolkit/LATEST/pt-table-checksum.html)\n-   [pt-table-sync](https://www.percona.com/doc/percona-toolkit/LATEST/pt-table-sync.html)\n-   [pt-table-usage](https://www.percona.com/doc/percona-toolkit/LATEST/pt-table-usage.html)\n-   [pt-upgrade](https://www.percona.com/doc/percona-toolkit/LATEST/pt-upgrade.html)\n-   [pt-variable-advisor](https://www.percona.com/doc/percona-toolkit/LATEST/pt-variable-advisor.html)\n-   [pt-visual-explain](https://www.percona.com/doc/percona-toolkit/LATEST/pt-visual-explain.html)\n-   [pt-slave-find](https://www.percona.com/doc/percona-toolkit/LATEST/pt-slave-find.html)\n-   [pt-slave-restart](https://www.percona.com/doc/percona-toolkit/LATEST/pt-slave-restart.html)\n-   [pt-stalk](https://www.percona.com/doc/percona-toolkit/LATEST/pt-stalk.html)\n-   [pt-summary](https://www.percona.com/doc/percona-toolkit/LATEST/pt-summary.html)\n-   [pt-table-checksum](https://www.percona.com/doc/percona-toolkit/LATEST/pt-table-checksum.html)\n-   [pt-table-sync](https://www.percona.com/doc/percona-toolkit/LATEST/pt-table-sync.html)\n-   [pt-table-usage](https://www.percona.com/doc/percona-toolkit/LATEST/pt-table-usage.html)\n-   [pt-upgrade](https://www.percona.com/doc/percona-toolkit/LATEST/pt-upgrade.html)\n-   [pt-variable-advisor](https://www.percona.com/doc/percona-toolkit/LATEST/pt-variable-advisor.html)\n-   [pt-visual-explain](https://www.percona.com/doc/percona-toolkit/LATEST/pt-visual-explain.html)\n<https://www.percona.com/software/database-tools/percona-toolkit>\n\n## Event Reduce**\n\nAn algorithm to optimize database queries that run multiple times\n<https://github.com/pubkey/event-reduce>\n\n## SQLCheck**\n\nSQL anti-patterns can slow down queries, but often it takes experienced DBAs and developers poring over code to identify and resolve them.\nFour categories of anti-pattern:\n\ni.  Logical database design\n\nii. Physical database design\n\niii. Query\n\niv. Application development\nSqlcheck can be targeted at varying risk levels, categorized as low, medium, or high risk. This is helpful if your list of anti-patterns is large, since you can prioritize the queries with the greatest performance impact.All you need to do to get started is gather a list of your distinct queries into a file and then pass them as an argument to the tool.\n<https://github.com/jarulraj/sqlcheck>\n\n## Gh-ost (Ghost)**\n\nIf like 99 percent of MySQL DBAs you have faced implementing a change to a MySQL table while fearing the impact on production, then you should consider[Gh-ost](https://github.com/github/gh-ost) (GitHub Online Schema Migration). Gh-ost provides MySQL schema changes without blocking writes, without using triggers, and with the ability to pause and resume the migration!\nWhy is this so important? Since MySQL 5.6 shipped with new[ALTER TABLE ... ALGORITHM=INPLACE](https://dev.mysql.com/doc/refman/5.6/en/alter-table.html)DDL (Data Definition Language) functionality, it became possible to modify a table without blocking writes for common operations such as adding an index (B-tree). However, there remain a few conditions where[writes (DML statements) are blocked](https://dev.mysql.com/doc/refman/5.7/en/innodb-create-index-overview.html#innodb-online-ddl-summary-grid), most notably the addition of aFULLTEXTindex, the encryption of the tablespace, and the conversion of a column type.\nOther popular online schema change tools, such as Percona's[pt-online-schema-change](https://www.percona.com/doc/percona-toolkit/LATEST/pt-online-schema-change.html), work by implementing a set of three triggers (INSERT,UPDATE, andDELETE) on the master to keep a shadow copy table in sync with changes. This introduces a small performance penalty due to write amplification, but more significantly requires seven instances of metadata locks. These effectively stall DML (Data Manipulation Language) events.\nSince Gh-ost operates using the binary log, it is not susceptible to the[trigger-based drawbacks](https://github.com/github/gh-ost/blob/master/doc/why-triggerless.md). Finally Gh-ost is able to effectively[throttle activity to zero events](https://github.com/github/gh-ost/blob/master/doc/interactive-commands.md#examples), allowing you to pause the schema migration for a while if your server begins to struggle, and resume when the activity bubble moves on.\nSo how does Gh-ost work? By default, Gh-ost connects to a replica (slave), identifies the master, and applies the migration on the master. It receives changes on a replica to the source table in[binlog_format=ROW](https://dev.mysql.com/doc/en/binary-log-setting.html), parses the log, and converts these statements to be re-executed on the master's shadow table.It keeps track of the row counts on the replica and identifies when it is time to perform an atomic cutover (switch tables).\n\n![gh ost general flow](media/MySQL_SQL---MySQL-Tools-image1.jpg)\n\n**Gh-ost operation modes**\nGh-ost provides an alternative mode where you execute the migration directly on the master (whether it has slaves or not), read back the master'sbinlog_format=ROWevents, and then re-apply them to the shadow table.\nA final option is available to run the migration only on the replica without impacting the master, so you can test or otherwise validate the migration.\n\n![gh ost operation modes](media/MySQL_SQL---MySQL-Tools-image2.jpg)\n\n**Gh-ost general flow**\nNote that if your schema has foreign keys then Gh-ost may not operate cleanly, as this configuration is not supported.\n<https://github.com/github/gh-ost>\n<https://www.infoworld.com/article/3241730/top-5-open-source-tools-for-mysql-administrators.html>\n\n## Maintenance Scripts**\n\n<https://jonlabelle.com/snippets/view/shell/mysql-database-maintenance-script>\n\nBackup + Optimize - <https://github.com/mmerian/MySQL-Maint/blob/master/mysql_maint.sh>\nInnoDB stores data using a page-allocation method and does not suffer from fragmentation in the same way that legacy storage engines (such as MyISAM) will. When considering whether or not to run optimize, consider the workload of transactions that your server will process:\n-   Some level of fragmentation is expected. InnoDB only fills pages 93% full, to leave room for updates without having to split pages.\n-   Delete operations might leave gaps that leave pages less filled than desired, which could make it worthwhile to optimize the table.\n-   Updates to rows usually rewrite the data within the same page, depending on the data type and row format, when sufficient space is available. See Section 14.10.5, \"How Compression Works for InnoDB Tables\" and Section 14.12.1, \"Overview of InnoDB Row Storage\".\n-   High-concurrency workloads might leave gaps in indexes over time, as InnoDB retains multiple versions of the same data due through its MVCC mechanism. See Section 14.5.12, \"InnoDB Multi-Versioning\".\n**Orchestrator**\n\norchestratoris a MySQL high availability and replication management tool, runs as a service and provides command line access, HTTP API and Web interface.\n<https://github.com/openark/orchestrator>\n\n## CueObserve**\n\nAnomaly detection on SQL data warehouses and databases\n\n<https://github.com/cuebook/cueobserve>\n\n<https://cueobserve.cuebook.ai>"},{"fields":{"slug":"/Databases/SQL-Databases/MySQL/Scaling---Optimizations/","title":"Scaling / Optimizations"},"frontmatter":{"draft":false},"rawBody":"# Scaling / Optimizations\n\nCreated: 2020-10-12 01:27:01 +0500\n\nModified: 2022-07-13 14:51:02 +0500\n\n---\n\n**What exactly needs to Scale?**\n\n1.  Tables (Data)\n    -   Partitioning, Co-location, Reference Tables\n\n2.  SQL (Reads)\n    -   How do we express and optimze distributed SQL\n\n3.  Transactions (Writes)\n    -   Cross Shared updates/deletes, Global Atomic Transactions\nAble to search ~5cr entries in 7 mins of average time with 4-5 where conditions\n**Data Partitioning**\n-   **Horizontal Partitioning**\n-   **Vertical Partitioning**-   Pick a column\n    -   Date\n    -   Id (customer_id, card_id)\n-   Pick a method\n    -   Hash\n    -   Range\n**Limitations**\n-   ALTER TABLE ... ORDER BY.AnALTER TABLE ... ORDER BYcolumnstatement run against a partitioned table causes ordering of rows only within each partition.\n-   Prohibited constructs.The following constructs are not permitted in partitioning expressions:\n    -   Stored procedures, stored functions, UDFs, or plugins.\n    -   Declared variables or user variables.\n-   The query cache is not supported for partitioned tables, and is automatically disabled for queries involving partitioned tables. The query cache cannot be enabled for such queries.\n-   Partitioned tables using the[InnoDB](https://dev.mysql.com/doc/refman/5.7/en/innodb-storage-engine.html)storage engine do not support foreign keys. More specifically, this means that the following two statements are true:\n    -   No definition of anInnoDBtable employing user-defined partitioning may contain foreign key references; noInnoDBtable whose definition contains foreign key references may be partitioned.\n    -   NoInnoDBtable definition may contain a foreign key reference to a user-partitioned table; noInnoDBtable with user-defined partitioning may contain columns referenced by foreign keys.\n<https://dev.mysql.com/doc/mysql-partitioning-excerpt/5.7/en/partitioning-limitations.html>\n\n## Performance Tuning**\n-   Often the default value is the best value\n-   Ensure all tables have a PRIMARY KEY\n-   InnoDB organizes the data according to the PRIMARY KEY:\n    -   The PRIMARY KEY is included in all secondary indexes in order to be able to locate the actual row\n    -   Smaller PRIMARY KEY gives smaller secondary indexes\n    -   A mostly sequential PRIMARY KEY is generally recommended to avoid inserting rows between existing rows\n-   max_execution_time limit for long running SQL Queries\n<https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html>\n\n## Optimizations**\n-   **Always do bulk inserts/updates wherever possible**\n\n<https://blog.jooq.org/2018/04/19/the-performance-difference-between-sql-row-by-row-updating-batch-updating-and-bulk-updating>\nif autoincrement is used, then for every query there is roundtrip to database to check the autoincrement value, so use bulk insert if possible-   **Explicitly ORDER BY After GROUP BY**\n\nBy default, the database sorts all 'GROUP BY col1, col2, ...' queries as if you specified 'ORDER BY col1, col2, ...' in the query as well. If a query includes a GROUP BY clause but you want to avoid the overhead of sorting the result, you can suppress sorting by specifying 'ORDER BY NULL'.\n\nSELECT age, COUNT(*) FROM employees WHERE age > 50\nGROUP BY age **ORDER BY NULL;**\nInstead of:\n\nSELECT age, COUNT(*) FROM employees WHERE age > 50\nGROUP BY age;-   **Avoid Comparing Columns From Different Types**\n\nJoining or filtering using columns of different types/lengths/signedness in the same condition may cause performance degradation. The database will have to perform a cast for each of these values before performing the comparison, which may also prevent the use of an index for these columns. Make sure to alter the column types so that common comparisons will be done between two columns of the same type.-   **Avoid Correlated Subqueries**\n\nA correlated subquery is a subquery that contains a reference (column: loan_id) to a table that also appears in the outer query. Usually correlated queries can be rewritten with a join clause, which is the best practice. The database optimizer handles joins much better than correlated subqueries. Therefore, rephrasing the query with a join will allow the optimizer to use the most efficient execution plan for the query.-   **Avoid Subqueries**\n\nWe advise against using subqueries as they are not optimized well by the optimizer. Therefore, it's recommended to join a newly created temporary table that holds the data, which also includes the relevant search index.-   **Use UNION ALL instead of UNION**\n\nAlways use UNION ALL unless you need to eliminate duplicaterecords. By using UNION ALL, you'll avoid the expensive distinct operation the database applies when using a UNION clause.-   The LOAD DATA INFILE statement reads rows from a text file into a table at a very high speed.\nANALYZE TABLE payments;\n\nOPTIMIZE TABLE table_name;\n\nCHECK TABLE table_name;\n\nREPAIR TABLE table_name;\n<https://www.mysqltutorial.org/mysql-database-table-maintenance-statements.aspx>\n\n## Defragmentation**\n\nOPTIMIZE TABLE sttash_website_LIVE.email_instance_moratorium;\nDuring optimization, MySQL will create a temporary table for the table, and after the optimization it will delete the original table, and rename this temporary table to the original table.\nYou can use[OPTIMIZE TABLE](https://dev.mysql.com/doc/refman/8.0/en/optimize-table.html)to reclaim the unused space and to defragment the data file. After extensive changes to a table, this statement may also improve performance of statements that use the table, sometimes significantly.\nThis statement requires[SELECT](https://dev.mysql.com/doc/refman/8.0/en/privileges-provided.html#priv_select)and[INSERT](https://dev.mysql.com/doc/refman/8.0/en/privileges-provided.html#priv_insert)privileges for the table.\n<https://www.thegeekstuff.com/2016/04/mysql-optimize-table>\n\n<https://dev.mysql.com/doc/refman/8.0/en/optimize-table.html>-   Optimizing data size\n    -   **Table Columns**\n        -   Use the most efficient (smallest) data types possible. MySQL has many specialized types that save disk space and memory. For example, use the smaller integer types if possible to get smaller tables.[MEDIUMINT](https://dev.mysql.com/doc/refman/8.0/en/integer-types.html)is often a better choice than[INT](https://dev.mysql.com/doc/refman/8.0/en/integer-types.html)because a[MEDIUMINT](https://dev.mysql.com/doc/refman/8.0/en/integer-types.html)column uses 25% less space.\n        -   Declare columns to beNOT NULLif possible. It makes SQL operations faster, by enabling better use of indexes and eliminating overhead for testing whether each value isNULL. You also save some storage space, one bit per column. If you really needNULLvalues in your tables, use them. Just avoid the default setting that allowsNULLvalues in every column.\n    -   **Row Format**\n    -   **Indexes**\n        -   The primary index of a table should be as short as possible. This makes identification of each row easy and efficient. ForInnoDBtables, the primary key columns are duplicated in each secondary index entry, so a short primary key saves considerable space if you have many secondary indexes.\n        -   Create only the indexes that you need to improve query performance. Indexes are good for retrieval, but slow down insert and update operations. If you access a table mostly by searching on a combination of columns, create a single composite index on them rather than a separate index for each column. The first part of the index should be the column most used. If youalwaysuse many columns when selecting from the table, the first column in the index should be the one with the most duplicates, to obtain better compression of the index.\n        -   If it is very likely that a long string column has a unique prefix on the first number of characters, it is better to index only this prefix, using MySQL's support for creating an index on the leftmost part of the column. Shorter indexes are faster, not only because they require less disk space, but because they also give you more hits in the index cache, and thus fewer disk seeks\n    -   **Joins**\n        -   In some circumstances, it can be beneficial to split into two a table that is scanned very often. This is especially true if it is a dynamic-format table and it is possible to use a smaller static format table that can be used to find the relevant rows when scanning the table.\n        -   Declare columns with identical information in different tables with identical data types, to speed up joins based on the corresponding columns.\n        -   Keep column names simple, so that you can use the same name across different tables and simplify join queries. For example, in a table namedcustomer, use a column name ofnameinstead ofcustomer_name. To make your names portable to other SQL servers, consider keeping them shorter than 18 characters.\n    -   **Normalization**\n        -   Normally, try to keep all data nonredundant (observing what is referred to in database theory asthird normal form). Instead of repeating lengthy values such as names and addresses, assign them unique IDs, repeat these IDs as needed across multiple smaller tables, and join the tables in queries by referencing the IDs in the join clause.\n        -   If speed is more important than disk space and the maintenance costs of keeping multiple copies of data, for example in a business intelligence scenario where you analyze all the data from large tables, you can relax the normalization rules, duplicating information or creating summary tables to gain more speed.\n<https://dev.mysql.com/doc/refman/8.0/en/data-size.html>\n[SQL Server Performance Essentials -- Full Course](https://www.youtube.com/watch?v=HvxmF0FUwrM)\n"},{"fields":{"slug":"/Databases/SQL-Databases/Postgres/Documentation/","title":"Documentation"},"frontmatter":{"draft":false},"rawBody":"# Documentation\n\nCreated: 2019-12-19 20:38:05 +0500\n\nModified: 2020-02-09 23:55:13 +0500\n\n---\n\n[Preface](https://www.postgresql.org/docs/12/preface.html)\n\n[1. What IsPostgreSQL?](https://www.postgresql.org/docs/12/intro-whatis.html)\n\n[2. A Brief History ofPostgreSQL](https://www.postgresql.org/docs/12/history.html)\n\n[3. Conventions](https://www.postgresql.org/docs/12/notation.html)\n\n[4. Further Information](https://www.postgresql.org/docs/12/resources.html)\n\n[5. Bug Reporting Guidelines](https://www.postgresql.org/docs/12/bug-reporting.html)\n\n[I. Tutorial](https://www.postgresql.org/docs/12/tutorial.html)\n\n[1. Getting Started](https://www.postgresql.org/docs/12/tutorial-start.html)\n\n[2. TheSQLLanguage](https://www.postgresql.org/docs/12/tutorial-sql.html)\n\n[3. Advanced Features](https://www.postgresql.org/docs/12/tutorial-advanced.html)\n\n[II. The SQL Language](https://www.postgresql.org/docs/12/sql.html)\n\n[4. SQL Syntax](https://www.postgresql.org/docs/12/sql-syntax.html)\n\n[5. Data Definition](https://www.postgresql.org/docs/12/ddl.html)\n\n[6. Data Manipulation](https://www.postgresql.org/docs/12/dml.html)\n\n[7. Queries](https://www.postgresql.org/docs/12/queries.html)\n\n[8. Data Types](https://www.postgresql.org/docs/12/datatype.html)\n\n[9. Functions and Operators](https://www.postgresql.org/docs/12/functions.html)\n\nPREPARE TRANSACTION--prepare the current transaction for two-phase commit\n\n<https://www.highgo.ca/2020/01/28/understanding-prepared-transactions-and-handling-the-orphans>\n[10. Type Conversion](https://www.postgresql.org/docs/12/typeconv.html)\n\n[11. Indexes](https://www.postgresql.org/docs/12/indexes.html)\n\n[12. Full Text Search](https://www.postgresql.org/docs/12/textsearch.html)\n\n[13. Concurrency Control](https://www.postgresql.org/docs/12/mvcc.html)\n\n[13.1. Introduction](https://www.postgresql.org/docs/current/mvcc-intro.html)\n\n[13.2. Transaction Isolation](https://www.postgresql.org/docs/current/transaction-iso.html)\n\n[13.2.1. Read Committed Isolation Level](https://www.postgresql.org/docs/current/transaction-iso.html#XACT-READ-COMMITTED)\n\n[13.2.2. Repeatable Read Isolation Level](https://www.postgresql.org/docs/current/transaction-iso.html#XACT-REPEATABLE-READ)\n\n[13.2.3. Serializable Isolation Level](https://www.postgresql.org/docs/current/transaction-iso.html#XACT-SERIALIZABLE)\n\n[**13.3. Explicit Locking**](https://www.postgresql.org/docs/current/explicit-locking.html)\n\n[**13.3.1. Table-Level Locks**](https://www.postgresql.org/docs/current/explicit-locking.html#LOCKING-TABLES)\n\n[**13.3.2. Row-Level Locks**](https://www.postgresql.org/docs/current/explicit-locking.html#LOCKING-ROWS)\n\n[**13.3.3. Page-Level Locks**](https://www.postgresql.org/docs/current/explicit-locking.html#LOCKING-PAGES)\n\n[**13.3.4. Deadlocks**](https://www.postgresql.org/docs/current/explicit-locking.html#LOCKING-DEADLOCKS)\n\n[**13.3.5. Advisory Locks**](https://www.postgresql.org/docs/current/explicit-locking.html#ADVISORY-LOCKS)\n<https://www.postgresql.org/docs/current/explicit-locking.html>\n[13.4. Data Consistency Checks at the Application Level](https://www.postgresql.org/docs/current/applevel-consistency.html)\n\n[13.4.1. Enforcing Consistency with Serializable Transactions](https://www.postgresql.org/docs/current/applevel-consistency.html#SERIALIZABLE-CONSISTENCY)\n\n[13.4.2. Enforcing Consistency with Explicit Blocking Locks](https://www.postgresql.org/docs/current/applevel-consistency.html#NON-SERIALIZABLE-CONSISTENCY)\n\n[13.5. Caveats](https://www.postgresql.org/docs/current/mvcc-caveats.html)\n\n[13.6. Locking and Indexes](https://www.postgresql.org/docs/current/locking-indexes.html)\n[14. Performance Tips](https://www.postgresql.org/docs/12/performance-tips.html)\n\n[15. Parallel Query](https://www.postgresql.org/docs/12/parallel-query.html)\n\n[III. Server Administration](https://www.postgresql.org/docs/12/admin.html)\n\n[16. Installation from Source Code](https://www.postgresql.org/docs/12/installation.html)\n\n[17. Installation from Source Code onWindows](https://www.postgresql.org/docs/12/install-windows.html)\n\n[18. Server Setup and Operation](https://www.postgresql.org/docs/12/runtime.html)\n\n[19. Server Configuration](https://www.postgresql.org/docs/12/runtime-config.html)\n\n[20. Client Authentication](https://www.postgresql.org/docs/12/client-authentication.html)\n\n[21. Database Roles](https://www.postgresql.org/docs/12/user-manag.html)\n\n[22. Managing Databases](https://www.postgresql.org/docs/12/managing-databases.html)\n\n[23. Localization](https://www.postgresql.org/docs/12/charset.html)\n\n[24. Routine Database Maintenance Tasks](https://www.postgresql.org/docs/12/maintenance.html)\n\n[25. Backup and Restore](https://www.postgresql.org/docs/12/backup.html)\n\n[26. High Availability, Load Balancing, and Replication](https://www.postgresql.org/docs/12/high-availability.html)\n\n[27. Monitoring Database Activity](https://www.postgresql.org/docs/12/monitoring.html)\n\n[28. Monitoring Disk Usage](https://www.postgresql.org/docs/12/diskusage.html)\n\n[29. Reliability and the Write-Ahead Log](https://www.postgresql.org/docs/12/wal.html)\n\n[30. Logical Replication](https://www.postgresql.org/docs/12/logical-replication.html)\n\n[31. Just-in-Time Compilation (JIT)](https://www.postgresql.org/docs/12/jit.html)\n\n[32. Regression Tests](https://www.postgresql.org/docs/12/regress.html)\n\n[IV. Client Interfaces](https://www.postgresql.org/docs/12/client-interfaces.html)\n\n[33.libpq- C Library](https://www.postgresql.org/docs/12/libpq.html)\n\n[34. Large Objects](https://www.postgresql.org/docs/12/largeobjects.html)\n\n[35.ECPG- EmbeddedSQLin C](https://www.postgresql.org/docs/12/ecpg.html)\n\n[36. The Information Schema](https://www.postgresql.org/docs/12/information-schema.html)\n\n[V. Server Programming](https://www.postgresql.org/docs/12/server-programming.html)\n\n[37. ExtendingSQL](https://www.postgresql.org/docs/12/extend.html)\n\n[38. Triggers](https://www.postgresql.org/docs/12/triggers.html)\n\n[39. Event Triggers](https://www.postgresql.org/docs/12/event-triggers.html)\n\n[40. The Rule System](https://www.postgresql.org/docs/12/rules.html)\n\n[41. Procedural Languages](https://www.postgresql.org/docs/12/xplang.html)\n\n[42.PL/pgSQL-SQLProcedural Language](https://www.postgresql.org/docs/12/plpgsql.html)\n\n[43. PL/Tcl - Tcl Procedural Language](https://www.postgresql.org/docs/12/pltcl.html)\n\n[44. PL/Perl - Perl Procedural Language](https://www.postgresql.org/docs/12/plperl.html)\n\n[45. PL/Python - Python Procedural Language](https://www.postgresql.org/docs/12/plpython.html)\n\n[46. Server Programming Interface](https://www.postgresql.org/docs/12/spi.html)\n\n[47. Background Worker Processes](https://www.postgresql.org/docs/12/bgworker.html)\n\n[48. Logical Decoding](https://www.postgresql.org/docs/12/logicaldecoding.html)\n\n[49. Replication Progress Tracking](https://www.postgresql.org/docs/12/replication-origins.html)\n\n[VI. Reference](https://www.postgresql.org/docs/12/reference.html)\n\n[I. SQL Commands](https://www.postgresql.org/docs/12/sql-commands.html)\n\n[II. PostgreSQL Client Applications](https://www.postgresql.org/docs/12/reference-client.html)\n\n[III. PostgreSQL Server Applications](https://www.postgresql.org/docs/12/reference-server.html)\n\n[VII. Internals](https://www.postgresql.org/docs/12/internals.html)\n\n[50. Overview of PostgreSQL Internals](https://www.postgresql.org/docs/12/overview.html)\n\n[51. System Catalogs](https://www.postgresql.org/docs/12/catalogs.html)\n\n[52. Frontend/Backend Protocol](https://www.postgresql.org/docs/12/protocol.html)\n\n[53. PostgreSQL Coding Conventions](https://www.postgresql.org/docs/12/source.html)\n\n[54. Native Language Support](https://www.postgresql.org/docs/12/nls.html)\n\n[55. Writing a Procedural Language Handler](https://www.postgresql.org/docs/12/plhandler.html)\n\n[56. Writing a Foreign Data Wrapper](https://www.postgresql.org/docs/12/fdwhandler.html)\n\n[57. Writing a Table Sampling Method](https://www.postgresql.org/docs/12/tablesample-method.html)\n\n[58. Writing a Custom Scan Provider](https://www.postgresql.org/docs/12/custom-scan.html)\n\n[59. Genetic Query Optimizer](https://www.postgresql.org/docs/12/geqo.html)\n\n[60. Table Access Method Interface Definition](https://www.postgresql.org/docs/12/tableam.html)\n\n[61. Index Access Method Interface Definition](https://www.postgresql.org/docs/12/indexam.html)\n\n[62. Generic WAL Records](https://www.postgresql.org/docs/12/generic-wal.html)\n\n[63. B-Tree Indexes](https://www.postgresql.org/docs/12/btree.html)\n\n[64. GiST Indexes](https://www.postgresql.org/docs/12/gist.html)\n\n[65. SP-GiST Indexes](https://www.postgresql.org/docs/12/spgist.html)\n\n[66. GIN Indexes](https://www.postgresql.org/docs/12/gin.html)\n\n[67. BRIN Indexes](https://www.postgresql.org/docs/12/brin.html)\n\n[68. Database Physical Storage](https://www.postgresql.org/docs/12/storage.html)\n\n[69. System Catalog Declarations and Initial Contents](https://www.postgresql.org/docs/12/bki.html)\n\n[70. How the Planner Uses Statistics](https://www.postgresql.org/docs/12/planner-stats-details.html)\n\n[VIII. Appendixes](https://www.postgresql.org/docs/12/appendixes.html)\n\n[A.PostgreSQLError Codes](https://www.postgresql.org/docs/12/errcodes-appendix.html)\n\n[B. Date/Time Support](https://www.postgresql.org/docs/12/datetime-appendix.html)\n\n[C.SQLKey Words](https://www.postgresql.org/docs/12/sql-keywords-appendix.html)\n\n[D. SQL Conformance](https://www.postgresql.org/docs/12/features.html)\n\n[E. Release Notes](https://www.postgresql.org/docs/12/release.html)\n\n[F. Additional Supplied Modules](https://www.postgresql.org/docs/12/contrib.html)\n\n[G. Additional Supplied Programs](https://www.postgresql.org/docs/12/contrib-prog.html)\n\n[H. External Projects](https://www.postgresql.org/docs/12/external-projects.html)\n\n[I. The Source Code Repository](https://www.postgresql.org/docs/12/sourcerepo.html)\n\n[J. Documentation](https://www.postgresql.org/docs/12/docguide.html)\n\n[K.PostgreSQLLimits](https://www.postgresql.org/docs/12/limits.html)\n\n[L. Acronyms](https://www.postgresql.org/docs/12/acronyms.html)\n<https://www.postgresql.org/docs/current/index.html>\n"},{"fields":{"slug":"/Databases/Time-Series-DB/InfluxDB/Administration/","title":"Administration"},"frontmatter":{"draft":false},"rawBody":"# Administration\n\nCreated: 2019-01-11 15:51:58 +0500\n\nModified: 2019-01-11 15:52:08 +0500\n\n---\n\n1.  [Configuring InfluxDB](https://docs.influxdata.com/influxdb/v1.7/administration/config/)\n\n2.  [Upgrading](https://docs.influxdata.com/influxdb/v1.7/administration/upgrading/)\n\n3.  [Enabling HTTPS](https://docs.influxdata.com/influxdb/v1.7/administration/https_setup/)\n\n4.  [Logging and tracing](https://docs.influxdata.com/influxdb/v1.7/administration/logs/)\n\n5.  [Ports](https://docs.influxdata.com/influxdb/v1.7/administration/ports/)\n\n6.  [Backing up and restoring](https://docs.influxdata.com/influxdb/v1.7/administration/backup_and_restore/)\n\n7.  [Managing security](https://docs.influxdata.com/influxdb/v1.7/administration/security/)\n\n8.  [Server monitoring](https://docs.influxdata.com/influxdb/v1.7/administration/server_monitoring/)\n\n9.  [Stability and compatibility](https://docs.influxdata.com/influxdb/v1.7/administration/stability_and_compatibility/)\n\n10. [Manage subscriptions](https://docs.influxdata.com/influxdb/v1.7/administration/subscription-management/)\r\n"},{"fields":{"slug":"/Databases/Time-Series-DB/InfluxDB/Commands---Influx-Query-Language-(InfluxQL)/","title":"Commands / Influx Query Language (InfluxQL)"},"frontmatter":{"draft":false},"rawBody":"# Commands / Influx Query Language (InfluxQL)\n\nCreated: 2019-01-11 15:52:41 +0500\n\nModified: 2019-12-28 23:48:57 +0500\n\n---\n\n1.  [Sample data](https://docs.influxdata.com/influxdb/v1.7/query_language/data_download/)\n\n2.  [Data exploration](https://docs.influxdata.com/influxdb/v1.7/query_language/data_exploration/)\n\n3.  [Schema exploration](https://docs.influxdata.com/influxdb/v1.7/query_language/schema_exploration/)\n\n4.  [Data management](https://docs.influxdata.com/influxdb/v1.7/query_language/database_management/)\n\n5.  [Continuous Queries](https://docs.influxdata.com/influxdb/v1.7/query_language/continuous_queries/)\n\n6.  [Functions](https://docs.influxdata.com/influxdb/v1.7/query_language/functions/)\n\n7.  [Mathematical operators](https://docs.influxdata.com/influxdb/v1.7/query_language/math_operators/)\n\n8.  [InfluxQL reference](https://docs.influxdata.com/influxdb/v1.7/query_language/spec/)\nGetting into database (login to influx cli)\n\n$ influx\n\n$ influx -precision rfc3339\n**INFLUXQL**\n\nInfluxQL is an SQL-like query language for interacting with data in InfluxDB.\n**SHOW Commands**\n\nSHOW [DATABASES, DIAGNOSTICS, MEASUREMENTS, QUERIES, SERIES, SHARDS, STATS, SUBSCRIPTIONS, TAG, USERS]\n**Schema Exploration**\n\nCREATE DATABASE parameter_series;\n\nSHOW DATABASES;\n\nSHOW MEASUREMENTS;\n\n**SHOW FIELD KEYS**\n\nSHOW FIELD KEYS ON \"telegraf\"\n\nSHOW FIELD KEYS ON \"telegraf\" FROM \"apache_access_log\"\n\nSHOW RETENTION POLICIES\n\nSHOW RETENTION POLICIES ON \"telegraf\"\n\nSHOW TAG KEYS\n\nSHOW SERIES\n\nSHOW SERIES ON telegraf (huge amount of data)\n\nSHOW TAG VALUES\n\nSHOW TAG VALUES ON \"telegraf\" WITH KEY = \"topic\";\n\nINSERT temp,tag1='tag1',tag2='tag2' value=22.2\n<https://docs.influxdata.com/influxdb/v1.7/query_language/schema_exploration>\n\n## Examples**\n\nSELECT client_ip FROM \"apache_access_log\" (huge amount of data)\n\nSELECT client_ip FROM \"apache_access_log\" ORDER BY time DESC LIMIT 10\nCREATE RETENTION POLICY \"device_ret\" ON \"telegraf\" DURATION 4w SHARD DURATION 2d REPLICATION 1 DEFAULT\n\nALTER RETENTION POLICY \"device_ret\" ON \"telegraf\" DURATION 4w SHARD DURATION 2d DEFAULT\n\nDROP RETENTION POLICY \"autogen\" ON \"hawkbit\"\nselect id,max(writePointsOk) from \"shard\" where \"database\"='telegraf' group by id;\n\nselect \"database\",diskBytes,fieldsCreate,id,writePointsOk from \"shard\" where \"database\"='telegraf' and time > now() -10s\n**InfluxDB**\n\ndocker run --rm -d --name influxdb --net=influxdb -p 8083:8083 -p 8086:8086 influxdb\n**Curl queries**\n\ncurl -G <http://localhost:8086/query> -u todd:influxdb4ever --data-urlencode \"q=SHOW DATABASES\"\n**Chronograf**\n\ndocker run --rm -d --name chronograf -p 8888:8888 --net=influxdb chronograf\n**Continuous Queries**\n\nContinuous Queries (CQ) are InfluxQL queries that run automatically and periodically on realtime data and store query results in a specified measurement.\nCQs were designed to aggregate the data you want to keep in a new measurement (referred to as downsampling). Your time series data comes in thousands or millions of points; you don't want to store them all forever unless absolutely necessary because the disk requirements quickly get out of hand. CQs offer a way for you to keep the summaries of your data without keeping all of the individual points.With CQs, you can have the full resolution data expire with a retention policy (or you can drop it manually) and you keep only what you need.\n<https://docs.influxdata.com/influxdb/v1.7/query_language/continuous_queries>\n"},{"fields":{"slug":"/Databases/Time-Series-DB/InfluxDB/Concepts/","title":"Concepts"},"frontmatter":{"draft":false},"rawBody":"# Concepts\n\nCreated: 2019-07-01 23:36:24 +0500\n\nModified: 2019-07-02 14:53:17 +0500\n\n---\n\n**In-memory indexing and the Time-Structured Merge Tree (TSM)**\n\nThe InfluxDB storage engine looks very similar to a LSM Tree. It has a write ahead log and a collection of read-only data files which are similar in concept to SSTables in an LSM Tree. TSM files contain sorted, compressed series data.\nInfluxDB will create a[shard](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#shard)for each block of time. For example, if you have a[retention policy](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#retention-policy-rp)with an unlimited duration, shards will be created for each 7 day block of time. Each of these shards maps to an underlying storage engine database. Each of these databases has its own[WAL](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#wal-write-ahead-log)and TSM files.\n**Storage Engine**\n\nThe storage engine ties a number of components together and provides the external interface for storing and querying series data. It is composed of a number of components that each serve a particular role:\n-   **In-Memory Index -** The in-memory index is a shared index across shards that provides the quick access to[measurements](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#measurement),[tags](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#tag), and[series](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#series). The index is used by the engine, but is not specific to the storage engine itself.\n-   **WAL -** The WAL is a write-optimized storage format that allows for writes to be durable, but not easily queryable. Writes to the WAL are appended to segments of a fixed size.\n-   **Cache -** The Cache is an in-memory representation of the data stored in the WAL. It is queried at runtime and merged with the data stored in TSM files.\n-   **TSM Files -** TSM files store compressed series data in a columnar format.\n-   **FileStore -** The FileStore mediates access to all TSM files on disk. It ensures that TSM files are installed atomically when existing ones are replaced as well as removing TSM files that are no longer used.\n-   **Compactor -** The Compactor is responsible for converting less optimized Cache and TSM data into more read-optimized formats. It does this by compressing series, removing deleted data, optimizing indices and combining smaller files into larger ones.\n-   **Compaction Planner -** The Compaction Planner determines which TSM files are ready for a compaction and ensures that multiple concurrent compactions do not interfere with each other.\n-   **Compression -** Compression is handled by various Encoders and Decoders for specific data types. Some encoders are fairly static and always encode the same type the same way; others switch their compression strategy based on the shape of the data.\n-   **Writers/Readers -** Each file type (WAL segment, TSM files, tombstones, etc..) has Writers and Readers for working with the formats.\n[**Write Ahead Log (WAL)**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#write-ahead-log-wal)\n\nThe WAL is organized as a bunch of files that look like_000001.wal. The file numbers are monotonically increasing and referred to as WAL segments. When a segment reaches 10MB in size, it is closed and a new one is opened. Each WAL segment stores multiple compressed blocks of writes and deletes.\nWhen a write comes in the new points are serialized, compressed using Snappy, and written to a WAL file. The file isfsync'd and the data is added to an in-memory index before a success is returned. This means that batching points together is required to achieve high throughput performance. (Optimal batch size seems to be 5,000-10,000 points per batch for many use cases.)\nEach entry in the WAL follows a[TLV standard](https://en.wikipedia.org/wiki/Type-length-value)with a single byte representing the type of entry (write or delete), a 4 byteuint32for the length of the compressed block, and then the compressed block.\n**TLV Standard**\n\nWithin[data communication protocols](https://en.wikipedia.org/wiki/Data_communication_protocol),TLV(type-length-valueortag-length-value) is an encoding scheme used for optional information element in a certain protocol.\n\nThe type and length are fixed in size (typically 1-4 bytes), and the value field is of variable size. These fields are used as follows:\n\n**Type**\n\nA binary code, often simply alphanumeric, which indicates the kind of field that this part of the message represents;\n\n**Length**\n\nThe size of the value field (typically in bytes);\n\n**Value**\n\nVariable-sized series of bytes which contains data for this part of the message.\n\nSome advantages of using a TLV representation data system solution are:\n-   TLV sequences are easily searched using generalized parsing functions;\n-   New message elements which are received at an older node can be safely skipped and the rest of the message can be parsed. This is similar to the way that unknown[XML](https://en.wikipedia.org/wiki/XML)tags can be safely skipped;\n-   TLV elements can be placed in any order inside the message body;\n-   TLV elements are typically used in a binary format which makes parsing faster and the data smaller than in comparable text based protocols.\n<https://en.wikipedia.org/wiki/Type-length-value>\n[**Cache**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#cache)\n\nThe Cache is an in-memory copy of all data points current stored in the WAL. The points are organized by the key, which is the measurement,[tag set](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#tag-set), and unique[field](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#field). Each field is kept as its own time-ordered range. The Cache data is not compressed while in memory.\nQueries to the storage engine will merge data from the Cache with data from the TSM files. Queries execute on a copy of the data that is made from the cache at query processing time. This way writes that come in while a query is running won't affect the result.\nDeletes sent to the Cache will clear out the given key or the specific time range for the given key.\nThe Cache exposes a few controls for snapshotting behavior. The two most important controls are the memory limits. There is a lower bound,[cache-snapshot-memory-size](https://docs.influxdata.com/influxdb/v1.7/administration/config#cache-snapshot-memory-size-25m), which when exceeded will trigger a snapshot to TSM files and remove the corresponding WAL segments. There is also an upper bound,[cache-max-memory-size](https://docs.influxdata.com/influxdb/v1.7/administration/config#cache-max-memory-size-1g), which when exceeded will cause the Cache to reject new writes. These configurations are useful to prevent out of memory situations and to apply back pressure to clients writing data faster than the instance can persist it. The checks for memory thresholds occur on every write.\nThe other snapshot controls are time based. The idle threshold,[cache-snapshot-write-cold-duration](https://docs.influxdata.com/influxdb/v1.7/administration/config#cache-snapshot-write-cold-duration-10m), forces the Cache to snapshot to TSM files if it hasn't received a write within the specified interval.\nThe in-memory Cache is recreated on restart by re-reading the WAL files on disk.\n[**TSM files**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#tsm-files)\n\nTSM files are a collection of read-only files that are memory mapped. The structure of these files looks very similar to an SSTable in LevelDB or other LSM Tree variants.\nA TSM file is composed of four sections: header, blocks, index, and footer.\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Header â”‚ Blocks â”‚ Index â”‚ Footer â”‚\nâ”‚5 bytes â”‚ N bytes â”‚ N bytes â”‚ 4 bytes â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nThe Header is a magic number to identify the file type and a version number.\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Header â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Magic â”‚ Version â”‚\nâ”‚ 4 bytes â”‚ 1 byte â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nBlocks are sequences of pairs of CRC32 checksums and data. The block data is opaque to the file. The CRC32 is used for block level error detection. The length of the blocks is stored in the index.\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Blocks â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Block 1 â”‚ Block 2 â”‚ Block N â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ CRC â”‚ Data â”‚ CRC â”‚ Data â”‚ CRC â”‚ Data â”‚\nâ”‚ 4 bytes â”‚ N bytes â”‚ 4 bytes â”‚ N bytes â”‚ 4 bytes â”‚ N bytes â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nFollowing the blocks is the index for the blocks in the file. The index is composed of a sequence of index entries ordered lexicographically by key and then by time. The key includes the measurement name, tag set, and one field. Multiple fields per point creates multiple index entries in the TSM file. Each index entry starts with a key length and the key, followed by the block type (float, int, bool, string) and a count of the number of index block entries that follow for that key. Each index block entry is composed of the min and max time for the block, the offset into the file where the block is located and the size of the block. There is one index block entry for each block in the TSM file that contains the key.\nThe index structure can provide efficient access to all blocks as well as the ability to determine the cost associated with accessing a given key. Given a key and timestamp, we can determine whether a file contains the block for that timestamp. We can also determine where that block resides and how much data must be read to retrieve the block. Knowing the size of the block, we can efficiently provision our IO statements.\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Index â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¤\nâ”‚ Key Len â”‚ Key â”‚ Type â”‚ Count â”‚Min Time â”‚Max Time â”‚ Offset â”‚ Size â”‚...â”‚\nâ”‚ 2 bytes â”‚ N bytes â”‚1 byteâ”‚2 bytesâ”‚ 8 bytes â”‚ 8 bytes â”‚8 bytes â”‚4 bytes â”‚ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜\n\nThe last section is the footer that stores the offset of the start of the index.\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Footer â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚Index Ofsâ”‚\nâ”‚ 8 bytes â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n[**Compression**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#compression)\n\nEach block is compressed to reduce storage space and disk IO when querying. A block contains the timestamps and values for a given series and field. Each block has one byte header, followed by the compressed timestamps and then the compressed values.\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Type â”‚ Len â”‚ Timestamps â”‚ Values â”‚\nâ”‚1 Byte â”‚VByteâ”‚ N Bytes â”‚ N Bytes â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nThe timestamps and values are compressed and stored separately using encodings dependent on the data type and its shape. Storing them independently allows timestamp encoding to be used for all timestamps, while allowing different encodings for different field types. For example, some points may be able to use run-length encoding whereas other may not.\nEach value type also contains a 1 byte header indicating the type of compression for the remaining bytes. The four high bits store the compression type and the four low bits are used by the encoder if needed.\n[**Compactions**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#compactions)\n\nCompactions are recurring processes that migrate data stored in a write-optimized format into a more read-optimized format. There are a number of stages of compaction that take place while a shard is hot for writes:\n-   **Snapshots -** Values in the Cache and WAL must be converted to TSM files to free memory and disk space used by the WAL segments. These compactions occur based on the cache memory and time thresholds.\n-   **Level Compactions -** Level compactions (levels 1-4) occur as the TSM files grow. TSM files are compacted from snapshots to level 1 files. Multiple level 1 files are compacted to produce level 2 files. The process continues until files reach level 4 and the max size for a TSM file. They will not be compacted further unless deletes, index optimization compactions, or full compactions need to run. Lower level compactions use strategies that avoid CPU-intensive activities like decompressing and combining blocks. Higher level (and thus less frequent) compactions will re-combine blocks to fully compact them and increase the compression ratio.\n-   **Index Optimization -** When many level 4 TSM files accumulate, the internal indexes become larger and more costly to access. An index optimization compaction splits the series and indices across a new set of TSM files, sorting all points for a given series into one TSM file. Before an index optimization, each TSM file contained points for most or all series, and thus each contains the same series index. After an index optimization, each TSM file contains points from a minimum of series and there is little series overlap between files. Each TSM file thus has a smaller unique series index, instead of a duplicate of the full series list. In addition, all points from a particular series are contiguous in a TSM file rather than spread across multiple TSM files.\n-   **Full Compactions -** Full compactions run when a shard has become cold for writes for long time, or when deletes have occurred on the shard. Full compactions produce an optimal set of TSM files and include all optimizations from Level and Index Optimization compactions. Once a shard is fully compacted, no other compactions will run on it unless new writes or deletes are stored.\n\n[**Writes**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#writes)\n\nWrites are appended to the current WAL segment and are also added to the Cache. Each WAL segment has a maximum size. Writes roll over to a new file once the current file fills up. The cache is also size bounded; snapshots are taken and WAL compactions are initiated when the cache becomes too full. If the inbound write rate exceeds the WAL compaction rate for a sustained period, the cache may become too full, in which case new writes will fail until the snapshot process catches up.\nWhen WAL segments fill up and are closed, the Compactor snapshots the Cache and writes the data to a new TSM file. When the TSM file is successfully written andfsync'd, it is loaded and referenced by the FileStore.\n[**Updates**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#updates)\n\nUpdates (writing a newer value for a point that already exists) occur as normal writes. Since cached values overwrite existing values, newer writes take precedence. If a write would overwrite a point in a prior TSM file, the points are merged at query runtime and the newer write takes precedence.\n[**Deletes**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#deletes)\n\nDeletes occur by writing a delete entry to the WAL for the measurement or series and then updating the Cache and FileStore. The Cache evicts all relevant entries. The FileStore writes a tombstone file for each TSM file that contains relevant data. These tombstone files are used at startup time to ignore blocks as well as during compactions to remove deleted entries.\nQueries against partially deleted series are handled at query time until a compaction removes the data fully from the TSM files.\n[**Queries**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#queries)\n\nWhen a query is executed by the storage engine, it is essentially a seek to a given time associated with a specific series key and field. First, we do a search on the data files to find the files that contain a time range matching the query as well containing matching series.\nOnce we have the data files selected, we next need to find the position in the file of the series key index entries. We run a binary search against each TSM index to find the location of its index blocks.\nIn common cases the blocks will not overlap across multiple TSM files and we can search the index entries linearly to find the start block from which to read. If there are overlapping blocks of time, the index entries are sorted to ensure newer writes will take precedence and that blocks can be processed in order during query execution.\nWhen iterating over the index entries the blocks are read sequentially from the blocks section. The block is decompressed and we seek to the specific point.\n[**Properties of time series data**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#properties-of-time-series-data)\n\nThe workload of time series data is quite different from normal database workloads. There are a number of factors that conspire to make it very difficult to scale and remain performant:\n-   Billions of individual data points\n-   High write throughput\n-   High read throughput\n-   Large deletes (data expiration)\n-   Mostly an insert/append workload, very few updates\nThe first and most obvious problem is one of scale. In DevOps, IoT, or APM it is easy to collect hundreds of millions or billions of unique data points every day.\nFor example, let's say we have 200 VMs or servers running, with each server collecting an average of 100 measurements every 10 seconds. Given there are 86,400 seconds in a day, a single measurement will generate 8,640 points in a day per server. That gives us a total of 172,800,000 (200 * 100 * 8,640) individual data points per day. We find similar or larger numbers in sensor data use cases.\nThe volume of data means that the write throughput can be very high. We regularly get requests for setups than can handle hundreds of thousands of writes per second. Some larger companies will only consider systems that can handle millions of writes per second.\nAt the same time, time series data can be a high read throughput use case. It's true that if you're tracking 700,000 unique metrics or time series you can't hope to visualize all of them. That leads many people to think that you don't actually read most of the data that goes into the database. However, other than dashboards that people have up on their screens, there are automated systems for monitoring or combining the large volume of time series data with other types of data.\nInside InfluxDB, aggregate functions calculated on the fly may combine tens of thousands of distinct time series into a single view. Each one of those queries must read each aggregated data point, so for InfluxDB the read throughput is often many times higher than the write throughput.\nGiven that time series is mostly an append-only workload, you might think that it's possible to get great performance on a B+Tree. Appends in the keyspace are efficient and you can achieve greater than 100,000 per second. However, we have those appends happening in individual time series. So the inserts end up looking more like random inserts than append only inserts.\nOne of the biggest problems we found with time series data is that it's very common to delete all data after it gets past a certain age. The common pattern here is that users have high precision data that is kept for a short period of time like a few days or months. Users then downsample and aggregate that data into lower precision rollups that are kept around much longer.\nThe naive implementation would be to simply delete each record once it passes its expiration time. However, that means that once the first points written reach their expiration date, the system is processing just as many deletes as writes, which is something most storage engines aren't designed for.\nLet's dig into the details of the two types of storage engines we tried and how these properties had a significant impact on our performance.\n[**LevelDB and log structured merge trees**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#leveldb-and-log-structured-merge-trees)\n\nWhen the InfluxDB project began, we picked LevelDB as the storage engine because we had used it for time series data storage in the product that was the precursor to InfluxDB. We knew that it had great properties for write throughput and everything seemed to \"just work\".\n\nLevelDB is an implementation of a log structured merge tree (LSM tree) that was built as an open source project at Google. It exposes an API for a key-value store where the key space is sorted. This last part is important for time series data as it allowed us to quickly scan ranges of time as long as the timestamp was in the key.\nLSM Trees are based on a log that takes writes and two structures known as Mem Tables and SSTables. These tables represent the sorted keyspace. SSTables are read only files that are continuously replaced by other SSTables that merge inserts and updates into the keyspace.\n\nThe two biggest advantages that LevelDB had for us were high write throughput and built in compression. However, as we learned more about what people needed with time series data, we encountered a few insurmountable challenges.\nThe first problem we had was that LevelDB doesn't support hot backups. If you want to do a safe backup of the database, you have to close it and then copy it. The LevelDB variants RocksDB and HyperLevelDB fix this problem, but there was another more pressing problem that we didn't think they could solve.\nOur users needed a way to automatically manage data retention. That meant we needed deletes on a very large scale. In LSM Trees, a delete is as expensive, if not more so, than a write. A delete writes a new record known as a tombstone. After that queries merge the result set with any tombstones to purge the deleted data from the query return. Later, a compaction runs that removes the tombstone record and the underlying deleted record in the SSTable file.\nTo get around doing deletes, we split data across what we call shards, which are contiguous blocks of time. Shards would typically hold either one day or seven days worth of data. Each shard mapped to an underlying LevelDB. This meant that we could drop an entire day of data by just closing out the database and removing the underlying files.\nUsers of RocksDB may at this point bring up a feature called ColumnFamilies. When putting time series data into Rocks, it's common to split blocks of time into column families and then drop those when their time is up. It's the same general idea: create a separate area where you can just drop files instead of updating indexes when you delete a large block of data. Dropping a column family is a very efficient operation. However, column families are a fairly new feature and we had another use case for shards.\nOrganizing data into shards meant that it could be moved within a cluster without having to examine billions of keys. At the time of this writing, it was not possible to move a column family in one RocksDB to another. Old shards are typically cold for writes so moving them around would be cheap and easy. We would have the added benefit of having a spot in the keyspace that is cold for writes so it would be easier to do consistency checks later.\nThe organization of data into shards worked great for a while, until a large amount of data went into InfluxDB. LevelDB splits the data out over many small files. Having dozens or hundreds of these databases open in a single process ended up creating a big problem. Users that had six months or a year of data would run out of file handles. It's not something we found with the majority of users, but anyone pushing the database to its limits would hit this problem and we had no fix for it. There were simply too many file handles open.\n[**BoltDB and mmap B+Trees**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#boltdb-and-mmap-b-trees)\n\nAfter struggling with LevelDB and its variants for a year we decided to move over to BoltDB, a pure Golang database heavily inspired by LMDB, a mmap B+Tree database written in C. It has the same API semantics as LevelDB: a key value store where the keyspace is ordered. Many of our users were surprised. Our own posted tests of the LevelDB variants vs. LMDB (a mmap B+Tree) showed RocksDB as the best performer.\nHowever, there were other considerations that went into this decision outside of the pure write performance. At this point our most important goal was to get to something stable that could be run in production and backed up. BoltDB also had the advantage of being written in pure Go, which simplified our build chain immensely and made it easy to build for other OSes and platforms.\nThe biggest win for us was that BoltDB used a single file as the database. At this point our most common source of bug reports were from people running out of file handles. Bolt solved the hot backup problem and the file limit problems all at the same time.\nWe were willing to take a hit on write throughput if it meant that we'd have a system that was more reliable and stable that we could build on. Our reasoning was that for anyone pushing really big write loads, they'd be running a cluster anyway.\nWe released versions 0.9.0 to 0.9.2 based on BoltDB. From a development perspective it was delightful. Clean API, fast and easy to build in our Go project, and reliable. However, after running for a while we found a big problem with write throughput. After the database got over a few GB, writes would start spiking IOPS.\nSome users were able to get past this by putting InfluxDB on big hardware with near unlimited IOPS. However, most users are on VMs with limited resources in the cloud. We had to figure out a way to reduce the impact of writing a bunch of points into hundreds of thousands of series at a time.\nWith the 0.9.3 and 0.9.4 releases our plan was to put a write ahead log (WAL) in front of Bolt. That way we could reduce the number of random insertions into the keyspace. Instead, we'd buffer up multiple writes that were next to each other and then flush them at once. However, that only served to delay the problem. High IOPS still became an issue and it showed up very quickly for anyone operating at even moderate work loads.\nHowever, our experience building the first WAL implementation in front of Bolt gave us the confidence we needed that the write problem could be solved. The performance of the WAL itself was fantastic, the index simply could not keep up. At this point we started thinking again about how we could create something similar to an LSM Tree that could keep up with our write load.\nThus was born the **Time Structured Merge Tree.**\n<https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#the-new-influxdb-storage-engine-and-lsm-refined>\n"},{"fields":{"slug":"/Databases/Time-Series-DB/InfluxDB/Influx/","title":"Influx"},"frontmatter":{"draft":false},"rawBody":"# Influx\n\nCreated: 2018-05-17 17:06:10 +0500\n\nModified: 2018-12-24 15:30:58 +0500\n\n---\n\nThe complete time series platform.\n![InfluxData Functional Architecture 10T & Sensors Systems Metrics & Events Apps Message Queues InfluxData Platform ACT Visualize Notify Automate Decision Makers Systems 10T ](media/InfluxDB_Influx-image1.png)\n**TICK Stack**\n\n![Chronograf Complete Interface for the InfluxData Platform Query Results System Stats Databases Networking Message Queues Apps Telegraf Agent for Collecting and Reporting Metrics and Events Plugins InfluxDB Purpose Built Time Series Database Clustered Kapacitor Real-time Streaming Data Processing Engine Define Alert Alerting Frameworks Pull Based Service Discovery Azure / AWS Kubemetexw Anomaly Detection Clustered User Machine Defined Learning Functions ](media/InfluxDB_Influx-image2.png)\n1.  Telegraf\n\n**Telegraf is the Agent for Collecting & Reporting Metrics & Data**\n\nIt is part of the TICK stack and is a plugin-driven server agent for collecting and reporting metrics. Telegraf has plugins or integrations to source a variety of metrics directly from the system it's running on, pull metrics from third-party APIs, or even listen for metrics via a StatsD and Kafka consumer services. It also has output plugins to send metrics to a variety of other datastores, services, and message queues, including InfluxDB, Graphite, OpenTSDB, Datadog, Librato, Kafka, MQTT, NSQ, and many others.\n\n2.  InfluxDB\n\n**InfluxDB is the Time Series Database in the[TICK Stack](https://www.influxdata.com/time-series-platform/)**\n\nInfluxDB is used as a data store for any use case involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics. Conserve space on your machine by configuring InfluxDB to keep data for a defined length of time, automatically expiring & deleting any unwanted data from the system. InfluxDB also offers a SQL-like query language for interacting with data.\n\n3.  Chronograf\n\n**Chronograf is the Complete Interface for the InfluxData Platform**\n\nChronograf is the user interface component of InfluxData's[TICK Stack](https://www.influxdata.com/time-series-platform/). It makes the monitoring and alerting for your infrastructure easy to setup and maintain. It is simple to use and includes templates and libraries to allow you to rapidly build dashboards with real-time visualizations of your data.\n\n4.  Kapacitor\n\n**Kapacitor is a Real-time Streaming Data Processing Engine**\n\nKapacitor is a native data processing engine in the[TICK Stack](https://www.influxdata.com/time-series-platform/). It can process both stream and batch data from InfluxDB. It lets you plug in your own custom logic or user-defined functions to process alerts with dynamic thresholds, match metrics for patterns, compute statistical anomalies, and perform specific actions based on these alerts like dynamic load rebalancing. It also integrates with HipChat, OpsGenie, Alerta, Sensu, PagerDuty, Slack, and[more](https://www.influxdata.com/products/integrations/).**References**\n\n<https://www.influxdata.com/products>"},{"fields":{"slug":"/Databases/Time-Series-DB/InfluxDB/Kapacitor/","title":"Kapacitor"},"frontmatter":{"draft":false},"rawBody":"# Kapacitor\n\nCreated: 2019-05-08 13:53:38 +0500\n\nModified: 2020-01-07 19:07:29 +0500\n\n---\n\nKapacitor is an open source data processing framework that makes it easy to create alerts, run ETL jobs and detect anomalies. Kapacitor is the final piece of the[TICK stack](https://influxdata.com/time-series-platform/).\n**Key Features**\n-   Process both streaming data and batch data.\n-   Query data from InfluxDB on a schedule, and receive data via the[line protocol](https://docs.influxdata.com/influxdb/v1.4/write_protocols/line/)and any other method InfluxDB supports.\n-   Perform any transformation currently possible in[InfluxQL](https://docs.influxdata.com/influxdb/v1.4/query_language/spec/).\n-   Store transformed data back in InfluxDB.\n-   Add custom user defined functions to detect anomalies.\n-   Integrate with HipChat, OpsGenie, Alerta, Sensu, PagerDuty, Slack, and more.\n**Commands**\n\nhelm install stable/kapacitor --name foo --namespace bar\nUsage: kapacitor [options] [command] [args]\n**record** Record the result of a query or a snapshot of the current stream data.\n\n**define** Create/update a task.\n\n**define-template** Create/update a template.\n\n**define-topic-handler** Create/update an alert handler for a topic.\n\nreplay Replay a recording to a task.\n\nreplay-live Replay data against a task without recording it.\n\nwatch Watch logs for a task.\n\nlogs Follow arbitrary Kapacitor logs.\n\nenable Enable and start running a task with live data.\n\ndisable Stop running a task.\n\nreload Reload a running task with an updated task definition.\n\npush Publish a task definition to another Kapacitor instance. Not implemented yet.\n\ndelete Delete tasks, templates, recordings, replays, topics or topic-handlers.\n\n**list** List information about tasks, templates, recordings, replays, topics, topic-handlers or service-tests.\n\n**show** Display detailed information about a task.\n\nshow-template Display detailed information about a template.\n\nshow-topic-handler Display detailed information about an alert handler for a topic.\n\nshow-topic Display detailed information about an alert topic.\n\nbackup Backup the Kapacitor database.\n\nlevel Sets the logging level on the kapacitord server.\n\nstats Display various stats about Kapacitor.\n\nversion Displays the Kapacitor version info.\n\nvars Print debug vars in JSON format.\n\nservice-tests Test a service.\nExamples\n\nkapacitor list tasks\n\nkapacitor show chronograf-v1-1370efff-9133-4cff-b94f-541a5e3d9dd0\n**Creating Alerts**\n\nAlerts in Chronograf correspond to Kapacitor tasks designed specifically to trigger alerts whenever the data stream values rise above or fall below designated thresholds. Please note that only the most common alerting use cases are manageable through Chronograf. These include:\n-   Thresholds with static ceilings, floors and ranges.\n-   Relative thresholds based on unit or percentage changes.\n-   Deadman switches.\n\nMore refined alerts and other tasks need to be defined directly in Kapacitor.\nKapacitor's alert system allows a publish-and-subscribe design pattern to be used. Alerts are published to atopicandhandlerssubscribe to it.\n"},{"fields":{"slug":"/Databases/Time-Series-DB/InfluxDB/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2019-01-11 15:53:28 +0500\n\nModified: 2020-01-17 19:40:28 +0500\n\n---\n\n# Supported Protocols\n\n1.  [CollectD](https://docs.influxdata.com/influxdb/v1.7/supported_protocols/collectd/)\n\n2.  [Graphite](https://docs.influxdata.com/influxdb/v1.7/supported_protocols/graphite/)\n\n3.  [OpenTSDB](https://docs.influxdata.com/influxdb/v1.7/supported_protocols/opentsdb/)\n\n4.  [Prometheus](https://docs.influxdata.com/influxdb/v1.7/supported_protocols/prometheus/)\n\n5.  [UDP](https://docs.influxdata.com/influxdb/v1.7/supported_protocols/udp/)\n# High Availability\n-   Clustering\n# Guides\n\n1.  [Writing data with the HTTP API](https://docs.influxdata.com/influxdb/v1.7/guides/writing_data/)\n\n2.  [Querying data with the HTTP API](https://docs.influxdata.com/influxdb/v1.7/guides/querying_data/)\n\n3.  [Downsampling and data retention](https://docs.influxdata.com/influxdb/v1.7/guides/downsampling_and_retention/)\n\n4.  [Hardware sizing guidelines](https://docs.influxdata.com/influxdb/v1.7/guides/hardware_sizing/)\n\n5.  [Calculating percentages in a query](https://docs.influxdata.com/influxdb/v1.7/guides/calculating_percentages/)\n**Performance Tuning**\n\n1.  Change the shard duration\n\n2.  Series cardinality\n\n3.  Batch points (5000 points per batch)\n\n4.  Down-sample your data\n\n5.  Condense like data, separate unlike data\n\n6.  Be precise (time precision)\n"},{"fields":{"slug":"/Databases/Time-Series-DB/InfluxDB/Tools/","title":"Tools"},"frontmatter":{"draft":false},"rawBody":"# Tools\n\nCreated: 2019-01-11 15:52:26 +0500\n\nModified: 2019-01-11 15:52:34 +0500\n\n---\n\n1.  [InfluxDB command line interface (CLI/shell)](https://docs.influxdata.com/influxdb/v1.7/tools/shell/)\n\n2.  [InfluxDB HTTP API reference](https://docs.influxdata.com/influxdb/v1.7/tools/api/)\n\n3.  [InfluxDB API client libraries](https://docs.influxdata.com/influxdb/v1.7/tools/api_client_libraries/)\n\n4.  [Influx Inspect disk utility](https://docs.influxdata.com/influxdb/v1.7/tools/influx_inspect/)\n\n5.  [Grafana graphs and dashboards](http://docs.grafana.org/datasources/influxdb/)\r\n"},{"fields":{"slug":"/Databases/Time-Series-DB/InfluxDB/Write-Protocols/","title":"Write Protocols"},"frontmatter":{"draft":false},"rawBody":"# Write Protocols\n\nCreated: 2019-01-11 15:53:11 +0500\n\nModified: 2019-01-11 15:53:20 +0500\n\n---\n\n1.  [Line Protocol reference](https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_reference/)\n\n2.  [InfluxDB Line Protocol tutorial](https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_tutorial/)\r\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/1.-Probability-Models-and-Axioms/","title":"1. Probability Models and Axioms"},"frontmatter":{"draft":false},"rawBody":"# 1. Probability Models and Axioms\n\nCreated: 2018-06-01 23:54:37 +0500\n\nModified: 2018-06-25 19:42:22 +0500\n\n---\n\nBasic structure of probability models, including the sample space and the axioms that any probabilistic model should obey, together with some consequences of the axioms and some simple examples.\n**Probabilistic Model**\n\nA probabilistic model is a quantitative description of a situation, a phenomenon or an experiment whose outcome is uncertain.\nPutting together such a model involves two key steps -\n\n1.  We need to describe the possible outcomes of the experiment. This is done by specifying a so-called sample space.\n\n2.  We specify a probability law, which assigns probabilities to outcomes or to collections of outcomes. (tells us whether one outcome is much more likely than some other outcome)\nProbabilities have to satisfy certain basic properties in order to be meaningful, these are the axioms of probability theory. For example probabilities cannot be negative.\n![Sample space List (set) of possible outcomes, List must be: Mutually exclusive Collectively exhaustive --- At the \"right\" granularity â€¢ T Qmc( ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image1.png)\nSample spaces are sets. And a set can be discrete, finite, infinite, continuous, and so on.\n![Sample space: discrete/ finite example Two rolls of a tetrahedral die Y = Second roll moo X = First roll sequential description 1.3 4.4 ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image2.png)\n![Sample space: continuous example (x, y) such that C) x, y S 1 1 1 ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image3.png)\n![Probability axioms Event: a subset of the Probability is assigned Axioms: sample space to events --- Nonnegativity: P (A) 0 --- Normalization: P (Q) 1 --- (Finite) additivity: (to be strengthened later) If An B = Ã˜, then U B) = P(A) + P(B) Avb ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image4.png)\n![Some simple Axioms P(Q) = 1 consequences of the axioms Consequences For disjoint events: C) = P(A) + P(B) +P(C) and similarly for k disjoint events = P(SI) + + pcs k) ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image5.png)\n![å å¼ æ–— 0 ã„ å½“ 0 å¼ vâ€¢vo - ã– ã• ãƒŸ u v)d ãƒ¼ (g)d å (v)d â…¡ ãƒŸ ã‚³ v)d S -0 ä¸€ xe 9q1 ã€ 0 soouonbOStIOO 9 ã€ (a)d å (v)d â…¥ ãƒŸ ã‚³ ã“ 4 9 9 ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image6.png)\n![More consequences of the axioms uBuC) = PCI) + B) + BC n C) â€¢ avg v c (J (B QA c) u (cQ/$nBC) c ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image7.png)\n![Probability calculation: discrete/ finite example Two rolls of a tetrahedral die â€¢ Let every possible outcome have probability 1/16 Y = Second roll azoz ataoo azaz Let Z 16 = min(X, Y) 1/16 1 3 2 X = First roll 4 ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image8.png)\n**Discrete Uniform Law -** It says that when our sample space is discrete, and when the outcomes in our sample space are equally likely, then to compute the probability of any event A, we can simply count the number of outcomes in A and divide it by the total number of possible outcomes.\n![Discrete uniform law Assume Q consists of n Assume A consists of k equally likely elements o o o o o 1 prob = n o ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image9.png)\n![Probability calculation: continuous example (x, y) such that O S 1 y 1 1 Uniform probability law: 1 2 Probability 2 1 2 = Area 8 ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image10.png)\n\nDiscrete and Infinite sample space - We toss a coin until we see a head\n\n![Probability calculation: discrete but infinite sample space Sample space: {1, 2, 1/2 --- We are given P (n) = 1/4 1/8 2 92 - 2 ) P(outcome is even) = Y 9 6 22 1/16 ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image11.png)\n![Mathematical subtleties Countable Additivity Axiom: If Al, 112, 113,... is an infinite sequence of disjoint events, then P(AI CJ U U...) pc Al) + pc A 2) + p(A3) + . Additivity holds only for \"countable\" sequences of events The unit square (simlarly, the real line, etc.) is not countable (its elements cannot be arranged in a sequence) \"Area\" is a legitimate probability law on the unit square, as long as we do not try to assign probabilities/ areas to \"very strange\" sets ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image12.png)\n![The role of probability theory A framework for analyzing phenomena with uncertain outcomes Rules for consistent reasoning Used for predictions and decisions Predictions Real world Decisions Data Probability theory (Analysis) Models Inference/Statistics ](media/Intro---Syllabus_1.-Probability-Models-and-Axioms-image13.png)\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets/","title":"1.1 Set, Sequences, Limits and Series, (un)countable sets"},"frontmatter":{"draft":false},"rawBody":"# 1.1 Set, Sequences, Limits and Series, (un)countable sets\n\nCreated: 2018-06-02 15:05:02 +0500\n\nModified: 2018-06-25 19:48:03 +0500\n\n---\n\n![Sets A collection of distinct elements f z â‚¬ 12 : ---Q : UM\" versa e sefr : sel s2c= 01 c (sc)C- ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image1.png)\n\n![Unions and intersections SUT SQT xeSuT xe SQ T sooue 00 me T ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image2.png)\n\n![Set properties SUT sn (T U) (sc)c - Sun Ã˜, Q, Sno ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image3.png)\n\n![De Morgan's laws ( SOT Y : s Cu T -SOT SCOT C ( SVT) c r c (SnT)Cez+ St) Ts U sfÃ¬ (Ã§sn)C- UTC ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image4.png)\n![Mathematical background: Seque4ace Sequences and their limits Q, e S s-IR do \"ene ex/sds or E em IQ, ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image5.png)\nSequence can converge to a number\n![Mathematical background: When does a sequence converge? If ai ai+l, for all i, then either: --- the sequence \"converges to N\" --- the sequence converges to some real number a If la; ---al bi, for all i, and bi O, then ai -9 a ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image6.png)\n-   monotonic series - If a series is an increasing sequence or decreasing sequence, we call it monotonic. A monotonic sequence always converge to either a finite number or infinity\n\n![Mathematical background: n ai Infinite series v l'61ed e F Isos If ai > O: limit exists if terms do not all limit need not exist have the same sign: limit may exist but be different if we sum in a different order --- Fact: limit exists and independent of order of summation if lazâ€¢l < oc ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image7.png)\n\n![Mathematical background: 1 + 02 + 00 Do Geometric series 1 1 S Å„ 00 5 or ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image8.png)\n![About the i ~ 1 , ä¸¿ ~ 1 å 1 ä¸€ 1 å 1 ä¸€ 1 å 1 ä¸€ 1 | 4 0 0 order Of summation In serles 0 0 0 0 0 0 with multiple indices 0 0 0 0 0 0 0 0 0 0 0 ä¸€ 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 å 1 0 å 1 ä¸€ 1 0 ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image9.png)\n![About the order of summation in series with multiple indices (i,j): O o O O o O O o o 0 o o O 0 o o O 0 o o O O o O O o O O o O 0 O o O 0 o o O i 0 o o O i ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image10.png)\n\nCountable - Able to arrange all the elements of a sample space in a sequence\n\n![Countable versus uncountable infinite sets Countable: can be put in 1-1 correspondence with positive integers 123 positive integers integers O, 1 pairs of positive integers --- rational numbers q, with O < q < 1 % 3/19 Uncountable: not countable --- the interval [O, 1] --- the reals, the plane,... a, 0 o o 0 o o o o o o o o O o o O i ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image11.png)\n![The reals are uncountable Cantor's diagonalization argument ole c.omae 3, H? fop ](media/Intro---Syllabus_1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets-image12.png)\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule/","title":"10. Conditioning on a random variable; Independence; Bayes' rule"},"frontmatter":{"draft":false},"rawBody":"# 10. Conditioning on a random variable; Independence; Bayes' rule\n\nCreated: 2018-06-27 00:42:16 +0500\n\nModified: 2018-06-29 22:11:16 +0500\n\n---\n\n![LECTURE 10: Conditioning on a random variable; Conditioning X on Y --- Total probability theorem --- Total expectation theorem Independence independent normals A comprehensive example Four variants of the Bayes rule Indepen ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image1.png)\n\n![Conditional PDFs, given another r.v. py(y) if py(y) > O PxlY Definition: fxIY(x I y) if fy(y) > O fY(y) fxIA(x) â€¢ O, where P(A) > O ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image2.png)\n\n![Comments on conditional fxIY(x I y) --- fy(y) fxlY(x I y) 2 0 Think of value of Y as fixed at some y shape of fxy(. I y): slice of the joint fxlY(x I y) dx fy(y) Multiplication rule: ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image3.png)\n\n![Total probability and expectation px(x) I y) y Expected value rule. theorems fx(x) xfxlY(a ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image4.png)\n\n![Independence y) = px(x) .fx(x) fy(y), for all x, y for all x and y y) fxlY(x I Y) fy(y) equivalent to: fxly(x I y) = fx(x), for all y with fy(y) > O an E[XY] If X, Y are independent: ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image5.png)\n\nBy analogy with the discrete case, we will say that two jointly continuous random variables are independent if the joint PDF is equal to the product of the marginal PDFs\n![Stick-breaking example Break a stick of length C twice first break at X: uniform in --- second break at Y: uniform in [O, X] fx(x) 1 fylx(y I x) ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image6.png)\n\n![Stick-breaking example fy(y) Using total expectation theorem: J/ ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image7.png)\n\nIntuition - If we break a stick once, the expected value or what we're left with is half of what we started with. But if we break it once more, then we expect it on the average to be cut by a factor again of 1/2. And so we expect to be left with a stick that has length 1/4 of what we started with.\n![Independent standard normals 277 277 ex p Ã­ -i (224 72)3 0.2 0.15 0.1 0.05 ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image8.png)\n\n![Independent normals 1 2770x0!] 1 202 0.1 0.08 c 0.06 0.04 0.02 ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image9.png)\n\nTo conclude, the joint PDF of two independent normals has the shape of a bell. The center of the bell is determined by the means. Furthermore, the bell is stretched in the x and y directions by an amount that is determined by the variances of x and y. However, the stretching is always along the coordinate axes.\n\nIf you wanted a bell that stretches in some diagonal direction, or if you have contours that are ellipses but with some different kind of axes, then you will have dependence between the two random variables. In that case, we will be dealing with a so-called bivariate normal distribution.\n![The Bayes rule pxIY( â€¢ I y) x unobserved value prior â€¢ ) --- a theme with variations Inference a: observed value y model Â¯ px(x) I x) pxlY(x I Y) fx(x) fy(y) fx(x) ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image10.png)\n\n![The Bayes rule --- one discrete and one continuous random K: discrete - r ( 80) (F Y: continuous ! Y s 7+8) (K:E PK(k) I k) PIClY(k I y) = PRI ylk) = ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image11.png)\n\n![The Bayes rule --- discrete unknown, continuous measurem unkown K: equally likely to be ---1 or +1 measurement Y: Y=IC+VV; w Y 1K: I Ott (l, J) Probability that K = 1, given that Y y? PIC(k) = 1/2 I k) 'Â¯ PIClY(k I y) fy(y) ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image12.png)\n\n![The Bayes rule --- continuous unknown, discrete measuren- measurement K: Bernoulli with parameter Y 7 1 fylK(y I k) = PK(k) unkown Y: uniform on [O, 1] Distribution of Y given that fy(y) PIC(I) PKIY(I I y) 7 2 ](media/Intro---Syllabus_10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule-image13.png)\n\nThe joint PMF\n\nThe Conditional PMF given an event\n\nConditional PMF of one random variable given another\nJoint PDF of X and Y\n\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/11.-Derived-Distributions/","title":"11. Derived Distributions"},"frontmatter":{"draft":false},"rawBody":"# 11. Derived Distributions\n\nCreated: 2018-06-29 22:39:33 +0500\n\nModified: 2021-10-04 23:28:36 +0500\n\n---\n\n![UNIT 6: Further topics on r.v.'s Derived distributions Y = g(x) Covariance and correlation A deeper view of conditioning ](media/Intro---Syllabus_11.-Derived-Distributions-image1.png)\n1.  Derived distributions - how to find the distribution, that is the PMF or the PDF of a random variable that is defined as a function of other random variables known as distributions\n\n2.  Covariance and correlation - which help us describe in easily quantifiable manner the strength of the relation between two dependent random variables\n\n3.  Conditional expectation - Can be viewed as a special kind of random variable.![LECTURE 11: Derived distributions Given the distribution of X, Y = g(x) find the distribution of --- the discrete case --- the continuous case --- general approach, using CDFs --- the linear case: Y = ax + b --- general formula when g is monotonic Given the (joint) distribution of X and Y, find the distribution of ](media/Intro---Syllabus_11.-Derived-Distributions-image2.png)\n\n![Derived distributions --- the discrete case Y g(x) 0.4 0.3 4 prob 0.2 3 0.1 2 y 5 4 3 2 py(y) ](media/Intro---Syllabus_11.-Derived-Distributions-image3.png)\n\n![A linear function of a discrete r.v. 3/6 pz(z) 2/6 1 1/6 1 1/61 2 Y = 2X+3 : 2+3 ---2 1 316 ](media/Intro---Syllabus_11.-Derived-Distributions-image4.png)\n\n![A linear function of a continuous r.v. 2/3 13 1 1 fy(y) ](media/Intro---Syllabus_11.-Derived-Distributions-image5.png)\n\n![A linear function of a continuous r.v. -1-19 (x J-px( ](media/Intro---Syllabus_11.-Derived-Distributions-image6.png)\n\n![A linear function of a normal r.v. is normal X JV(/I, 02) fx(x) fy(y) 1 e 27r a fx(a 2/202 e Tri ](media/Intro---Syllabus_11.-Derived-Distributions-image7.png)\n\n![A general function g(X) of a continuous r.v. Two-step procedure: Find the CDF of Y: Fy(y) s y) = f (7Cx) S)) CIFY Differentiate: fy(y) --- dy ](media/Intro---Syllabus_11.-Derived-Distributions-image8.png)\n\n![Example: Y = X3; (7) = 1 ( M 47) _ X uniform on [O, 2] (x3s 7) (x y'/ 3) ](media/Intro---Syllabus_11.-Derived-Distributions-image9.png)\n\n![Example: Y a/ X â€¢ You go to the gym and set the speed X of the treadmill to a number between 5 and 10 km/ hr (with a uniform distribut Find the PDF of the time it takes to run 10km. 10 f 10 10 ](media/Intro---Syllabus_11.-Derived-Distributions-image10.png)\n\n![A general formula for the PDF of Y g(X) when g is mono Assume g strictly inc Ing and differentiable Y = g(:r) (Ysa ](media/Intro---Syllabus_11.-Derived-Distributions-image11.png)\n\n![Example: Y X 2; X uniform on [O, 1] fy (y) f x (y) dy ](media/Intro---Syllabus_11.-Derived-Distributions-image12.png)\n\n![An intuitive explanation for the monotonic case dg slope [y, !J+Ã¶2] fxcÃ¦)Sl f, (z) oca ](media/Intro---Syllabus_11.-Derived-Distributions-image13.png)\n\n![A nonmonotonic example: Y The continuous case: The discrete case: PY(9) = c X : 22) +1 ](media/Intro---Syllabus_11.-Derived-Distributions-image14.png)\n\n![A function of multiple r.v.'s: Z = g(X, Y) Same methodology: find CDF of Z â€¢ Let Z Y/ X; X, Y independent, uniform on 2 ](media/Intro---Syllabus_11.-Derived-Distributions-image15.png)\n**Joint Effects**\n\nIf you believe that your dependent variable is affected by interactions between independent variables, you can capture the cross effects by looking at products of the independent variables. Thus, if you are regression PE ratios against market cap and expected growth rates, you can do the following:\n\nPE = a + b Growth Rate + c Mkt Cap + d (Growth Rate * Mkt Cap)\n\nDerivative of a CDF is a PDF\nDiscrete Formula\n\nContinuous Random Variable Formula\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation/","title":"12. Sums of independent r.v.'s; Covariance and Correlation"},"frontmatter":{"draft":false},"rawBody":"# 12. Sums of independent r.v.'s; Covariance and Correlation\n\nCreated: 2018-07-19 02:24:22 +0500\n\nModified: 2018-07-19 02:46:27 +0500\n\n---\n\n![LECTURE 12: Sums of independent random variables; Covariance and correlation The PMF/PDF of (X and Y independent) --- the discrete case --- the continuous case --- the mechanics --- the sum of independent normals Covariance and correlation --- definitions ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image1.png)\n\n![The distribution of X + Y: the discrete case pz(z) --- Z=X+Y; x, Y independent, discrete pz(3) (0,3) known PMFs rx (o) g ) (D py(2) pct.. (2,1) 30 00 ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image2.png)\n\n![Discrete convolution mechanics PX 1/3 3/6 2/6 1/6 3/6 2/3 2/6 3/6 2/6 1/6 1/6 pz(z) _ To find Flip (horizontz Put it undernc Right-shift ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image3.png)\n\n![The distribution of X + Y: the continuous case Z = X + Y; X, Y independent, continuous known PDFs onditional on X = x: Z pz(z) --- fz(z) (223) Joint PDF of Z and X: ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image4.png)\n\n![fx(x) fz(z) (algebra) The sum of independent normal r v s X Y N(ÃŸy, 2), independent (y---/ly) exp { exp { fx 1 e 27t 1 27'r(a2; -Jr a; ) fy(y) Â¯ 1 e 27 ay 2/203 1 exp { z 202 ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image5.png)\n\n![Covariance Zero-mean, discrete X and --- if independent: E[XY] = xy<O Definition for ge cov(x, Y) = E (X -E â€¢ independent (converse is not ti (0,1) ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image6.png)\n\nCovariance in general tells us whether two random variables tend to move together, both being high or both being low, in some average or typical sense.\n![Covariance properties ÑĞ¾Ñ† Ñ…, Ñ…) --- Ğ‘Ğ“ (Ñ…- -ÑĞ´Ğ³ĞºÑ˜)2 cov(aX + Ğ¬, Ğ£) : Ğ° ÑĞ¾ ÑĞ¾Ñ† Ñ…, Ğ£) = Ğ• (Ğ¥ ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image7.png)\n\n![The variance of a sum of random variables + (ma - gLX21)2 ) COV ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image8.png)\n\n![The variance of a sum of random variables var(X1 + X2) var(X1) + var(X2) + 2 cov(X1, X2) var(X1 + + x n) = Cas5uwe 2 z.) Co v (roxa ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image8.png)\n\n![The Correlation coefficient Dimensionless version of covariance: Measure of the degree of \"association\" Independent p = O, \"uncorrelated\" (converse is not true) p(x, Y) ax cov(x, Y) between X and Y ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image8.png)\n\n![Proof of key properties of the correlation coefficient p(x, Y) ax â€¢ Assume, for simplicity, zero means and unit variances, so that pY)2 --- ECK 2] -2 p + p 2 If Ipl = 1, then 92 X = Y ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image8.png)\n\n![Interpreting the correlation coefficient p(x, Association does not imply causation or influence X: math aptitude Y: musical ability Correlation often reflects underlying, common, hidden factor --- Assume, Z, V, W are independent â€¢Y=Z+VV Assume, for simplicity, that Z, V, W have zero means, unit Var(x) = 2 => ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image9.png)\n\n![Correlations matter... A real-estate investment company invests $10M in each of 10 At each state i, the return on its investment is a random varia with mean 1 and standard deviation 1.3 (in millions). 10 var(X1 If the Xi are uncorrelated, then: : 1600) a(X1 + var(XI â€¢ â€¢ â€¢ + â€¢MIO) Â¯ If for i i, = 0.9: Co V ](media/Intro---Syllabus_12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation-image10.png)\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/13.-Conditional-expectation-and-variance-revisited/","title":"13. Conditional expectation and variance revisited"},"frontmatter":{"draft":false},"rawBody":"# 13. Conditional expectation and variance revisited\n\nCreated: 2018-07-19 03:25:33 +0500\n\nModified: 2018-08-17 20:21:41 +0500\n\n---\n\n![LECTURE 13: Conditional expectation and variance revisite Application: Sum of a random number of in A more abstract version of the conditional expectation --- view it as a random variable --- the law of iterated expectations A more abstract version of the conditional variance --- view it as a random variable --- the law of total variance Sum of a random number ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image1.png)\n\n![Conditional expectation Function h = x2, for all a; e.g., h(x) as a random I Y y] EXPXIY(X I y) â€¢ g(y) (integral in continuous case) Remarks: variable Random variable X; h(X) is the r.v. that t if X happens to take is the r.v. that takes th if Y happens to take th E[xIY.] Definition: ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image2.png)\n\n![The mean of E[X I Y]: Law of iterated expectations 2 20) (X) 7 ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image3.png)\n\n**Law of iterated expectations -** Expectation of a conditional expectation, is the same as the unconditional expectation\n![Stick-breaking example Stick example: stick of length C break at uniformly chosen point Y fY(y) break what is left at uniformly chosen point X fxIY(x I y) ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image4.png)\n\n![Forecast revisions Suppose forecasts are made by calculating expected value, given any available information X: February sales Forecast in the beginning of the year: x End of January: will get new information, value y of Y Revised forecast: ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image5.png)\n\n![The conditional variance as a random variable var(X) var(x I Y = y) = E [(X I Y = 1 Y = y] var(X I Y) is the r.v. that takes the value var(X I Y y), when Example: X uniform on [O, Y] var(x I Y var(X I Y) ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image6.png)\n\n![Derivation of the law of total variance var(X) E var(X I Y)] + I Y]) var(x I Y = y) = E [X 2) var(X I Y) = ) E var(x I Y) 4 â€¢ var(X) 2 2 ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image7.png)\n\n![A simple example fx@) var(X) I Y)] + 9/16 '/2 var(x I Y: var(x I Y) --- var(X I Y: E var(x I Y)] Ä±z 2 2 ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image8.png)\n\n![Section means and variances Two sections of a class: y 1 (10 students); y = 2 (20 stuc Xi: score of student i Experiment: pick a student at random (uniformly) random variables: X and Y 60 Data: I Y = 1] = 90 10 (90 â€¢ 30 y = 2: 20 E 90 i=ll Jo +60 '/3 ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image9.png)\n\n![Section means and variances (ctd.) 90, Elx I Y] 60, W.P. 1/3 W.P. 2/3 10 1 10 20 E Elx I Y]] 70 Elx] -(90-70) +- More data: var(x I Y--- var(x I Y = 2) 90)2 10 var(x I Y) E var(x I Y)] 30 1 E (Xi 60)2 20 i=11 ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image10.png)\n\n![Sum of a random number of independent r.v.'s N: number of stores visited (N is a nonnegative integer r.v.) Let ELY IN = n] Total expectation theorem: ELY] = E n] Xi: money spent in --- Xi independent, id independent of N ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image11.png)\n\n![Variance of sum of a random number of independent r.v.'s var(Y) I N)] EIN] var(X) var(Y) I N] NE[X] N]) --- i/Qr (G Ex)) var(Y I N) : 01 Van x ](media/Intro---Syllabus_13.-Conditional-expectation-and-variance-revisited-image12.png)\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/14.-Intro-to-Bayesian-Inference/","title":"14. Intro to Bayesian Inference"},"frontmatter":{"draft":false},"rawBody":"# 14. Intro to Bayesian Inference\n\nCreated: 2018-08-17 20:32:04 +0500\n\nModified: 2022-04-07 21:18:39 +0500\n\n---\n\n**The power of Bayesian statistics**\n-   Outcome characterization - This is the distribution of things that happened\n-   Latent factor analysis - These are the things affect your outcome\n-   Decision making - Given all the potential outcomes here's the most optimal choice we should make today\n\n<https://www.youtube.com/watch?v=pJH_2y9J9-I>\na.  We apply the Bayes rule to find the posterior distribution of an unknown random variable given one or multiple observations of related random variables.\n\nb.  We discuss the most common methods for coming up with a point estimate of the unknown random variable (Maximum a Posteriori probability estimate, Least Mean Squares estimate, and Linear Least Mean Squares estimate).\n\nc.  We consider the question of performance analysis, namely, the calculation of the probability of error in hypothesis testing problems or the calculation of the mean squared error in estimation problems.\n\nd.  To illustrate the methodology, we pay special attention to a few canonical problems such as linear normal models and the problem of estimating the unknown bias of a coin.\n![LECTURE 14: Introduction to Bayesian inference The big picture motivation, applications --- problem types (hypothesis testing, estimation, etc.) The general framework Bayes' rule posterior (4 versions) --- point estimates (MAP, CMS) performance measures) ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image1.png)\n\n![Inference: the big picture redictions Real world Decisions Data Probability theory (Analysis) Models Inference/ Statistics ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image2.png)\n\n![Inference then and now Then: 10 patients were treated: 3 died 10 patients were not treated: 5 died Therefore Now: Big data Big models ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image3.png)\n\n![A sample of application domains Design and interpretation of experiments --- polling STATE COUNT 17 SOLIDLY DEMOCRATIC 23 sot 11 TOSSUP ELECTORAL VOTE C 237 LIKELY DEMOCRATIC 191 Ll 110 TOSSUP ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image4.png)\n\n![A sample of application domains marketing, advertising o recommendation systems Netflix competition persoms ononooaooosâ€¢ aaaasaanoool aaaanaoaasol onoaoÃ¸sasaaâ€¢ oaoaonosanol oaoÃ¸snzÃ¸aÃ¸oâ€¢ n onannÃ¸oÃ¸oâ€¢ ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image5.png)\n\n![](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image6.png)\n\n![A â€¢ sample of application Life sciences genomics domains Cytokines EPC) --- systems bio Svvival FÂ«tors (e.g- IGFI) Hormmes. Transmitters (e.g.. interleuk'b. serÂ«onin. eE.) Growth F Syntbol ASBII FTI-IL17 PHF8 O = GPCR = PKC Akka JAKS STABS Bc1-xL Caspase 9 Apoptosis BCI.2 Raf ME LOC16998) , SLC16A2 ZNF6 CAPZAIP ZETB33 MCTSI Et,F4 LOC266694 cyclase MEKK 17 h. .41 Myc: Mad: Max Max CREB Gene ARE mdm2 ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image7.png)\n\n![A sample of application domains Modeling and monitoring the oceans Modeling and monitoring global climate Modeling and monitoring pollution Interpreting data from physics experiments ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image8.png)\n\n![A sample of application domains Signal processing communication systems (noisy ... ) --- speech processing and understanding image processing and understanding --- tracking of objects positioning systems (e.g. GPS) ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image9.png)\n\n![Model building versus inferring unobserved variables X â€¢ = as + W Model building: w S, observe X Predictions Real world Decisions Data Inference/St know \"signal\" infer a Variable estimation: ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image10.png)\n\n![Hypothesis testing versus estimation Hypothesis testing: unknown takes one of few possible values aim at small probability of incorrect decision Is it an airplane or a bird? Estimation: numerical unknown(s) ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image11.png)\n\n![The Bayesian inference Unknown e --- treated as a random framework variable prior distribution pe or fe Observation X observation model Pxle or f x. Use appropriate version of the Bayes rule to find pelx(.lX = x) or felx(.lX = x) Prior D Where does thc --- symmetry known range earlier studies --- subjective or ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image12.png)\n\n![The output of Bayesian inference The complete answer is a posterior distribution: PMF pelx(â€¢ I x) felx(â€¢ I x) or PDF ELECT( ROM?' PelX(â€¢ I x) Prior D felX(â€¢ I x) 6% 3% 2% 1% 0% O o ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image13.png)\n\n![Point estimates in Bayesian inference The complete answer is a posterior distribution: PMF pelx(â€¢ I x) felx(â€¢ I x) or PDF PelX(â€¢ I x) felX(â€¢ I x) estinu (numb estim (randc â€¢ Maximum a posteriori probability (MAP): I c) = maxpelx(0 1 c) ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image14.png)\n\n![Discrete e, discrete X values of e: pelx(0 1 x) 0.1 1 MAP rule: alternative hypotheses 0.6 2 0.3 3 0 pelx(0 1 x) --- px(x) = El conditional prob of error smallest under the M/ overall probability of err e) = Ep(e ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image15.png)\n\n![pelx(0 1 x) Discrete e, continuous x Standard example: --- send signal e e {1, 2, 3} w N indep. of O fxle(x | 0) = fw(x O) 0.1 0.6 0.3 pelx(0 1 x) fx(x) = Epe( conditional prob of e smallest under the overall probability o ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image16.png)\n\n![Continuous e, continuous X linear normal models estimation of a noisy signal e and W: independent normals felx(0 1 x) fx(x) = fei multi-dimensional versions (many normal parameters, man estimating the parameter of a uniform intere ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image17.png)\n\n![Inferring the unknown bias of a coin and the Beta distribut Standard example: --- coin with bias e; prior fe(â€¢) fix n; K =number of heads Assume fe(.) is uniform in [O, 1] felK(0 1 k) 1 0 d(n, k) 01 k PFC k) \"Beta 1 felK(0 1 k) = PK(k) = fe( distribution, with parameters (3 ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image18.png)\n\n![Inferring the unknown bias of a coin: Standard example: --- coin with bias e; prior fe(â€¢) fix n; K =number of heads Assume fe(.) is uniform in [O, 1] 1 Ok (1 --- O) feuc(0 1 k) d(n, k) MAP estimate: 0M A p = point estimates 0 ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image19.png)\n\n![Summary â€¢ Problem data: pe(â€¢), pxle(â€¢ I pelx(â€¢ I x) Given the value a; of X: find, e.g., using appropriate version of the Bayes rule (q cLl'ce Estimator e = g(X) Estimate D = g(x) MAP: 0M AP = gMAp(x) maximizes pelx(0 1 x) L MS: DLMs= Q Ms(x) = I X = ](media/Intro---Syllabus_14.-Intro-to-Bayesian-Inference-image20.png)\n[Bayes theorem, and making probability intuitive](https://www.youtube.com/watch?v=HZGCoVF3YvM)-   Bayes' theorem describe the probability of an event occurring, based upon prior knowledge of other variables related to that event\n    -   In effect, it is a conditional probability, with the probability of an event conditioned on the information/knowledge you have\n    -   Since the information/knowledge that different individuals can have about an event can vary, Bayes' thorem allows for differences in probability estimates for the same event across individuals\n-   In Bayesian Inference, you update the probability of an event happening as you receive new evidence or information\n    -   The probability that you assign to an event before you receive the new information represent your priors\n    -   The probability that you assign to that same event after receiving and processing new information represent your posterior estimate\n\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/2.-Conditioning-and-Independence/","title":"2. Conditioning and Independence"},"frontmatter":{"draft":false},"rawBody":"# 2. Conditioning and Independence\n\nCreated: 2018-06-02 23:42:36 +0500\n\nModified: 2018-08-17 19:58:33 +0500\n\n---\n\nConditioning leads to revised (\"conditional\") probabilities that take into account partial information on the outcome of a probabilistic experiment. Conditioning is a very useful tool that allows us to \"divide and conquer\" complex problems. Independence is used to model situations involving non-interacting probabilistic phenomena and also plays an important role in building complex models from more elementary ones.\n# Conditioning and Bayes' rule\n-   **Conditional Probability**\n\nThe conditional probability of an event given another event is the probability of their intersection divided by the probability of the conditioning event.\n-   Three important tools\n    -   Multiplication rule\n    -   Total probability theorem\n    -   Bayes' rule - provides a systematic way for incorporating new evidence into a probability model. Foundation of the field of inference. It is a guide on how to process data and make inferences about unobserved quantities or phenomena.\n![The idea of conditioning Use new information to revise a model Assume 12 equally likely outcomes If told B occurred: 13 0 12 12 2 12 12 12 12 ](media/Intro---Syllabus_2.-Conditioning-and-Independence-image1.png)\n![Definition of conditional probability An B 2 4 12 12 I B) \"probability of A, given that B occurred\" P(AnB) defined only when P(B) > O 3 ](media/Intro---Syllabus_2.-Conditioning-and-Independence-image2.png)\n\nP ( A | B ) = Probability of **A given B**\n\nP ( A | B ) = Here B is called the conditioning event\n\n![Example: two rolls of a a-sided die 4 3 Y = Second roll 2 1 3 2 1 X = First roll 4 1/16 Let B be the event: min(X, Y) Let M = max(X, Y) 2 5 ](media/Intro---Syllabus_2.-Conditioning-and-Independence-image3.png)\n\n**Conditional Probabilities share probabilities of ordinary probabilities**\n-   Conditional probabilities must be non-negative\n\n![Conditional probabilities share properties of ordinary probabilities assuming P (B) > O (sing) mg) Eco - eco If Anc=Ã¸, then P(AuCIL) I B) +P(C I B) f ( (fiijvaÃ¼))_ f (aog)+f (cog) ](media/Intro---Syllabus_2.-Conditioning-and-Independence-image4.png)\n![Models based on conditional probabilities Event A: Airplane is flying above Event B: Something registers on radar screen PCB I â€¢ P(AnB) 90.9500.) 0.05. 0.99 PCB I A) = An B A n BC b .05 P(AC) 0.3 q 0.05 0.95 0.99 0.01 0.10 0.90 P(An B) c ](media/Intro---Syllabus_2.-Conditioning-and-Independence-image5.png)\n![](media/Intro---Syllabus_2.-Conditioning-and-Independence-image6.png)\n![Total probability theorem 13 Partition of sample space into Al, 742, Have P ( AZ), for every i Have P (B I AD, for every i P(B) I) (BOA2) avenope ](media/Intro---Syllabus_2.-Conditioning-and-Independence-image7.png)\n![Bayes' rule Partition of sample space into Al, 742, A3 Have P (AD, for every i initial \"beliefs\" Have P (B I AD, for every i revised \"beliefs,\" given that B occurred: f (Ateqg) P(AZ I B) = ](media/Intro---Syllabus_2.-Conditioning-and-Independence-image8.png)\n![Bayes' rule and inference --- Thomas Bayes, presbyterian minister (c. 1701-1761) \"Bayes' theorem,\" published posthumously --- systematic approach for incorporating new evidence Bayesian inference â€¢ --- initial beliefs P(Ai) on possible causes of an observed event B --- model of the world under each Ai: PCB I Ai) model draw conclusions about causes inference P(Ai I B). ](media/Intro---Syllabus_2.-Conditioning-and-Independence-image9.png)\n\nInference - Having observed B, we make inferences as to how likely a particular scenario Ai, is going to be.\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/3.-Independence/","title":"3. Independence"},"frontmatter":{"draft":false},"rawBody":"# 3. Independence\n\nCreated: 2018-06-03 18:16:38 +0500\n\nModified: 2018-06-05 01:24:30 +0500\n\n---\n\nIntroduces the concepts of independence of two events, independence of multiple events, and pairwise independence, together with examples related to coin tossing and system reliability.\n![A model based 3 tosses of a on conditional probabilities biased coin: P(H) = p, P(T) = 1 Multiplication rule: Total probability: E (Ha J H,) 2 P: ? (HOT,) P(TIIT) = p) p) p p P I-p HIT THT TTH P(l head) = p (l -p) Bayes rule: aeoÃ…) P(first toss is H | 1 head) --- C (L Read) ](media/Intro---Syllabus_3.-Independence-image1.png)\n\nIn the above example, the probability of the first coin toss didn't effect the probabilities what might happen in the second coin toss. This is called independence.\n![Independence of two events â€¢ Intuitive \"definition\": P (B I A) P (B) occurrence of A provides no new information about B B) = P(A) â€¢ PCB) Definition of independence: Symmetric with respect to A and B --- implies P (A I B) P (A) applies even if P(Ã„) = O 13 Independent? ](media/Intro---Syllabus_3.-Independence-image2.png)\nIn the above set example, the two disjoint sets A and B are dependent on each other, because if A occured than we are sure that B doesn't occurred and vice-versa.\n![Independence of event complements Definition of independence: If and B are independent, --- Intuitive argument then and BC are independent. Formal proof 2 (40B) + L (b) + t [Atl BC) 2 (A) 2(4) (I - 2 ](media/Intro---Syllabus_3.-Independence-image3.png)\n![Conditional independence Conditional independence, given C, is defined as independence under the probability law I C) Assume A and B are independent If we are told that C occurred, are and B independent? flJÃ– ](media/Intro---Syllabus_3.-Independence-image4.png)\n\nIn the last image, if C occurred, then A and B are disjoint, therefore we know that if A occurred after C than B definitely didn't occur and vice-versa. Therefore A and B are not independent.\n![Conditioning may affect independence Two unfair coins, A and B: I coin A) = 0.9, P(ll I coin B) 0.1 choose either coin with equal probability 0.9 Com A 5 0.5 Coin B 0.1 0.1 0.9 0.9 0.1 0.9 0.1 0.1 0.1 0.9 losses Are coin tosses independent? Compare: P(toss 11 = +0 _ 5 *0.) 005 P (toss 11 = Il I first 10 tosses are heads) IA) = 003 ](media/Intro---Syllabus_3.-Independence-image5.png)\n![Independence of a collection of events Intuitive \"definition\": Information on some of the events does not change probabilities related to the remaining events Definition: Events Al, A2, ...,An are called independent if: n Am) = â€¢ â€¢ â€¢ P(Arn) for any distinct indices P(Ã„I n PC Al) â€¢ P(A3) P (Al n 112) = P(AI) â€¢ P(A2) P(A2) â€¢ P(A3) pairwise independence P(AI n A2 n A3) = PC Al) â€¢ P(A2) â€¢ P(A3) ](media/Intro---Syllabus_3.-Independence-image6.png)\n![Independence vs. pairwise independence Two independent fair coin tosses First toss is 11 --- 112: Second toss is Il P(lll) = P(112) = 1/2 3 C: the two tosses had the same result H, T T '1/2 Cc) 11T z 2 Ill, 112, and C are pairwise independent, but not independent ](media/Intro---Syllabus_3.-Independence-image7.png)\n![Reliability Pi: probability that unit i is independent units PI PI (I: up dowevl probability that system is \"up\" ? 15 q T') : U u 2 U ](media/Intro---Syllabus_3.-Independence-image8.png)\n![The king's sibling 0 The king comes from a family of two children. What is the probability that his sibling is female? bop 2/3 025 ( 6)? 0 ](media/Intro---Syllabus_3.-Independence-image9.png)\n\n![](media/Intro---Syllabus_3.-Independence-image10.png)\n\nBoth are disjoint, therefore using additivity axiom\n\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/4.-Counting/","title":"4. Counting"},"frontmatter":{"draft":false},"rawBody":"# 4. Counting\n\nCreated: 2018-06-06 00:27:41 +0500\n\nModified: 2018-06-09 02:14:09 +0500\n\n---\n\nThe basic principle of counting, uses it to count subsets, permutations, combinations, and partitions, and applies it to some probability problems.\n![LECTURE 4: Counting Discrete uniform law --- Assume Q consists of n equally likely elements --- Assume A consists of k elements Then: P(A) = o o o o prol number of elements of A number of elements of Q Basic counting principle Applications k n permutations number of subsets ](media/Intro---Syllabus_4.-Counting-image1.png)\n\n![Basic counting principle 4 shirts 3 ties 2 jackets Number of possible attires? r stages ni choices at stage i 24 - ](media/Intro---Syllabus_4.-Counting-image2.png)\n\n![Basic counting principle examples â€¢ Number of license plates with 2 letters followed by 3 digits: 26 if repetition is prohibited: 2 6 â€¢25 â€¢ â€¢8 â€¢ Permutations: Number of ways of ordering n elements: 01-1 m-Q ](media/Intro---Syllabus_4.-Counting-image3.png)\n\n![Example Find the probability that: six rolls of a (six-sided) die all give different numbers. (Assume all outcomes equally likely.) (2,324) I ](media/Intro---Syllabus_4.-Counting-image4.png)\n\n![Combinations â€¢ Definition: number of k-element subsets of a given n-element set Two ways of constructing an ordered --- Choose the k items one at a time Choose k items, then order them sequence of k distinct iten ](media/Intro---Syllabus_4.-Counting-image5.png)\n\n![](media/Intro---Syllabus_4.-Counting-image6.png)\n\n![Binomial coefficient Binomial probabilities n 2 1 independent coin tosses; P(H) = p P(HTTHHH) = P(k heads) â€¢ PC particular sequence) = â€¢ PC particular k---head sequence) P(k heads) = p K ( ( - p) m k- 5eyueucc5 ](media/Intro---Syllabus_4.-Counting-image7.png)\n\n![A coin tossing problem Given that there were 3 heads in 10 tosses, what is the probability that the first two tosses were heads? the first 2 tosses were heads event B: 3 out of 10 tosses were heads event First solution: E (Hi omc H 40>scs 8 8 ](media/Intro---Syllabus_4.-Counting-image8.png)\nIn above solution, H1 H2 and one H in tosses 3,..,10 are independent of each other, therefore probability of two independent events are the product of there individual proabablities.\n\n![â€¢ Second solution: Conditional probability law (on B) is uniform ) D Sep. 3-Qead g p3(l -p) + 6 c 130). ](media/Intro---Syllabus_4.-Counting-image9.png)\n![Partitions n > 1 distinct items; r 2 1 persons n items to person i i , ... , nr are given nonnegative integers here n 1 Â¯ With n 1 â€¢ = n, Ordering n items: Deal ni to each person 01 r el 1 i, and then order ](media/Intro---Syllabus_4.-Counting-image10.png)\n\n![Example: 52-card deck, dealt (fairly) to four players. Find P(each player gets an ace) Outcomes are: number of outcomes: BIO Constructing an outcome wit one ace for each person: --- distribute the aces --- distribute the remaining 48 cards 1 12.1 12 ](media/Intro---Syllabus_4.-Counting-image11.png)\n\n![Example: 52-card deck, dealt (fairly) to four players. Find P(each player gets an ace) Stack the deck, aces on top .26 13 oooot ooooc ](media/Intro---Syllabus_4.-Counting-image12.png)\n![](media/Intro---Syllabus_4.-Counting-image13.png)\n\nBinomial Coefficient is a special case of multinomial coefficient with r=2\n\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/5.-Probability-Mass-Functions-and-Expectations/","title":"5. Probability Mass Functions and Expectations"},"frontmatter":{"draft":false},"rawBody":"# 5. Probability Mass Functions and Expectations\n\nCreated: 2018-06-09 14:45:43 +0500\n\nModified: 2018-06-13 23:39:51 +0500\n\n---\n\n![LECTURE 5: Discrete random variables: probability mass functions and expectations Random variables: the idea and the definition Discrete: take values in finite or countable set Probability mass function (PMF) Random variable examples Bernoulli Uniform Binomial ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image1.png)\n\n![Random variables: the idea 62 ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image2.png)\n\n![Random variables: the formalism A random variable (\"r.v.\") associates a value (a number) to every possible outcome Mathematically: A function from the sample space Q to the real It can take discrete or continuous values random variable X We can have several random variables defined on the same saml Notation: numerical value ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image3.png)\n\n![Probability mass function (PMF) of a discrete r.v. X It is the \"probability law\" or \"probability distribution\" of X â€¢ If we fix some x, then \"X = x\" is an event px(x) = Properties: e Q s.t. x(w) --- px(x) 2 0 ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image4.png)\n\n![PMF calculation â€¢ Two rolls of a tetrahedral die nana Y = Second roll 3 2 1 4 X = First roll pz(z) Let every possible outcome ha Find pz(z) repeat for all z: --- collect all possible outcomes for --- add their probabilities pa (3) = 3) a/ 16 ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image5.png)\nA random variable that takes a value of 0 or 1, with certain probabilities. Such a probability is called a Bernoulli random variable.\n\n![The simplest random variable: Bernoulli with parameter p e [O, 1, w.p. p O, w.p. I---p px(l) = p â€¢ Models a trial that results in success/ failure, Heads/ Tails, etc. â€¢ Indicator rev. of an event A: IA = 1 iff A occurs ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image6.png)\nSome useful random variable -\n\n1.  Discrete uniform random variable\n\nIt takes a value in a certain range, and each one of the values in that range has the same probability\n\n2.  Binomial random variable\n\n3.  Geometric random variable\n![Discrete uniform random variable; Parameters: integers Experiment: Pick one of + 1, Sample space: {a, a + 1, . b} Random variable X: X (w) Model of: complete ignorance parameters a, b b at random; all equally lik J: 52 22 c Special case constant/det ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image7.png)\n\n![Binomial random variable; parameters: positive integer n; p e â€¢ n independent tosses of a coin with PC Heads) = p Experiment: Sample space: Set of sequences of H and T, of length n Random variable X: number of Heads observed Model of: number of successes in a given number of independer P P IIHH 11TH HTT 3 2 ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image8.png)\n\n![035 0.3 p = 0.5 075 0.04 0.2 Î¿. 15 0.05 0.7 0.6 p = 0.2 p = 0.2 0.5 04 05 1.5 2.5 3.5 025 0.2 0.15 ÎŸÎ¤Î• 0 35 0.3 025 0.2 p = 0.5 Î¹Î¿ 017 0.06 0.03 0.02 0.01 10 o. 14 o. 12 20 ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image9.png)\n\n![Geometric random variable; parameter p: O < p 1 â€¢ Experiment: infinitely many independent tosses of a coin; P(HeaC Sample space: Set of infinite sequences of H and T Random variable X: number of tosses until the first Heads Model of: waiting times; number of trials until a success px(k) = px(k) P(no Heads ever) ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image10.png)\n\nIn the above experiment where P(no heads ever) is extremely unlikely since (1-p)^k^ tends to 0 for k tends to infiinity.\nMean of a random variable is a single number that provides some kind of summary of a random variable by telling us what it is on the average\n\n![Expectation/ mean of a random variable Motivation: Play a game 1000 times. Random gain at each play described by: \"Average\" gain: ) * 2004 2.5004 9-300 1000 1, 2, 4, w.p. z w.p. w.p. 2.110 â€¢ 10 Definition: E[x] 10 Â¯ XPx(X) Interpretation: of independent repetiti ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image11.png)\n![Expectation of a Bernoulli r.v. 1, O, w.p. P w.p. 1 If X is the indicator of an event A, IS f A occuas ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image12.png)\n\n![Expectation of a uniform r.v. Uniform on O, 1 1 Definition: ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image13.png)\n\nWhenever we have a PMF which is symmetric around a certain point, then the expected value will be the center of symmetry.\n![Expectation as a population average n students Weight of ith student: Experiment: pick a student at random, all equally likely Random variable X: weight of selected student assume the are distinct PX(Xi) = ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image14.png)\n\n![Elementary properties of expectations If X 0, then E[x] 2 0 for w: If a < x < b, Soc w: a E[x] then Definition: -Q > px(Ã¦) ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image15.png)\n-   If X >= 0, then E[X] >= 0. If a random variable is non-negative, it's expected value is non-negative\n-   Expectation has a linearity property\n![The expected value rule, for calculating E[g(X)] â€¢ Let X be a r.v. and let Y = g(X) â€¢ Averaging over y: E[Y] = 3, (0.1+0.2) + q prob 3-0.) q â€¢0.5 Averaging over Proof: E[x2] 0.4 0.3 0.2 0.1 2, ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image16.png)\n\n![Linearity of expectation: E[aX + b] = aE[X] + b aver Q oje SQ Q '7 +100 Intuitive â€¢ Derivation, based on the expected value rule: ELY | 2 cc) ](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image17.png)\n\n![](media/Intro---Syllabus_5.-Probability-Mass-Functions-and-Expectations-image18.png)\n\nNo reason to believe that one value\n\nis more likely than the other\nPMF notation instead of simple probability notation"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s/","title":"6. Variance; Conditioning of an event; Multiple r.v.'s"},"frontmatter":{"draft":false},"rawBody":"# 6. Variance; Conditioning of an event; Multiple r.v.'s\n\nCreated: 2018-06-10 01:48:28 +0500\n\nModified: 2018-06-26 19:08:56 +0500\n\n---\n\n![LECTURE 6: Variance; Conditioning on an event; Variance and its properties --- Variance of the Bernoulli and uniform PMFs Conditioning a r.v. on an event Conditional PMF, mean, variance --- Total expectation theorem Geometric PMF Memorylessness Mean value Multiple random variables --- Joint and marginal PMFs ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image1.png)\n**Variance -** Is a quantity that measures the amount of spread, of a dispersion of a probability mass function. It is defined as the expected value of the squared distance from the mean. Always non negative.\n\n**Intuition -** How far away the outcome of the random variable happens to be from the mean of that random variable\n\n![Variance --- a measure of the spread of a PMF Random variable X, with mean â€¢ Distance from the mean: X --- p â€¢ Average distance from the mean? var(X) â€¢ Definition of variance: Calculation, using the expected value rule, ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image2.png)\n\nOne way to calculate the distance from the mean is E[X-u] = E[X] - u (Using linearity of expressions), u-u = 0.\n\nTherefore we want the average absolute value of the distance from the mean. Therefore we have created variance which is defined as the expected value of the squared distance from the mean.\nVariance is a bit hard to interpret, because it is in wrong units. If capital X corresponds to meters, then the variance has units of meters squared. A more intuitive quantity is the square root of the variance, which is called **standard deviation.** It has the same units as the random variable and captures the width of the distribution.\n![Properties of the variance var(aX + b) = a2var(X) Notation: Let Y = X+b vat (M) ( --- Q-f..4 Let A useful formula: var(X) --- -fJ)2 E[x2] - ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image3.png)\n\nIf we add a constant to a random variable, the variance remains unchanged (Since constant just moves the entire PMF right or left by some amount, without changing its shape)\n\nIf we multiply a random variable by a, the variance gets multiplied by a squared\n![Variance 1, 0, var(X) var(X) of the a: E[x2] Bernoulli Efriz p (l 17) z 0 p + (o-p)2â€¢ (Ä±- 2 2 (E CXI) 2 ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image4.png)\n\n**Intuition -** The variance is a measure of the amount of randomness. A coin is most random if it is fair, i.e. when p is equal to 1/2. The variance of a coin flip is biggest if that coin is fair. (as it is seen by the variance vs probability plot)\n![Variance of the uniform px(x) 1 o 1 n a; var(x) -(gCY9) - 02 ; b -Q 1 12 x) ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image5.png)\n\n![Conditional PMF and expectation, given an event Condition on an event A use conditional probabilities px(x) = x) pxIA(x) = I A) EPXIA(X) I A] = Eg(x) pxIA(x) ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image6.png)\n\n![Example of conditioning â€¢ pxIA(x) 1 2 3 4 Let 1 2 3 E[X] = 5 4 3 ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image7.png)\n\n![Total expectation theorem P(B) = P(AI) I Al) + ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image8.png)\n\n![Total expectation theorem Al n {X P(B) = P(AI) I Al) + Px@) P (Al) p XI Al (x) + . ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image9.png)\n\nTotal Expectation Theorem tells us that the expected value of a random variable can be calculated by considering different scenarios, finding the expected value under each of the possible scenarios and weigh the scenarios according to their respective probabilities.\n![TotaI expectation exampIe Ğ Ñ… (Ñ‚) 1/9 Ğ¾ 1/9 1 1/9 2 2/9 Ğ± 2/9 7 2/9 8 2 ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image10.png)\n\n![Conditioning a geometric random variable X: number of independent coin px(k) 1 2 3 '156789 tosses until first head; P(H) = Memorylessness: Number of remainin conditioned on Tails is Geometric, with Conditioned on X > 1, X--- 1 is geometric with parameter p 14 ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image11.png)\n\n![Conditioned on X > 1, X --- 1 is geometric with parameter p (L-p)tp (3) X-IIY>J ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image12.png)\n\n![Conditioned on X > n, X ---n is geometric with parameter p (L-p)tp (3) x-IIX>) ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image13.png)\n![The mean of the geometric PX (k) 1 E[x] --- E kpx(k) 1 2 3 8 9 k ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image14.png)\n\n**Intuition -** If p is small, this means that the odds of seeing heads is small. Then in that case, we need to wait longer and longer until we see heads for the first time.\n![Multiple random variables and joint PMFs rim a Q X: PX 2 20 4 1/20 2/20 2/ o Joint PMF: 20 20 px(x) --- 2 1 1/20 x ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image15.png)\n\n![More than two random variables z) = P (X = x and Y = y and Z PX(x) = E y, z) ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image16.png)\n\n![Functions of multiple random variables Z = g(x, Y) PMF: pz(z) = Y (#7, Expected value rule: E[g(X, Y)] E E.g(x, y) y) ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image17.png)\n\n![Linearity of expectations E[ax + b] = aE[x] + b + Y] E[x] + ELY] 22 Y CBD + ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image18.png)\n\n![Linearity of expectations E[ax + b] = aE[X] + b + Y] --- E[x] + ELY] ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image19.png)\n\n![The mean of the binomial X: binomial with parameters n, p number of successes in n independent trials Xi=l if ith trial is a success; E[x] = (indicator variable) Xi = O otherwise ](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image20.png)\n**nu**\n\n![](media/Intro---Syllabus_6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s-image21.png)\nBecause X is a bernoulli, for X=0, X^2^ = 0 and for X = 1, X^2^ = 1\nconditional mean or conditional expectation\nExpected value rule, remain true in the conditional model\nSame as Total Probability Theorem but translated in PMF notation\n\npX(x) = PMF at that particular x\nBy Symmetry, the expected value will be in the middle, so the expected value is 1 for conditional universe {0,1,2} and 7 for conditional universe {6,7,8}\nby linearity of expectations\nInterpretation of the above formula - With p X,Y a specific (x,y) pair will occur. And when that occurs, the value of our random variable is a certain number g(x,y). And the combination of these two terms gives us a contribution to the expected value. Now we consider all possible (x,y) pairs that may occur, and we sum over all these (x,y) pairs.\nThe expected value of the sum of two random variables is equal to the sum of their expectations.\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/7.-Conditioning-on-a-rv;-Independence-of-r.v.'s/","title":"7. Conditioning on a rv; Independence of r.v.'s"},"frontmatter":{"draft":false},"rawBody":"# 7. Conditioning on a rv; Independence of r.v.'s\n\nCreated: 2018-06-13 02:20:27 +0500\n\nModified: 2018-06-13 22:35:18 +0500\n\n---\n\n![LECTURE 7: Conditioning on a random variable; Conditional PMFs Conditional expectations --- Total expectation theorem Independence of r.v.'s Expectation properties --- Variance properties The variance of the binomial Independ ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image1.png)\n\n![Conditional PMFs pxlA(x I A) --- pxlY(x I y) = pxly(x I y) = P (X = x I Y = y) z defined for y such that py(y) > O (L) 5/20 (112) = o 4 3 1/20 2/20 2/20 4/20 2/20 1/20 py(y) 2/20 ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image2.png)\n\n![Conditional PMFs involving more than two r.v.'s Self-explanatory notation L ( Y = 72 2 = a-) I z) J? 22) Multiplication rule pan B n c) P(A) I A) I An B) ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image3.png)\n\n![Conditional expectation Expected value rule = EXPXIA(X) Â¯ E g(x) pxlA(x) ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image4.png)\n\n![Total probability and expectation theorems Al, ,An: partition of Q px(x) = P(AI) (x) + â€¢ â€¢ â€¢ + P(An) pxIAn(x) px(x) = pxIY(x I y) â€¢ E[X] = P(AI) I Al] + + P(An) I An] ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image5.png)\n\n![Independence of two events: n B) = P(A) â€¢ PCB) of a r.v. and an event: P(X = x and A) P(X = x) P(A) ort 'z of two r.v. 's: = x and Y = y) = x) â€¢ = y y) = px(x) Py(y), for all x, y ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image6.png)\n\n![Example: independence and conditional independence 4 3 2 1/20 2/20 1 2/20 4/20 1/20 1/20 2 2/20 1/20 3/20 3 Independent? NO 2/20 PX (l) : 3/20 1/20 x 4 What if we condition on X < 2 ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image7.png)\n\n![Independence and expectations â€¢ In general: E[g(X, Y)] Exceptions: E[ax + b] = aE[x] + b E[XY] --- If X, Y are independent: g(X) and h(Y) are also independent: ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image8.png)\n\n![Independence and variances Always true: var(aX) = a2var(X) var (x + a) â€¢ In general: var(X + Y) # var(X) + var(Y) = var(X) If X, Y are independent: var(X + Y) = var(X) + var(Y) Y 22 = var (x) Examples: ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image9.png)\n\n![Variance of the binomial X: binomial with parameters n, p number of successes in n independent trials Xi=l if ith trial is a success; Xi = O otherwise 2 n , VQP(Ã„I (indicator variable) . t v (x A(XW) ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image10.png)\n\n![The hat problem n people throw their hats in a box and then --- All permutations equally likely Equivalent to picking one hat at a time X: number of people who get their own hat FindEl +0 if i selects own hat 1, O, otherwise. pick one at randon 1 PX(k) ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image11.png)\n\n![The variance in the hat problem X: number of people who get their own hat --- Find var(X) if i selects own hat 1, O, otherwise. var(X) = E[x2] - x2 For i # j: E[XtXj] = g [X, & ] = f (X, l) (X) ](media/Intro---Syllabus_7.-Conditioning-on-a-rv;-Independence-of-r.v.'s-image12.png)\n\n**Joint PMF**\n**Conditional PMF**\n\n**Marginal PMF**\n\nBy keeping y fixed\nIn PMF Notation\nJoint PMF factors out as a product of the marginal PMFs of the two random variable\nEntry of the joint PMF is equal to the product of the corresponding entries of the marginal PMFs. Therefore the two random variable X and Y are independent in the conditional universe.\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/8.-Probability-density-functions/","title":"8. Probability density functions"},"frontmatter":{"draft":false},"rawBody":"# 8. Probability density functions\n\nCreated: 2018-06-27 00:41:39 +0500\n\nModified: 2018-06-27 02:11:47 +0500\n\n---\n\n![UNIT 5: Continuous random variables â€¢ â€¢ Why continuous? physical quantities are often continuous --- the power of calculus convenient approximations Same threads as in the discrete case: definitions, notation properties of expectation and variance conditioning and independence --- total probability/expectation theorem Additional elements: ](media/Intro---Syllabus_8.-Probability-density-functions-image1.png)\n![LECTURE 8: Continuous random variables and probability â€¢ Probability density functions Properties Examples Expectation and its properties --- The expected value rule Linearity Variance and its properties Uniform and exponential random variables Cumulative distribution functions ](media/Intro---Syllabus_8.-Probability-density-functions-image2.png)\n\n![Probability density functions (PDFs) px(x) b x: a<x<b PDF fx(x) b b px(x) 0 Definition: A random variable is continuous if it can be desc ](media/Intro---Syllabus_8.-Probability-density-functions-image3.png)\n\nPDF's are not probabilities, they are densities. Their unit is *probability per unit length*\n![Probability density functions .fx(a) â€¢ Ã´ (PDFs) PDF fx@) fx@) 0 .fx@) da; ](media/Intro---Syllabus_8.-Probability-density-functions-image4.png)\n\n![Example: PX (x) 1 continuous uniform PDF :r fx(x) fx(x) Generalization: piecewise constant PDF ](media/Intro---Syllabus_8.-Probability-density-functions-image5.png)\n\n![Expectation/ mean of a continuous random variable PDF fx(x) a a; b â€¢ Interpretation: Average in large of independent repetitions of the number experiment Fine print: Assume ](media/Intro---Syllabus_8.-Probability-density-functions-image6.png)\n\n![Properties of expectations then E[X] 0 a E[x] then Expected value rule: (x)dx ](media/Intro---Syllabus_8.-Probability-density-functions-image7.png)\n\n![Variance and its properties var(X) - â€)2] â€¢ Definition of variance: Calculation using the expected value rule, = var(X) --- Standard deviation: OX = var(aX + b) a2var(X) var(X) ](media/Intro---Syllabus_8.-Probability-density-functions-image8.png)\n\n![Continuous uniform random variable; fx@) 1 a :rfx (x) dx b . oltX 22.--1---- - E[x21 --- 1 parameters a, b E[x] 2 3 var(X) 1 12 ](media/Intro---Syllabus_8.-Probability-density-functions-image9.png)\nIMP, used for many real world applications\n\n![Exponential random variable; parameter > O fx(x) = o, fx(x) small o px(k) (1 large 12 3 4 o ](media/Intro---Syllabus_8.-Probability-density-functions-image10.png)\n\n![Cumulative distribution function (CDF) Fx(x) x) CDF definition: Continuous random variables: Fx(x) x) fx(t) dt 3 fx(x) 1 a ](media/Intro---Syllabus_8.-Probability-density-functions-image11.png)\n\n![Cumulative distribution function (CDF) Fx(x) x) CDF definition: Discrete random variables: = = Epx(k) px(k) Fx(x) 1/4 1 2 1/2 3 ](media/Intro---Syllabus_8.-Probability-density-functions-image12.png)\n\n![General CDF properties Fx(x) x) Non-decreasing Fx(x) tends to 1, as x ---i Fx(x) tends to O, as x ---+ ---N ](media/Intro---Syllabus_8.-Probability-density-functions-image13.png)\n\n![Normal (Gaussian) random variables Important in the theory of probability Central limit theorem Prevalent in applications Convenient analytical properties Model of noise consisting of many, small independent noise ](media/Intro---Syllabus_8.-Probability-density-functions-image14.png)\n\n![Standard normal (Gaussian) random variables Standard normal NCO, 1): .fx(x) 1 x2/2 27t C Q-CCUIU5t ](media/Intro---Syllabus_8.-Probability-density-functions-image15.png)\n\n![General normal (Gaussian) random variables 1 General normal fx(x) -1 ](media/Intro---Syllabus_8.-Probability-density-functions-image16.png)\n\n![Linear functions of a normal random variable X 02) Let ELY] = + b Fact (will prove later in this course): Special ca ](media/Intro---Syllabus_8.-Probability-density-functions-image17.png)\n\n![Standard normal tables No closed form available for CDF but have tables, for the standard normal 9 ( ) C) (2.9) ( 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 .00 300 .5398 .5793 .6179 .6554 .6915 47257 .7580 .7881 .8159 .8643 .8849 49032 .9192 49332 .9452 , 9554 49641 .9713 .9772 S .9861 .01 .5040 .5438 .5832 -6217 .6591 .6950 .7291 .7611 .7910 .8186 .8438 .8665 .8869 -9049 .9207 .9345 -9463 .9564 -9649 -9719 .9778 -9826 -9864 .02 -5080 .5478 .5871 -6255 -6628 .6985 .7324 .7642 .7939 -8212 .8461 .8686 .8888 -9066 .9222 -9357 -9474 .9573 , 9656 -9726 .9783 -9830 -9868 .03 -5120 .5517 .5910 .6293 .6664 .7019 .7357 .7673 .7967 .8238 .8485 .8708 .8907 .9082 .9236 .9370 .9484 .9582 .9664 .9732 .9788 -9834 .9871 -5160 .5557 .5948 .6331 .6700 .7054 -7389 .7704 .7995 .8264 .8508 .8729 .8925 .9251 -9382 -9495 .9591 .9671 -9738 .9793 -9838 .9875 ](media/Intro---Syllabus_8.-Probability-density-functions-image18.png)\n\n![Standardizing a random variable â€¢ â€¢ Let X have mean and variance > O Let ELY] o VQQ (y) - ](media/Intro---Syllabus_8.-Probability-density-functions-image19.png)\n\n![Calculating normal probabilities Express an event of interest in terms of standard normal 2 2 o.o o.l 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 l.o 1.1 1.2 1.3 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 .oo .3000 .5398 .3793 .6179 .6534 .6915 .7237 .75S0 .78S1 .S 39 .sn:g .S(i43 .9032 .9192 .9332 .9432 ,9534 â€9641 .9713 .9772 .9861 .01 .5040 .5438 .5832 .6217 .6591 .6950 .7291 .7611 .7910 .SIS6 -8438 -8665 .9049 .9207 .9345 .9463 .9564 .9649 .9719 .9778 .9826 .9864 .02 .5080 .5478 .5871 .6255 .6628 .6985 .7324 .7642 .7939 â€8212 .8461 .8686 .8888 .9066 .9222 .9337 .9474 .9573 .9656 .9726 .9783 .9830 .9868 .03 .3120 .5517 .5910 .6293 .6664 .7019 .7357 .7673 .7967 .S238 .8485 .8708 .S907 .9082 .9236 .9370 .9484 .9582 .9664 .9732 .9788 .9834 .9871 .5160 .5557 .5948 .6331 .6700 .7054 .7389 .7704 .7995 .8264 .8508 .8729 .8925 .9251 .9382 .9495 .9591 .9671 .9738 .9793 .9838 .9875 ](media/Intro---Syllabus_8.-Probability-density-functions-image20.png)\n\nKey step is to take the event of interest and by subtracting the mean and dividing by the standard deviation express the same event in an equivalent form, but which now involves a standard normal random variable. And then we can finally use the standard normal tables.\n\n![](media/Intro---Syllabus_8.-Probability-density-functions-image21.png)\n\n**0**\nThis formula tells us that if we have the CDF, we can calculate the PDF and vice-versa\nSpecial degenerate discrete random variable\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s/","title":"9. Conditioning on an event; Multiple continuous r.v.'s"},"frontmatter":{"draft":false},"rawBody":"# 9. Conditioning on an event; Multiple continuous r.v.'s\n\nCreated: 2018-06-27 00:42:03 +0500\n\nModified: 2018-07-19 01:13:41 +0500\n\n---\n\n![LECTURE 9: Conditioning on an event; Conditioning a r.v. on an event --- Conditional PDF Multiple continuot Conditional expectation and the expected value rule Exponential PDF: memorylessness --- Total probability and expectation theorems Mixed distributions Jointly continuous r.v.'s and joint PDFs From the joints to the marginals ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image1.png)\n\n![](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image2.png)\n\n![Conditional PDF of X, -t (z Ã„eA) s) given that X e A fxlxeA(X) â€¢ / ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image3.png)\n\n![Conditional expectation Expected value rule: of X, given an event E[X] Ixfx(x) dx xfxIA(x) clx E[g(X)] dx ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image4.png)\n\n![](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image5.png)\n\n![Memorylessness of the exponential PDF â€¢ Do you prefer a used or a new \"exponential\" â€¢ Bulb lifetime T: exponential(Ã„) for x > O we are told that T > t r.v. X: remaining lifetime for > O E (Tye) light bulb? Prokat ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image6.png)\n\nUsed light bulb does not remember and it is not affected by how long it has been running. And this is the memorylessness property of exponential random variables.\n![Memorylessness of the exponential PDF fT(x) = for x > O P(OfTfÃ¶) 8 similar to an independent coin flip, every Ã¶ time steps, ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image7.png)\n\n![Total probability Al n B A2 n 13 and expectation theorems px@) P(AI) + â€¢ + P(An) fx@) + â€¢ + P(An)) ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image8.png)\n\n![Example Bill goes to the supermarket shortly, with at a time uniformly distributed between O or with probability 2/3, later in the day at a time uniformly distributed between 6 um f Co, probability 1/3, and 2 hours from n and 8 hours from n 2 3 fx(x) 1b fx(x) --- ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image9.png)\n\n![Mixed distributions uniform on [O, 2], 1, Y discrete Z continuous with probability 1/2 with probability 1/2 Y, with probability p Z, with probability 1 ---p + (1-12) F? (z) Is X discrete Is X continu X is ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image10.png)\n\n![Mixed distributions uniform on [O, 2], 1, 1 2 with probability 1/2 4 with probability 1/2 A Fx(x) + Fx(x) 112 ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image11.png)\n\n![Jointly continuous r.v.'s and joint PDFs = x and Y = y) 2 0 PX,Y (x, y) f x, Y (x, y) da ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image12.png)\n\n![Visualizing a joint PDF ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image13.png)\n\n![On joint PDFs y) dx dy f (x, y) clx dy y): probability per unit area ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image14.png)\n\n![From the joint to the px(x) y) marginals fx(x) y) dy ---00 ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image15.png)\n\n![Uniform joint PDF on 1 area of S 0, a set S if (x, y) e S, otherwise. atea ( Al) S) atea ( S) 4 3 2 1/2 1 1/4 fy(y) ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image16.png)\n\n![More than two random variables PX(x) y, z) ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image17.png)\n\n![Functions of multiple random variables Expected value rule: Y)] E Ego, y) Linearity of expectations Efx + Yl Efxl + EfYl lg(x, y)) E[ax + b] aE[X] + b ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image18.png)\n\n![The joint CDF Fx(x) x) fx(t) dt 02Fx Y Ox Oy dFx fx(x) dx ( s , 4) 01 s ](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image19.png)\n\nGreen PDF is the form of a conditional PDF\n\nTotal area under green curve is 1.\nHere Conditional probability of event X occuring in a conditional universe where A occurred is 0, since event A and x are disjoint (x lies outside A)\n\n![](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image20.png)\n\nCase when x lies outside A\n\n![](media/Intro---Syllabus_9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s-image21.png)\nCase when x lies inside A\nSums become integrals, and PMFs are replaced by PDFs\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/Additional-Theoretical-Material-1/","title":"Additional Theoretical Material"},"frontmatter":{"draft":false},"rawBody":"# Additional Theoretical Material\n\nCreated: 2018-08-12 09:06:44 +0500\n\nModified: 2018-08-17 20:22:02 +0500\n\n---\n\n![A linear function of two independent continuous random fN+Y(z) = The PDF of ---Y? (for independent X, Y) SY(x/2) ](media/Intro---Syllabus_Additional-Theoretical-Material-1-image1.png)\n![(a) Suppose for simplicity that X is integer-valued. The intuitive idea is to partition the unit interval into subintervals, with the kth interval having length PX (k). Whenever U falls in the kth interval, we assign the value k to the random variable X. Mathematically, this translates to the following. We assign to X the value k whenever the value u of the random variable U satisfies F(k---l) < u F (k). With this choice, = k) = - 1) < U Fx(k)) = F(k) - - 1), and P (X k) = F (k), so that F is indeed the CDF of the random value X we have generated. (b) In the continuous case, given the value u of U, we assign to X a value that satisfies F@) u. (Such a value exists and is unique because we assumed that F is strictly increasing on the relevant interval.) In terms, of random variables, we have the relation F (X) = U. Since F is continuous and monotonic, we have Therefore, if and only if F (X) F@). where the last equality holds because U is uniform. Thus, X has the desired CDF. (c) The exponential CDF with parameter = 1 takes the form F@) --- for 0. Thus, to generate values of X, we start with the value u of U and assign to X a value that satisfies 1 --- --- u. Solving for x, we find log(l --- u). ](media/Intro---Syllabus_Additional-Theoretical-Material-1-image3.png)\n![(a) Showing that two random variables are equal entails showing that no what the outcome of the experiment, the two random variables take same values. Therefore, we need to show that for any y* for which py(y O, the resulting values I Y = y* ] and = y* ] of two random variables of interest are the same. Recall the expected value rule: We apply this to a conditional model in which the event Y = y* is kn to have occurred: Now, note that for y y* , P (X = x, Y = y I Y = y* ) = O; in PMF notai if y y*. On the other hand, for y = y* , we have = x, Y = = = = xlY in P MF notation, y* I y* ) = PXIY(x I y*). Using these facts, the double summation in the expected value rule, becc a single summation, and We have therefore verified the desired equality. Note: These results are true for all kinds of random variables (continu mixed, etc.). A general proof would require an appropriate variant of expected value rule, but would be awkward because one would hav consider all possible combinations of cases. In a more advanced treatil ](media/Intro---Syllabus_Additional-Theoretical-Material-1-image4.png)\n\n![Multiplying both sides by x, and then summing over all we obtain E[XIY = y] = Ih(Y) z]. Now, if the event Y = y is known to have occurred, then E[X I Y] t the value E[X I Y = y]. In that case, the random variable h(Y) takes value h(y) = z, so that the random variable E[X I h(Y)] takes the v E[X I h(Y) = z]. We have already shown that E[X I Y = y] is the s as E[X I h(Y) = z]. Hence, the random variables E[X I Y] and E[X I h always take the same values, and are therefore equal. ](media/Intro---Syllabus_Additional-Theoretical-Material-1-image5.png)\n\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/Additional-Theoretical-Material/","title":"Additional Theoretical Material"},"frontmatter":{"draft":false},"rawBody":"# Additional Theoretical Material\n\nCreated: 2018-06-26 23:35:45 +0500\n\nModified: 2018-06-27 00:38:25 +0500\n\n---\n\n# Functions\n\n![Functions (3 set 0} pou'2> app 04 e pov ](media/Intro---Syllabus_Additional-Theoretical-Material-image1.png)\n-   Every element of A is mapped to exactly one element of B, not more.\n-   It is possible that some elements of B, do not correspond to any elements of A\n![Notation and terminology the function f : defined by f (x) the function f : defined by f (z) the function f 2 2 Info the fu the fu the fu ](media/Intro---Syllabus_Additional-Theoretical-Material-image2.png)# The Variance of the geometric PMF\n\n**The variance of the geometric PMF.**IfXis a geometric random variable with parameterp>0, show that\n\n![var (X) ](media/Intro---Syllabus_Additional-Theoretical-Material-image3.png)\n![The variance of the geometric PMF X: geometric with parameter p Given X > 1, X --- 1 has the same geometric PMF (unconditional PMF of X) Given X > 1, conditional PMF of X: same as uncondtional PMF of X + 1 pod +0 -p) ( cc 72] var(X) = 2 P ](media/Intro---Syllabus_Additional-Theoretical-Material-image4.png)\n\nThe way to think about X is the number of coin flips that it takes until we obtain heads for the first time, where p is the probability of heads at each toss.\n\nMemorylessness property - If I tell you that X is bigger than 1 which means that the first trial was a failure (we obtained tails). Given that event, the remaining number of tosses has the same geometric PMF as if we were just starting at this time. So it has the same geometric PMF as the unconditional PMF of X.\nVariance of a random variable is equal to the expected value of the square of that random variable minus the square of the expected value\n# The inclusion-exclusion formula\n![The inclusion-exclusion formula. Let A A i , An be events. Show that n il Here a sum such as stands for a triple sum, over all triples (il , i2 , i3) that satisfy ](media/Intro---Syllabus_Additional-Theoretical-Material-image5.png)\n![The inclusion-exclusion formula P(AI IJ A2) P(AI) + P(A2) - P(AI n A2) â€¢ PC Al u A3) = P(A2) +P(A3) (p(A1 n A2) + ](media/Intro---Syllabus_Additional-Theoretical-Material-image6.png)\n\n![The inclusion-exclusion formula â€¢ P(AI u A3) P(AI) +P(A2) + P(A3) (p(A1 n A2) +P(A â€¢ Ä¾cÄ… 02 00 ](media/Intro---Syllabus_Additional-Theoretical-Material-image7.png)\n\n# \n\n# Independence of random variable vs independence of events\n\nLetAandBbe two events, and letIAandIBbe the associated indicator variables. Show that eventsAandBare independent if and only if the random variablesIAandIBare independent.\n\n![Independence of events versus random variables â€¢ Events A and B â€¢ Indicator variables X = IA and Y = 1B 0 e can 5 X and Y are independent if and only if A and B are independent (and similarly for n events and indicator variables) = PX(I) py(l) = px(l) PY(O) = px(O) py(l) = px(o) PY(O) It ](media/Intro---Syllabus_Additional-Theoretical-Material-image8.png)\n**Meaning of Independence of two events -** The probability of their intersection is the product of their individual probabilities\n\nThis should not be used, *the function f* should be called, because *the function f(x)* is a number that is the output of *the function f*\nMemorylessness property\nUsing Demorgan's law on above function"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/Summary---Unit-4/","title":"Summary - Unit 4"},"frontmatter":{"draft":false},"rawBody":"# Summary - Unit 4\n\nCreated: 2018-06-27 00:40:05 +0500\n\nModified: 2021-10-24 18:08:42 +0500\n\n---\n\n![UNIT 4: Discrete random variables --- Summary px(x), y), r.v.'s and PMFs: PXIY(x I y), Expectation: E[X], E[X I A], E[X IY = y] Expected value rule: Y)], Y) IA], Linearity: + I)Y] = + Variance: var(X), var(X I A), var (X IY = y) Independence of r.v.'s: Px,Y = PX â€¢ PY PXIA(X) var(X) = E[XY] = Elx] â€¢ E[Y] Multiplication rule Total probability theorem var(x + Y) = var(X) + var(Y) Y, z) = pz(z) PY I I px(x) = Epy(y) PX IY y ](media/Intro---Syllabus_Summary---Unit-4-image1.png)\n[MIT6041XT114-S0401_100](https://www.youtube.com/watch?v=Iqvs0kNthrs)\n\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/Summary---Unit-5/","title":"Summary - Unit 5"},"frontmatter":{"draft":false},"rawBody":"# Summary - Unit 5\n\nCreated: 2018-06-29 22:33:48 +0500\n\nModified: 2018-06-29 22:36:19 +0500\n\n---\n\n![UNIT 5: Continuous random variables --- Summary fx(x), y), fXlY(x ly), r.v.'s and PDFs: Expectation: E[X], E[X I A], E[X IY = y] Expected value rule: Y)], Y) IA], Linearity: E[ax + W] = aE[Y] + bE[Y] Variance: var(X), var(X I A), var(X IY = y) Independence of r.v.'s: fx,Y = fx fy fXIA(x) var(X) = E[XY] = Elx] â€¢ E[Y] Multiplication rule Total probability theorem var(x + Y) = var(X) + var(Y) f XG) Elx] Â¯ fz(z) fYIZ(y I JI f Y (y) f XIV (a fY(y) Elx ](media/Intro---Syllabus_Summary---Unit-5-image1.png)\n\n![What was new? Replace: --- sums by integrals --- PMFs by PDFs Densities are not probabilities: PG X Ã¶) fx(x) â€¢ Ã¶ Conditioning on events {Y = y} that have zero probability CDF: Fx(x) = x) Bayes' rule variations and mixed (discrete/continuous) models ](media/Intro---Syllabus_Summary---Unit-5-image2.png)\n**References**\n\n[MIT6041XT114-S0502_100](https://www.youtube.com/watch?v=P3_W6ZsL-A8)\n![](media/Intro---Syllabus_Summary---Unit-5-image3.png)\n\nContinuous Random Variables\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/Summary---Unit-6/","title":"Summary - Unit 6"},"frontmatter":{"draft":false},"rawBody":"# Summary - Unit 6\n\nCreated: 2018-08-17 20:26:07 +0500\n\nModified: 2018-08-17 20:26:59 +0500\n\n---\n\n![UNIT 6: Further topics on r.v.'s --- Summary â€¢ Derived distributions Covariance and correlation A deeper view of conditioning ](media/Intro---Syllabus_Summary---Unit-6-image1.png)\n\n![UNIT 6: Further topics on r.v.'s --- Summary Derived distributions: Y = g(X): find CDF of Y; can go directly when g is monotoni Y = ax + b: simple formula Z = g(x, Y): same meth0( Z = X + Y (X, Y independent): convolution formula fz(z) = - pz(z) = Epx(x) PY(z - x) sum of independent normals is normal ](media/Intro---Syllabus_Summary---Unit-6-image2.png)\n\n![UNIT 6: Further topics on r.v.'s --- Summary Covariance and correlation cov(x, Y) = E (X - E[x]) â€¢ (Y - E[Y]) p(x, Y) --- Ipl linearity properties used to find var(X1 + ](media/Intro---Syllabus_Summary---Unit-6-image3.png)\n\n![UNIT 6: Further topics on r.v.'s --- Summary â€¢ A deeper view of conditioning ly], var(x ly) as random variables Law of iterated expectations: E E[X I Y]] = E[X] Law of total variance: var(X) I Y)) + I â€¢ Sum of a random number of independent r.v.'s: E[Y] = ELN] â€¢ E[x] var(Y) = E[N] var(X) + var(N) ](media/Intro---Syllabus_Summary---Unit-6-image4.png)**References**\n\n[MIT6041XT114-US0601_100](https://www.youtube.com/watch?v=-hVAAv2khAs)\n![](media/Intro---Syllabus_Summary---Unit-6-image5.png)"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/Unit-1---Solved-Problems/","title":"Unit 1 - Solved Problems"},"frontmatter":{"draft":false},"rawBody":"# Unit 1 - Solved Problems\n\nCreated: 2018-06-02 17:17:28 +0500\n\nModified: 2022-04-07 21:19:04 +0500\n\n---\n\n**The probability of difference of two sets**\n\n![p å†– å†– 40B ã€‚ ä¸€ 0 å†– QB ä¸€ ä¸€ â…¡ p å†– ä¸€ + p å†– B ä¸€ ä¸¨ 0 Â· p å†– 40B ä¸€ ä¸€ ](media/Intro---Syllabus_Unit-1---Solved-Problems-image1.png)\n![æ›° ãƒª ã£ ã‚½ ã‚’ ã€‚ ãƒƒ 0 ä¸‰ ã¤ ã« ) ä¹™ ã¦ - ã€‚ z æœ ) å¯ ](media/Intro---Syllabus_Unit-1---Solved-Problems-image2.png)\n**Geniuses and chocolates.**Out of the students in a class, 60% are geniuses, 70% love chocolate, and 40% fall into both categories. Determine the probability that a randomly selected student is neither a genius nor a chocolate lover.\n\n![9 ](media/Intro---Syllabus_Unit-1---Solved-Problems-image3.png)\n**Uniform probabilities on a square.**Romeo and Juliet have a date at a given time, and each will arrive at the meeting place with a delay between 0 and 1 hour, with all pairs of delays being \"equally likely,\" that is, according to a uniform probability law on the unit square. The first to arrive will wait for 15 minutes and will leave if the other has not arrived. What is the probability that they will meet?\n\n![0 ã¤ M40 . ã€ ç‰© ã€ å© 1 ã« ã£ ) ã‚Š ã€ - ã« ãƒ ã„ ã‚¹ ãƒ¬ ãƒ 0. ç¬¬ 01 å›½ ã® d ã® ãƒ« ã¤ å© ã‚Š ](media/Intro---Syllabus_Unit-1---Solved-Problems-image4.png)\n**Bonferroni's inequality**\n\n![Bonferroni's inequality. (a) Prove that for any two events Al and A2, we have P(AI n A2) 2 P(AI) + P(A2) ---1. (b) Generalize to the case of n events Al , A2, ... , An, by showing that ](media/Intro---Syllabus_Unit-1---Solved-Problems-image5.png)\n![Interpreting the union bound and the Suppose that: --- very few of the students are smart --- very few students are beautiful Then: very few students are smart or Suppose that: most of the students are smart --- most students are beautiful Bonferroni inequality V/42)g beautiful Then: most students are smart and beautiful P(AI n A2) P(AI) + - 1 ](media/Intro---Syllabus_Unit-1---Solved-Problems-image6.png)\nUsing union bound for P(A1^c^ U A2^c^) <= P(A1^c^) + P(A2^c^)\n![(t ãƒ¼ ã“ ãƒ¼ ( ã« ã“ d å ãƒ» : å (Iv)d ( ä¸‰ u : ã¨ (v)d ãƒ» 1 ãƒ¼ ( ) d å ã£ ã“ d ã‚“ ( ã© tv)d 'Ollenbout ä¸€ uo è¾· u09 0 ãƒ‹ ãƒˆ ](media/Intro---Syllabus_Unit-1---Solved-Problems-image7.png)\n\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/Unit-2---Solved-Problems/","title":"Unit 2 - Solved Problems"},"frontmatter":{"draft":false},"rawBody":"# Unit 2 - Solved Problems\n\nCreated: 2018-06-05 01:37:56 +0500\n\nModified: 2018-06-06 00:26:42 +0500\n\n---\n\n# Conditional probability example\n\nWe roll two fair 6-sided dice. Each one of the 36 possible outcomes is assumed to be equally likely.\n\n(a) Find the probability that doubles are rolled (i.e., both dice have the same number).\n\n(b) Given that the roll results in a sum of 4 or less, find the conditional probability that doubles are rolled.\n\n(c) Find the probability that at least one die roll is a 6.\n\n(d) Given that the two dice land on different numbers, find the conditional probability that at least one die roll is a 6.\n![---.Î¶---------Ïƒ-ÎÏ Î¸Î)ÎÎ¿Ï…Î±Î00 ](media/Intro---Syllabus_Unit-2---Solved-Problems-image1.png)# A chess tournament problem\n\nThis year's Belmont chess champion is to be selected by the following procedure. Bo and Ci, the leading challengers, first play a two-game match. If one of them wins both games, he gets to play a two-game*second round*with Al, the current champion. Al retains his championship unless a second round is required and the challenger beats Al in both games. If Al wins the initial game of the second round, no more games are played.\n\nFurthermore, we know the following:\n\nâˆ™The probability that Bo will beat Ci in any particular game is 0.6.\n\nâˆ™The probability that Al will beat Bo in any particular game is 0.5.\n\nâˆ™The probability that Al will beat Ci in any particular game is 0.7.\n\nAssume no tie games are possible and all games are independent.\n\n1. Determine the a priori probabilities that\n\n(a) the second round will be required.\n\n(b) Bo will win the first round.\n\n(c) Al will retain his championship this year.\n\n2. Given that the second round is required, determine the conditional probabilities that\n\n(a) Bo is the surviving challenger.\n\n(b) Al retains his championship.\n\n3. Given that the second round was required and that it comprised only one game, what is the conditional probability that it was Bo who won the first round?\n![ÙŠØ­Ø·. TÃ¶wrnamenf eØŸ aaf< -Ù‚ Ø¨Ø± Ø©)) ÙØ¥Ø§Ø§Ù„Ø§ Ø£Ù†Ù‡ Ù¤Ù¡Ù¡Ù¡) - -Ù©7Ù). Ø¥Ø«Ø§ - ÙŠ Ù„ Ùˆ . 7 . ?. -.fCi2 . ](media/Intro---Syllabus_Unit-2---Solved-Problems-image2.png)\n![](media/Intro---Syllabus_Unit-2---Solved-Problems-image3.png)\n# A coin tossing puzzle\n\nA coin is tossed twice. Alice claims that the event of getting two Heads is at least as likely if we know that the first toss is Heads than if we know that at least one of the tosses is Heads. Is she right? Does it make a difference if the coin is fair or unfair? How can we generalize Alice's reasoning?\n![A ä¹™ ä¸¿ åŒ• ](media/Intro---Syllabus_Unit-2---Solved-Problems-image4.png)\n\nSolution - We can see that Alice claim is true without ever considering the probability of Head if it is biased or not.\nAlso P ( A intersection B | A ) = 1/2 and P ( A intersection B | A U B ) = 1/3, for a fair coin, where\n\nA = 1st toss is head and B = 2nd toss is head\n\n![](media/Intro---Syllabus_Unit-2---Solved-Problems-image5.png)\n# The Monty Hall problem\n\nThis is a much discussed puzzle, based on an old American game show. You are told that a prize is equally likely to be found behind any one of three closed doors in front of you. You point to one of the doors. A friend opens for you one of the remaining two doors, after making sure that the prize is not behind it. At this point, you can stick to your initial choice, or switch to the other unopened door. You win the prize if it lies behind your final choice of a door. Consider the following strategies:\n-   Stick to your initial choice.\n-   **Switch to the other unopened door.**\n-   You first point to door 1. If door 2 is opened, you do not switch. If door 3 is opened, you switch.\n\nWhich is the best strategy?\n![](media/Intro---Syllabus_Unit-2---Solved-Problems-image6.png)# A random walker\n\nImagine a drunk tightrope walker, who manages to keep his balance, but takes a step forward with probabilitypand takes a step back with probability(1âˆ’p).\n\n(a) What is the probability that after two steps, the tightrope walker will be at the same place on the rope as where he started?\n\n(b) What is the probability that after three steps, the tightrope walker will be one step forward from where he started?\n\n(c) Given that after three steps he has managed to move ahead one step, what is the probability that the first step he took was a step forward?\n![V \" äº  ä¸„ 2 ã€ ä¸€ 0 ](media/Intro---Syllabus_Unit-2---Solved-Problems-image7.png)\n![\" ä¸€ ä¹› ä¹› Z ã€ ä¹› EF ä¸€ ](media/Intro---Syllabus_Unit-2---Solved-Problems-image8.png)# Communication over a noisy channel\n\nA source transmits a message (a string of symbols) over a noisy communication channel. Each symbol is0or1with probabilitypand1âˆ’p, respectively, and is received incorrectly with probabilityÏµ0andÏµ1, respectively (see the figure below). Errors in different symbol transmissions are independent.\n\n![](media/Intro---Syllabus_Unit-2---Solved-Problems-image9.png)\n\n(a) What is the probability that thekth symbol is received correctly?\n\n(b) What is the probability that the string of symbols1011is received correctly?\n\n(c) In an effort to improve reliability, each symbol is transmitted three times and the received string is decoded by majority rule. In other words, a0(or1) is transmitted as000(or111, respectively), and it is decoded at the receiver as a0(or1) if and only if the received three-symbol string contains at least two0's (or1's, respectively). What is the probability that a0is correctly decoded?\n\n(d) For what values ofÏµ0is there an improvement in the probability of correct decoding of a0when the scheme of part (c) is used?\n\n(e) Suppose that the scheme of part (c) is used. What is the probability that a symbol was0given that the received string is101?\n\n![P ã‚‚ ã” O ã€ 000 o- ä¸€ 80 0 - 000 ã€ 7 0 â‘£ ã‚’ ã« ( ã€‚ ) 7 å½“ ( é¬˜ ç”Ÿ ãƒ I â‘© ](media/Intro---Syllabus_Unit-2---Solved-Problems-image10.png)# Network reliability\n\nAn electrical system consists of identical components, each of which is operational with probabilityp, independent of other components. The components are connected in three subsystems, as shown in the figure. The system is operational if there is a path that starts at pointA, ends at pointB, and consists of operational components. What is the probability of this happening?\n\n![0 ](media/Intro---Syllabus_Unit-2---Solved-Problems-image11.png)\n![sena ](media/Intro---Syllabus_Unit-2---Solved-Problems-image12.png)\n![(4) ÙˆØµÙˆØ±) 1- 4(eÂ» Ù‡ (Ù„Ø§Ø¨ÙŠÙ‡) Ø§ ](media/Intro---Syllabus_Unit-2---Solved-Problems-image13.png)\n![ãƒŸ æ‹¿ â†“ e ã¦ ( ãˆ ãƒ» ) P 0 ã€‚ 4 b)= P P : ã€ ç‰© p å  ãƒ¼ æ—¥ ãƒ¼ ç”³ ã‚¹ ã€ ã ã¹ ](media/Intro---Syllabus_Unit-2---Solved-Problems-image14.png)\n![](media/Intro---Syllabus_Unit-2---Solved-Problems-image15.png)\n"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/Unit-3---Solved-Problems/","title":"Unit 3 - Solved Problems"},"frontmatter":{"draft":false},"rawBody":"# Unit 3 - Solved Problems\n\nCreated: 2018-06-09 13:28:06 +0500\n\nModified: 2018-06-09 14:38:22 +0500\n\n---\n\n# The birthday problem\n\nConsidernpeople who are attending a party. We assume that every person has an equal probability of being born on any day during the year, independently of everyone else, and ignore the additional complication presented by leap years (i.e., nobody is born on February 29). What is the probability that each person has a distinct birthday?\n![( ä¸€ s ä¸† ($2Ã‰) å¸ ](media/Intro---Syllabus_Unit-3---Solved-Problems-image1.png)\n![1.0 0.8 0.6 0.4 0.2 0.0 20 40 100 100 10-5 104 10-9 -10 80 100 120 Z CDI)nClb/e) Â¯ (345) n ](media/Intro---Syllabus_Unit-3---Solved-Problems-image2.png)\n# Rooks on a chessboard\n\nEight rooks are placed in distinct squares of an8Ã—8chessboard, with all possible placements being equally likely. Find the probability that all the rooks are safe from one another, i.e., that there is no row or column with more than one rook.\n![8 (l)fU a Chessboard (03 (02 57 spots ](media/Intro---Syllabus_Unit-3---Solved-Problems-image3.png)\n![Chess(eoard (04 spof-s. (JrllÄ™orw Ew ](media/Intro---Syllabus_Unit-3---Solved-Problems-image4.png)\n1.  Counting Principle\n\n2.  Discrete Uniform Law\n# Hypergeometric probabilities\n\nAn urn containsnballs, out of which exactlymare red. We selectkof the balls at random, without replacement (i.e., selected balls are not put back into the urn before the next selection). What is the probability thatiof the selected balls are red?\n![~ ã‚‹ ã‚¤ ä¸¿ C ãƒ‹ BJQ åˆ€ ãŸ . ) ](media/Intro---Syllabus_Unit-3---Solved-Problems-image5.png)\n![ã¨ ) ](media/Intro---Syllabus_Unit-3---Solved-Problems-image6.png)\n# Multinomial probabilities\n\nAn urn contains balls ofrdifferent colors. We drawnballs, with different draws being independent. For any given draw, there is a probabilitypi,i=1,...,r, of getting a ball of colori. Here, thepi's are nonnegative numbers that sum to 1.\n\n(For such independence to be possible, you may think of an urn that has infinitely many balls, so that removing one does not change the probabilitiespi, or you can think about drawing \"with replacement\": the chosen ball is put back into the urn before the next draw.)\n\nLetn1,...,nrbe nonnegative integers that sum ton. What is the probability that we obtain exactlyniballs of colori, for eachi=1,...,r?\n![The multinomial probabilities balls of different colors: i probability of picking a ball of color i is Pi draw n balls, independently given nonnegative numbers ntâ€¢, with ni + -k nr = n Find: P ni balls of color 1, Special case r 2; colors: m, k lead; balls of color 2, balls of color \"heads\" , \"tails\" ](media/Intro---Syllabus_Unit-3---Solved-Problems-image7.png)\n\nThis is the multinomial probability for the special case where we are dealing with two colors.\n![The multinomial probabilities \"type\" (711' , nr 3 2 PI P2 P particular sequence of sequence of type (711, 112, , nr) P get type (711, nil â€¢ 712! partition of {1, into subsets of sizes ni, , nr â€¢ PI P2 ](media/Intro---Syllabus_Unit-3---Solved-Problems-image8.png)"},{"fields":{"slug":"/Mathematics/Probability/Intro---Syllabus/Unit-4---Solved-Problems/","title":"Unit 4 - Solved Problems"},"frontmatter":{"draft":false},"rawBody":"# Unit 4 - Solved Problems\n\nCreated: 2018-06-13 23:26:05 +0500\n\nModified: 2018-06-13 23:54:53 +0500\n\n---\n\n![PMF of a function of a random variable. Consider a random variable X such that Px@) --- where a > 0 is a real parameter. 1. Find a. 2. What is the PMF of the random variable Z o, force {-3, -2, -1, 1, 2, 3}, otherwise, ](media/Intro---Syllabus_Unit-4---Solved-Problems-image1.png)\n![1. The sum of the values of the P MF of a random variable over all values that it takes with positive probability must be equal to 1. Hence, we have 1 3 1 28 a which implies that a = 28. 2. The following table shows the value of Z for a given value of X and the probability of that event. PX -3 9/28 zlx 9 -2 1/7 4 -1 1 1/28 1/28 1 1 2 3 1/7 9/28 4 9 We see that Z can take only three possible values with non-zero proba- bility, namely 1, 4, and 9. In addition, for each value, there correspond two values of X. So we have, for example, pz(9) = P (Z = 9) = P(X -3) + = 3) = px(-3) +px(3). Hence the PMF of Z is given by 1/14, 2/7, Pz(z) 9/14, o, if z = 1, if z = 4, if z = 9, otherwise. ](media/Intro---Syllabus_Unit-4---Solved-Problems-image2.png)\n![](media/Intro---Syllabus_Unit-4---Solved-Problems-image3.png)\n\n8/a 18/a\n\n![](media/Intro---Syllabus_Unit-4---Solved-Problems-image5.png)\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/gRPC/Commands/","title":"Commands"},"frontmatter":{"draft":false},"rawBody":"# Commands\n\nCreated: 2020-05-07 01:35:45 +0500\n\nModified: 2020-10-02 20:17:17 +0500\n\n---\n\n**brew install protobuf**\n\n>>> protoc\n\n### for go\n```\nexport GO111MODULE=on # Enable module-aware mode\ngo get google.golang.org/grpc@v1.28.1\ngo get github.com/golang/protobuf/protoc-gen-go\n\nexport PATH=\"$PATH:$(go env GOPATH)/bin\"\nprotoc --proto_path=proto proto/*.proto --go_out=plugins=grpc:pb\n<https://grpc.io/docs/quickstart/go>\n```\n\n### for python\n```\nimport memory_message_pb2\n\nm = memory_message_pb2.Memory()\n\nm.value = 10\n\nm.unit = 10\n\nm.SerializeToString()\npip install google\n\nfrom google.protobuf.json_format import MessageToJson\n\nMessageToJson(m)\n```\n\n## Tools\n\n<https://github.com/fullstorydev/grpcurl>\n\n## python2to3.py\n```\nimport logging\n\nimport os\nlogging.basicConfig(level=logging.DEBUG)\npath = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n\npath = os.path.join(path, \"pb\")\n\nlogging.info(path)\n\nfile_path = f\"{path}/*_pb2*.py\"\n\nlogging.info(file_path)\n\nos.system(f\"2to3 -wn -f import {file_path}\")\n```\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/gRPC/Comparision/","title":"Comparision"},"frontmatter":{"draft":false},"rawBody":"# Comparision\n\nCreated: 2020-05-04 01:08:59 +0500\n\nModified: 2020-05-04 01:12:16 +0500\n\n---\n\n**Performance benchmarks**\n\ngRPC is roughly 7 times faster than REST when receiving data & roughly 10 times faster than REST when sending data for this specific payload. This is mainly due to the tight packing of the Protocol Buffers and the use of HTTP/2 by gRPC. However I had to spend roughly 45 mins implementing this simple gRPC Service, where I only spent around 10 mins building the WebAPI.\n<https://medium.com/@EmperorRXF/evaluating-performance-of-rest-vs-grpc-1b8bdf0b22da>\n\n## High-level comparison**\n\n| **Feature**                | **gRPC**                                                                                                                           | **HTTP APIs with JSON**       |\n|----------------------|-------------------------|--------------------------|\n| **Contract**               | Required (.proto)                                                                                                                  | Optional (OpenAPI)            |\n| **Protocol**               | HTTP/2                                                                                                                             | HTTP                          |\n| **Payload**                | [Protobuf (small, binary)](https://docs.microsoft.com/en-us/aspnet/core/grpc/comparison?view=aspnetcore-3.1#performance)           | JSON (large, human readable)  |\n| **Prescriptiveness**       | [Strict specification](https://docs.microsoft.com/en-us/aspnet/core/grpc/comparison?view=aspnetcore-3.1#strict-specification)      | Loose. Any HTTP is valid.     |\n| **Streaming**              | [Client, server, bi-directional](https://docs.microsoft.com/en-us/aspnet/core/grpc/comparison?view=aspnetcore-3.1#streaming)       | Client, server                |\n| **Browser support**        | [No (requires grpc-web)](https://docs.microsoft.com/en-us/aspnet/core/grpc/comparison?view=aspnetcore-3.1#limited-browser-support) | Yes                           |\n| **Security**               | Transport (TLS)                                                                                                                    | Transport (TLS)               |\n| **Client code-generation** | [Yes](https://docs.microsoft.com/en-us/aspnet/core/grpc/comparison?view=aspnetcore-3.1#code-generation)                            | OpenAPI + third-party tooling |\n**gRPC recommended scenarios**\n-   **Microservices--** gRPC is designed for low latency and high throughput communication. gRPC is great for lightweight microservices where efficiency is critical.\n-   **Point-to-point real-time communication--** gRPC has excellent support for bi-directional streaming. gRPC services can push messages in real-time without polling.\n-   **Polyglot environments--** gRPC tooling supports all popular development languages, making gRPC a good choice for multi-language environments.\n-   **Network constrained environments--** gRPC messages are serialized with Protobuf, a lightweight message format. A gRPC message is always smaller than an equivalent JSON message.\n**gRPC weaknesses**\n-   Limited browser support\n-   Not human readable\n<https://docs.microsoft.com/en-us/aspnet/core/grpc/comparison>\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/gRPC/Concepts/","title":"Concepts"},"frontmatter":{"draft":false},"rawBody":"# Concepts\n\nCreated: 2020-05-22 12:44:50 +0500\n\nModified: 2020-05-22 12:47:00 +0500\n\n---\n\n**Flow Control**\n\nFlow control is the mechanism to **throttle the traffic** in order to protect endpoints that are under resource constraints.-   Technically it's a scaling problem\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/gRPC/Guides/","title":"Guides"},"frontmatter":{"draft":false},"rawBody":"# Guides\n\nCreated: 2020-05-03 15:31:06 +0500\n\nModified: 2020-05-07 16:01:40 +0500\n\n---\n-   [Authentication](https://grpc.io/docs/guides/auth/)\n-   [Benchmarking](https://grpc.io/docs/guides/benchmarking/)\n-   [Contribution Guidelines](https://grpc.io/docs/guides/contributing/)\n-   [Error Handling](https://grpc.io/docs/guides/error/)\n-   [gRPC Concepts](https://grpc.io/docs/guides/concepts/)\n**Others**\n-   Custom data types\n    -   Enum\n    -   Message\n    -   Nested or not nested?\n    -   Well-known types\n-   Multiple proto files\n    -   Package\n    -   Import\n-   Repeated fields\n-   Oneof fields\n-   Go package option\n"},{"fields":{"slug":"/Computer-Science/Networking/Others/gRPC/Others/","title":"Others"},"frontmatter":{"draft":false},"rawBody":"# Others\n\nCreated: 2020-05-06 09:13:38 +0500\n\nModified: 2020-11-02 23:16:07 +0500\n\n---\n\n**How to automate grpc generation using git hooks, why grpc over REST + JSON**\n\n<https://medium.com/red-crane/grpc-and-why-it-can-save-you-development-time-436168fd0cbc>\nJSON is not useful where:\n-   Readability doesn't makes any sense and performance matters\n-   A standard API contract between machines\n-   No type safety\n**Python**\n\n<https://grpc.github.io/grpc/python>\n\n<https://medium.com/@biplav.nep/grpc-using-flask-restful-code-2ed5607ae9a>\n\n## grpc-web**\n\n<https://www.freecodecamp.org/news/how-to-use-grpc-web-with-react-1c93feb691b5>\n\n<https://medium.com/@karandeepahluwalia/grpc-web-and-react-a-match-made-in-heaven-51a7a8ec86e6>\n<https://www.youtube.com/watch?v=_RiZkNR__60&ab_channel=GOTOConferences>\n\n![](media/gRPC_Others-image1.png)\n![](media/gRPC_Others-image2.png)\n![](media/gRPC_Others-image3.png)\n![API Specification with gRPC Default gRPC service definitions caters to: Service, Remote methods and Types. O Advanced API specification options for: o o O O o Authentication schemes are JWT, Basic Auth and API Key Authorization Granular authorization at method level. Rate limiting/Throttling Applied for per-service or per-method basis. Versioning Policy Enforcement ](media/gRPC_Others-image4.png)\n![Cancellation When either the client or server application wants to terminate the RPC this can be done by Cance//ing the RPC. No further RPCs can be done over that connection. When one party cancels the RPC, the other party can determine it by checking the context of the RPC. E.g. stream. Context ( ) . Err ( ) o context . Canceled. ](media/gRPC_Others-image5.png)\n![Multiplexing o Running multiple gRPC services on the same gRPC server. gRPC Server App grpcServer grpc . NewServer () // Register Order Management service on gRPC orderMgtServer ordermgt_pb. RegisterOrderManagementServer ( grpcServer , &orderMgtServer { ) // Register Greeter Service on gRPC orderMgtServer hello_pb.RegisterGreeterServer (grpcServer, &he110Server{ } ) Request Response Service Serqce # N ](media/gRPC_Others-image6.png)\n![Metadata Information directly related to the service's business logic and consumer is part of the remote method invocation arguments. Use Metadata to share information about the RPC calls that are not related to the business context of the RPC (e.g. Security Headers) Structured as K-V pairs. Exchanged as gRPC headers. ](media/gRPC_Others-image7.png)\n![Deadlines A deadline is expressed in absolute time from the beginning of a request and applied across multiple service invocations. gRPC client applications sets deadline when invoking remote functions. aRPC Client ADP clientDeadIine time .Now() . Add (time. Duration (2 * time . Second) ) context . WithDeadIine (context . Background () , ctx, cancel clientDead1ine) // Invoke RPC ](media/gRPC_Others-image8.png)\n![gRPC Interceptors o Recpest Mechanism to execute some common logic before or after the execution of Respmse the remote function for server or client application. Server Side and Client Side interceptors. Unary Interceptors Phases : preprocessing, invoking the RPC o method, and postprocessing Streaming interceptors Intercepts any streaming RPC o Useful for logging, authentication, metrics etc. Interceptor 'N ](media/gRPC_Others-image9.png)"},{"fields":{"slug":"/Computer-Science/Networking/Protocols/TCP-(Connection-Oriented-Protocol)/Flow-Control/","title":"Flow Control"},"frontmatter":{"draft":false},"rawBody":"# Flow Control\n\nCreated: 2019-06-24 08:26:47 +0500\n\nModified: 2022-03-14 20:43:39 +0500\n\n---\n\nFlow Control basically means thatTCPwill ensure that a sender is not overwhelming a receiver by sending packets faster than it can consume. It's pretty similar to what's normally called*Back pressure*in the Distributed Systems literature. The idea is that a node receiving data will send some kind of feedback to the node sending the data to let it know about its current condition.\nIt's important to understand that this isnotthe same as*Congestion Control*. Although there's some overlap between the mechanismsTCPuses to provide both services, they are distinct features. Congestion control is about preventing a node from overwhelming the network (i.e. the links between two nodes), while Flow Control is about the end-node.\n**Additive increase, Multiplicative decrease**\n**rwnd - Receiver Window and cwnd - Congestion Window**\n\nCongestion Window (cwnd) is a TCP state variable that limits the amount of data the[TCP](https://en.wikipedia.org/wiki/Transmission_Control_Protocol)can send into the network before receiving an[ACK](https://en.wikipedia.org/wiki/Acknowledgement_(data_networks)). The Receiver Window (rwnd) is a variable that advertises the amount of data that the destination side can receive. Together, the two variables are used to regulate data flow in TCP connections, minimize congestion, and improve network performance.\n<https://blog.stackpath.com/glossary-cwnd-and-rwnd>\n\n## How it works**\n\nWhen we need to send data over a network, this is normally what happens.\n\n![dO 8 UO!JOO!lddV 0 N d01 V UO!3DO!lddV ](media/TCP-(Connection-Oriented-Protocol)_Flow-Control-image1.png)\n\nThe sender application writes data to a socket, the transport layer (in our case,TCP) will wrap this data in a segment and hand it to the network layer (e.g.IP), that will somehow route this packet to the receiving node.\nOn the other side of this communication, the network layer will deliver this piece of data toTCP, that will make it available to the receiver application as an exact copy of the data sent, meaning if will not deliver packets out of order, and will wait for a retransmission in case it notices a gap in the byte stream.\nIf we zoom in, we will see something like this.\n\n![- ã€ 3 q PI-IOS dO dO TCP Socket 00S d31 8 UO!JOO!lddV V LIO!JOO!lddV ](media/TCP-(Connection-Oriented-Protocol)_Flow-Control-image2.png)\n\nTCPstores the data it needs to send in thesend buffer, and the data it receives in thereceive buffer. When the application is ready, it will then read data from the receive buffer.\nFlow Control is all about making sure we don't send more packets when the receive buffer is already full, as the receiver wouldn't be able to handle them and would need to drop these packets.\nTo control the amount of data thatTCPcan send, the receiver will advertise its**Receive Window (rwnd)**, that is, the spare room in the receive buffer.\n\n![Data in the buffer Receive Window TCP Receive Buffer ](media/TCP-(Connection-Oriented-Protocol)_Flow-Control-image3.png)\n\nEvery timeTCPreceives a packet, it needs to send anackmessage to the sender, acknowledging it received that packet correctly, and with thisackmessage it sends the value of the current receive window, so the sender knows if it can keep sending data.\n**The sliding window**\n\nTCPuses a sliding window protocol to control the number of bytes in flight it can have. In other words, the number of bytes that were sent but not yetacked.\nLet's say we want to send a 150000 bytes file from node A to node B.TCPcould break this file down into 100 packets, 1500 bytes each. Now let's say that when the connection between node A and B is established, node B advertises a receive window of 45000 bytes, because it really wants to help us with our math here.\nSeeing that,TCPknows it can send the first 30 packets (1500 * 30 = 45000) before it receives an acknowledgment. If it gets anackmessage for the first 10 packets (meaning we now have only 20 packets in flight), and the receive window present in theseackmessages is still 45000, it can send the next 10 packets, bringing the number of packets in flight back to 30, that is the limit defined by the receive window. In other words, at any given point in time it can have 30 packets in flight, that were sent but not yetacked.\n\n![Window 10 ](media/TCP-(Connection-Oriented-Protocol)_Flow-Control-image4.png)\n\nExample of a sliding window. As soon as packet 3 is acked, we can slide the window to the right and send the packet 8.\nNow, if for some reason the application reading these packets in node B slows down,TCPwill still ack the packets that were correctly received, but as these packets need to be stored in the receive buffer until the application decides to read them, the receive window will be smaller, so even if TCP receives the acknowledgment for the next 10 packets (meaning there are currently 20 packets, or 30000 bytes, in flight), but the receive window value received in thisackis now 30000 (instead of 45000), it will not send more packets, as the number of bytes in flight is already equal to the latest receive window advertised.\nThe sender will always keep this invariant:\n\nLastByteSent - LastByteAcked <= ReceiveWindowAdvertised\n**Visualizing the Receive Window**\n\nJust to see this behavior in action, let's write a very simple application that reads data from a socket and watch how the receive window behaves when we make this application slower. We will use Wireshark to see these packets,netcat to send data to this application, and ago program to read data from the socket.\nHere's the simple go program that reads and prints the data received:\n\n```\npackage main\n\nimport (\n\"bufio\"\n\"fmt\"\n\"net\"\n)\n\nfunc main() {\nlistener, _ := net.Listen(\"tcp\", \"localhost:3040\")\nconn, _ := listener.Accept()\n\nfor {\nmessage, _ := bufio.NewReader(conn).ReadBytes('n')\nfmt.Println(string(message))\n}\n}\n```\n\nThis program will simply listen to connections on port 3040 and print the string received.\n\nWe can then use net cat to send data to this application:\n\n$ nc localhost 3040\n\nNow let's run this command to create a stream of data. It will simply add the string \"foo\" to a file, that we will use to send to this application:\n\n$ while true; do echo \"foo\" > stream.txt; done\nAnd now let's send this data to the application:\n\ntail -f stream.txt | nc localhost 3040\nThe application is still fast enough to keep up with the work, though. So let's make it a bit slower to see what happens:\n\n```\npackage main\n\nimport (\n\"bufio\"\n\"fmt\"\n\"net\"\n\"time\"\n)\n\nfunc main() {\nlistener, _ := net.Listen(\"tcp\", \"localhost:3040\")\nconn, _ := listener.Accept()\n\nfor {\nmessage, _ := bufio.NewReader(conn).ReadBytes('n')\nfmt.Println(string(message))\n+ time.Sleep(1 * time.Second)\n}\n}\n```\n\nNow we are sleeping for 1 second before we read data from the receive buffer. If we runnetcatagain and observeWireshark, it doesn't take long until the receive buffer is full andTCPstarts advertising a 0 window size:\n\n![No](media/TCP-(Connection-Oriented-Protocol)_Flow-Control-image5.png)\n\nAt this moment TCP will stop transmitting data, as the receiver's buffer is full.\n\n**The persist timer**\n\nThere's still one problem, though. After the receiver advertises a zero window, if it doesn't send any otherackmessage to the sender (or if theackis lost), it will never know when it can start sending data again. We will have a deadlock situation, where the receiver is waiting for more data, and the sender is waiting for a message saying it can start sending data again.\nTo solve this problem, whenTCPreceives a zero-window message it starts the *persist timer*, that will periodically send a small packet to the receiver (usually calledWindowProbe), so it has a chance to advertise a nonzero window size.\n\n![tcp.port](media/TCP-(Connection-Oriented-Protocol)_Flow-Control-image6.png)\n\nWhen there's some spare space in the receiver's buffer again it can advertise a non-zero window size and the transmission can continue.\n\n**Recap**\n-   TCP's flow control is a mechanism to ensure the sender is not overwhelming the receiver with more data than it can handle;\n-   With everyackmessage the receiver advertises its current receive window;\n-   The receive window is the spare space in the receive buffer, that is,rwnd = ReceiveBuffer - (LastByteReceived -- LastByteReadByApplication);\n-   TCPwill use a sliding window protocol to make sure it never has more bytes in flight than the window advertised by the receiver;\n-   When the window size is 0,TCPwill stop transmitting data and will start the persist timer;\n-   It will then periodically send a smallWindowProbemessage to the receiver to check if it can start receiving data again;\n-   When it receives a non-zero window size, it resumes the transmission.\n<https://www.brianstorti.com/tcp-flow-control>\n\n\n## TCP Slow Start\n\nSlow start is a mechanism which balances the speed of a TCP network connection. It escalates the amount of data transmitted until it finds the network's maximum carrying capacity.cwdnstands for the Congestion Window.\nBitTorrent uses tcp slow start\nslow start\" increases total throughput by keeping networks busy as in \"sliding window\" and also it solves end-to-end flow control by allowing the receiver to restrict transmission until it has sufficient buffer space to accommodate more data. Whenever the receiver sends ACK, the available buffer space is attached to the ACK (which is known as 'window advertisement'), so that the sender can decide its window size.\nIn TCP, the window size is defined as the minimum value between**cwnd(congestion window size)** andwindow advertisement.At any time, the window size cannot be greater thanmaximum cwndwhich is fixed.\nNot only when the sender starts to transmit at the first time, but also after collision and after idle periods between the sender and the receiver, TCP transmits its data in \"slow start\" fashion.\n![How Does BitTorrent Work? a Plain English Guide](media/TCP-(Connection-Oriented-Protocol)_Flow-Control-image7.png)<https://www.isi.edu/nsnam/DIRECTED_RESEARCH/DR_HYUNAH/D-Research/slow-start-tcp.html>\n\n## Flow Control vs Congestion Control\n\n![Flow Control Congestion Control Protect receiver from overloaded Protect the network itself Goal Trigger Overhead Performance of the receiver Small Bandwidth; loss rate Large Control Congestion Control ](media/TCP-(Connection-Oriented-Protocol)_Flow-Control-image8.png)\n\n## TCP Backlog\n\nWait, what is the TCP backlog? An application wishing to accept incoming TCP connections must issue thelistensyscall. This syscall instructs the OS to proceed with the TCP 3-way handshake when it receives initial SYN packets. During a handshake, metadata for a connection is maintained in a queue called the TCP backlog. The length of this queue dictates how many TCP connections can be establishing concurrently. The application issues accept syscalls to dequeue connections once they are established; this action frees space in the backlog for new connections.\nA backlog length of 1 means the queue will be full while just a single connection is establishing and then waiting to be dequeued. When the queue is full, the OS will drop initial SYN packets from other new connections. A client will resend an initial SYN packet if it doesn't receive a response, but after a couple attempts, it will stop and report failure to connect.\n\n![](media/TCP-(Connection-Oriented-Protocol)_Flow-Control-image9.png)\n\nWhy would setting the backlog length to 1 occasionally result in HTTP 502s? Most of the time, there are already established connections between the ALB and our API service, so the backlog length does not matter. However, when our service is unusually idle or is about to be upgraded, these connections are closed. A burst of API requests from just a single client at this point could cause the ALB to attempt to establish new TCP connections concurrently. Contention for the single slot in the TCP backlog would cause some of these connections to eventually report failure. The ALB responds to this failure by returning HTTP 502.\n<http://veithen.io/2014/01/01/how-tcp-backlog-works-in-linux.html>\n\nWhen an application puts a socket into LISTEN state using the[listen](http://linux.die.net/man/2/listen)syscall, it needs to specify a backlog for that socket. The backlog is usually described as the limit for the queue of incoming connections.\n\n![TCP state diagram](media/TCP-(Connection-Oriented-Protocol)_Flow-Control-image10.png)\n\nBecause of the 3-way handshake used by TCP, an incoming connection goes through an intermediate state SYN RECEIVED before it reaches the ESTABLISHED state and can be returned by the[accept](http://linux.die.net/man/2/accept)syscall to the application (see the part of the[TCP state diagram](http://commons.wikimedia.org/wiki/File:Tcp_state_diagram_fixed.svg)reproduced above). This means that a TCP/IP stack has two options to implement the backlog queue for a socket in LISTEN state:\n\n1.  The implementation uses a single queue, the size of which is determined by thebacklogargument of thelistensyscall. When a SYN packet is received, it sends back a SYN/ACK packet and adds the connection to the queue. When the corresponding ACK is received, the connection changes its state to ESTABLISHED and becomes eligible for handover to the application. This means that the queue can contain connections in two different state: SYN RECEIVED and ESTABLISHED. Only connections in the latter state can be returned to the application by theacceptsyscall.\n\n2.  The implementation uses two queues, a SYN queue (or incomplete connection queue) and an accept queue (or complete connection queue). Connections in state SYN RECEIVED are added to the SYN queue and later moved to the accept queue when their state changes to ESTABLISHED, i.e. when the ACK packet in the 3-way handshake is received. As the name implies, theacceptcall is then implemented simply to consume connections from the accept queue. In this case, thebacklogargument of thelistensyscall determines the size of the accept queue.\n**Note that a \"listen backlog\" of 100 connections doesn't mean that your server can only handle 100 simultaneous (or total) connections - this is instead dependent on the number of configured processes or threads. The listen backlog is a socket setting telling the kernel how to limit the number of outstanding (as yet unaccapted) connections in the listen queue of a listening socket. If the number of pending connections exceeds the specified size, new ones are automatically rejected. A functioning server regularly servicing its connections should not require a large backlog size.**\n\n"},{"fields":{"slug":"/placeholder/","title":"This Is a Placeholder File for Mdx"},"frontmatter":{"draft":true},"rawBody":"---\ntitle: This Is a Placeholder File for Mdx\ndraft: true\ntags:\n  - gatsby-theme-primer-wiki-placeholder\n---\n"}]}}}