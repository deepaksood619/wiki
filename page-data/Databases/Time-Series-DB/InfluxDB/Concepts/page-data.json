{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/Databases/Time-Series-DB/InfluxDB/Concepts/",
    "result": {"data":{"mdx":{"id":"d660127c-d65a-579b-83df-06b24011d010","tableOfContents":{"items":[{"url":"#concepts","title":"Concepts"}]},"fields":{"title":"Concepts","slug":"/Databases/Time-Series-DB/InfluxDB/Concepts/","url":"https://deepaksood619.github.io/wiki/wiki/Databases/Time-Series-DB/InfluxDB/Concepts/","editUrl":"https://github.com/deepaksood619/wiki/tree/main/Databases/Time-Series-DB/InfluxDB/Concepts.md","lastUpdatedAt":"2022-12-12T07:09:25.000Z","lastUpdated":"12/12/2022","gitCreatedAt":"2022-12-11T06:36:19.000Z","shouldShowTitle":false},"frontmatter":{"title":"","description":null,"imageAlt":null,"tags":[],"date":null,"dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"concepts\"\n  }, \"Concepts\"), mdx(\"p\", null, \"Created: 2019-07-01 23:36:24 +0500\"), mdx(\"p\", null, \"Modified: 2019-07-02 14:53:17 +0500\"), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"In-memory indexing and the Time-Structured Merge Tree (TSM)\")), mdx(\"p\", null, \"The InfluxDB storage engine looks very similar to a LSM Tree. It has a write ahead log and a collection of read-only data files which are similar in concept to SSTables in an LSM Tree. TSM files contain sorted, compressed series data.\\nInfluxDB will create a\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#shard\"\n  }, \"shard\"), \"for each block of time. For example, if you have a\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#retention-policy-rp\"\n  }, \"retention policy\"), \"with an unlimited duration, shards will be created for each 7 day block of time. Each of these shards maps to an underlying storage engine database. Each of these databases has its own\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#wal-write-ahead-log\"\n  }, \"WAL\"), \"and TSM files.\\n\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Storage Engine\")), mdx(\"p\", null, \"The storage engine ties a number of components together and provides the external interface for storing and querying series data. It is composed of a number of components that each serve a particular role:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"In-Memory Index -\"), \" The in-memory index is a shared index across shards that provides the quick access to\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#measurement\"\n  }, \"measurements\"), \",\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#tag\"\n  }, \"tags\"), \", and\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#series\"\n  }, \"series\"), \". The index is used by the engine, but is not specific to the storage engine itself.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"WAL -\"), \" The WAL is a write-optimized storage format that allows for writes to be durable, but not easily queryable. Writes to the WAL are appended to segments of a fixed size.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Cache -\"), \" The Cache is an in-memory representation of the data stored in the WAL. It is queried at runtime and merged with the data stored in TSM files.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"TSM Files -\"), \" TSM files store compressed series data in a columnar format.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"FileStore -\"), \" The FileStore mediates access to all TSM files on disk. It ensures that TSM files are installed atomically when existing ones are replaced as well as removing TSM files that are no longer used.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Compactor -\"), \" The Compactor is responsible for converting less optimized Cache and TSM data into more read-optimized formats. It does this by compressing series, removing deleted data, optimizing indices and combining smaller files into larger ones.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Compaction Planner -\"), \" The Compaction Planner determines which TSM files are ready for a compaction and ensures that multiple concurrent compactions do not interfere with each other.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Compression -\"), \" Compression is handled by various Encoders and Decoders for specific data types. Some encoders are fairly static and always encode the same type the same way; others switch their compression strategy based on the shape of the data.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Writers/Readers -\"), \" Each file type (WAL segment, TSM files, tombstones, etc..) has Writers and Readers for working with the formats.\\n\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#write-ahead-log-wal\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"Write Ahead Log (WAL)\")))), mdx(\"p\", null, \"The WAL is organized as a bunch of files that look like_000001.wal. The file numbers are monotonically increasing and referred to as WAL segments. When a segment reaches 10MB in size, it is closed and a new one is opened. Each WAL segment stores multiple compressed blocks of writes and deletes.\\nWhen a write comes in the new points are serialized, compressed using Snappy, and written to a WAL file. The file isfsync'd and the data is added to an in-memory index before a success is returned. This means that batching points together is required to achieve high throughput performance. (Optimal batch size seems to be 5,000-10,000 points per batch for many use cases.)\\nEach entry in the WAL follows a\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Type-length-value\"\n  }, \"TLV standard\"), \"with a single byte representing the type of entry (write or delete), a 4 byteuint32for the length of the compressed block, and then the compressed block.\\n\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TLV Standard\")), mdx(\"p\", null, \"Within\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Data_communication_protocol\"\n  }, \"data communication protocols\"), \",TLV(type-length-valueortag-length-value) is an encoding scheme used for optional information element in a certain protocol.\"), mdx(\"p\", null, \"The type and length are fixed in size (typically 1-4 bytes), and the value field is of variable size. These fields are used as follows:\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Type\")), mdx(\"p\", null, \"A binary code, often simply alphanumeric, which indicates the kind of field that this part of the message represents;\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Length\")), mdx(\"p\", null, \"The size of the value field (typically in bytes);\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Value\")), mdx(\"p\", null, \"Variable-sized series of bytes which contains data for this part of the message.\"), mdx(\"p\", null, \"Some advantages of using a TLV representation data system solution are:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"TLV sequences are easily searched using generalized parsing functions;\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"New message elements which are received at an older node can be safely skipped and the rest of the message can be parsed. This is similar to the way that unknown\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://en.wikipedia.org/wiki/XML\"\n  }, \"XML\"), \"tags can be safely skipped;\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"TLV elements can be placed in any order inside the message body;\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"TLV elements are typically used in a binary format which makes parsing faster and the data smaller than in comparable text based protocols.\\n\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://en.wikipedia.org/wiki/Type-length-value\"\n  }, \"https://en.wikipedia.org/wiki/Type-length-value\"), mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#cache\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"Cache\")))), mdx(\"p\", null, \"The Cache is an in-memory copy of all data points current stored in the WAL. The points are organized by the key, which is the measurement,\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#tag-set\"\n  }, \"tag set\"), \", and unique\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#field\"\n  }, \"field\"), \". Each field is kept as its own time-ordered range. The Cache data is not compressed while in memory.\\nQueries to the storage engine will merge data from the Cache with data from the TSM files. Queries execute on a copy of the data that is made from the cache at query processing time. This way writes that come in while a query is running won't affect the result.\\nDeletes sent to the Cache will clear out the given key or the specific time range for the given key.\\nThe Cache exposes a few controls for snapshotting behavior. The two most important controls are the memory limits. There is a lower bound,\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/administration/config#cache-snapshot-memory-size-25m\"\n  }, \"cache-snapshot-memory-size\"), \", which when exceeded will trigger a snapshot to TSM files and remove the corresponding WAL segments. There is also an upper bound,\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/administration/config#cache-max-memory-size-1g\"\n  }, \"cache-max-memory-size\"), \", which when exceeded will cause the Cache to reject new writes. These configurations are useful to prevent out of memory situations and to apply back pressure to clients writing data faster than the instance can persist it. The checks for memory thresholds occur on every write.\\nThe other snapshot controls are time based. The idle threshold,\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/administration/config#cache-snapshot-write-cold-duration-10m\"\n  }, \"cache-snapshot-write-cold-duration\"), \", forces the Cache to snapshot to TSM files if it hasn't received a write within the specified interval.\\nThe in-memory Cache is recreated on restart by re-reading the WAL files on disk.\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#tsm-files\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"TSM files\"))), mdx(\"p\", null, \"TSM files are a collection of read-only files that are memory mapped. The structure of these files looks very similar to an SSTable in LevelDB or other LSM Tree variants.\\nA TSM file is composed of four sections: header, blocks, index, and footer.\"), mdx(\"p\", null, \"\\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\\n\\u2502 Header \\u2502 Blocks \\u2502 Index \\u2502 Footer \\u2502\\n\\u25025 bytes \\u2502 N bytes \\u2502 N bytes \\u2502 4 bytes \\u2502\\n\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\"), mdx(\"p\", null, \"The Header is a magic number to identify the file type and a version number.\"), mdx(\"p\", null, \"\\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\\n\\u2502 Header \\u2502\\n\\u251C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2524\\n\\u2502 Magic \\u2502 Version \\u2502\\n\\u2502 4 bytes \\u2502 1 byte \\u2502\\n\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\"), mdx(\"p\", null, \"Blocks are sequences of pairs of CRC32 checksums and data. The block data is opaque to the file. The CRC32 is used for block level error detection. The length of the blocks is stored in the index.\"), mdx(\"p\", null, \"\\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\\n\\u2502 Blocks \\u2502\\n\\u251C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2524\\n\\u2502 Block 1 \\u2502 Block 2 \\u2502 Block N \\u2502\\n\\u251C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u253C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u253C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2524\\n\\u2502 CRC \\u2502 Data \\u2502 CRC \\u2502 Data \\u2502 CRC \\u2502 Data \\u2502\\n\\u2502 4 bytes \\u2502 N bytes \\u2502 4 bytes \\u2502 N bytes \\u2502 4 bytes \\u2502 N bytes \\u2502\\n\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\"), mdx(\"p\", null, \"Following the blocks is the index for the blocks in the file. The index is composed of a sequence of index entries ordered lexicographically by key and then by time. The key includes the measurement name, tag set, and one field. Multiple fields per point creates multiple index entries in the TSM file. Each index entry starts with a key length and the key, followed by the block type (float, int, bool, string) and a count of the number of index block entries that follow for that key. Each index block entry is composed of the min and max time for the block, the offset into the file where the block is located and the size of the block. There is one index block entry for each block in the TSM file that contains the key.\\nThe index structure can provide efficient access to all blocks as well as the ability to determine the cost associated with accessing a given key. Given a key and timestamp, we can determine whether a file contains the block for that timestamp. We can also determine where that block resides and how much data must be read to retrieve the block. Knowing the size of the block, we can efficiently provision our IO statements.\"), mdx(\"p\", null, \"\\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\\n\\u2502 Index \\u2502\\n\\u251C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2524\\n\\u2502 Key Len \\u2502 Key \\u2502 Type \\u2502 Count \\u2502Min Time \\u2502Max Time \\u2502 Offset \\u2502 Size \\u2502...\\u2502\\n\\u2502 2 bytes \\u2502 N bytes \\u25021 byte\\u25022 bytes\\u2502 8 bytes \\u2502 8 bytes \\u25028 bytes \\u25024 bytes \\u2502 \\u2502\\n\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2518\"), mdx(\"p\", null, \"The last section is the footer that stores the offset of the start of the index.\"), mdx(\"p\", null, \"\\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\\n\\u2502 Footer \\u2502\\n\\u251C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2524\\n\\u2502Index Ofs\\u2502\\n\\u2502 8 bytes \\u2502\\n\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#compression\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"Compression\"))), mdx(\"p\", null, \"Each block is compressed to reduce storage space and disk IO when querying. A block contains the timestamps and values for a given series and field. Each block has one byte header, followed by the compressed timestamps and then the compressed values.\"), mdx(\"p\", null, \"\\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\\n\\u2502 Type \\u2502 Len \\u2502 Timestamps \\u2502 Values \\u2502\\n\\u25021 Byte \\u2502VByte\\u2502 N Bytes \\u2502 N Bytes \\u2502\\n\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\"), mdx(\"p\", null, \"The timestamps and values are compressed and stored separately using encodings dependent on the data type and its shape. Storing them independently allows timestamp encoding to be used for all timestamps, while allowing different encodings for different field types. For example, some points may be able to use run-length encoding whereas other may not.\\nEach value type also contains a 1 byte header indicating the type of compression for the remaining bytes. The four high bits store the compression type and the four low bits are used by the encoder if needed.\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#compactions\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"Compactions\"))), mdx(\"p\", null, \"Compactions are recurring processes that migrate data stored in a write-optimized format into a more read-optimized format. There are a number of stages of compaction that take place while a shard is hot for writes:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Snapshots -\"), \" Values in the Cache and WAL must be converted to TSM files to free memory and disk space used by the WAL segments. These compactions occur based on the cache memory and time thresholds.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Level Compactions -\"), \" Level compactions (levels 1-4) occur as the TSM files grow. TSM files are compacted from snapshots to level 1 files. Multiple level 1 files are compacted to produce level 2 files. The process continues until files reach level 4 and the max size for a TSM file. They will not be compacted further unless deletes, index optimization compactions, or full compactions need to run. Lower level compactions use strategies that avoid CPU-intensive activities like decompressing and combining blocks. Higher level (and thus less frequent) compactions will re-combine blocks to fully compact them and increase the compression ratio.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Index Optimization -\"), \" When many level 4 TSM files accumulate, the internal indexes become larger and more costly to access. An index optimization compaction splits the series and indices across a new set of TSM files, sorting all points for a given series into one TSM file. Before an index optimization, each TSM file contained points for most or all series, and thus each contains the same series index. After an index optimization, each TSM file contains points from a minimum of series and there is little series overlap between files. Each TSM file thus has a smaller unique series index, instead of a duplicate of the full series list. In addition, all points from a particular series are contiguous in a TSM file rather than spread across multiple TSM files.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Full Compactions -\"), \" Full compactions run when a shard has become cold for writes for long time, or when deletes have occurred on the shard. Full compactions produce an optimal set of TSM files and include all optimizations from Level and Index Optimization compactions. Once a shard is fully compacted, no other compactions will run on it unless new writes or deletes are stored.\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#writes\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"Writes\"))), mdx(\"p\", null, \"Writes are appended to the current WAL segment and are also added to the Cache. Each WAL segment has a maximum size. Writes roll over to a new file once the current file fills up. The cache is also size bounded; snapshots are taken and WAL compactions are initiated when the cache becomes too full. If the inbound write rate exceeds the WAL compaction rate for a sustained period, the cache may become too full, in which case new writes will fail until the snapshot process catches up.\\nWhen WAL segments fill up and are closed, the Compactor snapshots the Cache and writes the data to a new TSM file. When the TSM file is successfully written andfsync'd, it is loaded and referenced by the FileStore.\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#updates\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"Updates\"))), mdx(\"p\", null, \"Updates (writing a newer value for a point that already exists) occur as normal writes. Since cached values overwrite existing values, newer writes take precedence. If a write would overwrite a point in a prior TSM file, the points are merged at query runtime and the newer write takes precedence.\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#deletes\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"Deletes\"))), mdx(\"p\", null, \"Deletes occur by writing a delete entry to the WAL for the measurement or series and then updating the Cache and FileStore. The Cache evicts all relevant entries. The FileStore writes a tombstone file for each TSM file that contains relevant data. These tombstone files are used at startup time to ignore blocks as well as during compactions to remove deleted entries.\\nQueries against partially deleted series are handled at query time until a compaction removes the data fully from the TSM files.\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#queries\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"Queries\"))), mdx(\"p\", null, \"When a query is executed by the storage engine, it is essentially a seek to a given time associated with a specific series key and field. First, we do a search on the data files to find the files that contain a time range matching the query as well containing matching series.\\nOnce we have the data files selected, we next need to find the position in the file of the series key index entries. We run a binary search against each TSM index to find the location of its index blocks.\\nIn common cases the blocks will not overlap across multiple TSM files and we can search the index entries linearly to find the start block from which to read. If there are overlapping blocks of time, the index entries are sorted to ensure newer writes will take precedence and that blocks can be processed in order during query execution.\\nWhen iterating over the index entries the blocks are read sequentially from the blocks section. The block is decompressed and we seek to the specific point.\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#properties-of-time-series-data\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"Properties of time series data\"))), mdx(\"p\", null, \"The workload of time series data is quite different from normal database workloads. There are a number of factors that conspire to make it very difficult to scale and remain performant:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Billions of individual data points\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"High write throughput\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"High read throughput\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Large deletes (data expiration)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Mostly an insert/append workload, very few updates\\nThe first and most obvious problem is one of scale. In DevOps, IoT, or APM it is easy to collect hundreds of millions or billions of unique data points every day.\\nFor example, let's say we have 200 VMs or servers running, with each server collecting an average of 100 measurements every 10 seconds. Given there are 86,400 seconds in a day, a single measurement will generate 8,640 points in a day per server. That gives us a total of 172,800,000 (200 \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \" 100 \"), \" 8,640) individual data points per day. We find similar or larger numbers in sensor data use cases.\\nThe volume of data means that the write throughput can be very high. We regularly get requests for setups than can handle hundreds of thousands of writes per second. Some larger companies will only consider systems that can handle millions of writes per second.\\nAt the same time, time series data can be a high read throughput use case. It's true that if you're tracking 700,000 unique metrics or time series you can't hope to visualize all of them. That leads many people to think that you don't actually read most of the data that goes into the database. However, other than dashboards that people have up on their screens, there are automated systems for monitoring or combining the large volume of time series data with other types of data.\\nInside InfluxDB, aggregate functions calculated on the fly may combine tens of thousands of distinct time series into a single view. Each one of those queries must read each aggregated data point, so for InfluxDB the read throughput is often many times higher than the write throughput.\\nGiven that time series is mostly an append-only workload, you might think that it's possible to get great performance on a B+Tree. Appends in the keyspace are efficient and you can achieve greater than 100,000 per second. However, we have those appends happening in individual time series. So the inserts end up looking more like random inserts than append only inserts.\\nOne of the biggest problems we found with time series data is that it's very common to delete all data after it gets past a certain age. The common pattern here is that users have high precision data that is kept for a short period of time like a few days or months. Users then downsample and aggregate that data into lower precision rollups that are kept around much longer.\\nThe naive implementation would be to simply delete each record once it passes its expiration time. However, that means that once the first points written reach their expiration date, the system is processing just as many deletes as writes, which is something most storage engines aren't designed for.\\nLet's dig into the details of the two types of storage engines we tried and how these properties had a significant impact on our performance.\\n\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#leveldb-and-log-structured-merge-trees\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"LevelDB and log structured merge trees\")))), mdx(\"p\", null, \"When the InfluxDB project began, we picked LevelDB as the storage engine because we had used it for time series data storage in the product that was the precursor to InfluxDB. We knew that it had great properties for write throughput and everything seemed to \\\"just work\\\".\"), mdx(\"p\", null, \"LevelDB is an implementation of a log structured merge tree (LSM tree) that was built as an open source project at Google. It exposes an API for a key-value store where the key space is sorted. This last part is important for time series data as it allowed us to quickly scan ranges of time as long as the timestamp was in the key.\\nLSM Trees are based on a log that takes writes and two structures known as Mem Tables and SSTables. These tables represent the sorted keyspace. SSTables are read only files that are continuously replaced by other SSTables that merge inserts and updates into the keyspace.\"), mdx(\"p\", null, \"The two biggest advantages that LevelDB had for us were high write throughput and built in compression. However, as we learned more about what people needed with time series data, we encountered a few insurmountable challenges.\\nThe first problem we had was that LevelDB doesn't support hot backups. If you want to do a safe backup of the database, you have to close it and then copy it. The LevelDB variants RocksDB and HyperLevelDB fix this problem, but there was another more pressing problem that we didn't think they could solve.\\nOur users needed a way to automatically manage data retention. That meant we needed deletes on a very large scale. In LSM Trees, a delete is as expensive, if not more so, than a write. A delete writes a new record known as a tombstone. After that queries merge the result set with any tombstones to purge the deleted data from the query return. Later, a compaction runs that removes the tombstone record and the underlying deleted record in the SSTable file.\\nTo get around doing deletes, we split data across what we call shards, which are contiguous blocks of time. Shards would typically hold either one day or seven days worth of data. Each shard mapped to an underlying LevelDB. This meant that we could drop an entire day of data by just closing out the database and removing the underlying files.\\nUsers of RocksDB may at this point bring up a feature called ColumnFamilies. When putting time series data into Rocks, it's common to split blocks of time into column families and then drop those when their time is up. It's the same general idea: create a separate area where you can just drop files instead of updating indexes when you delete a large block of data. Dropping a column family is a very efficient operation. However, column families are a fairly new feature and we had another use case for shards.\\nOrganizing data into shards meant that it could be moved within a cluster without having to examine billions of keys. At the time of this writing, it was not possible to move a column family in one RocksDB to another. Old shards are typically cold for writes so moving them around would be cheap and easy. We would have the added benefit of having a spot in the keyspace that is cold for writes so it would be easier to do consistency checks later.\\nThe organization of data into shards worked great for a while, until a large amount of data went into InfluxDB. LevelDB splits the data out over many small files. Having dozens or hundreds of these databases open in a single process ended up creating a big problem. Users that had six months or a year of data would run out of file handles. It's not something we found with the majority of users, but anyone pushing the database to its limits would hit this problem and we had no fix for it. There were simply too many file handles open.\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#boltdb-and-mmap-b-trees\"\n  }, mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"BoltDB and mmap B+Trees\"))), mdx(\"p\", null, \"After struggling with LevelDB and its variants for a year we decided to move over to BoltDB, a pure Golang database heavily inspired by LMDB, a mmap B+Tree database written in C. It has the same API semantics as LevelDB: a key value store where the keyspace is ordered. Many of our users were surprised. Our own posted tests of the LevelDB variants vs. LMDB (a mmap B+Tree) showed RocksDB as the best performer.\\nHowever, there were other considerations that went into this decision outside of the pure write performance. At this point our most important goal was to get to something stable that could be run in production and backed up. BoltDB also had the advantage of being written in pure Go, which simplified our build chain immensely and made it easy to build for other OSes and platforms.\\nThe biggest win for us was that BoltDB used a single file as the database. At this point our most common source of bug reports were from people running out of file handles. Bolt solved the hot backup problem and the file limit problems all at the same time.\\nWe were willing to take a hit on write throughput if it meant that we'd have a system that was more reliable and stable that we could build on. Our reasoning was that for anyone pushing really big write loads, they'd be running a cluster anyway.\\nWe released versions 0.9.0 to 0.9.2 based on BoltDB. From a development perspective it was delightful. Clean API, fast and easy to build in our Go project, and reliable. However, after running for a while we found a big problem with write throughput. After the database got over a few GB, writes would start spiking IOPS.\\nSome users were able to get past this by putting InfluxDB on big hardware with near unlimited IOPS. However, most users are on VMs with limited resources in the cloud. We had to figure out a way to reduce the impact of writing a bunch of points into hundreds of thousands of series at a time.\\nWith the 0.9.3 and 0.9.4 releases our plan was to put a write ahead log (WAL) in front of Bolt. That way we could reduce the number of random insertions into the keyspace. Instead, we'd buffer up multiple writes that were next to each other and then flush them at once. However, that only served to delay the problem. High IOPS still became an issue and it showed up very quickly for anyone operating at even moderate work loads.\\nHowever, our experience building the first WAL implementation in front of Bolt gave us the confidence we needed that the write problem could be solved. The performance of the WAL itself was fantastic, the index simply could not keep up. At this point we started thinking again about how we could create something similar to an LSM Tree that could keep up with our write load.\\nThus was born the \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Time Structured Merge Tree.\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#the-new-influxdb-storage-engine-and-lsm-refined\"\n  }, \"https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#the-new-influxdb-storage-engine-and-lsm-refined\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"# Concepts\n\nCreated: 2019-07-01 23:36:24 +0500\n\nModified: 2019-07-02 14:53:17 +0500\n\n---\n\n**In-memory indexing and the Time-Structured Merge Tree (TSM)**\n\nThe InfluxDB storage engine looks very similar to a LSM Tree. It has a write ahead log and a collection of read-only data files which are similar in concept to SSTables in an LSM Tree. TSM files contain sorted, compressed series data.\nInfluxDB will create a[shard](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#shard)for each block of time. For example, if you have a[retention policy](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#retention-policy-rp)with an unlimited duration, shards will be created for each 7 day block of time. Each of these shards maps to an underlying storage engine database. Each of these databases has its own[WAL](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#wal-write-ahead-log)and TSM files.\n**Storage Engine**\n\nThe storage engine ties a number of components together and provides the external interface for storing and querying series data. It is composed of a number of components that each serve a particular role:\n-   **In-Memory Index -** The in-memory index is a shared index across shards that provides the quick access to[measurements](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#measurement),[tags](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#tag), and[series](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#series). The index is used by the engine, but is not specific to the storage engine itself.\n-   **WAL -** The WAL is a write-optimized storage format that allows for writes to be durable, but not easily queryable. Writes to the WAL are appended to segments of a fixed size.\n-   **Cache -** The Cache is an in-memory representation of the data stored in the WAL. It is queried at runtime and merged with the data stored in TSM files.\n-   **TSM Files -** TSM files store compressed series data in a columnar format.\n-   **FileStore -** The FileStore mediates access to all TSM files on disk. It ensures that TSM files are installed atomically when existing ones are replaced as well as removing TSM files that are no longer used.\n-   **Compactor -** The Compactor is responsible for converting less optimized Cache and TSM data into more read-optimized formats. It does this by compressing series, removing deleted data, optimizing indices and combining smaller files into larger ones.\n-   **Compaction Planner -** The Compaction Planner determines which TSM files are ready for a compaction and ensures that multiple concurrent compactions do not interfere with each other.\n-   **Compression -** Compression is handled by various Encoders and Decoders for specific data types. Some encoders are fairly static and always encode the same type the same way; others switch their compression strategy based on the shape of the data.\n-   **Writers/Readers -** Each file type (WAL segment, TSM files, tombstones, etc..) has Writers and Readers for working with the formats.\n[**Write Ahead Log (WAL)**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#write-ahead-log-wal)\n\nThe WAL is organized as a bunch of files that look like_000001.wal. The file numbers are monotonically increasing and referred to as WAL segments. When a segment reaches 10MB in size, it is closed and a new one is opened. Each WAL segment stores multiple compressed blocks of writes and deletes.\nWhen a write comes in the new points are serialized, compressed using Snappy, and written to a WAL file. The file isfsync'd and the data is added to an in-memory index before a success is returned. This means that batching points together is required to achieve high throughput performance. (Optimal batch size seems to be 5,000-10,000 points per batch for many use cases.)\nEach entry in the WAL follows a[TLV standard](https://en.wikipedia.org/wiki/Type-length-value)with a single byte representing the type of entry (write or delete), a 4 byteuint32for the length of the compressed block, and then the compressed block.\n**TLV Standard**\n\nWithin[data communication protocols](https://en.wikipedia.org/wiki/Data_communication_protocol),TLV(type-length-valueortag-length-value) is an encoding scheme used for optional information element in a certain protocol.\n\nThe type and length are fixed in size (typically 1-4 bytes), and the value field is of variable size. These fields are used as follows:\n\n**Type**\n\nA binary code, often simply alphanumeric, which indicates the kind of field that this part of the message represents;\n\n**Length**\n\nThe size of the value field (typically in bytes);\n\n**Value**\n\nVariable-sized series of bytes which contains data for this part of the message.\n\nSome advantages of using a TLV representation data system solution are:\n-   TLV sequences are easily searched using generalized parsing functions;\n-   New message elements which are received at an older node can be safely skipped and the rest of the message can be parsed. This is similar to the way that unknown[XML](https://en.wikipedia.org/wiki/XML)tags can be safely skipped;\n-   TLV elements can be placed in any order inside the message body;\n-   TLV elements are typically used in a binary format which makes parsing faster and the data smaller than in comparable text based protocols.\n<https://en.wikipedia.org/wiki/Type-length-value>\n[**Cache**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#cache)\n\nThe Cache is an in-memory copy of all data points current stored in the WAL. The points are organized by the key, which is the measurement,[tag set](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#tag-set), and unique[field](https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#field). Each field is kept as its own time-ordered range. The Cache data is not compressed while in memory.\nQueries to the storage engine will merge data from the Cache with data from the TSM files. Queries execute on a copy of the data that is made from the cache at query processing time. This way writes that come in while a query is running won't affect the result.\nDeletes sent to the Cache will clear out the given key or the specific time range for the given key.\nThe Cache exposes a few controls for snapshotting behavior. The two most important controls are the memory limits. There is a lower bound,[cache-snapshot-memory-size](https://docs.influxdata.com/influxdb/v1.7/administration/config#cache-snapshot-memory-size-25m), which when exceeded will trigger a snapshot to TSM files and remove the corresponding WAL segments. There is also an upper bound,[cache-max-memory-size](https://docs.influxdata.com/influxdb/v1.7/administration/config#cache-max-memory-size-1g), which when exceeded will cause the Cache to reject new writes. These configurations are useful to prevent out of memory situations and to apply back pressure to clients writing data faster than the instance can persist it. The checks for memory thresholds occur on every write.\nThe other snapshot controls are time based. The idle threshold,[cache-snapshot-write-cold-duration](https://docs.influxdata.com/influxdb/v1.7/administration/config#cache-snapshot-write-cold-duration-10m), forces the Cache to snapshot to TSM files if it hasn't received a write within the specified interval.\nThe in-memory Cache is recreated on restart by re-reading the WAL files on disk.\n[**TSM files**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#tsm-files)\n\nTSM files are a collection of read-only files that are memory mapped. The structure of these files looks very similar to an SSTable in LevelDB or other LSM Tree variants.\nA TSM file is composed of four sections: header, blocks, index, and footer.\n\n┌────────┬────────────────────────────────────┬─────────────┬──────────────┐\n│ Header │ Blocks │ Index │ Footer │\n│5 bytes │ N bytes │ N bytes │ 4 bytes │\n└────────┴────────────────────────────────────┴─────────────┴──────────────┘\n\nThe Header is a magic number to identify the file type and a version number.\n\n┌───────────────────┐\n│ Header │\n├─────────┬─────────┤\n│ Magic │ Version │\n│ 4 bytes │ 1 byte │\n└─────────┴─────────┘\n\nBlocks are sequences of pairs of CRC32 checksums and data. The block data is opaque to the file. The CRC32 is used for block level error detection. The length of the blocks is stored in the index.\n\n┌───────────────────────────────────────────────────────────┐\n│ Blocks │\n├───────────────────┬───────────────────┬───────────────────┤\n│ Block 1 │ Block 2 │ Block N │\n├─────────┬─────────┼─────────┬─────────┼─────────┬─────────┤\n│ CRC │ Data │ CRC │ Data │ CRC │ Data │\n│ 4 bytes │ N bytes │ 4 bytes │ N bytes │ 4 bytes │ N bytes │\n└─────────┴─────────┴─────────┴─────────┴─────────┴─────────┘\n\nFollowing the blocks is the index for the blocks in the file. The index is composed of a sequence of index entries ordered lexicographically by key and then by time. The key includes the measurement name, tag set, and one field. Multiple fields per point creates multiple index entries in the TSM file. Each index entry starts with a key length and the key, followed by the block type (float, int, bool, string) and a count of the number of index block entries that follow for that key. Each index block entry is composed of the min and max time for the block, the offset into the file where the block is located and the size of the block. There is one index block entry for each block in the TSM file that contains the key.\nThe index structure can provide efficient access to all blocks as well as the ability to determine the cost associated with accessing a given key. Given a key and timestamp, we can determine whether a file contains the block for that timestamp. We can also determine where that block resides and how much data must be read to retrieve the block. Knowing the size of the block, we can efficiently provision our IO statements.\n\n┌────────────────────────────────────────────────────────────────────────────┐\n│ Index │\n├─────────┬─────────┬──────┬───────┬─────────┬─────────┬────────┬────────┬───┤\n│ Key Len │ Key │ Type │ Count │Min Time │Max Time │ Offset │ Size │...│\n│ 2 bytes │ N bytes │1 byte│2 bytes│ 8 bytes │ 8 bytes │8 bytes │4 bytes │ │\n└─────────┴─────────┴──────┴───────┴─────────┴─────────┴────────┴────────┴───┘\n\nThe last section is the footer that stores the offset of the start of the index.\n\n┌─────────┐\n│ Footer │\n├─────────┤\n│Index Ofs│\n│ 8 bytes │\n└─────────┘\n\n[**Compression**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#compression)\n\nEach block is compressed to reduce storage space and disk IO when querying. A block contains the timestamps and values for a given series and field. Each block has one byte header, followed by the compressed timestamps and then the compressed values.\n\n┌───────┬─────┬─────────────────┬──────────────────┐\n│ Type │ Len │ Timestamps │ Values │\n│1 Byte │VByte│ N Bytes │ N Bytes │\n└───────┴─────┴─────────────────┴──────────────────┘\n\nThe timestamps and values are compressed and stored separately using encodings dependent on the data type and its shape. Storing them independently allows timestamp encoding to be used for all timestamps, while allowing different encodings for different field types. For example, some points may be able to use run-length encoding whereas other may not.\nEach value type also contains a 1 byte header indicating the type of compression for the remaining bytes. The four high bits store the compression type and the four low bits are used by the encoder if needed.\n[**Compactions**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#compactions)\n\nCompactions are recurring processes that migrate data stored in a write-optimized format into a more read-optimized format. There are a number of stages of compaction that take place while a shard is hot for writes:\n-   **Snapshots -** Values in the Cache and WAL must be converted to TSM files to free memory and disk space used by the WAL segments. These compactions occur based on the cache memory and time thresholds.\n-   **Level Compactions -** Level compactions (levels 1-4) occur as the TSM files grow. TSM files are compacted from snapshots to level 1 files. Multiple level 1 files are compacted to produce level 2 files. The process continues until files reach level 4 and the max size for a TSM file. They will not be compacted further unless deletes, index optimization compactions, or full compactions need to run. Lower level compactions use strategies that avoid CPU-intensive activities like decompressing and combining blocks. Higher level (and thus less frequent) compactions will re-combine blocks to fully compact them and increase the compression ratio.\n-   **Index Optimization -** When many level 4 TSM files accumulate, the internal indexes become larger and more costly to access. An index optimization compaction splits the series and indices across a new set of TSM files, sorting all points for a given series into one TSM file. Before an index optimization, each TSM file contained points for most or all series, and thus each contains the same series index. After an index optimization, each TSM file contains points from a minimum of series and there is little series overlap between files. Each TSM file thus has a smaller unique series index, instead of a duplicate of the full series list. In addition, all points from a particular series are contiguous in a TSM file rather than spread across multiple TSM files.\n-   **Full Compactions -** Full compactions run when a shard has become cold for writes for long time, or when deletes have occurred on the shard. Full compactions produce an optimal set of TSM files and include all optimizations from Level and Index Optimization compactions. Once a shard is fully compacted, no other compactions will run on it unless new writes or deletes are stored.\n\n[**Writes**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#writes)\n\nWrites are appended to the current WAL segment and are also added to the Cache. Each WAL segment has a maximum size. Writes roll over to a new file once the current file fills up. The cache is also size bounded; snapshots are taken and WAL compactions are initiated when the cache becomes too full. If the inbound write rate exceeds the WAL compaction rate for a sustained period, the cache may become too full, in which case new writes will fail until the snapshot process catches up.\nWhen WAL segments fill up and are closed, the Compactor snapshots the Cache and writes the data to a new TSM file. When the TSM file is successfully written andfsync'd, it is loaded and referenced by the FileStore.\n[**Updates**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#updates)\n\nUpdates (writing a newer value for a point that already exists) occur as normal writes. Since cached values overwrite existing values, newer writes take precedence. If a write would overwrite a point in a prior TSM file, the points are merged at query runtime and the newer write takes precedence.\n[**Deletes**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#deletes)\n\nDeletes occur by writing a delete entry to the WAL for the measurement or series and then updating the Cache and FileStore. The Cache evicts all relevant entries. The FileStore writes a tombstone file for each TSM file that contains relevant data. These tombstone files are used at startup time to ignore blocks as well as during compactions to remove deleted entries.\nQueries against partially deleted series are handled at query time until a compaction removes the data fully from the TSM files.\n[**Queries**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#queries)\n\nWhen a query is executed by the storage engine, it is essentially a seek to a given time associated with a specific series key and field. First, we do a search on the data files to find the files that contain a time range matching the query as well containing matching series.\nOnce we have the data files selected, we next need to find the position in the file of the series key index entries. We run a binary search against each TSM index to find the location of its index blocks.\nIn common cases the blocks will not overlap across multiple TSM files and we can search the index entries linearly to find the start block from which to read. If there are overlapping blocks of time, the index entries are sorted to ensure newer writes will take precedence and that blocks can be processed in order during query execution.\nWhen iterating over the index entries the blocks are read sequentially from the blocks section. The block is decompressed and we seek to the specific point.\n[**Properties of time series data**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#properties-of-time-series-data)\n\nThe workload of time series data is quite different from normal database workloads. There are a number of factors that conspire to make it very difficult to scale and remain performant:\n-   Billions of individual data points\n-   High write throughput\n-   High read throughput\n-   Large deletes (data expiration)\n-   Mostly an insert/append workload, very few updates\nThe first and most obvious problem is one of scale. In DevOps, IoT, or APM it is easy to collect hundreds of millions or billions of unique data points every day.\nFor example, let's say we have 200 VMs or servers running, with each server collecting an average of 100 measurements every 10 seconds. Given there are 86,400 seconds in a day, a single measurement will generate 8,640 points in a day per server. That gives us a total of 172,800,000 (200 * 100 * 8,640) individual data points per day. We find similar or larger numbers in sensor data use cases.\nThe volume of data means that the write throughput can be very high. We regularly get requests for setups than can handle hundreds of thousands of writes per second. Some larger companies will only consider systems that can handle millions of writes per second.\nAt the same time, time series data can be a high read throughput use case. It's true that if you're tracking 700,000 unique metrics or time series you can't hope to visualize all of them. That leads many people to think that you don't actually read most of the data that goes into the database. However, other than dashboards that people have up on their screens, there are automated systems for monitoring or combining the large volume of time series data with other types of data.\nInside InfluxDB, aggregate functions calculated on the fly may combine tens of thousands of distinct time series into a single view. Each one of those queries must read each aggregated data point, so for InfluxDB the read throughput is often many times higher than the write throughput.\nGiven that time series is mostly an append-only workload, you might think that it's possible to get great performance on a B+Tree. Appends in the keyspace are efficient and you can achieve greater than 100,000 per second. However, we have those appends happening in individual time series. So the inserts end up looking more like random inserts than append only inserts.\nOne of the biggest problems we found with time series data is that it's very common to delete all data after it gets past a certain age. The common pattern here is that users have high precision data that is kept for a short period of time like a few days or months. Users then downsample and aggregate that data into lower precision rollups that are kept around much longer.\nThe naive implementation would be to simply delete each record once it passes its expiration time. However, that means that once the first points written reach their expiration date, the system is processing just as many deletes as writes, which is something most storage engines aren't designed for.\nLet's dig into the details of the two types of storage engines we tried and how these properties had a significant impact on our performance.\n[**LevelDB and log structured merge trees**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#leveldb-and-log-structured-merge-trees)\n\nWhen the InfluxDB project began, we picked LevelDB as the storage engine because we had used it for time series data storage in the product that was the precursor to InfluxDB. We knew that it had great properties for write throughput and everything seemed to \"just work\".\n\nLevelDB is an implementation of a log structured merge tree (LSM tree) that was built as an open source project at Google. It exposes an API for a key-value store where the key space is sorted. This last part is important for time series data as it allowed us to quickly scan ranges of time as long as the timestamp was in the key.\nLSM Trees are based on a log that takes writes and two structures known as Mem Tables and SSTables. These tables represent the sorted keyspace. SSTables are read only files that are continuously replaced by other SSTables that merge inserts and updates into the keyspace.\n\nThe two biggest advantages that LevelDB had for us were high write throughput and built in compression. However, as we learned more about what people needed with time series data, we encountered a few insurmountable challenges.\nThe first problem we had was that LevelDB doesn't support hot backups. If you want to do a safe backup of the database, you have to close it and then copy it. The LevelDB variants RocksDB and HyperLevelDB fix this problem, but there was another more pressing problem that we didn't think they could solve.\nOur users needed a way to automatically manage data retention. That meant we needed deletes on a very large scale. In LSM Trees, a delete is as expensive, if not more so, than a write. A delete writes a new record known as a tombstone. After that queries merge the result set with any tombstones to purge the deleted data from the query return. Later, a compaction runs that removes the tombstone record and the underlying deleted record in the SSTable file.\nTo get around doing deletes, we split data across what we call shards, which are contiguous blocks of time. Shards would typically hold either one day or seven days worth of data. Each shard mapped to an underlying LevelDB. This meant that we could drop an entire day of data by just closing out the database and removing the underlying files.\nUsers of RocksDB may at this point bring up a feature called ColumnFamilies. When putting time series data into Rocks, it's common to split blocks of time into column families and then drop those when their time is up. It's the same general idea: create a separate area where you can just drop files instead of updating indexes when you delete a large block of data. Dropping a column family is a very efficient operation. However, column families are a fairly new feature and we had another use case for shards.\nOrganizing data into shards meant that it could be moved within a cluster without having to examine billions of keys. At the time of this writing, it was not possible to move a column family in one RocksDB to another. Old shards are typically cold for writes so moving them around would be cheap and easy. We would have the added benefit of having a spot in the keyspace that is cold for writes so it would be easier to do consistency checks later.\nThe organization of data into shards worked great for a while, until a large amount of data went into InfluxDB. LevelDB splits the data out over many small files. Having dozens or hundreds of these databases open in a single process ended up creating a big problem. Users that had six months or a year of data would run out of file handles. It's not something we found with the majority of users, but anyone pushing the database to its limits would hit this problem and we had no fix for it. There were simply too many file handles open.\n[**BoltDB and mmap B+Trees**](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#boltdb-and-mmap-b-trees)\n\nAfter struggling with LevelDB and its variants for a year we decided to move over to BoltDB, a pure Golang database heavily inspired by LMDB, a mmap B+Tree database written in C. It has the same API semantics as LevelDB: a key value store where the keyspace is ordered. Many of our users were surprised. Our own posted tests of the LevelDB variants vs. LMDB (a mmap B+Tree) showed RocksDB as the best performer.\nHowever, there were other considerations that went into this decision outside of the pure write performance. At this point our most important goal was to get to something stable that could be run in production and backed up. BoltDB also had the advantage of being written in pure Go, which simplified our build chain immensely and made it easy to build for other OSes and platforms.\nThe biggest win for us was that BoltDB used a single file as the database. At this point our most common source of bug reports were from people running out of file handles. Bolt solved the hot backup problem and the file limit problems all at the same time.\nWe were willing to take a hit on write throughput if it meant that we'd have a system that was more reliable and stable that we could build on. Our reasoning was that for anyone pushing really big write loads, they'd be running a cluster anyway.\nWe released versions 0.9.0 to 0.9.2 based on BoltDB. From a development perspective it was delightful. Clean API, fast and easy to build in our Go project, and reliable. However, after running for a while we found a big problem with write throughput. After the database got over a few GB, writes would start spiking IOPS.\nSome users were able to get past this by putting InfluxDB on big hardware with near unlimited IOPS. However, most users are on VMs with limited resources in the cloud. We had to figure out a way to reduce the impact of writing a bunch of points into hundreds of thousands of series at a time.\nWith the 0.9.3 and 0.9.4 releases our plan was to put a write ahead log (WAL) in front of Bolt. That way we could reduce the number of random insertions into the keyspace. Instead, we'd buffer up multiple writes that were next to each other and then flush them at once. However, that only served to delay the problem. High IOPS still became an issue and it showed up very quickly for anyone operating at even moderate work loads.\nHowever, our experience building the first WAL implementation in front of Bolt gave us the confidence we needed that the write problem could be solved. The performance of the WAL itself was fantastic, the index simply could not keep up. At this point we started thinking again about how we could create something similar to an LSM Tree that could keep up with our write load.\nThus was born the **Time Structured Merge Tree.**\n<https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/#the-new-influxdb-storage-engine-and-lsm-refined>\n","excerpt":"Concepts Created: 2019-07-01 23:36:24 +0500 Modified: 2019-07-02 14:53:17 +0500 In-memory indexing and the Time-Structured Merge Tree (TSM)…","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/Databases/Time-Series-DB/InfluxDB/Concepts/","sidebarItems":[{"title":"Categories","items":[{"title":"Computer-Science","url":"","items":[{"title":"Courses","url":"","items":[{"title":"365 Data Science Program","url":"/Computer-Science/Courses/365-Data-Science-Program/","items":[]},{"title":"365 DS - Advanced Stastistical Methods in Python","url":"/Computer-Science/Courses/365-DS---Advanced-Stastistical-Methods-in-Python/","items":[]},{"title":"365 DS - Mathematics","url":"/Computer-Science/Courses/365-DS---Mathematics/","items":[]},{"title":"Coursera - Algorithms Part - 1","url":"/Computer-Science/Courses/Coursera---Algorithms-Part---1/","items":[]},{"title":"Coursera - Algorithms Part - 2","url":"/Computer-Science/Courses/Coursera---Algorithms-Part---2/","items":[]},{"title":"Coursera - How Google does ML","url":"/Computer-Science/Courses/Coursera---How-Google-does-ML/","items":[]},{"title":"Data Integration Specialist - AWS","url":"/Computer-Science/Courses/Data-Integration-Specialist---AWS/","items":[]},{"title":"Intro to Microsoft Excel / Google Sheets","url":"/Computer-Science/Courses/Intro-to-Microsoft-Excel---Google-Sheets/","items":[]},{"title":"Mordern Algorithm Design","url":"/Computer-Science/Courses/Mordern-Algorithm-Design/","items":[]},{"title":"Nutanix Hybrid Cloud","url":"/Computer-Science/Courses/Nutanix-Hybrid-Cloud/","items":[]},{"title":"SE Radio","url":"/Computer-Science/Courses/SE-Radio/","items":[]},{"title":"Self-Driving Nanodegree","url":"/Computer-Science/Courses/Self-Driving-Nanodegree/","items":[]},{"title":"Udemy - Python for data structures algorithms","url":"/Computer-Science/Courses/Udemy---Python-for-data-structures-algorithms/","items":[]}]},{"title":"Decentralized-Applications","url":"","items":[{"title":"Bitcoin / Cryptocurrency / Web3","url":"/Computer-Science/Decentralized-Applications/Bitcoin---Cryptocurrency---Web3/","items":[]},{"title":"Blockchain","url":"/Computer-Science/Decentralized-Applications/Blockchain/","items":[]},{"title":"Ethereum","url":"/Computer-Science/Decentralized-Applications/Ethereum/","items":[]},{"title":"Intro","url":"/Computer-Science/Decentralized-Applications/Intro/","items":[]},{"title":"Others","url":"/Computer-Science/Decentralized-Applications/Others/","items":[]}]},{"title":"Distributed-System","url":"","items":[{"title":"CAP Theorem","url":"/Computer-Science/Distributed-System/CAP-Theorem/","items":[]},{"title":"Clocks","url":"/Computer-Science/Distributed-System/Clocks/","items":[]},{"title":"Consensus Protocols","url":"/Computer-Science/Distributed-System/Consensus-Protocols/","items":[]},{"title":"Consistency","url":"/Computer-Science/Distributed-System/Consistency/","items":[]},{"title":"Designing Distributed Systems","url":"/Computer-Science/Distributed-System/Designing-Distributed-Systems/","items":[]},{"title":"Distributed Logging","url":"/Computer-Science/Distributed-System/Distributed-Logging/","items":[]},{"title":"Fallacies and Problems","url":"/Computer-Science/Distributed-System/Fallacies-and-Problems/","items":[]},{"title":"Intro","url":"/Computer-Science/Distributed-System/Intro/","items":[]},{"title":"Others","url":"/Computer-Science/Distributed-System/Others/","items":[]},{"title":"Vocabulary","url":"/Computer-Science/Distributed-System/Vocabulary/","items":[]}]},{"title":"General","url":"","items":[{"title":"Coding Guidelines / Code Reviews / Clean Code","url":"/Computer-Science/General/Coding-Guidelines---Code-Reviews---Clean-Code/","items":[]},{"title":"Common","url":"/Computer-Science/General/Common/","items":[]},{"title":"Newsletter / Learning Resources","url":"/Computer-Science/General/Newsletter---Learning-Resources/","items":[]},{"title":"Others","url":"/Computer-Science/General/Others/","items":[]},{"title":"Outline","url":"/Computer-Science/General/Outline/","items":[]},{"title":"Standards","url":"/Computer-Science/General/Standards/","items":[]}]},{"title":"Interview-Question","url":"","items":[{"title":"Blogs / Conferences / Blogging / Presentation / Tech Thursdays","url":"/Computer-Science/Interview-Question/Blogs---Conferences---Blogging---Presentation---Tech-Thursdays/","items":[]},{"title":"Coding Interview Questions","url":"/Computer-Science/Interview-Question/Coding-Interview-Questions/","items":[]},{"title":"Others","url":"/Computer-Science/Interview-Question/Others/","items":[]},{"title":"System Design - Autocomplete or TypeAhead","url":"/Computer-Science/Interview-Question/System-Design---Autocomplete-or-TypeAhead/","items":[]},{"title":"System Design - Google Search","url":"/Computer-Science/Interview-Question/System-Design---Google-Search/","items":[]},{"title":"System Design - Messenger / WhatsApp","url":"/Computer-Science/Interview-Question/System-Design---Messenger---WhatsApp/","items":[]},{"title":"System Design - MMOG - Game","url":"/Computer-Science/Interview-Question/System-Design---MMOG---Game/","items":[]},{"title":"System Design - Others","url":"/Computer-Science/Interview-Question/System-Design---Others/","items":[]},{"title":"System Design - Parking Lot","url":"/Computer-Science/Interview-Question/System-Design---Parking-Lot/","items":[]},{"title":"System Design - TinyURL","url":"/Computer-Science/Interview-Question/System-Design---TinyURL/","items":[]},{"title":"System Design - Twitter","url":"/Computer-Science/Interview-Question/System-Design---Twitter/","items":[]},{"title":"System Design - Uber Lyft ride sharing services","url":"/Computer-Science/Interview-Question/System-Design---Uber-Lyft-ride-sharing-services/","items":[]},{"title":"TopTal","url":"/Computer-Science/Interview-Question/TopTal/","items":[]}]},{"title":"IoT","url":"","items":[{"title":"Device Management","url":"/Computer-Science/IoT/Device-Management/","items":[]},{"title":"Edge Computing","url":"/Computer-Science/IoT/Edge-Computing/","items":[]},{"title":"EdgeXFoundary","url":"/Computer-Science/IoT/EdgeXFoundary/","items":[]},{"title":"Industrial IoT (IIoT)","url":"/Computer-Science/IoT/Industrial-IoT-(IIoT)/","items":[]},{"title":"IoT Intro","url":"/Computer-Science/IoT/IoT-Intro/","items":[]},{"title":"Others","url":"/Computer-Science/IoT/Others/","items":[]}]},{"title":"Networking","url":"","items":[{"title":"MQTT","url":"","items":[{"title":"Client, Broker & Connection Establishment","url":"/Computer-Science/Networking/MQTT/Client,-Broker-&-Connection-Establishment/","items":[]},{"title":"Intro","url":"/Computer-Science/Networking/MQTT/Intro/","items":[]},{"title":"Keep Alive & Client Take-Over","url":"/Computer-Science/Networking/MQTT/Keep-Alive-&-Client-Take-Over/","items":[]},{"title":"Last Will and Testament","url":"/Computer-Science/Networking/MQTT/Last-Will-and-Testament/","items":[]},{"title":"Libraries","url":"/Computer-Science/Networking/MQTT/Libraries/","items":[]},{"title":"Messages","url":"/Computer-Science/Networking/MQTT/Messages/","items":[]},{"title":"MQTT - SN","url":"/Computer-Science/Networking/MQTT/MQTT---SN/","items":[]},{"title":"MQTT 5.0","url":"/Computer-Science/Networking/MQTT/MQTT-5.0/","items":[]},{"title":"MQTT over WebSockets","url":"/Computer-Science/Networking/MQTT/MQTT-over-WebSockets/","items":[]},{"title":"Others","url":"/Computer-Science/Networking/MQTT/Others/","items":[]},{"title":"Paho Client","url":"/Computer-Science/Networking/MQTT/Paho-Client/","items":[]},{"title":"Persistent Session & Queuing Messages","url":"/Computer-Science/Networking/MQTT/Persistent-Session-&-Queuing-Messages/","items":[]},{"title":"Publish Subscribe Pattern","url":"/Computer-Science/Networking/MQTT/Publish-Subscribe-Pattern/","items":[]},{"title":"Publish, Subscribe & Unsubscribe","url":"/Computer-Science/Networking/MQTT/Publish,-Subscribe-&-Unsubscribe/","items":[]},{"title":"QoS Levels","url":"/Computer-Science/Networking/MQTT/QoS-Levels/","items":[]},{"title":"Retained Messages","url":"/Computer-Science/Networking/MQTT/Retained-Messages/","items":[]},{"title":"Scaling","url":"/Computer-Science/Networking/MQTT/Scaling/","items":[]},{"title":"Security","url":"/Computer-Science/Networking/MQTT/Security/","items":[]},{"title":"Topics & Best Practices","url":"/Computer-Science/Networking/MQTT/Topics-&-Best-Practices/","items":[]}]},{"title":"Networking-Concepts","url":"","items":[{"title":"Addressing Methods / cast protocols","url":"/Computer-Science/Networking/Networking-Concepts/Addressing-Methods---cast-protocols/","items":[]},{"title":"Book - Computer Networks","url":"/Computer-Science/Networking/Networking-Concepts/Book---Computer-Networks/","items":[]},{"title":"CIDR","url":"/Computer-Science/Networking/Networking-Concepts/CIDR/","items":[]},{"title":"Data Center Networking","url":"/Computer-Science/Networking/Networking-Concepts/Data-Center-Networking/","items":[]},{"title":"Forward Error Correction","url":"/Computer-Science/Networking/Networking-Concepts/Forward-Error-Correction/","items":[]},{"title":"Intro","url":"/Computer-Science/Networking/Networking-Concepts/Intro/","items":[]},{"title":"IP","url":"/Computer-Science/Networking/Networking-Concepts/IP/","items":[]},{"title":"Network Sockets/Ports","url":"/Computer-Science/Networking/Networking-Concepts/Network-Sockets-Ports/","items":[]},{"title":"Networking Fabric","url":"/Computer-Science/Networking/Networking-Concepts/Networking-Fabric/","items":[]},{"title":"OSI Layers","url":"/Computer-Science/Networking/Networking-Concepts/OSI-Layers/","items":[]},{"title":"Others","url":"/Computer-Science/Networking/Networking-Concepts/Others/","items":[]},{"title":"Questions","url":"/Computer-Science/Networking/Networking-Concepts/Questions/","items":[]},{"title":"Routing","url":"/Computer-Science/Networking/Networking-Concepts/Routing/","items":[]},{"title":"Sockets","url":"/Computer-Science/Networking/Networking-Concepts/Sockets/","items":[]},{"title":"TCP/IP","url":"/Computer-Science/Networking/Networking-Concepts/TCP-IP/","items":[]}]},{"title":"Others","url":"","items":[{"title":"5G Wireless Networking","url":"/Computer-Science/Networking/Others/5G-Wireless-Networking/","items":[]},{"title":"Apache Avro","url":"/Computer-Science/Networking/Others/Apache-Avro/","items":[]},{"title":"Apache Parquet","url":"/Computer-Science/Networking/Others/Apache-Parquet/","items":[]},{"title":"Comparisions","url":"/Computer-Science/Networking/Others/Comparisions/","items":[]},{"title":"Data formats","url":"/Computer-Science/Networking/Others/Data-formats/","items":[]},{"title":"Falcor","url":"/Computer-Science/Networking/Others/Falcor/","items":[]},{"title":"File Formats","url":"/Computer-Science/Networking/Others/File-Formats/","items":[]},{"title":"gRPC","url":"/Computer-Science/Networking/Others/gRPC/","items":[{"title":"Commands","url":"/Computer-Science/Networking/Others/gRPC/Commands/","items":[]},{"title":"Comparision","url":"/Computer-Science/Networking/Others/gRPC/Comparision/","items":[]},{"title":"Concepts","url":"/Computer-Science/Networking/Others/gRPC/Concepts/","items":[]},{"title":"Guides","url":"/Computer-Science/Networking/Others/gRPC/Guides/","items":[]},{"title":"Others","url":"/Computer-Science/Networking/Others/gRPC/Others/","items":[]}]},{"title":"Others","url":"/Computer-Science/Networking/Others/Others/","items":[]},{"title":"Protocol Buffers Protobuf","url":"/Computer-Science/Networking/Others/Protocol-Buffers-Protobuf/","items":[]},{"title":"RPC","url":"/Computer-Science/Networking/Others/RPC/","items":[]},{"title":"Serialization/Deserialization","url":"/Computer-Science/Networking/Others/Serialization-Deserialization/","items":[]},{"title":"VPN","url":"/Computer-Science/Networking/Others/VPN/","items":[]}]},{"title":"Protocols","url":"","items":[{"title":"AMQP","url":"/Computer-Science/Networking/Protocols/AMQP/","items":[]},{"title":"DNS Domain Name System","url":"/Computer-Science/Networking/Protocols/DNS-Domain-Name-System/","items":[]},{"title":"GraphQL","url":"/Computer-Science/Networking/Protocols/GraphQL/","items":[]},{"title":"HTTP / HTTPS","url":"/Computer-Science/Networking/Protocols/HTTP---HTTPS/","items":[]},{"title":"Http Status Code","url":"/Computer-Science/Networking/Protocols/Http-Status-Code/","items":[]},{"title":"HTTP/3 QUIC","url":"/Computer-Science/Networking/Protocols/HTTP-3-QUIC/","items":[]},{"title":"Messaging","url":"/Computer-Science/Networking/Protocols/Messaging/","items":[]},{"title":"OpenAPI","url":"/Computer-Science/Networking/Protocols/OpenAPI/","items":[]},{"title":"OpenThread","url":"/Computer-Science/Networking/Protocols/OpenThread/","items":[]},{"title":"Others","url":"/Computer-Science/Networking/Protocols/Others/","items":[]},{"title":"Protocols","url":"/Computer-Science/Networking/Protocols/Protocols/","items":[]},{"title":"Protocols Intro","url":"/Computer-Science/Networking/Protocols/Protocols-Intro/","items":[]},{"title":"REST Representational State Transfer / RESTFul","url":"/Computer-Science/Networking/Protocols/REST-Representational-State-Transfer---RESTFul/","items":[]},{"title":"Rsocket","url":"/Computer-Science/Networking/Protocols/Rsocket/","items":[]},{"title":"TCP (Connection Oriented Protocol)","url":"/Computer-Science/Networking/Protocols/TCP-(Connection-Oriented-Protocol)/","items":[{"title":"Flow Control","url":"/Computer-Science/Networking/Protocols/TCP-(Connection-Oriented-Protocol)/Flow-Control/","items":[]}]},{"title":"UDP","url":"/Computer-Science/Networking/Protocols/UDP/","items":[]},{"title":"Video / Live Streaming","url":"/Computer-Science/Networking/Protocols/Video---Live-Streaming/","items":[]},{"title":"Weave","url":"/Computer-Science/Networking/Protocols/Weave/","items":[]},{"title":"WebSockets","url":"/Computer-Science/Networking/Protocols/WebSockets/","items":[]},{"title":"ZeroMQ: Distributed Messaging","url":"/Computer-Science/Networking/Protocols/ZeroMQ--Distributed-Messaging/","items":[]}]}]},{"title":"Operating-System","url":"","items":[{"title":"Basic Computer Organization","url":"/Computer-Science/Operating-System/Basic-Computer-Organization/","items":[]},{"title":"Caches / Caching","url":"/Computer-Science/Operating-System/Caches---Caching/","items":[]},{"title":"Compilers","url":"/Computer-Science/Operating-System/Compilers/","items":[]},{"title":"Concepts","url":"/Computer-Science/Operating-System/Concepts/","items":[]},{"title":"Concurrency / Threading","url":"/Computer-Science/Operating-System/Concurrency---Threading/","items":[]},{"title":"Concurrency Models - Async","url":"/Computer-Science/Operating-System/Concurrency-Models---Async/","items":[]},{"title":"Concurrency Problems","url":"/Computer-Science/Operating-System/Concurrency-Problems/","items":[]},{"title":"Coroutines","url":"/Computer-Science/Operating-System/Coroutines/","items":[]},{"title":"CPU | GPU | TPU","url":"/Computer-Science/Operating-System/CPU---GPU---TPU/","items":[]},{"title":"Disk IO","url":"/Computer-Science/Operating-System/Disk-IO/","items":[]},{"title":"DRAM","url":"/Computer-Science/Operating-System/DRAM/","items":[]},{"title":"Intro","url":"/Computer-Science/Operating-System/Intro/","items":[]},{"title":"Journaling File System","url":"/Computer-Science/Operating-System/Journaling-File-System/","items":[]},{"title":"Memory","url":"/Computer-Science/Operating-System/Memory/","items":[]},{"title":"Memory Allocation","url":"/Computer-Science/Operating-System/Memory-Allocation/","items":[]},{"title":"Memory Layout","url":"/Computer-Science/Operating-System/Memory-Layout/","items":[]},{"title":"Memory Mapping mmap","url":"/Computer-Science/Operating-System/Memory-Mapping-mmap/","items":[]},{"title":"Microprocessor","url":"/Computer-Science/Operating-System/Microprocessor/","items":[]},{"title":"Others","url":"/Computer-Science/Operating-System/Others/","items":[]},{"title":"Parallel Processing","url":"/Computer-Science/Operating-System/Parallel-Processing/","items":[]},{"title":"RAID","url":"/Computer-Science/Operating-System/RAID/","items":[]},{"title":"Scheduling","url":"/Computer-Science/Operating-System/Scheduling/","items":[]},{"title":"Swap / Paging / Caching","url":"/Computer-Science/Operating-System/Swap---Paging---Caching/","items":[]},{"title":"Unix / Linux","url":"/Computer-Science/Operating-System/Unix---Linux/","items":[]},{"title":"Unix / Linux File System","url":"/Computer-Science/Operating-System/Unix---Linux-File-System/","items":[]},{"title":"Write Ahead Log, WAL","url":"/Computer-Science/Operating-System/Write-Ahead-Log,-WAL/","items":[]}]},{"title":"Others","url":"","items":[{"title":"BioInformatics/BioTechnology","url":"/Computer-Science/Others/BioInformatics-BioTechnology/","items":[]},{"title":"Computer Graphics","url":"/Computer-Science/Others/Computer-Graphics/","items":[]},{"title":"Digital Circuits","url":"/Computer-Science/Others/Digital-Circuits/","items":[]},{"title":"Game Development","url":"/Computer-Science/Others/Game-Development/","items":[]},{"title":"Others","url":"/Computer-Science/Others/Others/","items":[]},{"title":"Tech Books","url":"/Computer-Science/Others/Tech-Books/","items":[]}]},{"title":"Programming-Concepts","url":"","items":[{"title":"Cohesion and Coupling","url":"/Computer-Science/Programming-Concepts/Cohesion-and-Coupling/","items":[]},{"title":"Dependency Injection","url":"/Computer-Science/Programming-Concepts/Dependency-Injection/","items":[]},{"title":"Inversion of Control","url":"/Computer-Science/Programming-Concepts/Inversion-of-Control/","items":[]},{"title":"Marshalling","url":"/Computer-Science/Programming-Concepts/Marshalling/","items":[]},{"title":"Metaprogramming","url":"/Computer-Science/Programming-Concepts/Metaprogramming/","items":[]},{"title":"Others","url":"/Computer-Science/Programming-Concepts/Others/","items":[]},{"title":"Programming Styles","url":"/Computer-Science/Programming-Concepts/Programming-Styles/","items":[]},{"title":"Software/Coding / Development/Engineering","url":"/Computer-Science/Programming-Concepts/Software-Coding---Development-Engineering/","items":[{"title":"Code Smell","url":"/Computer-Science/Programming-Concepts/Software-Coding---Development-Engineering/Code-Smell/","items":[]},{"title":"Static Code Analysis","url":"/Computer-Science/Programming-Concepts/Software-Coding---Development-Engineering/Static-Code-Analysis/","items":[]}]},{"title":"Type Introspection and Reflection","url":"/Computer-Science/Programming-Concepts/Type-Introspection-and-Reflection/","items":[]},{"title":"Type Systems","url":"/Computer-Science/Programming-Concepts/Type-Systems/","items":[]}]},{"title":"Programming-Paradigms","url":"","items":[{"title":"Architectural - MVVM","url":"/Computer-Science/Programming-Paradigms/Architectural---MVVM/","items":[]},{"title":"Behavioral - Chain of Responsibility","url":"/Computer-Science/Programming-Paradigms/Behavioral---Chain-of-Responsibility/","items":[]},{"title":"Behavioral - Iterator","url":"/Computer-Science/Programming-Paradigms/Behavioral---Iterator/","items":[]},{"title":"Behavioral - Mediator","url":"/Computer-Science/Programming-Paradigms/Behavioral---Mediator/","items":[]},{"title":"Behavioral - Memento","url":"/Computer-Science/Programming-Paradigms/Behavioral---Memento/","items":[]},{"title":"Behavioral - Observer","url":"/Computer-Science/Programming-Paradigms/Behavioral---Observer/","items":[]},{"title":"Behavioral - Publisher Subscriber","url":"/Computer-Science/Programming-Paradigms/Behavioral---Publisher-Subscriber/","items":[]},{"title":"Behavioral - State","url":"/Computer-Science/Programming-Paradigms/Behavioral---State/","items":[]},{"title":"Behavioral - Strategy","url":"/Computer-Science/Programming-Paradigms/Behavioral---Strategy/","items":[]},{"title":"Concurrency - Reactor","url":"/Computer-Science/Programming-Paradigms/Concurrency---Reactor/","items":[]},{"title":"Creational - Abstract Factory","url":"/Computer-Science/Programming-Paradigms/Creational---Abstract-Factory/","items":[]},{"title":"Creational - Object Pool","url":"/Computer-Science/Programming-Paradigms/Creational---Object-Pool/","items":[]},{"title":"Design Patterns","url":"/Computer-Science/Programming-Paradigms/Design-Patterns/","items":[]},{"title":"Functional Programming","url":"/Computer-Science/Programming-Paradigms/Functional-Programming/","items":[]},{"title":"OOPS / SOLID","url":"/Computer-Science/Programming-Paradigms/OOPS---SOLID/","items":[]},{"title":"Others","url":"/Computer-Science/Programming-Paradigms/Others/","items":[]},{"title":"References","url":"/Computer-Science/Programming-Paradigms/References/","items":[]},{"title":"Structural -  Facade","url":"/Computer-Science/Programming-Paradigms/Structural----Facade/","items":[]},{"title":"Structural - Adapter","url":"/Computer-Science/Programming-Paradigms/Structural---Adapter/","items":[]},{"title":"Structural - Bridge","url":"/Computer-Science/Programming-Paradigms/Structural---Bridge/","items":[]},{"title":"Structural - Decorator","url":"/Computer-Science/Programming-Paradigms/Structural---Decorator/","items":[]},{"title":"Structural - Proxy","url":"/Computer-Science/Programming-Paradigms/Structural---Proxy/","items":[]},{"title":"Types of programming paradigm","url":"/Computer-Science/Programming-Paradigms/Types-of-programming-paradigm/","items":[]}]},{"title":"Security","url":"","items":[{"title":"Attacks","url":"/Computer-Science/Security/Attacks/","items":[]},{"title":"Authentication","url":"/Computer-Science/Security/Authentication/","items":[{"title":"Certificates","url":"/Computer-Science/Security/Authentication/Certificates/","items":[]},{"title":"HTTP Authentication","url":"/Computer-Science/Security/Authentication/HTTP-Authentication/","items":[]},{"title":"JWT","url":"/Computer-Science/Security/Authentication/JWT/","items":[]},{"title":"OAuth","url":"/Computer-Science/Security/Authentication/OAuth/","items":[]},{"title":"OpenID","url":"/Computer-Science/Security/Authentication/OpenID/","items":[]}]},{"title":"Concepts","url":"/Computer-Science/Security/Concepts/","items":[]},{"title":"Cryptography Intro","url":"/Computer-Science/Security/Cryptography-Intro/","items":[{"title":"Cryptographic Algorithms","url":"/Computer-Science/Security/Cryptography-Intro/Cryptographic-Algorithms/","items":[]},{"title":"Cryptographic Hash Functions","url":"/Computer-Science/Security/Cryptography-Intro/Cryptographic-Hash-Functions/","items":[]},{"title":"Cryptography Terms","url":"/Computer-Science/Security/Cryptography-Intro/Cryptography-Terms/","items":[]},{"title":"Diffie-Hellman Key Exchange","url":"/Computer-Science/Security/Cryptography-Intro/Diffie-Hellman-Key-Exchange/","items":[]},{"title":"Public-key cryptography","url":"/Computer-Science/Security/Cryptography-Intro/Public-key-cryptography/","items":[]}]},{"title":"Ethical Hacking","url":"/Computer-Science/Security/Ethical-Hacking/","items":[]},{"title":"Firewall WAF","url":"/Computer-Science/Security/Firewall-WAF/","items":[]},{"title":"Others","url":"/Computer-Science/Security/Others/","items":[]},{"title":"Systems Protection","url":"/Computer-Science/Security/Systems-Protection/","items":[]},{"title":"Tools","url":"/Computer-Science/Security/Tools/","items":[]},{"title":"Vault","url":"/Computer-Science/Security/Vault/","items":[]},{"title":"Vulnerabilities","url":"/Computer-Science/Security/Vulnerabilities/","items":[]}]},{"title":"System-Design","url":"","items":[{"title":"Addressing Failures","url":"/Computer-Science/System-Design/Addressing-Failures/","items":[]},{"title":"API Gateway","url":"/Computer-Science/System-Design/API-Gateway/","items":[]},{"title":"Architecture Guide","url":"/Computer-Science/System-Design/Architecture-Guide/","items":[{"title":"N-Tier Application Architecture","url":"/Computer-Science/System-Design/Architecture-Guide/N-Tier-Application-Architecture/","items":[]}]},{"title":"Cloud Native","url":"/Computer-Science/System-Design/Cloud-Native/","items":[]},{"title":"Enterprise Integration Patterns","url":"/Computer-Science/System-Design/Enterprise-Integration-Patterns/","items":[]},{"title":"Event driven architecture","url":"/Computer-Science/System-Design/Event-driven-architecture/","items":[]},{"title":"Intro","url":"/Computer-Science/System-Design/Intro/","items":[]},{"title":"Lambda Architecture","url":"/Computer-Science/System-Design/Lambda-Architecture/","items":[]},{"title":"Microservice Architecture","url":"/Computer-Science/System-Design/Microservice-Architecture/","items":[{"title":"Design Patterns","url":"/Computer-Science/System-Design/Microservice-Architecture/Design-Patterns/","items":[]},{"title":"Domain Driven Design","url":"/Computer-Science/System-Design/Microservice-Architecture/Domain-Driven-Design/","items":[]},{"title":"Example","url":"/Computer-Science/System-Design/Microservice-Architecture/Example/","items":[]}]},{"title":"Others","url":"/Computer-Science/System-Design/Others/","items":[]},{"title":"Rate Limiting","url":"/Computer-Science/System-Design/Rate-Limiting/","items":[]},{"title":"Reactive Microservices / Manifesto","url":"/Computer-Science/System-Design/Reactive-Microservices---Manifesto/","items":[]},{"title":"Serverless Architecture","url":"/Computer-Science/System-Design/Serverless-Architecture/","items":[]},{"title":"Trade offs","url":"/Computer-Science/System-Design/Trade-offs/","items":[]},{"title":"Twelve-Factor App","url":"/Computer-Science/System-Design/Twelve-Factor-App/","items":[]}]},{"title":"Testing","url":"","items":[{"title":"Intro","url":"/Computer-Science/Testing/Intro/","items":[]},{"title":"iperf3 Testing","url":"/Computer-Science/Testing/iperf3-Testing/","items":[]},{"title":"Load / Performance Testing/QA Tools","url":"/Computer-Science/Testing/Load---Performance-Testing-QA-Tools/","items":[]},{"title":"Mocking","url":"/Computer-Science/Testing/Mocking/","items":[]},{"title":"Postman","url":"/Computer-Science/Testing/Postman/","items":[]},{"title":"Terms","url":"/Computer-Science/Testing/Terms/","items":[]},{"title":"Test Pyramid","url":"/Computer-Science/Testing/Test-Pyramid/","items":[]},{"title":"Tools","url":"/Computer-Science/Testing/Tools/","items":[]}]}]},{"title":"Data-Structures","url":"","items":[{"title":"General","url":"","items":[{"title":"Data","url":"/Data-Structures/General/Data/","items":[]},{"title":"Disjoint-Set Data Structure","url":"/Data-Structures/General/Disjoint-Set-Data-Structure/","items":[]},{"title":"DS Intro","url":"/Data-Structures/General/DS-Intro/","items":[]},{"title":"Elementary Symbol Tables","url":"/Data-Structures/General/Elementary-Symbol-Tables/","items":[]},{"title":"Endianness","url":"/Data-Structures/General/Endianness/","items":[]},{"title":"Mutable/Immutable Data Structures","url":"/Data-Structures/General/Mutable-Immutable-Data-Structures/","items":[]}]},{"title":"Graph","url":"","items":[{"title":"Adjacency List","url":"/Data-Structures/Graph/Adjacency-List/","items":[]},{"title":"Adjacency Matrix","url":"/Data-Structures/Graph/Adjacency-Matrix/","items":[]},{"title":"Digraphs (Directed Graphs)","url":"/Data-Structures/Graph/Digraphs-(Directed-Graphs)/","items":[]},{"title":"Implementation","url":"/Data-Structures/Graph/Implementation/","items":[]},{"title":"Intro","url":"/Data-Structures/Graph/Intro/","items":[]},{"title":"Questions","url":"/Data-Structures/Graph/Questions/","items":[]},{"title":"Undirected Graph","url":"/Data-Structures/Graph/Undirected-Graph/","items":[]}]},{"title":"HashTable","url":"","items":[{"title":"Bloom Filters","url":"/Data-Structures/HashTable/Bloom-Filters/","items":[]},{"title":"Chord","url":"/Data-Structures/HashTable/Chord/","items":[]},{"title":"Count-min Sketch","url":"/Data-Structures/HashTable/Count-min-Sketch/","items":[]},{"title":"DHT - Distributed Hash Tables","url":"/Data-Structures/HashTable/DHT---Distributed-Hash-Tables/","items":[]},{"title":"Dictionaries","url":"/Data-Structures/HashTable/Dictionaries/","items":[]},{"title":"Hash Functions","url":"/Data-Structures/HashTable/Hash-Functions/","items":[]},{"title":"Hash Tables","url":"/Data-Structures/HashTable/Hash-Tables/","items":[]},{"title":"Hashing","url":"/Data-Structures/HashTable/Hashing/","items":[]},{"title":"Hashing Techniques","url":"/Data-Structures/HashTable/Hashing-Techniques/","items":[]},{"title":"HyperLogLog","url":"/Data-Structures/HashTable/HyperLogLog/","items":[]},{"title":"Kademlia","url":"/Data-Structures/HashTable/Kademlia/","items":[]},{"title":"List of Hash Functions","url":"/Data-Structures/HashTable/List-of-Hash-Functions/","items":[]},{"title":"Merkle Trees","url":"/Data-Structures/HashTable/Merkle-Trees/","items":[]},{"title":"Probabilistic Data Structure","url":"/Data-Structures/HashTable/Probabilistic-Data-Structure/","items":[]},{"title":"Questions","url":"/Data-Structures/HashTable/Questions/","items":[]}]},{"title":"Hierarchical-Data-Structure","url":"","items":[{"title":"2-3 Search Trees","url":"/Data-Structures/Hierarchical-Data-Structure/2-3-Search-Trees/","items":[]},{"title":"AVL Tree","url":"/Data-Structures/Hierarchical-Data-Structure/AVL-Tree/","items":[]},{"title":"B-Tree","url":"/Data-Structures/Hierarchical-Data-Structure/B-Tree/","items":[]},{"title":"Beap (Bi-Parental Heap)","url":"/Data-Structures/Hierarchical-Data-Structure/Beap-(Bi-Parental-Heap)/","items":[]},{"title":"Binary Heap","url":"/Data-Structures/Hierarchical-Data-Structure/Binary-Heap/","items":[]},{"title":"Binary Search Tree","url":"/Data-Structures/Hierarchical-Data-Structure/Binary-Search-Tree/","items":[]},{"title":"Binary Tree","url":"/Data-Structures/Hierarchical-Data-Structure/Binary-Tree/","items":[]},{"title":"Binomial Heap","url":"/Data-Structures/Hierarchical-Data-Structure/Binomial-Heap/","items":[]},{"title":"Fibonacci Heap","url":"/Data-Structures/Hierarchical-Data-Structure/Fibonacci-Heap/","items":[]},{"title":"Interval Search Tree","url":"/Data-Structures/Hierarchical-Data-Structure/Interval-Search-Tree/","items":[]},{"title":"k-ary heap / d-ary heap / d-way heap","url":"/Data-Structures/Hierarchical-Data-Structure/k-ary-heap---d-ary-heap---d-way-heap/","items":[]},{"title":"Kd-trees","url":"/Data-Structures/Hierarchical-Data-Structure/Kd-trees/","items":[]},{"title":"Left Leaning Red-Black BSTs (LLRB tree)","url":"/Data-Structures/Hierarchical-Data-Structure/Left-Leaning-Red-Black-BSTs-(LLRB-tree)/","items":[]},{"title":"Problems","url":"/Data-Structures/Hierarchical-Data-Structure/Problems/","items":[]},{"title":"Segment Tree","url":"/Data-Structures/Hierarchical-Data-Structure/Segment-Tree/","items":[]},{"title":"Space-partitioning trees","url":"/Data-Structures/Hierarchical-Data-Structure/Space-partitioning-trees/","items":[]},{"title":"Tree DS","url":"/Data-Structures/Hierarchical-Data-Structure/Tree-DS/","items":[]}]},{"title":"Linear-Data-Structure","url":"","items":[{"title":"Array","url":"/Data-Structures/Linear-Data-Structure/Array/","items":[]},{"title":"Bag Data Structure","url":"/Data-Structures/Linear-Data-Structure/Bag-Data-Structure/","items":[]},{"title":"Circular Buffer","url":"/Data-Structures/Linear-Data-Structure/Circular-Buffer/","items":[]},{"title":"Dequeue","url":"/Data-Structures/Linear-Data-Structure/Dequeue/","items":[]},{"title":"Indexed Priority Queue","url":"/Data-Structures/Linear-Data-Structure/Indexed-Priority-Queue/","items":[]},{"title":"Linked List","url":"/Data-Structures/Linear-Data-Structure/Linked-List/","items":[]},{"title":"Priority Queue","url":"/Data-Structures/Linear-Data-Structure/Priority-Queue/","items":[]},{"title":"Problems","url":"/Data-Structures/Linear-Data-Structure/Problems/","items":[]},{"title":"Queue FIFO","url":"/Data-Structures/Linear-Data-Structure/Queue-FIFO/","items":[]},{"title":"Randomized Queue","url":"/Data-Structures/Linear-Data-Structure/Randomized-Queue/","items":[]},{"title":"Sets","url":"/Data-Structures/Linear-Data-Structure/Sets/","items":[]},{"title":"Skip Lists","url":"/Data-Structures/Linear-Data-Structure/Skip-Lists/","items":[]},{"title":"Stack LIFO","url":"/Data-Structures/Linear-Data-Structure/Stack-LIFO/","items":[]}]},{"title":"Others","url":"","items":[{"title":"Bitmap","url":"/Data-Structures/Others/Bitmap/","items":[]},{"title":"LSM (Log Structured Merge Trees)","url":"/Data-Structures/Others/LSM-(Log-Structured-Merge-Trees)/","items":[]},{"title":"SSTables, Sorted String Tables","url":"/Data-Structures/Others/SSTables,-Sorted-String-Tables/","items":[]}]},{"title":"Trie","url":"","items":[{"title":"Compressed Trie","url":"/Data-Structures/Trie/Compressed-Trie/","items":[]},{"title":"Others","url":"/Data-Structures/Trie/Others/","items":[]},{"title":"Patricia Trie","url":"/Data-Structures/Trie/Patricia-Trie/","items":[]},{"title":"Questions","url":"/Data-Structures/Trie/Questions/","items":[]},{"title":"R-way Tries","url":"/Data-Structures/Trie/R-way-Tries/","items":[]},{"title":"Standard Trie","url":"/Data-Structures/Trie/Standard-Trie/","items":[]},{"title":"Suffix Array","url":"/Data-Structures/Trie/Suffix-Array/","items":[]},{"title":"Suffix Tree","url":"/Data-Structures/Trie/Suffix-Tree/","items":[]},{"title":"Ternary Search Tries (TST)","url":"/Data-Structures/Trie/Ternary-Search-Tries-(TST)/","items":[]}]}]},{"title":"Databases","url":"","items":[{"title":"Concepts","url":"","items":[{"title":"ACID and BASE","url":"/Databases/Concepts/ACID-and-BASE/","items":[]},{"title":"Concepts","url":"/Databases/Concepts/Concepts/","items":[]},{"title":"Concurrency Control","url":"/Databases/Concepts/Concurrency-Control/","items":[]},{"title":"Disk oriented vs in-memory DBs","url":"/Databases/Concepts/Disk-oriented-vs-in-memory-DBs/","items":[]},{"title":"History","url":"/Databases/Concepts/History/","items":[]},{"title":"Indexing","url":"/Databases/Concepts/Indexing/","items":[{"title":"Database Index","url":"/Databases/Concepts/Indexing/Database-Index/","items":[]},{"title":"Inverted Index","url":"/Databases/Concepts/Indexing/Inverted-Index/","items":[]},{"title":"MySQL Indexing","url":"/Databases/Concepts/Indexing/MySQL-Indexing/","items":[]}]},{"title":"Intro","url":"/Databases/Concepts/Intro/","items":[]},{"title":"Isolation Levels","url":"/Databases/Concepts/Isolation-Levels/","items":[]},{"title":"MVCC, MultiVersion Concurrency Control","url":"/Databases/Concepts/MVCC,-MultiVersion-Concurrency-Control/","items":[]},{"title":"Others","url":"/Databases/Concepts/Others/","items":[]},{"title":"RUM Conjecture","url":"/Databases/Concepts/RUM-Conjecture/","items":[]},{"title":"Types of Databases","url":"/Databases/Concepts/Types-of-Databases/","items":[]}]},{"title":"Modeling","url":"","items":[{"title":"Data Modeling","url":"/Databases/Modeling/Data-Modeling/","items":[]},{"title":"Database Workloads","url":"/Databases/Modeling/Database-Workloads/","items":[]},{"title":"ER - Tools","url":"/Databases/Modeling/ER---Tools/","items":[]},{"title":"ER Diagrams (Entity Relationships)","url":"/Databases/Modeling/ER-Diagrams-(Entity-Relationships)/","items":[]}]},{"title":"NoSQL-Databases","url":"","items":[{"title":"AWS DynamoDB","url":"/Databases/NoSQL-Databases/AWS-DynamoDB/","items":[{"title":"Cheetsheet","url":"/Databases/NoSQL-Databases/AWS-DynamoDB/Cheetsheet/","items":[]},{"title":"Core components","url":"/Databases/NoSQL-Databases/AWS-DynamoDB/Core-components/","items":[]},{"title":"Documentation","url":"/Databases/NoSQL-Databases/AWS-DynamoDB/Documentation/","items":[]},{"title":"Others","url":"/Databases/NoSQL-Databases/AWS-DynamoDB/Others/","items":[]},{"title":"Working","url":"/Databases/NoSQL-Databases/AWS-DynamoDB/Working/","items":[]}]},{"title":"Cassandra","url":"/Databases/NoSQL-Databases/Cassandra/","items":[{"title":"Commands","url":"/Databases/NoSQL-Databases/Cassandra/Commands/","items":[]},{"title":"Consistency","url":"/Databases/NoSQL-Databases/Cassandra/Consistency/","items":[]},{"title":"CQL (Cassandra Query Language)","url":"/Databases/NoSQL-Databases/Cassandra/CQL-(Cassandra-Query-Language)/","items":[]},{"title":"Data Model","url":"/Databases/NoSQL-Databases/Cassandra/Data-Model/","items":[]},{"title":"Design","url":"/Databases/NoSQL-Databases/Cassandra/Design/","items":[]},{"title":"Drivers / Clients","url":"/Databases/NoSQL-Databases/Cassandra/Drivers---Clients/","items":[]},{"title":"Questions","url":"/Databases/NoSQL-Databases/Cassandra/Questions/","items":[]},{"title":"Working","url":"/Databases/NoSQL-Databases/Cassandra/Working/","items":[]}]},{"title":"Column family","url":"/Databases/NoSQL-Databases/Column-family/","items":[]},{"title":"Druid","url":"/Databases/NoSQL-Databases/Druid/","items":[{"title":"Architecture","url":"/Databases/NoSQL-Databases/Druid/Architecture/","items":[]},{"title":"Cheatsheet","url":"/Databases/NoSQL-Databases/Druid/Cheatsheet/","items":[]},{"title":"Commands","url":"/Databases/NoSQL-Databases/Druid/Commands/","items":[]},{"title":"Documentation","url":"/Databases/NoSQL-Databases/Druid/Documentation/","items":[]},{"title":"Others","url":"/Databases/NoSQL-Databases/Druid/Others/","items":[]},{"title":"Paper","url":"/Databases/NoSQL-Databases/Druid/Paper/","items":[]}]},{"title":"MongoDB","url":"/Databases/NoSQL-Databases/MongoDB/","items":[{"title":"Commands","url":"/Databases/NoSQL-Databases/MongoDB/Commands/","items":[]},{"title":"Data Types","url":"/Databases/NoSQL-Databases/MongoDB/Data-Types/","items":[]},{"title":"Indexes","url":"/Databases/NoSQL-Databases/MongoDB/Indexes/","items":[]},{"title":"Others","url":"/Databases/NoSQL-Databases/MongoDB/Others/","items":[]},{"title":"Overview","url":"/Databases/NoSQL-Databases/MongoDB/Overview/","items":[]},{"title":"pymongo","url":"/Databases/NoSQL-Databases/MongoDB/pymongo/","items":[]}]},{"title":"NoSQL Databases","url":"/Databases/NoSQL-Databases/NoSQL-Databases/","items":[]},{"title":"Redis","url":"/Databases/NoSQL-Databases/Redis/","items":[{"title":"Best Practices","url":"/Databases/NoSQL-Databases/Redis/Best-Practices/","items":[]},{"title":"Commands","url":"/Databases/NoSQL-Databases/Redis/Commands/","items":[]},{"title":"Documentation","url":"/Databases/NoSQL-Databases/Redis/Documentation/","items":[]},{"title":"Others","url":"/Databases/NoSQL-Databases/Redis/Others/","items":[]},{"title":"Redis Concepts","url":"/Databases/NoSQL-Databases/Redis/Redis-Concepts/","items":[]},{"title":"Redis Data Types","url":"/Databases/NoSQL-Databases/Redis/Redis-Data-Types/","items":[]},{"title":"Redis Eviction Policies","url":"/Databases/NoSQL-Databases/Redis/Redis-Eviction-Policies/","items":[]},{"title":"Redis Queues","url":"/Databases/NoSQL-Databases/Redis/Redis-Queues/","items":[]},{"title":"Redis Streams / PUBSUB","url":"/Databases/NoSQL-Databases/Redis/Redis-Streams---PUBSUB/","items":[]},{"title":"redis-py","url":"/Databases/NoSQL-Databases/Redis/redis-py/","items":[]}]}]},{"title":"Others","url":"","items":[{"title":"Course - Advanced Database Systems","url":"/Databases/Others/Course---Advanced-Database-Systems/","items":[]},{"title":"Course - AWS Certified Database - Specialty","url":"/Databases/Others/Course---AWS-Certified-Database---Specialty/","items":[]},{"title":"Data Lake","url":"/Databases/Others/Data-Lake/","items":[]},{"title":"Data Warehousing","url":"/Databases/Others/Data-Warehousing/","items":[{"title":"Architecture","url":"/Databases/Others/Data-Warehousing/Architecture/","items":[]},{"title":"Characteristics","url":"/Databases/Others/Data-Warehousing/Characteristics/","items":[]},{"title":"Concepts","url":"/Databases/Others/Data-Warehousing/Concepts/","items":[]},{"title":"Databases","url":"/Databases/Others/Data-Warehousing/Databases/","items":[]},{"title":"Others","url":"/Databases/Others/Data-Warehousing/Others/","items":[]},{"title":"Warehouse Schemas","url":"/Databases/Others/Data-Warehousing/Warehouse-Schemas/","items":[]}]},{"title":"Databases - Others","url":"/Databases/Others/Databases---Others/","items":[]},{"title":"ETL (Extract Transform Load)","url":"/Databases/Others/ETL-(Extract-Transform-Load)/","items":[]},{"title":"MemSQL","url":"/Databases/Others/MemSQL/","items":[{"title":"Intro","url":"/Databases/Others/MemSQL/Intro/","items":[]}]},{"title":"Technologies / Tools","url":"/Databases/Others/Technologies---Tools/","items":[]},{"title":"YugabyteDB","url":"/Databases/Others/YugabyteDB/","items":[]}]},{"title":"SQL-Databases","url":"","items":[{"title":"AWS Aurora","url":"/Databases/SQL-Databases/AWS-Aurora/","items":[{"title":"Aurora Documentation","url":"/Databases/SQL-Databases/AWS-Aurora/Aurora-Documentation/","items":[]},{"title":"Others","url":"/Databases/SQL-Databases/AWS-Aurora/Others/","items":[]},{"title":"Storage","url":"/Databases/SQL-Databases/AWS-Aurora/Storage/","items":[]}]},{"title":"AWS Redshift","url":"/Databases/SQL-Databases/AWS-Redshift/","items":[{"title":"Architecture","url":"/Databases/SQL-Databases/AWS-Redshift/Architecture/","items":[]},{"title":"Deep dive / Best practices","url":"/Databases/SQL-Databases/AWS-Redshift/Deep-dive---Best-practices/","items":[]},{"title":"Documentation","url":"/Databases/SQL-Databases/AWS-Redshift/Documentation/","items":[]},{"title":"Others","url":"/Databases/SQL-Databases/AWS-Redshift/Others/","items":[]},{"title":"Pricing / Sizing","url":"/Databases/SQL-Databases/AWS-Redshift/Pricing---Sizing/","items":[]},{"title":"Redshift SQL Queries / Commands","url":"/Databases/SQL-Databases/AWS-Redshift/Redshift-SQL-Queries---Commands/","items":[]}]},{"title":"MySQL","url":"/Databases/SQL-Databases/MySQL/","items":[{"title":"11. MySQL Data Types","url":"/Databases/SQL-Databases/MySQL/11.-MySQL-Data-Types/","items":[]},{"title":"Connection Handling","url":"/Databases/SQL-Databases/MySQL/Connection-Handling/","items":[]},{"title":"Documentation","url":"/Databases/SQL-Databases/MySQL/Documentation/","items":[]},{"title":"Others","url":"/Databases/SQL-Databases/MySQL/Others/","items":[]},{"title":"Scaling / Optimizations","url":"/Databases/SQL-Databases/MySQL/Scaling---Optimizations/","items":[]},{"title":"SQL / MySQL Tools","url":"/Databases/SQL-Databases/MySQL/SQL---MySQL-Tools/","items":[]}]},{"title":"Normalization","url":"/Databases/SQL-Databases/Normalization/","items":[]},{"title":"Partitioning / Sharding","url":"/Databases/SQL-Databases/Partitioning---Sharding/","items":[]},{"title":"Postgres","url":"/Databases/SQL-Databases/Postgres/","items":[{"title":"Documentation","url":"/Databases/SQL-Databases/Postgres/Documentation/","items":[]}]},{"title":"RDBMS","url":"/Databases/SQL-Databases/RDBMS/","items":[]}]},{"title":"Time-Series-DB","url":"","items":[{"title":"InfluxDB","url":"/Databases/Time-Series-DB/InfluxDB/","items":[{"title":"Administration","url":"/Databases/Time-Series-DB/InfluxDB/Administration/","items":[]},{"title":"Commands / Influx Query Language (InfluxQL)","url":"/Databases/Time-Series-DB/InfluxDB/Commands---Influx-Query-Language-(InfluxQL)/","items":[]},{"title":"Concepts","url":"/Databases/Time-Series-DB/InfluxDB/Concepts/","items":[]},{"title":"Influx","url":"/Databases/Time-Series-DB/InfluxDB/Influx/","items":[]},{"title":"Kapacitor","url":"/Databases/Time-Series-DB/InfluxDB/Kapacitor/","items":[]},{"title":"Others","url":"/Databases/Time-Series-DB/InfluxDB/Others/","items":[]},{"title":"Tools","url":"/Databases/Time-Series-DB/InfluxDB/Tools/","items":[]},{"title":"Write Protocols","url":"/Databases/Time-Series-DB/InfluxDB/Write-Protocols/","items":[]}]},{"title":"Time Series Databases","url":"/Databases/Time-Series-DB/Time-Series-Databases/","items":[]},{"title":"TimeScaleDB","url":"/Databases/Time-Series-DB/TimeScaleDB/","items":[]}]}]},{"title":"Mathematics","url":"","items":[{"title":"Algebra","url":"","items":[{"title":"2.1 Functions","url":"/Mathematics/Algebra/2.1-Functions/","items":[]},{"title":"2.2 Complex Numbers","url":"/Mathematics/Algebra/2.2-Complex-Numbers/","items":[]},{"title":"2.7. Exponential & logarithms","url":"/Mathematics/Algebra/2.7.-Exponential-&-logarithms/","items":[]},{"title":"2.9. Series","url":"/Mathematics/Algebra/2.9.-Series/","items":[]},{"title":"Cheatsheet","url":"/Mathematics/Algebra/Cheatsheet/","items":[]},{"title":"Intro","url":"/Mathematics/Algebra/Intro/","items":[]},{"title":"Others","url":"/Mathematics/Algebra/Others/","items":[]},{"title":"Root","url":"/Mathematics/Algebra/Root/","items":[]},{"title":"Sets","url":"/Mathematics/Algebra/Sets/","items":[]}]},{"title":"Aptitude","url":"","items":[{"title":"Chinese Remainder Theorem","url":"/Mathematics/Aptitude/Chinese-Remainder-Theorem/","items":[]},{"title":"Cube Cutting","url":"/Mathematics/Aptitude/Cube-Cutting/","items":[]},{"title":"Distance Speed and Time","url":"/Mathematics/Aptitude/Distance-Speed-and-Time/","items":[]}]},{"title":"Calculus","url":"","items":[{"title":"Essence of Calculus - 3Blue1Brown","url":"/Mathematics/Calculus/Essence-of-Calculus---3Blue1Brown/","items":[]},{"title":"Functions","url":"/Mathematics/Calculus/Functions/","items":[]},{"title":"Gradient","url":"/Mathematics/Calculus/Gradient/","items":[]},{"title":"Intro","url":"/Mathematics/Calculus/Intro/","items":[]},{"title":"Others","url":"/Mathematics/Calculus/Others/","items":[]},{"title":"Product Rule for Derivatives","url":"/Mathematics/Calculus/Product-Rule-for-Derivatives/","items":[]},{"title":"Quotient Rule","url":"/Mathematics/Calculus/Quotient-Rule/","items":[]},{"title":"Tangent Line and the Derivative","url":"/Mathematics/Calculus/Tangent-Line-and-the-Derivative/","items":[]}]},{"title":"Combinatorics","url":"","items":[{"title":"Birthday Paradox","url":"/Mathematics/Combinatorics/Birthday-Paradox/","items":[]},{"title":"Conditional Probability","url":"/Mathematics/Combinatorics/Conditional-Probability/","items":[]},{"title":"Inclusion-Exclusion Principle","url":"/Mathematics/Combinatorics/Inclusion-Exclusion-Principle/","items":[]},{"title":"Intro","url":"/Mathematics/Combinatorics/Intro/","items":[]},{"title":"Permutation and Combination","url":"/Mathematics/Combinatorics/Permutation-and-Combination/","items":[]},{"title":"Pigeonhole Principle","url":"/Mathematics/Combinatorics/Pigeonhole-Principle/","items":[]},{"title":"Probability","url":"/Mathematics/Combinatorics/Probability/","items":[]}]},{"title":"General","url":"","items":[{"title":"Ackermann Function","url":"/Mathematics/General/Ackermann-Function/","items":[]},{"title":"Advanced Topics","url":"/Mathematics/General/Advanced-Topics/","items":[]},{"title":"Conjecture","url":"/Mathematics/General/Conjecture/","items":[]},{"title":"Discrete Mathematics","url":"/Mathematics/General/Discrete-Mathematics/","items":[]},{"title":"Fermat's Last Theorem","url":"/Mathematics/General/Fermat's-Last-Theorem/","items":[]},{"title":"Fermat's Little Theorem","url":"/Mathematics/General/Fermat's-Little-Theorem/","items":[]},{"title":"GCD","url":"/Mathematics/General/GCD/","items":[]},{"title":"Godel's Incompleteness Theorem","url":"/Mathematics/General/Godel's-Incompleteness-Theorem/","items":[]},{"title":"Golden Ratio - phi","url":"/Mathematics/General/Golden-Ratio---phi/","items":[]},{"title":"Greek Letters / Latin","url":"/Mathematics/General/Greek-Letters---Latin/","items":[]},{"title":"Handshaking Lemma","url":"/Mathematics/General/Handshaking-Lemma/","items":[]},{"title":"Logic","url":"/Mathematics/General/Logic/","items":[]},{"title":"Modulus / Modulo 10^9+7 (1000000007)","url":"/Mathematics/General/Modulus---Modulo-10^9+7-(1000000007)/","items":[]},{"title":"Numbers","url":"/Mathematics/General/Numbers/","items":[]},{"title":"Others","url":"/Mathematics/General/Others/","items":[]},{"title":"Outline","url":"/Mathematics/General/Outline/","items":[]},{"title":"Pie","url":"/Mathematics/General/Pie/","items":[]},{"title":"Properties","url":"/Mathematics/General/Properties/","items":[]}]},{"title":"Geometry","url":"","items":[{"title":"Analytic Geometry","url":"/Mathematics/Geometry/Analytic-Geometry/","items":[]},{"title":"Circles","url":"/Mathematics/Geometry/Circles/","items":[]},{"title":"Congruence","url":"/Mathematics/Geometry/Congruence/","items":[]},{"title":"Geometry Foundations","url":"/Mathematics/Geometry/Geometry-Foundations/","items":[]},{"title":"Others","url":"/Mathematics/Geometry/Others/","items":[]},{"title":"Right Triangles and Geometry","url":"/Mathematics/Geometry/Right-Triangles-and-Geometry/","items":[]},{"title":"Similarity","url":"/Mathematics/Geometry/Similarity/","items":[]},{"title":"Solid Geometry","url":"/Mathematics/Geometry/Solid-Geometry/","items":[]},{"title":"Transformations","url":"/Mathematics/Geometry/Transformations/","items":[]}]},{"title":"Linear-Algebra","url":"","items":[{"title":"3Blue1Brown","url":"/Mathematics/Linear-Algebra/3Blue1Brown/","items":[]},{"title":"Alternate Coordinate systems (bases)","url":"/Mathematics/Linear-Algebra/Alternate-Coordinate-systems-(bases)/","items":[]},{"title":"Cheetsheet","url":"/Mathematics/Linear-Algebra/Cheetsheet/","items":[]},{"title":"Matrix Transformations","url":"/Mathematics/Linear-Algebra/Matrix-Transformations/","items":[]},{"title":"Others","url":"/Mathematics/Linear-Algebra/Others/","items":[]},{"title":"Vectors and Spaces","url":"/Mathematics/Linear-Algebra/Vectors-and-Spaces/","items":[]}]},{"title":"Precalculus","url":"","items":[{"title":"Intro","url":"/Mathematics/Precalculus/Intro/","items":[]}]},{"title":"Probability","url":"","items":[{"title":"365 DS - Probability","url":"/Mathematics/Probability/365-DS---Probability/","items":[]},{"title":"Binomial Random Variables","url":"/Mathematics/Probability/Binomial-Random-Variables/","items":[]},{"title":"Central Limit Theorem","url":"/Mathematics/Probability/Central-Limit-Theorem/","items":[]},{"title":"Cheatsheet","url":"/Mathematics/Probability/Cheatsheet/","items":[]},{"title":"Intro","url":"/Mathematics/Probability/Intro/","items":[]},{"title":"Intro - Syllabus","url":"/Mathematics/Probability/Intro---Syllabus/","items":[{"title":"1. Probability Models and Axioms","url":"/Mathematics/Probability/Intro---Syllabus/1.-Probability-Models-and-Axioms/","items":[]},{"title":"1.1 Set, Sequences, Limits and Series, (un)countable sets","url":"/Mathematics/Probability/Intro---Syllabus/1.1-Set,-Sequences,-Limits-and-Series,-(un)countable-sets/","items":[]},{"title":"10. Conditioning on a random variable; Independence; Bayes' rule","url":"/Mathematics/Probability/Intro---Syllabus/10.-Conditioning-on-a-random-variable;-Independence;-Bayes'-rule/","items":[]},{"title":"11. Derived Distributions","url":"/Mathematics/Probability/Intro---Syllabus/11.-Derived-Distributions/","items":[]},{"title":"12. Sums of independent r.v.'s; Covariance and Correlation","url":"/Mathematics/Probability/Intro---Syllabus/12.-Sums-of-independent-r.v.'s;-Covariance-and-Correlation/","items":[]},{"title":"13. Conditional expectation and variance revisited","url":"/Mathematics/Probability/Intro---Syllabus/13.-Conditional-expectation-and-variance-revisited/","items":[]},{"title":"14. Intro to Bayesian Inference","url":"/Mathematics/Probability/Intro---Syllabus/14.-Intro-to-Bayesian-Inference/","items":[]},{"title":"2. Conditioning and Independence","url":"/Mathematics/Probability/Intro---Syllabus/2.-Conditioning-and-Independence/","items":[]},{"title":"3. Independence","url":"/Mathematics/Probability/Intro---Syllabus/3.-Independence/","items":[]},{"title":"4. Counting","url":"/Mathematics/Probability/Intro---Syllabus/4.-Counting/","items":[]},{"title":"5. Probability Mass Functions and Expectations","url":"/Mathematics/Probability/Intro---Syllabus/5.-Probability-Mass-Functions-and-Expectations/","items":[]},{"title":"6. Variance; Conditioning of an event; Multiple r.v.'s","url":"/Mathematics/Probability/Intro---Syllabus/6.-Variance;-Conditioning-of-an-event;-Multiple-r.v.'s/","items":[]},{"title":"7. Conditioning on a rv; Independence of r.v.'s","url":"/Mathematics/Probability/Intro---Syllabus/7.-Conditioning-on-a-rv;-Independence-of-r.v.'s/","items":[]},{"title":"8. Probability density functions","url":"/Mathematics/Probability/Intro---Syllabus/8.-Probability-density-functions/","items":[]},{"title":"9. Conditioning on an event; Multiple continuous r.v.'s","url":"/Mathematics/Probability/Intro---Syllabus/9.-Conditioning-on-an-event;-Multiple-continuous-r.v.'s/","items":[]},{"title":"Additional Theoretical Material","url":"/Mathematics/Probability/Intro---Syllabus/Additional-Theoretical-Material-1/","items":[]},{"title":"Additional Theoretical Material","url":"/Mathematics/Probability/Intro---Syllabus/Additional-Theoretical-Material/","items":[]},{"title":"Summary - Unit 4","url":"/Mathematics/Probability/Intro---Syllabus/Summary---Unit-4/","items":[]},{"title":"Summary - Unit 5","url":"/Mathematics/Probability/Intro---Syllabus/Summary---Unit-5/","items":[]},{"title":"Summary - Unit 6","url":"/Mathematics/Probability/Intro---Syllabus/Summary---Unit-6/","items":[]},{"title":"Unit 1 - Solved Problems","url":"/Mathematics/Probability/Intro---Syllabus/Unit-1---Solved-Problems/","items":[]},{"title":"Unit 2 - Solved Problems","url":"/Mathematics/Probability/Intro---Syllabus/Unit-2---Solved-Problems/","items":[]},{"title":"Unit 3 - Solved Problems","url":"/Mathematics/Probability/Intro---Syllabus/Unit-3---Solved-Problems/","items":[]},{"title":"Unit 4 - Solved Problems","url":"/Mathematics/Probability/Intro---Syllabus/Unit-4---Solved-Problems/","items":[]}]},{"title":"Monte Carlo Simulation","url":"/Mathematics/Probability/Monte-Carlo-Simulation/","items":[]},{"title":"Normal Distributions","url":"/Mathematics/Probability/Normal-Distributions/","items":[]},{"title":"Others","url":"/Mathematics/Probability/Others/","items":[]},{"title":"Probability Distribution","url":"/Mathematics/Probability/Probability-Distribution/","items":[]},{"title":"Random Variables","url":"/Mathematics/Probability/Random-Variables/","items":[]}]},{"title":"Statistics","url":"","items":[{"title":"Bivariate Analysis","url":"/Mathematics/Statistics/Bivariate-Analysis/","items":[]},{"title":"Confidence Intervals","url":"/Mathematics/Statistics/Confidence-Intervals/","items":[]},{"title":"Correlation and Covariance","url":"/Mathematics/Statistics/Correlation-and-Covariance/","items":[]},{"title":"Crash Course Statistics","url":"/Mathematics/Statistics/Crash-Course-Statistics/","items":[]},{"title":"Descriptive Statistics","url":"/Mathematics/Statistics/Descriptive-Statistics/","items":[]},{"title":"Discriminant Analysis","url":"/Mathematics/Statistics/Discriminant-Analysis/","items":[]},{"title":"Estimation Statistics","url":"/Mathematics/Statistics/Estimation-Statistics/","items":[]},{"title":"Glossary","url":"/Mathematics/Statistics/Glossary/","items":[]},{"title":"Hypothesis Testing","url":"/Mathematics/Statistics/Hypothesis-Testing/","items":[]},{"title":"Inferential Statistics","url":"/Mathematics/Statistics/Inferential-Statistics/","items":[]},{"title":"Intro","url":"/Mathematics/Statistics/Intro/","items":[]},{"title":"Nonparametric Statistics","url":"/Mathematics/Statistics/Nonparametric-Statistics/","items":[]},{"title":"Other Statistics","url":"/Mathematics/Statistics/Other-Statistics/","items":[]},{"title":"Sampling","url":"/Mathematics/Statistics/Sampling/","items":[]}]}]},{"title":"wiki","url":"/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/Computer-Science/Networking/Others/gRPC/Commands/","title":"Commands","lastUpdatedAt":"2022-12-14T07:08:50.000Z","lastUpdated":"12/14/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Databases/SQL-Databases/AWS-Redshift/Pricing---Sizing/","title":"Pricing / Sizing","lastUpdatedAt":"2022-12-14T07:03:59.000Z","lastUpdated":"12/14/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Data-Structures/Others/LSM-(Log-Structured-Merge-Trees)/","title":"LSM (Log Structured Merge Trees)","lastUpdatedAt":"2022-12-14T06:59:27.000Z","lastUpdated":"12/14/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Databases/NoSQL-Databases/Druid/Cheatsheet/","title":"Cheatsheet","lastUpdatedAt":"2022-12-14T06:59:27.000Z","lastUpdated":"12/14/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Databases/NoSQL-Databases/Redis/Commands/","title":"Commands","lastUpdatedAt":"2022-12-14T06:59:27.000Z","lastUpdated":"12/14/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Computer-Science/Courses/Intro-to-Microsoft-Excel---Google-Sheets/","title":"Intro to Microsoft Excel / Google Sheets","lastUpdatedAt":"2022-12-14T06:39:47.000Z","lastUpdated":"12/14/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Computer-Science/Courses/Mordern-Algorithm-Design/","title":"Mordern Algorithm Design","lastUpdatedAt":"2022-12-14T06:39:47.000Z","lastUpdated":"12/14/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Computer-Science/Courses/Nutanix-Hybrid-Cloud/","title":"Nutanix Hybrid Cloud","lastUpdatedAt":"2022-12-14T06:39:47.000Z","lastUpdated":"12/14/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Computer-Science/Courses/SE-Radio/","title":"SE Radio","lastUpdatedAt":"2022-12-14T06:39:47.000Z","lastUpdated":"12/14/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Computer-Science/Courses/Self-Driving-Nanodegree/","title":"Self-Driving Nanodegree","lastUpdatedAt":"2022-12-14T06:39:47.000Z","lastUpdated":"12/14/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}