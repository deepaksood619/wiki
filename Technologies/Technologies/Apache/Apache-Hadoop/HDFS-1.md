# HDFS

Created: 2018-10-13 11:56:45 +0500

Modified: 2020-04-11 15:21:24 +0500

---

The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size.



64 MB Chunk Size

Block Size - 128 MB



**References**

<https://www.aosabook.org/en/hdfs.html>

<https://www.tutorialspoint.com/hadoop/hadoop_hdfs_overview.htm>

<https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html>
